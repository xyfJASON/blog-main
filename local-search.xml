<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>[矩阵论]6.4广义逆与线性方程组</title>
    <link href="/blog-main/2023/12/20/%E7%9F%A9%E9%98%B5%E8%AE%BA-6-4%E5%B9%BF%E4%B9%89%E9%80%86%E4%B8%8E%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E7%BB%84/"/>
    <url>/blog-main/2023/12/20/%E7%9F%A9%E9%98%B5%E8%AE%BA-6-4%E5%B9%BF%E4%B9%89%E9%80%86%E4%B8%8E%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E7%BB%84/</url>
    
    <content type="html"><![CDATA[<p>对于方程组 <span class="math inline">\(Ax=b\)</span>，如果 <span class="math inline">\(A\)</span> 非奇异，则 <span class="math inline">\(x=A^{-1}b\)</span> 是唯一解。而在其他情况下，我们希望得到类似的结果。</p><ul><li>如果方程组相容，且其解有无数多个，我们希望求<strong>极小范数解</strong>，即 <span class="math inline">\(\min_{Ax=b}\Vert x\Vert\)</span>；</li><li>如果方程组不相容，即无解，那么我们希望求矛盾方程组的<strong>最小二乘解</strong>，即 <span class="math inline">\(\min \Vert Ax-b\Vert\)</span>；</li><li>一般而言，最小二乘解也不唯一，因此我们希望求<strong>极小范数最小二乘解</strong>，即 <span class="math inline">\(\min_{\min\Vert Ax-b\Vert}\Vert x\Vert\)</span>.</li></ul><p>注：本节所用范数均为 2 范数。</p><h2 id="线性方程组的相容性通解与-1-逆">线性方程组的相容性、通解与 1-逆</h2><p><strong>定理</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n},\,B\in\mathbb C^{p\times q},\,D\in\mathbb C^{m\times q}\)</span>，则矩阵方程 <span class="math inline">\(AXB=D\)</span> 相容的充要条件是： <span class="math display">\[AA^{(1)}DB^{(1)}B=D\]</span> 当方程相容时，通解为： <span class="math display">\[X=A^{(1)}DB^{(1)}+Y-A^{(1)}AYBB^{(1)}\]</span> 其中 <span class="math inline">\(Y\in\mathbb C^{n\times p}\)</span> 为任意矩阵。</p><div class="note note-secondary">            <p>证明：充分性，取 <span class="math inline">\(X=A^{(1)}DB^{(1)}\)</span> 即可；必要性，若 <span class="math inline">\(AXB=D\)</span> 有解，则 <span class="math inline">\(D=AXB=AA^{(1)}AXBB^{(1)}B=AA^{(1)}DB^{(1)}B\)</span>.</p><p>对于通解，首先显然 <span class="math inline">\(X=A^{(1)}DB^{(1)}+Y-A^{(1)}AYBB^{(1)}\)</span> 是方程的解；其次，若 <span class="math inline">\(X\)</span> 是方程的解，则取 <span class="math inline">\(Y=X\)</span> 即可写作通解形式。证毕。</p>          </div><p><strong>推论</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，取 <span class="math inline">\(A^{(1)}\in A\{1\}\)</span>，则： <span class="math display">\[A\{1\}=\{A^{(1)}+Z-A^{(1)}AZAA^{(1)}\mid Z\in\mathbb C^{n\times m}\}\]</span> <div class="note note-secondary">            <p>证明：任意 <span class="math inline">\(X\in A\{1\}\)</span> 满足矩阵方程 <span class="math inline">\(AXA=A\)</span>，代入上述定理的通解形式得： <span class="math display">\[\begin{align}X&amp;=A^{(1)}AA^{(1)}+Y-A^{(1)}AYAA^{(1)}\\&amp;=A^{(1)}AA^{(1)}+A^{(1)}+Z-A^{(1)}A(A^{(1)}+Z)AA^{(1)}&amp;Y=A^{(1)}+Z\\&amp;=A^{(1)}+Z+A^{(1)}AA^{(1)}-A^{(1)}AA^{(1)}AA^{(1)}-A^{(1)}AZAA^{(1)}\\&amp;=A^{(1)}+Z-A^{(1)}AZAA^{(1)}\end{align}\]</span> 证毕。</p>          </div></p><p><strong>定理</strong>：线性方程组 <span class="math inline">\(Ax=b\)</span> 相容的充要条件是： <span class="math display">\[AA^{(1)}b=b\]</span> 通解为： <span class="math display">\[x=A^{(1)}b+(I-A^{(1)}A)y\]</span> 其中 <span class="math inline">\(y\in\mathbb C^{n}\)</span> 为任意向量。</p><div class="note note-secondary">            <p>上文定理取 <span class="math inline">\(X=x,\,B=1,\,D=b\)</span> 的特例。</p>          </div><p>上述定理是给定 <span class="math inline">\(A^{(1)}\)</span> 后求解方程的解，反过来，利用方程的解也可以给出 <span class="math inline">\(A^{(1)}\)</span>.</p><p><strong>定理</strong>：若对于任意满足 <span class="math inline">\(Ax=b\)</span> 相容的 <span class="math inline">\(b\)</span>，<span class="math inline">\(x=Xb\)</span> 都是解，则 <span class="math inline">\(X\in A\{1\}\)</span>.</p><div class="note note-secondary">            <p>证明：考虑 <span class="math inline">\(Ax=a_i\)</span>，其中 <span class="math inline">\(a_i\)</span> 为 <span class="math inline">\(A\)</span> 的列，由于 <span class="math inline">\(x=Xa_i\)</span> 是方程的解，所以 <span class="math inline">\(AXa_i=a_i\)</span>，于是 <span class="math inline">\(AXA=A\)</span>，故 <span class="math inline">\(X\in A\{1\}\)</span>. 证毕。</p>          </div><h2 id="相容方程组的极小范数解与-14-逆">相容方程组的极小范数解与 1,4-逆</h2><p><strong>引理</strong>：相容方程组 <span class="math inline">\(Ax=b\)</span> 的极小范数解唯一，且这个唯一解在 <span class="math inline">\(R(A^H)\)</span> 中。</p><div class="note note-secondary">            <p>证明：由于 <span class="math inline">\(R(A^H)=N(A)^\perp\)</span>，所以设 <span class="math inline">\(x=y+z\)</span>，其中 <span class="math inline">\(y=P_{R(A^H)}x\in R(A^H),\,z=P_{N(A)}x\in N(A)\)</span>，于是： <span class="math display">\[\Vert x\Vert^2=\Vert y+z\Vert^2=\Vert y\Vert^2+\Vert z\Vert^2\geq \Vert y\Vert^2\]</span> 由于 <span class="math inline">\(Az=0\implies Ay=b\)</span>，即 <span class="math inline">\(y\)</span> 也是方程的解，所以为了让 <span class="math inline">\(x\)</span> 是极小范数解，只能是 <span class="math inline">\(z=0\)</span>，因此 <span class="math inline">\(x=y\in R(A^H)\)</span>.</p><p>唯一性。设 <span class="math inline">\(x&#39;\in R(A^H)\)</span> 且 <span class="math inline">\(Ax&#39;=b\)</span>，则 <span class="math inline">\(A(x-x&#39;)=0\)</span>，即 <span class="math inline">\(x-x&#39;\in N(A)=R^{\perp}(A^H)\)</span>. 又 <span class="math inline">\(x-x&#39;\in R(A^H)\)</span>，故 <span class="math inline">\(x-x&#39;=0\)</span>. 证毕。</p>          </div><p><strong>引理</strong>：集合 <span class="math inline">\(A\{1,4\}\)</span> 由矩阵方程 <span class="math inline">\(XA=A^{(1,4)}A\)</span> 的所有解组成，其中 <span class="math inline">\(A^{(1,4)}\in A\{1,4\}\)</span>.</p><div class="note note-secondary">            <p>证明：<span class="math inline">\(AXA=AA^{(1,4)}A=A\)</span>，所以 <span class="math inline">\(X\in A\{1\}\)</span>；<span class="math inline">\((XA)^H=(A^{(1,4)}A)^H=A^{(1,4)}A=XA\)</span>，所以 <span class="math inline">\(X\in A\{4\}\)</span>. 综上 <span class="math inline">\(X\in A\{1,4\}\)</span>.</p><p>另一方面，若 <span class="math inline">\(X\in A\{1,4\}\)</span>，则 <span class="math display">\[A^{(1,4)}A=A^{(1,4)}AXA=(A^{(1,4)}A)^H(XA)^H=A^H(A^{(1,4)})^HA^HX^H=(AA^{(1,4)}A)^HX^H=A^HX^H=XA\]</span> 即 <span class="math inline">\(X\)</span> 是方程的解。证毕。</p>          </div><div class="note note-success">            <p>该定理说明尽管 <span class="math inline">\(A^{(1,4)}\)</span> 不唯一，但是 <span class="math inline">\(A^{(1,4)}A\)</span> 唯一。</p>          </div><p><strong>推论</strong>：<span class="math inline">\(A^{(1,4)}A=P_{R(A^H)}\)</span>.</p><p><strong>定理</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n},\,A^{(1,4)}\in A\{1,4\}\)</span>，则： <span class="math display">\[A\{1,4\}=\{A^{(1,4)}+Z(I-AA^{(1,4)})\mid Z\in\mathbb C^{n\times m}\}\]</span> <div class="note note-secondary">            <p>证明：根据引理，任意 <span class="math inline">\(X\in A\{1,4\}\)</span> 满足方程 <span class="math inline">\(XA=A^{(1,4)}A\)</span>，代入通解形式得： <span class="math display">\[\begin{align}X&amp;=A^{(1,4)}AA^{(1,4)}+Y-YAA^{(1,4)}\\&amp;=A^{(1,4)}AA^{(1,4)}+A^{(1,4)}+Z-(A^{(1,4)}+Z)AA^{(1,4)}&amp;Y=A^{(1,4)}+Z\\&amp;=A^{(1,4)}+Z+A^{(1,4)}AA^{(1,4)}-(A^{(1,4)}+Z)AA^{(1,4)}\\&amp;=A^{(1,4)}+Z(I-AA^{(1,4)})\end{align}\]</span> 证毕。</p>          </div></p><p><strong>定理</strong>：设 <span class="math inline">\(Ax=b\)</span> 相容，则 <span class="math inline">\(x=A^{(1,4)}b\)</span> 为极小范数解；反之，若对于任意 <span class="math inline">\(b\in R(A)\)</span>，<span class="math inline">\(x=Xb\)</span> 都是极小范数解，则 <span class="math inline">\(X\in A\{1,4\}\)</span>.</p><div class="note note-secondary">            <p>证明：由第一节定理知 <span class="math inline">\(x=A^{(1,4)}b\)</span> 一定是解。设 <span class="math inline">\(Au=b\)</span>，则 <span class="math inline">\(x=A^{(1,4)}b=A^{(1,4)}Au=(A^{(1,4)}A)^Hu=A^H(A^{(1,4)})^Hu\in R(A^H)\)</span>，于是根据本节引理知 <span class="math inline">\(x\)</span> 为唯一极小范数解。</p><p>反之，考虑 <span class="math inline">\(Ax=a_i\)</span>，由于 <span class="math inline">\(x=Xa_i\)</span> 是方程的极小范数解，所以 <span class="math inline">\(Xa_i=A^{(1,4)}a_i\)</span>，故 <span class="math inline">\(XA=A^{(1,4)}A\)</span>，根据引理知 <span class="math inline">\(X\in A\{1,4\}\)</span>. 证毕。</p>          </div><h2 id="矛盾方程组的最小二乘解与-13-逆">矛盾方程组的最小二乘解与 1,3-逆</h2><p><strong>引理</strong>：集合 <span class="math inline">\(A\{1,3\}\)</span> 由矩阵方程 <span class="math inline">\(AX=AA^{(1,3)}\)</span> 的所有解组成，其中 <span class="math inline">\(A^{(1,3)}\in A\{1,3\}\)</span>.</p><div class="note note-secondary">            <p>证明：<span class="math inline">\(AXA=AA^{(1,3)}A=A\)</span>，故 <span class="math inline">\(X\in A\{1\}\)</span>；<span class="math inline">\((AX)^H=(AA^{(1,3)})^H=AA^{(1,3)}=AX\)</span>，故 <span class="math inline">\(X\in A\{3\}\)</span>. 综上 <span class="math inline">\(X\in A\{1,3\}\)</span>.</p><p>另一方面，若 <span class="math inline">\(X\in A\{1,3\}\)</span>，则： <span class="math display">\[AA^{(1,3)}=AXAA^{(1,3)}=(AX)^H(AA^{(1,3)})^H=X^HA^H(A^{(1,3)})^HA^H=X^H(AA^{(1,3)}A)^H=X^HA^H=AX\]</span> 即 <span class="math inline">\(X\)</span> 是方程的解。证毕。</p>          </div><div class="note note-success">            <p>该定理说明尽管 <span class="math inline">\(A^{(1,3)}\)</span> 不唯一，但是 <span class="math inline">\(AA^{(1,3)}\)</span> 唯一。</p>          </div><p><strong>推论</strong>：<span class="math inline">\(AA^{(1,3)}=P_{R(A)}\)</span>.</p><p><strong>定理</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n},\,A^{(1,3)}\in A\{1,3\}\)</span>，则： <span class="math display">\[A\{1,3\}=\{A^{(1,3)}+(I-A^{(1,3)}A)Z\mid Z\in\mathbb C^{n\times m}\}\]</span> <div class="note note-secondary">            <p>证明：根据引理，任意 <span class="math inline">\(X\in A\{1,3\}\)</span> 满足方程 <span class="math inline">\(AX=AA^{(1,3)}\)</span>，代入通解形式得： <span class="math display">\[\begin{align}X&amp;=A^{(1,3)}AA^{(1,3)}+Y-A^{(1,3)}AY\\&amp;=A^{(1,3)}AA^{(1,3)}+A^{(1,3)}+Z-A^{(1,3)}A(A^{(1,3)}+Z)&amp;Y=A^{(1,3)}+Z\\&amp;=A^{(1,3)}+Z+A^{(1,3)}AA^{(1,3)}-A^{(1,3)}A(A^{(1,3)}+Z)\\&amp;=A^{(1,3)}+(I-A^{(1,3)}A)Z\end{align}\]</span> 证毕。</p>          </div></p><p><strong>定理</strong>：设有方程 <span class="math inline">\(Ax=b\)</span>，则 <span class="math inline">\(x=A^{(1,3)}b\)</span> 为最小二乘解；反之，若对于任意 <span class="math inline">\(b\)</span>，<span class="math inline">\(x=Xb\)</span> 都是最小二乘解，则 <span class="math inline">\(X\in A\{1,3\}\)</span>.</p><p><strong>法方程</strong>：<span class="math inline">\(x\)</span> 是方程组 <span class="math inline">\(Ax=b\)</span> 的最小二乘解的充要条件为： <span class="math display">\[A^HAx=A^Hb\]</span></p><h2 id="矛盾方程组的极小范数最小二乘解与-a">矛盾方程组的极小范数最小二乘解与 <span class="math inline">\(A^+\)</span></h2><p><strong>定理</strong>：<span class="math inline">\(x=A^+b\)</span> 是方程组 <span class="math inline">\(Ax=b\)</span> 的唯一极小范数最小二乘解。反之，若对所有 <span class="math inline">\(b\)</span>，<span class="math inline">\(x=Xb\)</span> 都是方程 <span class="math inline">\(Ax=b\)</span> 的极小范数最小二乘解，则 <span class="math inline">\(X=A^+\)</span>.</p><p><strong>定理</strong>：若矩阵方程 <span class="math inline">\(AXB=D\)</span> 不相容，则其极小范数最小二乘解，即满足 <span class="math inline">\(\min_\limits{\min \Vert AXB-D\Vert}\Vert X\Vert\)</span> 的唯一解为 <span class="math inline">\(X=A^+DB^+\)</span>.</p><div class="note note-secondary">            <p>证明：方程两边同时行拉直： <span class="math display">\[\overline{\text{vec}}(AXB)=\overline{\text{vec}}(D)\implies (A\otimes B^T)\overline{\text{vec}}(X)=\overline{\text{vec}}(D)\]</span> 其极小范数最小二乘解为： <span class="math display">\[\overline{\text{vec}}(X)=(A\otimes B^T)^+\overline{\text{vec}}(D)=(A^+\otimes (B^T)^+)\overline{\text{vec}}(D)=(A^+\otimes (B^+)^T)\overline{\text{vec}}(D)\]</span> 于是反过来应用拉直算子得 <span class="math inline">\(X=A^+DB^+\)</span>. 证毕。</p><p>注：上述过程应用了 <span class="math inline">\((A\otimes B)^+=A^+\otimes B^+\)</span> 的结论，该结论可以通过定义验证。</p>          </div><h2 id="小结">小结</h2><h3 id="axb"><span class="math inline">\(Ax=b\)</span></h3><ul><li><span class="math inline">\(Ax=b\)</span> 相容的充要条件是 <span class="math inline">\(AA^{(1)}b=b\)</span></li><li>若 <span class="math inline">\(Ax=b\)</span> 相容，则通解为 <span class="math inline">\(x=A^{(1)}b+(I-A^{(1)}A)y\)</span></li><li>若 <span class="math inline">\(Ax=b\)</span> 相容，则极小范数解为 <span class="math inline">\(x=A^{(1,4)}b\)</span></li><li>若 <span class="math inline">\(Ax=b\)</span> 不相容，则最小二乘解为 <span class="math inline">\(x=A^{(1,3)}b\)</span></li><li>若 <span class="math inline">\(Ax=b\)</span> 不相容，则极小范数最小二乘解为 <span class="math inline">\(x=A^+b\)</span></li></ul><h3 id="axbd"><span class="math inline">\(AXB=D\)</span></h3><ul><li><span class="math inline">\(AXB=D\)</span> 相容的充要条件是 <span class="math inline">\(AA^{(1)}DB^{(1)}B=D\)</span></li><li>若 <span class="math inline">\(AXB=D\)</span> 相容，则通解为 <span class="math inline">\(X=A^{(1)}DB^{(1)}+Y-A^{(1)}AYBB^{(1)}\)</span></li><li>若 <span class="math inline">\(AXB=D\)</span> 不相容，则极小范数最小二乘解为 <span class="math inline">\(X=A^+DB^+\)</span></li></ul><h3 id="广义逆的集合表示">广义逆的集合表示</h3><ul><li><span class="math inline">\(A\{1\}=\{X\mid AXA=A\}=\{A^{(1)}+Z-A^{(1)}AZAA^{(1)}\mid Z\in\mathbb C^{n\times m}\}\)</span></li><li><span class="math inline">\(A\{1,3\}=\{X\mid AX=AA^{(1,3)}\}=\{A^{(1,3)}+(I-A^{(1,3)}A)Z\mid Z\in\mathbb C^{n\times m}\}\)</span></li><li><span class="math inline">\(A\{1,4\}=\{X\mid XA=A^{(1,4)}A\}=\{A^{(1,4)}+Z(I-AA^{(1,4)})\mid Z\in\mathbb C^{n\times m}\}\)</span></li><li><span class="math inline">\(A\{1,2\}=\{X\mid \text{rank}(X)=\text{rank}(A),\,X\in A\{1\}\}\)</span></li></ul>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]6.3广义逆矩阵的计算方法</title>
    <link href="/blog-main/2023/12/19/%E7%9F%A9%E9%98%B5%E8%AE%BA-6-3%E5%B9%BF%E4%B9%89%E9%80%86%E7%9F%A9%E9%98%B5%E7%9A%84%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95/"/>
    <url>/blog-main/2023/12/19/%E7%9F%A9%E9%98%B5%E8%AE%BA-6-3%E5%B9%BF%E4%B9%89%E9%80%86%E7%9F%A9%E9%98%B5%E7%9A%84%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="利用-hermite-标准形计算-1-逆和-12-逆">利用 Hermite 标准形计算 1-逆和 1,2-逆</h2><p><strong>定理</strong>：设 <span class="math inline">\(A\in\mathbb C_r^{m\times n}\)</span>，又设 <span class="math inline">\(Q\in\mathbb C_m^{m\times m}\)</span> 和 <span class="math inline">\(P\in\mathbb C_n^{n\times n}\)</span> 使得 <span class="math display">\[QAP=\begin{bmatrix}I_r&amp;K\\0&amp;0\end{bmatrix}\]</span> 成立（<span class="math inline">\(P\)</span> 可以只是一个列置换矩阵），则对任意 <span class="math inline">\(L\in\mathbb C^{(n-r)\times (m-r)}\)</span>，<span class="math inline">\(n\times m\)</span> 矩阵 <span class="math display">\[X=P\begin{bmatrix}I_r&amp;0\\0&amp;L\end{bmatrix}Q\]</span> 是 <span class="math inline">\(A\)</span> 的 1-逆，若令 <span class="math inline">\(L=0\)</span> 则 <span class="math inline">\(X\)</span> 是 <span class="math inline">\(A\)</span> 的 1,2-逆。</p><div class="note note-info">            <p>理论基础显然是上一节的 1-逆和 1,2-逆的通解形式，不过这里不要求 <span class="math inline">\(K=0\)</span>，相应代价就是通解中的 <span class="math inline">\(C,D\)</span> 这里必须是零，也就是说得到的是一种特解。</p>          </div><h2 id="满秩分解求广义逆矩阵">满秩分解求广义逆矩阵</h2><p><strong>定理</strong>：设 <span class="math inline">\(A\in\mathbb C_r^{m\times n}\)</span> 的满秩分解为 <span class="math inline">\(A=FG\)</span>，则：</p><ol type="1"><li><span class="math inline">\(G^{(i)}F^{(1)}\in A\{i\},\,i=1,2,4\)</span>.</li><li><span class="math inline">\(G^{(1)}F^{(i)}\in A\{i\},\,i=1,2,3\)</span>.</li><li><span class="math inline">\(G^{(1)}F^{+}\in A\{1,2,3\}\)</span>，<span class="math inline">\(G^{+}F^{(1)}\in A\{1,2,4\}\)</span>.</li><li><span class="math inline">\(A^+=G^+F^{(1,3)}=G^{(1,4)}F^+\)</span>.</li><li><span class="math inline">\(A^+=G^+F^+=G^H(GG^H)^{-1}(F^HF)^{-1}F^H\)</span>.</li></ol><div class="note note-secondary">            <p>由于 <span class="math inline">\(F\)</span> 列满秩、<span class="math inline">\(G\)</span> 行满秩，根据上一节 1-逆的性质 7，有 <span class="math inline">\(F^{(1)}F=GG^{(1)}=I_r\)</span>. 利用这一点，由定义即可验证 1 与 2。</p><p>3 和 4 可由 1 和 2 得到。5 利用了上一节关于行满秩与列满秩的矩阵的 <span class="math inline">\(A^+\)</span> 公式。</p>          </div><h2 id="zlobec-公式计算-a">Zlobec 公式计算 <span class="math inline">\(A^+\)</span></h2><p><span class="math display">\[A^+=A^H(A^HAA^H)^{(1)}A^H\]</span></p><div class="note note-secondary">            <p>证明（利用通解形式）：设 <span class="math inline">\(A=U\begin{bmatrix}\Sigma&amp;0\\0&amp;0\end{bmatrix}V^H\)</span>，则： <span class="math display">\[A^HAA^H=V\begin{bmatrix}\Sigma&amp;0\\0&amp;0\end{bmatrix}U^HU\begin{bmatrix}\Sigma&amp;0\\0&amp;0\end{bmatrix}V^HV\begin{bmatrix}\Sigma&amp;0\\0&amp;0\end{bmatrix}U^H=V\begin{bmatrix}\Sigma^3&amp;0\\0&amp;0\end{bmatrix}U^H\]</span> 于是： <span class="math display">\[(A^HAA^H)^{(1)}=U\begin{bmatrix}\Sigma^{-3}&amp;C\\D&amp;E\end{bmatrix}V^H\]</span> 因此： <span class="math display">\[A^H(A^HAA^H)^{(1)}A^H=V\begin{bmatrix}\Sigma&amp;0\\0&amp;0\end{bmatrix}U^HU\begin{bmatrix}\Sigma^{-3}&amp;C\\D&amp;E\end{bmatrix}V^HV\begin{bmatrix}\Sigma&amp;0\\0&amp;0\end{bmatrix}U^H=V\begin{bmatrix}\Sigma^{-1}&amp;0\\0&amp;0\end{bmatrix}U^H\]</span> 这就是 <span class="math inline">\(A^+\)</span> 的通解形式。证毕。</p>          </div><p>如果用方程形式去证明，需要一些引理的帮助，显得非常麻烦，这里不做叙述。不过这些引理中有一些值得注意，写在下面。</p><p><strong>定理</strong>：设 <span class="math inline">\(A\in\mathbb C_r^{m\times n},\,U\in\mathbb C^{n\times p},\,V\in\mathbb C^{q\times m}\)</span>，则 <span class="math display">\[U(VAU)^{(1)}V\in A\{1\}\iff \text{rank}(VAU)=\text{rank}(A)\]</span> <div class="note note-info">            <p>这个定理是上一节 1-逆的性质 8 的扩展。回顾性质 8（做了变量替换）： <span class="math display">\[\begin{align}&amp;AU(AU)^{(1)}A=A\iff\text{rank}(AU)=\text{rank}(A)\\&amp;A(VA)^{(1)}VA=A\iff\text{rank}(VA)=\text{rank}(A)\end{align}\]</span> 第一条是在 <span class="math inline">\(A\)</span> 的右边乘上 <span class="math inline">\(U\)</span>，第二条是在 <span class="math inline">\(A\)</span> 的左边乘上 <span class="math inline">\(V\)</span>，而这个定理左右同时乘了 <span class="math inline">\(V\)</span> 和 <span class="math inline">\(U\)</span>.</p>          </div></p><div class="note note-secondary">            <p>证明：充分性。由 <span class="math inline">\(\text{rank}(VAU)=\text{rank}(A)\)</span> 知 <span class="math inline">\(R(VAU)=R(AU)=R(A),\,N(VAU)=N(VA)=N(A)\)</span>. 故存在 <span class="math inline">\(X,Y\)</span> 使得 <span class="math inline">\(A=AUX=YVA\)</span>，于是 <span class="math inline">\(AU(VAU)^{(1)}VA=YVAU(VAU)^{(1)}VAUX=YVAUX=YVA=A\)</span>.</p><p>必要性？？？TODO</p>          </div><p><strong>定理</strong>：对任意矩阵 <span class="math inline">\(A\)</span>，满足 <span class="math inline">\(X\in A\{1,2\}\)</span> 和 <span class="math inline">\(R(X)=R(A^H),\,N(X)=N(A^H)\)</span> 的唯一矩阵为 <span class="math inline">\(A^+\)</span>.</p><h2 id="greville-公式计算-a">Greville 公式计算 <span class="math inline">\(A^+\)</span></h2><p>Greville 公式是计算 <span class="math inline">\(A^+\)</span> 的<strong>增量</strong>公式。</p><p>设 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，记 <span class="math inline">\(a_k\)</span> 为 <span class="math inline">\(A\)</span> 的第 <span class="math inline">\(k\)</span> 列，<span class="math inline">\(A_k\)</span> 为 <span class="math inline">\(A\)</span> 的前 <span class="math inline">\(k\)</span> 列构成的子矩阵；又记： <span class="math display">\[d_k=A^+_{k-1}a_k,\quad c_k=a_k-A_{k-1}d_k\]</span> 则： <span class="math display">\[A^+_k=\begin{bmatrix}A^+_{k-1}-d_kb_k^H\\b_k^H\end{bmatrix},\quad\text{where}\quad b_k^H=\begin{cases}c_k^+,&amp;c_k\neq 0\\(1+d_k^Hd_k)^{-1}d_k^HA^+_{k-1},&amp;c_k=0\end{cases}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]6.2广义逆矩阵的存在、性质及构造</title>
    <link href="/blog-main/2023/12/17/%E7%9F%A9%E9%98%B5%E8%AE%BA-6-2%E5%B9%BF%E4%B9%89%E9%80%86%E7%9F%A9%E9%98%B5%E7%9A%84%E5%AD%98%E5%9C%A8%E3%80%81%E6%80%A7%E8%B4%A8%E5%8F%8A%E6%9E%84%E9%80%A0/"/>
    <url>/blog-main/2023/12/17/%E7%9F%A9%E9%98%B5%E8%AE%BA-6-2%E5%B9%BF%E4%B9%89%E9%80%86%E7%9F%A9%E9%98%B5%E7%9A%84%E5%AD%98%E5%9C%A8%E3%80%81%E6%80%A7%E8%B4%A8%E5%8F%8A%E6%9E%84%E9%80%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="广义逆的定义">广义逆的定义</h2><p><strong>定义</strong>：设矩阵 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，若矩阵 <span class="math inline">\(X\in\mathbb C^{n\times m}\)</span> 满足如下四个 Penrose 方程： <span class="math display">\[\begin{align}&amp;AXA=A\tag{1}\label{1}\\&amp;XAX=X\tag{2}\label{2}\\&amp;(AX)^H=AX\tag{3}\label{3}\\&amp;(XA)^H=XA\tag{4}\label{4}\\\end{align}\]</span></p><p>则称 <span class="math inline">\(X\)</span> 为 <span class="math inline">\(A\)</span> 的 Moore-Penrose 逆，记作 <span class="math inline">\(A^+\)</span>.</p><p>若 <span class="math inline">\(X\)</span> 值满足上述四个方程中的第 <span class="math inline">\((i),(j),\ldots,(l)\)</span> 个方程，则称 <span class="math inline">\(X\)</span> 为 <span class="math inline">\(A\)</span> 的 <span class="math inline">\(\{i,j,\ldots,l\}\)</span>-逆，记作 <span class="math inline">\(A^{(i,j,\ldots,l)}\)</span>，其全体记为 <span class="math inline">\(A\{i,j,\ldots,l\}\)</span>.</p><p>如下为 1-逆的示意图：</p><p><img src="1inv.png" width=50% /></p><p><strong>定理</strong>：对任意 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，<span class="math inline">\(A^+\)</span> 存在且唯一。</p><div class="note note-secondary">            <p>证明：存在性。对 <span class="math inline">\(A\)</span> 做奇异值分解 <span class="math inline">\(A=U\begin{bmatrix}\Sigma&amp;0\\0&amp;0\end{bmatrix}V^H\)</span>，取 <span class="math inline">\(X=V\begin{bmatrix}\Sigma^{-1}&amp;0\\0&amp;0\end{bmatrix}U^H\)</span>，可以验证 <span class="math inline">\(X\)</span> 满足 <span class="math inline">\(A^+\)</span> 的四个条件。</p><p>唯一性。设 <span class="math inline">\(X,Y\)</span> 均是 <span class="math inline">\(A^+\)</span>，则： <span class="math display">\[\begin{align}&amp;Y=YAY=Y(AY)^H=YY^HA^H=YY^H(AXA)^H=YY^HA^H(AX)^H=Y(AY)^HAX=YAYAX=YAX\\&amp;X=XAX=(XA)^HX=A^HX^HX=(AYA)^HX^HX=(YA)^HA^HX^HX=(YA)^H(XA)^HX=(YA)^HXAX=(YA)^HX=YAX\end{align}\]</span> 故 <span class="math inline">\(X=Y\)</span>. 证毕。</p>          </div><div class="note note-info">            <p>上述定理的证明过程也给出了 <span class="math inline">\(A^+\)</span> 的一种基于奇异值分解的计算方法： <span class="math display">\[A=U\begin{bmatrix}\Sigma&amp;0\\0&amp;0\end{bmatrix}V^H\implies A^+=V\begin{bmatrix}\Sigma^{-1}&amp;0\\0&amp;0\end{bmatrix}U^H\]</span></p>          </div><p><strong>定理</strong>： <span class="math display">\[\lim_{\delta\to0}(\delta^2I+A^HA)^{-1}A^H=A^+\]</span></p><div class="note note-secondary">            <p>证明：设 <span class="math inline">\(A=U\begin{bmatrix}\Sigma&amp;0\\0&amp;0\end{bmatrix}V^H\)</span>，则 <span class="math inline">\(A^HA=V\begin{bmatrix}\Sigma^2&amp;0\\0&amp;0\end{bmatrix}V^H\)</span>，于是 <span class="math inline">\(\delta^2I+A^HA=V\begin{bmatrix}\Sigma^2+\delta^2I&amp;0\\0&amp;\delta^2I\end{bmatrix}V^H\)</span>，因此： <span class="math display">\[\begin{align}(\delta^2I+A^HA)^{-1}A^H&amp;=V\begin{bmatrix}\left[\sigma_i^2+\delta^2\right]_{r\times r}&amp;0\\0&amp;\delta^2I\end{bmatrix}^{-1}V^HA^H\\&amp;=V\begin{bmatrix}\left[\frac{1}{\sigma_i^2+\delta^2}\right]_{r\times r}&amp;0\\0&amp;\delta^{-2}I\end{bmatrix}(AV)^H\\&amp;=V\begin{bmatrix}\left[\frac{1}{\sigma_i^2+\delta^2}\right]_{r\times r}&amp;0\\0&amp;\delta^{-2}I\end{bmatrix}\begin{bmatrix}\Sigma&amp;0\\0&amp;0\end{bmatrix}U^H\\&amp;=V\begin{bmatrix}\left[\frac{\sigma_i}{\sigma_i^2+\delta^2}\right]_{r\times r}&amp;0\\0&amp;0\end{bmatrix}U^H\\\end{align}\]</span> 于是当 <span class="math inline">\(\delta\to0\)</span> 时， <span class="math display">\[\lim_{\delta\to0}(\delta^2I+A^HA)^{-1}A^H=\lim_{\delta\to0}V\begin{bmatrix}\left[\frac{\sigma_i}{\sigma_i^2+\delta^2}\right]_{r\times r}&amp;0\\0&amp;0\end{bmatrix}U^H=V\begin{bmatrix}\Sigma^{-1}&amp;0\\0&amp;0\end{bmatrix}U^H=A^+\]</span> 证毕。</p>          </div><h2 id="广义逆的性质和构造">广义逆的性质和构造</h2><h3 id="重要引理与推论">重要引理与推论</h3><p><strong>引理</strong>： <span class="math display">\[\begin{align}N(A)\supset N(B)&amp;\iff \exists X,A=XB\\R(A)\subset R(B)&amp;\iff \exists X,A=BX\end{align}\]</span> <strong>推论</strong>： <span class="math display">\[\begin{align}&amp;\text{rank}(AB)=\text{rank}(A)\implies \exists X,A=ABX\\&amp;\text{rank}(BA)=\text{rank}(A)\implies \exists X,A=XBA\end{align}\]</span> <div class="note note-secondary">            <p>证明：由于 <span class="math inline">\(R(AB)\subset R(A)\)</span>，且 <span class="math inline">\(\dim R(AB)=\text{rank}(AB)=\text{rank}(A)=\dim R(A)\)</span>，故 <span class="math inline">\(R(AB)=R(A)\)</span>. 于是 <span class="math inline">\(R(AB)\supset R(A)\)</span>，根据引理知 <span class="math inline">\(\exists X,A=ABX\)</span>.</p><p>类似地，由于 <span class="math inline">\(N(BA)\supset N(A)\)</span>，且 <span class="math inline">\(\dim N(BA)=n-\text{rank}(BA)=n-\text{rank}(A)=\dim N(A)\)</span>，故 <span class="math inline">\(N(BA)=N(A)\)</span>. 于是 <span class="math inline">\(N(BA)\subset N(A)\)</span>，根据引理知 <span class="math inline">\(\exists X,A=XBA\)</span>. 证毕。</p>          </div></p><div class="note note-success">            <p>推论的这两个式子在证明中<strong>非常常用</strong>，即用更复杂的式子表示简单的矩阵，反而有助于证明。</p>          </div><h3 id="关于-1-逆的定理">关于 1-逆的定理</h3><p><strong>定理</strong>：矩阵 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span> 有唯一 1-逆的充要条件为 <span class="math inline">\(A\)</span> 是非奇异矩阵，且该 1-逆就是 <span class="math inline">\(A^{-1}\)</span>.</p><div class="note note-secondary">            <p>证明：充分性显然，必要性证明如下。设 <span class="math inline">\(Au=0\)</span>，<span class="math inline">\(AXA=A\)</span>，那么容易验证 <span class="math inline">\(X&#39;=X+u\cdot[1,0,\ldots,0]\)</span> 也满足 <span class="math inline">\(AX&#39;A=A\)</span>，由于 1-逆唯一，故 <span class="math inline">\(u=0\)</span>，即 <span class="math inline">\(N(A)=\{0\}\)</span>. 类似可以证明 <span class="math inline">\(N(A^H)=\{0\}\)</span>，于是 <span class="math inline">\(A\)</span> 列满秩且行满秩，故 <span class="math inline">\(A\)</span> 为可逆方阵。证毕。</p>          </div><p><strong>性质</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n},\,B\in\mathbb C^{n\times p},\,\lambda\in\mathbb C\)</span>，则：</p><ol type="1"><li><p><span class="math inline">\((A^{(1)})^H\in A^{H}\{1\}\)</span>.</p></li><li><p><span class="math inline">\(\lambda^+ A^{(1)}\in(\lambda A)\{1\}\)</span>. 其中 <span class="math inline">\(\lambda^{+}=\begin{cases}\lambda^{-1}&amp;\lambda\neq 0\\0&amp;\lambda=0\end{cases}\)</span>.</p></li><li><p>若 <span class="math inline">\(S\)</span> 和 <span class="math inline">\(T\)</span> 非奇异，则 <span class="math inline">\(T^{-1}A^{(1)}S^{-1}\in(SAT)\{1\}\)</span>.</p></li><li><p><span class="math inline">\(\text{rank}(A^{(1)})\geq \text{rank}(A)\)</span>.</p><div class="note note-secondary">            <p>证明：<span class="math inline">\(\text{rank}(A)=\text{rank}(AA^{(1)}A)\leq\text{rank}(A^{(1)})\)</span>. 证毕。</p>          </div></li><li><p><span class="math inline">\(AA^{(1)}\)</span> 和 <span class="math inline">\(A^{(1)}A\)</span> 均为幂等矩阵且与 <span class="math inline">\(A\)</span> 同秩。</p><div class="note note-secondary">            <p>证明：<span class="math inline">\(\text{rank}(AA^{(1)})\leq\text{rank}(A)=\text{rank}(AA^{(1)}A)\leq\text{rank}(AA^{(1)})\)</span>，故 <span class="math inline">\(\text{rank}(AA^{(1)})=\text{rank}(A)\)</span>.</p>          </div></li><li><p><span class="math inline">\(R(AA^{(1)})=R(A),\,N(A^{(1)}A)=N(A),\,R((A^{(1)}A)^H)=R(A^H)\)</span>.</p><div class="note note-secondary">            <p>证明：<span class="math inline">\(R(AA^{(1)})\subset R(A)=R(AA^{(1)}A)\subset R(AA^{(1)})\)</span>，故 <span class="math inline">\(R(AA^{(1)})=R(A)\)</span>.</p><p>类似地，<span class="math inline">\(N(A)\subset N(A^{(1)}A)\subset N(AA^{(1)}A)=N(A)\)</span>，故 <span class="math inline">\(N(A^{(1)}A)=N(A)\)</span>.</p>          </div></li><li><p><span class="math inline">\(A^{(1)}A=I_n\iff \text{rank}(A)=n\)</span>，<span class="math inline">\(AA^{(1)}=I_m\iff \text{rank}(A)=m\)</span>.</p><div class="note note-secondary">            <p>证明：根据性质 5，<span class="math inline">\(\text{rank}(A^{(1)}A)=\text{rank}(A)\)</span>，因此必要性：<span class="math inline">\(A^{(1)}A=I_n\implies\text{rank}(A^{(1)}A)=n\implies\text{rank}(A)=n\)</span>；充分性：<span class="math inline">\(\text{rank}(A)=n\implies \text{rank}(A^{(1)}A)=n\)</span>，即 <span class="math inline">\(A^{(1)}A\)</span> 可逆，又 <span class="math inline">\(A^{(1)}A\)</span> 幂等，故为单位阵。另一个类似。证毕。</p>          </div></li><li><p><span class="math inline">\(AB(AB)^{(1)}A=A\iff\text{rank}(AB)=\text{rank}(A)\)</span>，<span class="math inline">\(B(AB)^{(1)}AB=B\iff\text{rank}(AB)=\text{rank}(B)\)</span>.</p><div class="note note-secondary">            <p>证明：这实际上是上文重要推论的进一步阐述，即给出了存在的 <span class="math inline">\(X\)</span> 的具体形式。</p><p>充分性。根据前文推论，存在 <span class="math inline">\(X\)</span> 使得 <span class="math inline">\(A=ABX\)</span>，于是 <span class="math inline">\(AB(AB)^{(1)}A=AB(AB)^{(1)}ABX=ABX=A\)</span>.</p><p>必要性。<span class="math inline">\(\text{rank}(A)\geq\text{rank}(AB)\geq\text{rank}(AB(AB)^{(1)}A)=\text{rank}(A)\)</span>，故 <span class="math inline">\(\text{rank}(AB)=\text{rank}(A)\)</span>.</p>          </div></li></ol><h3 id="关于-12-逆的定理">关于 1,2-逆的定理</h3><p><strong>定理</strong>：设 <span class="math inline">\(Y,Z\in A\{1\}\)</span>，则 <span class="math inline">\(X=YAZ\in A\{1,2\}\)</span>.</p><div class="note note-secondary">            <p>证明：<span class="math inline">\(XAX=YAZAYAZ=YAYAZ=YAZ=X\)</span>，故 <span class="math inline">\(X\in A\{2\}\)</span>.</p><p>又 <span class="math inline">\(AXA=AYAZA=AZA=A\)</span>，故 <span class="math inline">\(X\in A\{1\}\)</span>. 综上，<span class="math inline">\(X\in A\{1,2\}\)</span>. 证毕。</p>          </div><div class="note note-secondary">            <p>证明 2（利用通解形式，见下文）：奇异值分解 <span class="math inline">\(A=U\begin{bmatrix}\Sigma&amp;0\\0&amp;0\end{bmatrix}V^H\)</span>，则 <span class="math inline">\(Y,Z\)</span> 可分别写作： <span class="math display">\[Y=V\begin{bmatrix}\Sigma&amp;C_1\\D_1&amp;E_1\end{bmatrix}U^H,\quad Z=V\begin{bmatrix}\Sigma&amp;C_2\\D_2&amp;E_2\end{bmatrix}U^H\]</span> 于是： <span class="math display">\[X=YAZ=V\begin{bmatrix}B^{-1}&amp;C_1\\D_1&amp;E_1\end{bmatrix}U^HU\begin{bmatrix}B&amp;0\\0&amp;0\end{bmatrix}V^HV\begin{bmatrix}B^{-1}&amp;C_2\\D_2&amp;E_2\end{bmatrix}U^H=V\begin{bmatrix}B^{-1}&amp;C_2\\D_1&amp;D_1BC_2\end{bmatrix}U^H\]</span> 这正是 1,2-逆的通解形式，故 <span class="math inline">\(X\in A\{1,2\}\)</span>. 证毕。</p>          </div><p><strong>定理</strong>：给定矩阵 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(X\in A\{1\}\)</span>，则 <span class="math inline">\(X\in A\{1,2\}\)</span> 的充要条件是 <span class="math inline">\(\text{rank}(X)=\text{rank}(A)\)</span>.</p><div class="note note-secondary">            <p>证明：充分性。由于 <span class="math inline">\(X\in A\{1\}\)</span>，故 <span class="math inline">\(A=AXA\)</span>，于是 <span class="math inline">\(\text{rank}(A)=\text{rank}(AXA)\leq\text{rank}(XA)\leq\text{rank}(X)=\text{rank}(A)\)</span>，故 <span class="math inline">\(\text{rank}(X)=\text{rank}(XA)\)</span>. 根据推论，存在 <span class="math inline">\(Y\)</span> 使得 <span class="math inline">\(X=XAY\)</span>，于是 <span class="math inline">\(XAX=XAXAY=XAY=X\)</span>，故 <span class="math inline">\(X\in A\{2\}\)</span>.</p><p>必要性。由于 <span class="math inline">\(A=AXA,\,XAX=X\)</span>，于是 <span class="math inline">\(\text{rank}(X)=\text{rank}(XAX)\leq\text{rank}(A)=\text{rank}(AXA)\leq\text{rank}(X)\)</span>，故 <span class="math inline">\(\text{rank}(X)=\text{rank}(A)\)</span>.</p><p>证毕。</p>          </div><h3 id="关于-123-逆和-124-逆的定理">关于 1,2,3-逆和 1,2,4-逆的定理</h3><p><strong>引理</strong>：对任意矩阵 <span class="math inline">\(A\)</span> 均有： <span class="math display">\[\text{rank}(A^HA)=\text{rank}(A)=\text{rank}(AA^H)\]</span> <div class="note note-secondary">            <p>证明：由于 <span class="math inline">\(A^HAx=0\implies x^HA^HAx=0\implies Ax=0\)</span>，所以 <span class="math inline">\(N(A^HA)\subset N(A)\)</span>. 又 <span class="math inline">\(N(A^HA)\supset N(A)\)</span>，于是 <span class="math inline">\(N(A^HA)=N(A)\)</span>，于是 <span class="math inline">\(\text{rank}(A^HA)=\text{rank}(A)\)</span>. 另一个类似。证毕。</p>          </div></p><p><strong>定理</strong>：设有矩阵 <span class="math inline">\(A\)</span>，则： <span class="math display">\[\begin{align}&amp;Y=(A^HA)^{(1)}A^H\in A\{1,2,3\}\\&amp;Z=A^H(AA^H)^{(1)}\in A\{1,2,4\}\end{align}\]</span> <div class="note note-secondary">            <p>证明：由于 <span class="math inline">\(\text{rank}(A^HA)=\text{rank}(A)=\text{rank}(AA^H)\)</span>，根据 1-逆的性质 8 有： <span class="math display">\[A=A(A^HA)^{(1)}A^HA,\quad A^H=A^HA(A^HA)^{(1)}A^H\]</span> 因此： <span class="math display">\[\begin{align}&amp;AYA=A(A^HA)^{(1)}A^HA=A&amp;\implies Y\in A\{1\}\\&amp;YAY=(A^HA)^{(1)}A^HA(A^HA)^{(1)}A^H=(A^HA)^{(1)}A^H=Y&amp;\implies Y\in A\{2\}\end{align}\]</span> 又存在 <span class="math inline">\(X\)</span> 使得 <span class="math inline">\(A=XA^HA\)</span>，故： <span class="math display">\[AY=A(A^HA)^{(1)}A^H=(XA^HA)(A^HA)^{(1)}(XA^HA)^H=XA^HA(A^HA)^{(1)}A^HAX^H=XA^HAX^H\]</span> 是 Hermite 矩阵，于是 <span class="math inline">\(Y\in A\{3\}\)</span>.</p><p><span class="math inline">\(Z\)</span> 可类似证明。证毕。</p>          </div></p><h3 id="关于-a-逆的定理">关于 <span class="math inline">\(A^+\)</span> 逆的定理</h3><p><strong>定理</strong>： <span class="math display">\[A^+=A^{(1,4)}AA^{(1,3)}\]</span> <div class="note note-secondary">            <p>证明：设 <span class="math inline">\(X=A^{(1,4)}AA^{(1,3)}\)</span>，根据关于 1,2-逆的定理知 <span class="math inline">\(X\in A\{1,2\}\)</span>. 另外， <span class="math display">\[AX=AA^{(1,4)}AA^{(1,3)}=AA^{(1,3)},\quad XA=A^{(1,4)}AA^{(1,3)}A=A^{(1,4)}A\]</span> 均是 Hermite 矩阵，从而得到结论。证毕。</p>          </div></p><div class="note note-secondary">            <p>证明 2（利用通解形式，见下文）：TODO</p>          </div><p><strong>定理</strong>：给定矩阵 <span class="math inline">\(A\)</span>，有：</p><ol type="1"><li><span class="math inline">\(\text{rank}(A^+)=\text{rank}(A)\)</span>.</li><li><span class="math inline">\((A^+)^+=A\)</span>.</li><li><span class="math inline">\((A^H)^+=(A^+)^H,\,(A^T)^+=(A^+)^T\)</span>.</li><li><span class="math inline">\((A^HA)^+=A^+(A^H)^+,\,(AA^H)^+=(A^H)^+A^+\)</span>.</li><li><span class="math inline">\(A^+=(A^HA)^+A^H=A^H(AA^H)^+\)</span>.</li><li><span class="math inline">\(R(A^+)=R(A^H),\,N(A^+)=N(A^H)\)</span>.</li></ol><div class="note note-secondary">            <p>证明：前 5 条都可以通过定义证明。</p><p>对于第 6 条，根据 1 可知 <span class="math inline">\(\text{rank}(A^+)=\text{rank}(A)=\text{rank}(A^H)\)</span>，根据 5 可知 <span class="math inline">\(R(A^+)\subset R(A^H),\,N(A^+)\supset N(A^H)\)</span>，于是 <span class="math inline">\(R(A^+)=R(A^H),\,N(A^+)=N(A^H)\)</span>. 证毕。</p>          </div><p><strong>推论</strong>：若 <span class="math inline">\(A\in\mathbb C_n^{m\times n}\)</span>，即列满秩，则 <span class="math inline">\(A^+=(A^HA)^{-1}A^H\)</span>；若 <span class="math inline">\(A\in\mathbb C_m^{m\times n}\)</span>，即行满秩，则 <span class="math inline">\(A^+=A^H(AA^H)^{-1}\)</span>.</p><p><strong>推论</strong>：若 <span class="math inline">\(\alpha\in\mathbb C^n\)</span>，且 <span class="math inline">\(\alpha\neq 0\)</span>，则 <span class="math inline">\(\alpha^+=(\alpha^H\alpha)^{-1}\alpha^H\)</span>，而 <span class="math inline">\((\alpha^H)^+=(\alpha^+)^H=\alpha(\alpha^H\alpha)^{-1}\)</span>.</p><h2 id="广义逆的通解形式">广义逆的通解形式</h2><div class="note note-success">            <p>第一节中给出的广义逆的定义是方程形式，本节则直接推导广义逆的通解形式。</p><p>做证明时，有时使用方程形式不容易想到思路，而使用通解只需要无脑计算即可。</p>          </div><p>设 <span class="math inline">\(A\in\mathbb C^{m\times n}_r\)</span>，存在 <span class="math inline">\(m\)</span> 阶可逆矩阵（或酉矩阵） <span class="math inline">\(P\)</span> 和 <span class="math inline">\(n\)</span> 阶可逆矩阵（或酉矩阵） <span class="math inline">\(Q\)</span> 使得 <span class="math inline">\(A=P\begin{bmatrix}B&amp;0\\0&amp;0\end{bmatrix}Q\)</span>，其中 <span class="math inline">\(B\)</span> 为 <span class="math inline">\(r\)</span> 阶可逆矩阵。</p><div class="note note-success">            <p>注：应用奇异值分解可以使得 <span class="math inline">\(A=U\begin{bmatrix}\Sigma&amp;0\\0&amp;0\end{bmatrix}V^H\)</span>，这是上面的特殊情形，因此<strong>做证明题时直接奇异值分解</strong>就行了。</p><p>但是做计算题时，奇异值分解比较麻烦，所以我们不必追求让 <span class="math inline">\(B\)</span> 称为对角矩阵，只需要使得 <span class="math inline">\(B\)</span> 可逆即可。可以通过如下方式计算 <span class="math inline">\(P,Q,B\)</span>： <span class="math display">\[\begin{bmatrix}A&amp;I_m\\I_n&amp;0\end{bmatrix}\xrightarrow[\text{列变换}]{\text{行变换}}\begin{bmatrix}\begin{bmatrix}B&amp;0\\0&amp;0\end{bmatrix}&amp;P\\Q&amp;0\end{bmatrix}\]</span> 也可以通过 QR 分解做：首先使用列置换矩阵 <span class="math inline">\(P\)</span> 使得 <span class="math inline">\(AP\)</span> 前 <span class="math inline">\(r\)</span> 列线性无关，则对 <span class="math inline">\(AP\)</span> 做 QR 分解得 <span class="math inline">\(AP=Q_1\begin{bmatrix}R_1&amp;G\\0&amp;0\end{bmatrix}\)</span>，其中 <span class="math inline">\(R_1\)</span> 为上三角矩阵。再对 <span class="math inline">\(\begin{bmatrix}R_1^H&amp;0\\G^H&amp;0\end{bmatrix}\)</span> 做 QR 分解得 <span class="math inline">\(\begin{bmatrix}R_1^H&amp;0\\G^H&amp;0\end{bmatrix}=Q_2\begin{bmatrix}R_2&amp;0\\0&amp;0\end{bmatrix}\)</span>，其中 <span class="math inline">\(R_2\)</span> 为上三角矩阵。于是： <span class="math display">\[AP=Q_1\begin{bmatrix}R_2^H&amp;0\\0&amp;0\end{bmatrix}Q_2^H\implies A=Q_1\begin{bmatrix}R_2^H&amp;0\\0&amp;0\end{bmatrix}Q_2^HP^T\]</span> 这样得到的 <span class="math inline">\(B\)</span> 是一个下三角矩阵。</p>          </div><table><thead><tr class="header"><th style="text-align: center;">广义逆</th><th style="text-align: center;">通解</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><span class="math inline">\(X\in A\{1\}\)</span></td><td style="text-align: center;"><span class="math inline">\(\exists\ C,D,E,\quad\;X=Q^{-1}\begin{bmatrix}B^{-1}&amp;C\\D&amp;E\end{bmatrix}P^{-1}\)</span></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(X\in A\{1,2\}\)</span></td><td style="text-align: center;"><span class="math inline">\(\exists\ C,D,\quad X=Q^{-1}\begin{bmatrix}B^{-1}&amp;C\\D&amp;DBC\end{bmatrix}P^{-1}\)</span></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(X\in A\{1,3\}\)</span></td><td style="text-align: center;"><span class="math inline">\(\exists\ D,E,\quad\;X=Q^{-1}\begin{bmatrix}B^{-1}&amp;0\\D&amp;E\end{bmatrix}P^{-1}\)</span></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(X\in A\{1,4\}\)</span></td><td style="text-align: center;"><span class="math inline">\(\exists\ C,E,\quad\;X=Q^{-1}\begin{bmatrix}B^{-1}&amp;C\\0&amp;E\end{bmatrix}P^{-1}\)</span></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(X\in A\{1,2,3\}\)</span></td><td style="text-align: center;"><span class="math inline">\(\exists\ D,\quad\;X=Q^{-1}\begin{bmatrix}B^{-1}&amp;0\\D&amp;0\end{bmatrix}P^{-1}\)</span></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(X\in A\{1,2,4\}\)</span></td><td style="text-align: center;"><span class="math inline">\(\exists\ C,\quad\;X=Q^{-1}\begin{bmatrix}B^{-1}&amp;C\\0&amp;0\end{bmatrix}P^{-1}\)</span></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(X\in A\{1,3,4\}\)</span></td><td style="text-align: center;"><span class="math inline">\(\exists\ E,\quad\;X=Q^{-1}\begin{bmatrix}B^{-1}&amp;0\\0&amp;E\end{bmatrix}P^{-1}\)</span></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(X\in A\{1,2,3,4\}\)</span></td><td style="text-align: center;"><span class="math inline">\(X=Q^{-1}\begin{bmatrix}B^{-1}&amp;0\\0&amp;0\end{bmatrix}P^{-1}\)</span></td></tr></tbody></table><p>如果 <span class="math inline">\(P,Q\)</span> 是酉矩阵，则 <span class="math inline">\(Q^{-1},P^{-1}\)</span> 写为 <span class="math inline">\(Q^H,P^H\)</span> 即可。</p><h2 id="广义逆的等价定义">广义逆的等价定义</h2><p><strong>定义</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，若矩阵 <span class="math inline">\(X\in\mathbb C^{n\times m}\)</span> 满足 <span class="math inline">\(AX=P_{R(A)},\,XA=P_{R(X)}\)</span>，其中 <span class="math inline">\(P_L\)</span> 是空间 <span class="math inline">\(L\)</span> 上的正交投影矩阵，则称 <span class="math inline">\(X\)</span> 为 <span class="math inline">\(A\)</span> 的 Moore 广义逆矩阵。</p><p><strong>定理</strong>：Moore 广义逆矩阵和 Penrose 广义逆矩阵是等价的。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]6.1投影矩阵及其应用</title>
    <link href="/blog-main/2023/12/17/%E7%9F%A9%E9%98%B5%E8%AE%BA-6-1%E6%8A%95%E5%BD%B1%E7%9F%A9%E9%98%B5%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/"/>
    <url>/blog-main/2023/12/17/%E7%9F%A9%E9%98%B5%E8%AE%BA-6-1%E6%8A%95%E5%BD%B1%E7%9F%A9%E9%98%B5%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="投影算子与投影矩阵">投影算子与投影矩阵</h2><h3 id="定义">定义</h3><p><strong>投影算子</strong>：设 <span class="math inline">\(\mathbb C^n=L\oplus M\)</span>，则对于任意 <span class="math inline">\(x\in\mathbb C^n\)</span> 有唯一分解 <span class="math inline">\(x=y+z,\,y\in L,\,z\in M\)</span>. 称将 <span class="math inline">\(x\)</span> 变为 <span class="math inline">\(y\)</span> 的变换为沿着 <span class="math inline">\(M\)</span> 到 <span class="math inline">\(L\)</span> 的投影算子，记作 <span class="math inline">\(P_{L,M}\)</span>，即： <span class="math display">\[P_{L,M}x=y\]</span> <img src="proj.png" width=30% /></p><p><strong>性质</strong>：<span class="math inline">\(R(P_{L,M})=L,\,N(P_{L,M})=M\)</span>. 注意 <span class="math inline">\(x-y\in M\)</span>.</p><p><strong>投影矩阵</strong>：投影算子 <span class="math inline">\(P_{L,M}\)</span> 在 <span class="math inline">\(\mathbb C^n\)</span> 的基 <span class="math inline">\((e_1,\ldots,e_n)\)</span> 下的矩阵称为投影矩阵。</p><h3 id="投影与幂等">投影与幂等</h3><p><strong>引理</strong>：设 <span class="math inline">\(A\in\mathbb C^{n\times n}\)</span> 是幂等矩阵，<span class="math inline">\(A^2=A\)</span>，则： <span class="math display">\[N(A)=R(I-A)\]</span> <div class="note note-secondary">            <p>证明：任取 <span class="math inline">\(x\in N(A)\)</span>，则 <span class="math inline">\(Ax=0\)</span>，则 <span class="math inline">\(x=Ax+(I-A)x=(I-A)x\in R(I-A)\)</span>，因此 <span class="math inline">\(N(A)\subset R(I-A)\)</span>；</p><p>任取 <span class="math inline">\(y\in\mathbb C^n\)</span>，<span class="math inline">\(A(I-A)y=(A-A^2)y=0\)</span>，故 <span class="math inline">\((I-A)y\in N(A)\)</span>，故 <span class="math inline">\(R(I-A)\subset N(A)\)</span>；</p><p>综上，<span class="math inline">\(N(A)=R(I-A)\)</span>. 证毕。</p>          </div></p><p><strong>定理</strong>：矩阵 <span class="math inline">\(P\)</span> 为投影矩阵的充要条件是 <span class="math inline">\(P\)</span> 为幂等矩阵，即： <span class="math display">\[P_{n\times n}=P_{L,M}\iff P^2=P\]</span> <div class="note note-secondary">            <p>证明：必要性。设 <span class="math inline">\(C^n=L\oplus M\)</span>，则对于任意 <span class="math inline">\(x\in\mathbb C^n\)</span>，存在唯一的分解 <span class="math inline">\(x=y+z,\,y\in L,\,z\in M\)</span>. 于是 <span class="math inline">\(P_{L,M}x=y\)</span>. 因此 <span class="math inline">\(P_{L,M}^2x=P_{L,M}y=y=P_{L,M}x\)</span>，即 <span class="math inline">\(P_{L,M}\)</span> 是幂等的。</p><p>充分性。任意 <span class="math inline">\(x\in\mathbb C^n\)</span> 可分解为 <span class="math inline">\(x=Px+(I-P)x\)</span>，根据引理知 <span class="math inline">\(N(P)=R(I-P)\)</span>，又 <span class="math inline">\(\mathbb C^n=R(P)\oplus N(P)\)</span>，所以这样的分解是唯一的，于是 <span class="math inline">\(P=P_{R(P),N(P)}\)</span>. 证毕。</p>          </div></p><h3 id="计算方法">计算方法</h3><p>取 <span class="math inline">\(L\)</span> 的一组基 <span class="math inline">\((q_1,\ldots,q_r)\)</span> 和 <span class="math inline">\(M\)</span> 的一组基 <span class="math inline">\((q_{r+1},\ldots,q_n)\)</span>，则任意向量 <span class="math inline">\(x\in\mathbb C^n\)</span> 可表示为： <span class="math display">\[x=(q_1,\ldots,q_r,q_{r+1},\ldots,q_n)y=Qy\]</span> 于是： <span class="math display">\[P_{L,M}x=QI_ry=QI_rQ^{-1}x\implies P_{L,M}=QI_rQ^{-1}\]</span> 其中 <span class="math inline">\(I_r\)</span> 表示前 <span class="math inline">\(r\)</span> 个对角元为 1、其余为 0 的对角矩阵。</p><div class="note note-info">            <p>可以看见上面的计算方法涉及到基的选取，但可以证明选取不同的基算出来的 <span class="math inline">\(P_{L,M}\)</span> 都是一样的。</p><p>假设另选一组基 <span class="math inline">\(\bar Q_L=(\bar q_1,\ldots,\bar q_r)\)</span> 和 <span class="math inline">\(\bar Q_M=(\bar q_{r+1},\ldots,\bar q_n)\)</span>，设 <span class="math inline">\(\bar Q_L=Q_LR_1,\,\bar Q_M=Q_MR_2\)</span>，则 <span class="math inline">\(\bar Q=Q\text{diag}(R_1,R_2)\)</span>，于是： <span class="math display">\[\bar P_{L,M}=\bar Q I_r\bar Q^{-1}=Q\text{diag}(R_1,R_2)I_r\text{diag}(R_1^{-1},R_2^{-1})Q^{-1}=QI_rQ^{-1}=P_{L,M}\]</span> 可见 <span class="math inline">\(P_{L,M}\)</span> 与基的选取无关。</p>          </div><h2 id="正交投影算子与正交投影矩阵">正交投影算子与正交投影矩阵</h2><h3 id="定义-1">定义</h3><p><strong>正交投影算子</strong>：设 <span class="math inline">\(L\)</span> 是 <span class="math inline">\(\mathbb C^n\)</span> 的子空间，则沿着 <span class="math inline">\(L^{\perp}\)</span> 到 <span class="math inline">\(L\)</span> 的投影算子 <span class="math inline">\(P_{L,L^{\perp}}\)</span> 为正交投影算子，简记为 <span class="math inline">\(P_L\)</span>.</p><p><img src="proj-o.png" width=30% /></p><p><strong>正交投影矩阵</strong>：正交投影算子 <span class="math inline">\(P_{L}\)</span> 在 <span class="math inline">\(\mathbb C^n\)</span> 的基 <span class="math inline">\(e_1,\ldots,e_n\)</span> 下的矩阵称为正交投影矩阵。</p><h3 id="正交投影与幂等-hermite">正交投影与幂等 Hermite</h3><p><strong>定理</strong>：矩阵 <span class="math inline">\(P\)</span> 为正交投影矩阵的充要条件是 <span class="math inline">\(P\)</span> 为幂等 Hermite 矩阵。</p><div class="note note-secondary">            <p>证明：必要性。若 <span class="math inline">\(P\)</span> 为正交投影矩阵，则根据上一节定理知它是幂等矩阵，于是 <span class="math inline">\(R(I-P)=N(P)\)</span>。又 <span class="math inline">\(R(P)\perp N(P)\)</span>，所以 <span class="math inline">\(R(P)\perp R(I-P)\)</span>，因此对于任意 <span class="math inline">\(x,y\in\mathbb C^n\)</span>，有： <span class="math display">\[x^HP^H(I-P)y=0\implies P^H(I-P)=0\implies P^H=P^HP\implies P=(P^HP)^H=P^HP=P^H\]</span> 即 <span class="math inline">\(P\)</span> 是 Hermite 矩阵。</p><p>充分性。若 <span class="math inline">\(P\)</span> 是幂等 Hermite 矩阵，则根据上一节定理知它是投影矩阵 <span class="math inline">\(P_{R(P),N(P)}\)</span>. 又由于 <span class="math inline">\(P^H=P\)</span>，所以： <span class="math display">\[P_{R(P),N(P)}=P_{R(P),N(P^H)}=P_{R(P),R^\perp(P)}\]</span> 即 <span class="math inline">\(P\)</span> 是正交投影矩阵。证毕。</p>          </div><h3 id="计算方法-1">计算方法</h3><p>取 <span class="math inline">\(L\)</span> 的一组基 <span class="math inline">\(X=(x_1,\ldots,x_r)\)</span>，<span class="math inline">\(L^{\perp}\)</span> 的一组基 <span class="math inline">\(y=(y_1,\ldots,y_{n-r})\)</span>，则 <span class="math inline">\(X^HY=Y^HX=O\)</span>. 根据上一节投影矩阵的计算方法知： <span class="math display">\[P_L=P_{L,L^\perp}=\left[\begin{array}{c:c}X&amp;Y\end{array}\right]\;I_r\;\left[\begin{array}{c:c}X&amp;Y\end{array}\right]^{-1}=\left[\begin{array}{c:c}X&amp;O\end{array}\right]\left[\begin{array}{c:c}X&amp;Y\end{array}\right]^{-1}\]</span> 由于： <span class="math display">\[\left[\begin{array}{c:c}X&amp;Y\end{array}\right]^{H}\left[\begin{array}{c:c}X&amp;Y\end{array}\right]=\left[\begin{array}{c}X^H\\\hdashline Y^H\end{array}\right]\left[\begin{array}{c:c}X&amp;Y\end{array}\right]=\left[\begin{array}{c:c}X^HX&amp;O\\\hdashline O&amp;Y^HY\end{array}\right]\]</span> 于是： <span class="math display">\[\left[\begin{array}{c:c}X&amp;Y\end{array}\right]^{-1}=\left[\begin{array}{c:c}(X^HX)^{-1}&amp;O\\\hdashline O&amp;(Y^HY)^{-1}\end{array}\right]\left[\begin{array}{c}X^H\\\hdashline Y^H\end{array}\right]=\left[\begin{array}{c}(X^HX)^{-1}X^H\\\hdashline(Y^HY)^{-1}Y^H\end{array}\right]\]</span> 因此： <span class="math display">\[P_L=\left[\begin{array}{c:c}X&amp;O\end{array}\right]\left[\begin{array}{c:c}X&amp;Y\end{array}\right]^{-1}=\left[\begin{array}{c:c}X&amp;O\end{array}\right]\left[\begin{array}{c}(X^HX)^{-1}X^H\\\hdashline(Y^HY)^{-1}Y^H\end{array}\right]=X(X^HX)^{-1}X^H\]</span> <div class="note note-info">            <p>同样的，正交投影矩阵的计算也与选取的基无关。假设有另一组基 <span class="math inline">\(\bar X=(\bar x_1,\ldots,\bar x_r)\)</span>，设 <span class="math inline">\(\bar X=XR\)</span>，则： <span class="math display">\[\bar P_L=\bar X({\bar X}^H\bar X)^{-1}{\bar X}^H=XR(R^HX^HXR)^{-1}R^HX^H=XRR^{-1}(X^HX)^{-1}(R^H)^{-1}R^HX^H=X(X^HX)^{-1}X^H=P_L\]</span> 可见 <span class="math inline">\(P_L\)</span> 与基的选取无关。</p>          </div></p><div class="note note-info">            <p>由于 <span class="math inline">\(X\)</span> 是列满秩矩阵，根据下一节的内容可知 <span class="math inline">\(X^+=(X^HX)^{-1}X\)</span>，所以 <span class="math inline">\(P_L=XX^+\)</span>.</p>          </div>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Wavelet Transform</title>
    <link href="/blog-main/2023/12/12/Wavelet-Transform/"/>
    <url>/blog-main/2023/12/12/Wavelet-Transform/</url>
    
    <content type="html"><![CDATA[<h2 id="尺度函数">尺度函数</h2><p>给定一个平方可积的实值函数 <span class="math inline">\(\varphi(x)\)</span>，考虑如下函数集合 <span class="math inline">\(\{\varphi_{j,k}(x)\}\)</span>： <span class="math display">\[\varphi_{j,k}(x)=2^{j/2}\varphi(2^jx-k)\]</span> 其中 <span class="math inline">\(k\in\mathbb Z\)</span> 表示平移量，决定函数的位置；<span class="math inline">\(j\in\mathbb Z\)</span> 指示沿 <span class="math inline">\(x\)</span> 轴的伸缩量，决定函数的宽度；另外，系数 <span class="math inline">\(2^{j/2}\)</span> 控制函数的高度。由于 <span class="math inline">\(\varphi_{j,k}(x)\)</span> 的形状由 <span class="math inline">\(j\)</span> 控制，因此我们称 <span class="math inline">\(\varphi(x)\)</span> 为<strong>尺度函数</strong>。</p><p>举个例子，<strong>Haar 尺度函数</strong>定义为： <span class="math display">\[\varphi(x)=\begin{cases}1&amp;0\leq x&lt;1\\0&amp;\text{otherwise}\end{cases}\]</span> 下图可视化了 Haar 尺度函数及其平移伸缩后的图像：</p><p><img src="haar-scaling.png" width=70% /></p><p>当我们固定 <span class="math inline">\(j=j_0\)</span> 后，改变 <span class="math inline">\(k\)</span>，就得到尺度函数的一系列间隔整数单位的平移副本，它们可以作为一组基张成一个函数空间： <span class="math display">\[V_{j_0}=\overline{\text{Span}\{\varphi_{j_0,k}(x)\}}\]</span> 因此，若 <span class="math inline">\(f(x)\in V_{j_0}\)</span>，则它能表示这组基的线性组合： <span class="math display">\[f(x)=\sum_{k}\alpha_k\varphi_{j_0,k}(x)\]</span> 例如，对于Haar 尺度函数，当 <span class="math inline">\(j=0\)</span> 时，可以想像所有的 <span class="math inline">\(\{\varphi_{0,k}(x)\}\)</span> 正好一个挨着一个铺满了 <span class="math inline">\(x\)</span> 轴。那么，只要一个函数 <span class="math inline">\(f(x)\)</span> 的“分辨率”不是太高，也即它在一个整数单位区间内不改变，那么就能表示为 <span class="math inline">\(\{\varphi_{0,k}(x)\}\)</span> 的线性组合。类似地，当 <span class="math inline">\(j=1\)</span> 时，<span class="math inline">\(\{\varphi_{1,k}(x)\}\)</span> 的“分辨率”更高了，只要 <span class="math inline">\(f(x)\)</span> 在半个整数单位区间内不改变，就可以表示为 <span class="math inline">\(\{\varphi_{1,k}(x)\}\)</span> 的线性组合。以此类推，随着 <span class="math inline">\(j\)</span> 增大，基函数 <span class="math inline">\(\{\varphi_{j,k}(x)\}\)</span> 的“分辨率”越来越高，能表示的 <span class="math inline">\(f(x)\)</span> 也就越来越精细。这就是<strong>多分辨率分析</strong>的意思。如下左图展示了一个可以表示为 <span class="math inline">\(\varphi_{1,0}(x),\varphi_{1,1}(x),\varphi_{1,4}(x)\)</span> 的线性组合的 <span class="math inline">\(f(x)\)</span>；右图则表明 <span class="math inline">\(\varphi_{0,0}(x)\)</span> 可以表示为 <span class="math inline">\(\varphi_{1,0}(x)\)</span> 和 <span class="math inline">\(\varphi_{1,1}(x)\)</span> 的线性组合。</p><p><img src="haar-scaling2.png" width=70% /></p><p>不过，为了多分辨率分析能够正常地进行下去，我们对尺度函数 <span class="math inline">\(\varphi(x)\)</span> 提出一些要求：</p><ol type="1"><li><p>尺度函数与其间隔整数的平移副本之间是正交的。正交即内积为零，函数空间中内积的定义为：</p><p><span class="math display">\[\langle f(x),g(x)\rangle=\int f^\ast(x)g(x)\mathrm dx\]</span> 因此，我们要求： <span class="math display">\[\langle \varphi_{j,k_1}(x),\varphi_{j,k_2}(x)\rangle=\int \varphi_{j,k_1}^\ast(x)\varphi_{j,k_2}(x)\mathrm dx=2^j\int\varphi^\ast(2^jx-k_1)\varphi(2^jx-k_2)\mathrm dx=0\]</span></p></li><li><p>低分辨率尺度函数张成的子空间嵌套在高分辨率尺度函数张成的子空间之内。 <span class="math display">\[V_{-\infty}\subset\cdots\subset V_{-1}\subset V_0\subset V_1\subset V_2\subset\cdots\subset V_{\infty}\]</span></p></li><li><p>唯一包含在所有空间中的函数是 <span class="math inline">\(f(x)=0\)</span>. <span class="math display">\[V_{-\infty}=\{0\}\]</span></p></li><li><p>任何函数都可以以任意精度表示。 <span class="math display">\[V_{\infty}=\{L^2(\mathbb R)\}\]</span> 其中 <span class="math inline">\(L^2(\mathbb R)\)</span> 表示所有平方可积函数。</p></li></ol><p><img src="nested.png" width=40% /></p><p>于是，在这些要求下，<span class="math inline">\(V_j\)</span> 的基函数可以表示为更高一级分辨率 <span class="math inline">\(V_{j+1}\)</span> 的基函数的线性组合： <span class="math display">\[\varphi_{j,k}(x)=\sum_{n}\alpha_n\varphi_{j+1,n}(x)\]</span> 当 <span class="math inline">\(j=k=0\)</span> 时，有： <span class="math display">\[\varphi(x)=\sum_nh_\varphi(n)\sqrt{2}\varphi(2x-n)\tag{1}\label{scaling}\]</span> 这里 <span class="math inline">\(h_\varphi(n)\)</span> 称作<strong>尺度函数系数</strong>，<span class="math inline">\(h_\varphi\)</span> 称作尺度向量。<strong>这个式子是多分辨率分析的基础</strong>，它给出了尺度函数与其两倍分辨率副本之间的关系。例如，对于 Haar 尺度函数，有 <span class="math inline">\(\varphi(x)=\varphi(2x)+\varphi(2x-1)\)</span>，因此 <span class="math inline">\(h_\varphi(0)=h_\varphi(1)=1/\sqrt{2}\)</span>.</p><h2 id="小波函数">小波函数</h2><p>上文提到，尺度函数的相邻两个分辨率张成的空间之间是嵌套的子空间关系，即 <span class="math inline">\(V_{j}\subset V_{j+1}\)</span>. 于是，我们可以找到 <span class="math inline">\(V_j\)</span> 的<strong>正交补</strong> <span class="math inline">\(W_j\)</span>，使得 <span class="math inline">\(V_{j+1}=V_j\oplus W_j\)</span>，如图所示：</p><p><img src="nested2.png" width=50% /></p><p>那么，所有平方可积函数构成的空间就可以表示如下： <span class="math display">\[\begin{align}L^2(\mathbb R)&amp;=V_0\oplus W_0\oplus W_1\oplus\cdots\\&amp;=V_1\oplus W_1\oplus W_2\oplus\cdots\\&amp;=V_{j_0}\oplus W_{j_0}\oplus W_{j_0+1}\oplus\cdots\\&amp;=\cdots\oplus W_{-1}\oplus W_0\oplus W_1\oplus\cdots\end{align}\]</span> 即一个起始的 <span class="math inline">\(V_{j_0}\)</span> 与无数个更大分辨率的 <span class="math inline">\(W_{j}\,(j\geq j_0)\)</span> 之直和。</p><p>在 <span class="math inline">\(W_j\)</span> 中，我们可以找到一组与尺度函数形式类似的正交基 <span class="math inline">\(\{\psi_{j,k}(x)\}\)</span>： <span class="math display">\[\psi_{j,k}(x)=2^{j/2}\psi(2^jx-k)\]</span> 其中 <span class="math inline">\(\psi(x)\)</span> 称作<strong>小波函数</strong>。由于 <span class="math inline">\(W_j\subset V_{j+1}\)</span>，仿照 <span class="math inline">\(\eqref{scaling}\)</span> 式，<span class="math inline">\(\psi(x)\)</span> 也能用其两倍分辨率的尺度函数表达： <span class="math display">\[\psi(x)=\sum_nh_\psi(n)\sqrt{2}\varphi(2x-n)\tag{2}\label{wavelet}\]</span> 其中 <span class="math inline">\(h_\psi(n)\)</span> 称作<strong>小波函数系数</strong>。</p><p>不过说了这么多，我们还是没有给出同尺度下 <span class="math inline">\(\varphi(x)\)</span> 与 <span class="math inline">\(\psi(x)\)</span> 之间到底是什么关系。利用 <span class="math inline">\(W_j\)</span> 是 <span class="math inline">\(V_j\)</span> 的正交补，有论文指出小波函数系数与尺度函数系数之间的关系为： <span class="math display">\[h_\psi(n)=(-1)^nh_\varphi(1-n)\tag{3}\label{coef-relation}\]</span> 我们还是拿 Haar 尺度函数举例。根据 <span class="math inline">\(\eqref{coef-relation}\)</span> 式，可以得到 Haar 小波函数系数为 <span class="math inline">\(h_\psi(0)=1/\sqrt{2},\,h_\psi(1)=-1/\sqrt{2}\)</span>. 因此，<strong>Haar 小波函数</strong>为： <span class="math display">\[\psi(x)=\begin{cases}1&amp;0\leq x&lt;0.5\\-1&amp;0.5\leq x&lt;1\\0&amp;\text{otherwise}\end{cases}\]</span> 下图展示了 Haar 小波函数及其平移伸缩后的图像，以及将 <span class="math inline">\(f(x)\in V_1\)</span> 分解为 <span class="math inline">\(V_0\)</span> 中的尺度函数与 <span class="math inline">\(W_0\)</span> 中的小波函数的线性组合。</p><p><img src="haar-wavelet.png" width=70% /></p><h2 id="小波级数展开">小波级数展开</h2><p>对任意 <span class="math inline">\(f(x)\in L^2(\mathbb R)\)</span>，考虑从分辨率 <span class="math inline">\(j_0\)</span> 开始的空间分解： <span class="math display">\[L^2(\mathbb R)=V_{j_0}\oplus W_{j_0}\oplus W_{j_0+1}\oplus\cdots\]</span> 则 <span class="math inline">\(f(x)\)</span> 可以分解为相应分辨率的小波函数和尺度函数的线性组合： <span class="math display">\[f(x)=\sum_k c_{j_0}(k)\varphi_{j_0,k}(x)+\sum_{j=j_0}^\infty\sum_k d_j(k)\psi_{j,k}(x)\]</span> 由于 <span class="math inline">\(\{\varphi_{j_0,k}(x),\psi_{j,k}(x)\}\)</span> 之间两两都是正交的，所以组合系数就是函数与基的内积： <span class="math display">\[\begin{align}&amp;c_{j_0}(k)=\langle f(x),\varphi_{j_0,k}(x)\rangle=\int f^\ast(x)\varphi_{j_0,k}(x)\mathrm dx\\&amp;d_{j}(k)=\langle f(x),\psi_{j,k}(x) \rangle=\int f^\ast(x)\psi_{j,k}(x)\mathrm dx\end{align}\]</span></p><h2 id="离散小波变换">离散小波变换</h2><p>考虑连续函数 <span class="math inline">\(f(x)\)</span> 的离散采样 <span class="math inline">\(\{f(n)\mid n=0,1,\ldots,M-1\}\)</span>. 假设 <span class="math inline">\(M=2^J\)</span> 为 2 的幂次，并取 <span class="math inline">\(j_0=0\)</span>，那么有离散小波变换对： <span class="math display">\[\begin{align}&amp;f(n)=\frac{1}{\sqrt{M}} W_\varphi(0,0)\varphi_{0,0}(n)+\frac{1}{\sqrt{M}}\sum_{j=0}^{J-1}\sum_{k=0}^{2^j-1}W_\psi(j,k)\psi_{j,k}(n)\\&amp;W_\varphi(0,0)=\frac{1}{\sqrt{M}}\sum_{n=0}^{M-1} f(n)\varphi_{0,0}(n)\\&amp;W_\psi(j,k)=\frac{1}{\sqrt{M}}\sum_{n=0}^{M-1} f(n)\psi_{j,k}(n)\end{align}\tag{4}\label{dwt}\]</span> 注意这里略有滥用记号：这里的 <span class="math inline">\(\varphi_{j,k}(n),\psi_{j,k}(n)\)</span> 指的是将 <span class="math inline">\([0,1]\)</span> 区间分成 <span class="math inline">\(M\)</span> 份后，第 <span class="math inline">\(n\)</span> 份位置处尺度函数和小波函数的值，即其实是前文的 <span class="math inline">\(\varphi_{j,k}(n/M)\)</span> 和 <span class="math inline">\(\psi_{j,k}(n/M)\)</span>.</p><p>例如，对于 Haar 尺度函数和小波函数，当 <span class="math inline">\(M=2\)</span> 时，<span class="math inline">\(\varphi_{0,0}(0)=\varphi_{0,0}(1)=1,\,\psi_{0,0}(0)=1,\,\psi_{0,0}(1)=-1\)</span>，或写作矩阵形式： <span class="math display">\[\begin{bmatrix}\varphi_{0,0}(0)&amp;\varphi_{0,0}(1)\\\psi_{0,0}(0)&amp;\psi_{0,0}(1)\end{bmatrix}=\begin{bmatrix}1&amp;1\\1&amp;-1\end{bmatrix}\]</span> 当 <span class="math inline">\(M=4\)</span> 时，相应矩阵为： <span class="math display">\[\begin{bmatrix}\varphi_{0,0}(0)&amp;\varphi_{0,0}(1)&amp;\varphi_{0,0}(2)&amp;\varphi_{0,0}(3)\\\psi_{0,0}(0)&amp;\psi_{0,0}(1)&amp;\psi_{0,0}(2)&amp;\psi_{0,0}(3)\\\psi_{1,0}(0)&amp;\psi_{1,0}(1)&amp;\psi_{1,0}(2)&amp;\psi_{1,0}(3)\\\psi_{1,1}(0)&amp;\psi_{1,1}(1)&amp;\psi_{1,1}(2)&amp;\psi_{1,1}(3)\end{bmatrix}=\begin{bmatrix}1&amp;1&amp;1&amp;1\\1&amp;1&amp;-1&amp;-1\\\sqrt{2}&amp;-\sqrt{2}&amp;0&amp;0\\0&amp;0&amp;\sqrt{2}&amp;-\sqrt{2}\end{bmatrix}\]</span> 作图可视化如下：</p><p><img src="haar-discrete.png" width=100% /></p><p>事实上，如果对上面的矩阵做一个归一化（使得每行都是单位向量），那么得到的矩阵称作 <strong>Haar 矩阵</strong>： <span class="math display">\[H_2=\frac{1}{\sqrt{2}}\begin{bmatrix}1&amp;1\\1&amp;-1\end{bmatrix},\quad H_4=\frac{1}{\sqrt{4}}\begin{bmatrix}1&amp;1&amp;1&amp;1\\1&amp;1&amp;-1&amp;-1\\\sqrt{2}&amp;-\sqrt{2}&amp;0&amp;0\\0&amp;0&amp;\sqrt{2}&amp;-\sqrt{2}\end{bmatrix}\]</span> 注意归一化系数其实就是离散小波变换 <span class="math inline">\(\eqref{dwt}\)</span> 式中的系数 <span class="math inline">\(1/\sqrt{M}\)</span>. 因此，视离散采样的 <span class="math inline">\(M\)</span> 个值为一个向量 <span class="math inline">\([f(0),f(1),\ldots,f(M-1)]^T\)</span> ，Haar 矩阵各行构成一组单位正交基，那么离散小波变换其实就是这个向量在这组基下的线性表示。</p><h2 id="子带编码">子带编码</h2><p>为了介绍快速小波变换，我们需要了解一个前置知识——子带编码。</p><p><img src="subband.png" width=80% /></p><p>如上图所示，输入是带限时间离散信号 <span class="math inline">\(f(n),\,n=0,1,2,\ldots\)</span>；信号经由分析滤波器 <span class="math inline">\(h_0(n)\)</span> 和 <span class="math inline">\(h_1(n)\)</span> 后下采样被分解成 <span class="math inline">\(f_\text{lp}(n)\)</span> 和 <span class="math inline">\(f_\text{hp}(n)\)</span>，即编码过程；分解的信号经由上采样和综合滤波器 <span class="math inline">\(g_0(n)\)</span> 和 <span class="math inline">\(g_1(n)\)</span> 输出 <span class="math inline">\(\hat f(n)\)</span>，即解码过程。其中 <span class="math inline">\(h_0(n),h_1(n)\)</span> 的理想传递函数 <span class="math inline">\(H_0\)</span> 和 <span class="math inline">\(H_1\)</span> 如上图下方所示。子带编码的目标是选择滤波器 <span class="math inline">\(h_0(n),h_1(n),g_0(n),g_1(n)\)</span> 以使得 <span class="math inline">\(\hat f(n)=f(n)\)</span>，即输出与输入相同，这时这些滤波器称作“完美重构”滤波器。</p><p>相关文献表明，“完美重构”滤波器需要满足以下两个条件：</p><ul><li><p><strong>交叉调制</strong>： <span class="math display">\[\begin{cases}g_0(n)=(-1)^nh_1(n)\\g_1(n)=(-1)^{n+1}h_0(n)\end{cases}\quad\text{or}\quad\begin{cases}g_0(n)=(-1)^{n+1}h_1(n)\\g_1(n)=(-1)^nh_0(n)\end{cases}\]</span></p></li><li><p><strong>双正交性</strong>： <span class="math display">\[\langle h_i(2n-k),g_j(k)\rangle=\delta(i-j)\delta(n),\quad i,j\in\{0,1\}\]</span> 或者展开写作： <span class="math display">\[\begin{align}&amp;\langle g_0(k),h_0(2n-k)\rangle=\delta(n)\\&amp;\langle g_1(k),h_1(2n-k)\rangle=\delta(n)\\&amp;\langle g_0(k),h_1(2n-k)\rangle=0\\&amp;\langle g_1(k),h_0(2n-k)\rangle=0\end{align}\]</span></p></li></ul><p>特别地，如果滤波器满足<strong>单位正交性</strong>： <span class="math display">\[\langle g_i(n),g_j(n+2m)\rangle=\delta(i-j)\delta(m),\quad i,j\in\{0,1\}\]</span> 那么可以证明它们还满足： <span class="math display">\[\begin{align}&amp;g_1(n)=(-1)^ng_0(2k-1-n)\\&amp;h_i(n)=g_i(2k-1-n),\quad i=\{0,1\}\end{align}\]</span></p><h2 id="快速小波变换">快速小波变换</h2><blockquote><p>快速小波变换是离散小波变换的快速算法，主要是发现并利用了相邻尺度离散小波变换的系数之间的关系。</p></blockquote><p>在离散小波变换 <span class="math inline">\(\eqref{dwt}\)</span> 式中，求解每个系数 <span class="math inline">\(W_\varphi(0,0)\)</span> 和 <span class="math inline">\(W_\psi(j,k)\)</span> 的复杂度都是 <span class="math inline">\(O(M)\)</span> 的，一共有 <span class="math inline">\(M\)</span> 个系数要计算，因此总复杂度是 <span class="math inline">\(O(M^2)\)</span> 的，而快速小波变换能将其减少到 <span class="math inline">\(O(M)\)</span>.</p><p>回顾 <span class="math inline">\(\eqref{scaling}\)</span> 式： <span class="math display">\[\varphi(x)=\sum_nh_\varphi(n)\sqrt{2}\varphi(2x-n)\]</span> 作变量代换：<span class="math inline">\(x\to 2^jx-k\)</span>，<span class="math inline">\(n\to m-2k\)</span>，得： <span class="math display">\[\begin{align}\varphi(2^jx-k)&amp;=\sum_n h_\varphi(n)\sqrt{2}\varphi(2(2^jx-k)-n)\\\&amp;=\sum_m h_\varphi(m-2k)\sqrt{2}\varphi(2^{j+1}x-m)\end{align}\]</span> 类似地，<span class="math inline">\(\eqref{wavelet}\)</span> 式也可以改写作： <span class="math display">\[\psi(2^jx-k)=\sum_m h_\psi(m-2k)\sqrt{2}\varphi(2^{j+1}x-m)\]</span> 代入离散小波变换的系数： <span class="math display">\[\begin{align}W_\psi(j,k)&amp;=\frac{1}{\sqrt{M}}\sum_{n=0}^{M-1} f(n)2^{j/2}\psi(2^jn-k)\\&amp;=\frac{1}{\sqrt{M}}\sum_{n=0}^{M-1} f(n)2^{j/2}\left(\sum_m h_\psi(m-2k)\sqrt{2}\varphi(2^{j+1}n-m)\right)\\&amp;=\sum_mh_\psi(m-2k)\left[\frac{1}{\sqrt{M}}\sum_{n=0}^{M-1} f(n)2^{(j+1)/2}\varphi(2^{j+1}n-m)\right]\\&amp;=\sum_mh_\psi(m-2k)W_\varphi(j+1,m)\end{align}\]</span> 这样就建立起了 <span class="math inline">\(W_\psi(j,k)\)</span> 与 <span class="math inline">\(W_\varphi(j+1,m)\)</span> 之间的关系。类似地，<span class="math inline">\(W_\varphi(j,k)\)</span> 与 <span class="math inline">\(W_\varphi(j+1,m)\)</span> 之间也有关系： <span class="math display">\[W_\varphi(j,k)=\sum_mh_\varphi(m-2k)W_\varphi(j+1,m)\]</span> 上述表达式可以看做是只在 <span class="math inline">\(n=2k,\,k\geq0\)</span> 处进行卷积，即： <span class="math display">\[\begin{align}&amp;W_\psi(j,k)=h_\psi(-n)\ast W_\varphi(j+1,n)\Bigg\vert_{n=2k,\,k\geq0}\\&amp;W_\varphi(j,k)=h_\varphi(-n)\ast W_\varphi(j+1,n)\Bigg\vert_{n=2k,\,k\geq0}\end{align}\]</span> 这等价于先正常卷积、再做 2 倍的下采样，如图所示：</p><p><img src="fwt.png" width=50% /></p><p>由于我们要对所有 <span class="math inline">\(j=0,1,\ldots,J-1\)</span> 求 <span class="math inline">\(W_\psi(j,k)\)</span>，所以可以反复将低通分量不断地分解为更低尺度的低通分量与高通分量，注意 <span class="math inline">\(f(n)\)</span> 本身可以视为一个起始低通分量 <span class="math inline">\(W_\varphi(J,n)\)</span>，如图所示：</p><p><img src="fwt2.png" width=80% /></p><p>由于 <span class="math inline">\(h_\psi(-n)\)</span> 与 <span class="math inline">\(h_\varphi(-n)\)</span> 的长度可以视为常数（例如 Haar 尺度和小波函数系数长度为 2），因此每个卷积操作与其序列长度成正比，于是总的时间复杂度为 <span class="math inline">\(O(2^J+2^{J-1}+\cdots+1)=O(2^{J+1})=O(M)\)</span>.</p><p>为了进行<strong>快速小波逆变换</strong>，注意到上述正变换的过程与子带编码的分析过程有着一致的形式——尺度函数系数 <span class="math inline">\(h_\varphi(-n)\)</span> 相当于子带编码中的低通分析滤波器 <span class="math inline">\(h_0(n)\)</span>，小波函数系数 <span class="math inline">\(h_\psi(-n)\)</span> 相当于高通分析滤波器 <span class="math inline">\(h_1(n)\)</span>. 于是，逆变换就是子带编码的综合过程。回忆子带编码中，完美重构要求 <span class="math inline">\(g_0(n)=h_0(-n),\,g_1(n)=h_1(-n)\)</span>，因此，在小波变换的语境下，综合滤波器分别为 <span class="math inline">\(g_0(n)=h_\varphi(n),\,g_1(n)=h_\psi(n)\)</span>，如下图所示：</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="ifwt.png" width=100% /></div><div class="group-image-wrap"><img src="ifwt2.png" width=100% /></div></div></div><p>同样的，不断迭代上述过程，我们最终就能够完美重构 <span class="math inline">\(f(n)=W_\varphi(J,n)\)</span>. 由于形式上的对称性，容易知道逆变换的复杂度也是 <span class="math inline">\(O(M)\)</span>. 因此，整个快速小波变换算法是 <span class="math inline">\(O(M)\)</span> 的，这比快速傅立叶变换的 <span class="math inline">\(O(M\log M)\)</span> 更优。</p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>数学</category>
      
      <category>图像处理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>image processing</tag>
      
      <tag>signal processing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]5.4矩阵的直积</title>
    <link href="/blog-main/2023/12/12/%E7%9F%A9%E9%98%B5%E8%AE%BA-5-4%E7%9F%A9%E9%98%B5%E7%9A%84%E7%9B%B4%E7%A7%AF/"/>
    <url>/blog-main/2023/12/12/%E7%9F%A9%E9%98%B5%E8%AE%BA-5-4%E7%9F%A9%E9%98%B5%E7%9A%84%E7%9B%B4%E7%A7%AF/</url>
    
    <content type="html"><![CDATA[<h3 id="矩阵直积的定义与性质">矩阵直积的定义与性质</h3><p><strong>定义</strong>：设 <span class="math inline">\(A=(a_{ij})\in\mathbb C^{m\times n}\)</span>，<span class="math inline">\(B=(b_{ij})\in\mathbb C^{p\times q}\)</span>，则称 <span class="math inline">\(A\)</span> 与 <span class="math inline">\(B\)</span> 的直积（Kronecker 积）为： <span class="math display">\[A\otimes B=\begin{bmatrix}a_{11}B&amp;a_{12}B&amp;\cdots&amp;a_{1n}B\\a_{21}B&amp;a_{22}B&amp;\cdots&amp;a_{2n}B\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\a_{m1}B&amp;a_{m2}B&amp;\cdots&amp;a_{mn}B\\\end{bmatrix}\in\mathbb C^{mp\times nq}\]</span> <div class="note note-info">            <p>简单说来，就是 <span class="math inline">\(a_{ij}b_{kl}\)</span> 放在 <span class="math inline">\(A\otimes B\)</span> 的 <span class="math inline">\(((i-1)m+k,(j-1)n+l)\)</span> 处。</p>          </div></p><p><strong>性质 1</strong>. <span class="math inline">\(k(A\otimes B)=(kA)\otimes B=A\otimes(kB)\)</span>.</p><p><strong>性质 2</strong>. 若 <span class="math inline">\(A,B\)</span> 同阶，则 <span class="math inline">\((A+B)\otimes C=A\otimes C+B\otimes C\)</span>，<span class="math inline">\(C\otimes(A+B)=C\otimes A+C\otimes B\)</span>.</p><p><strong>性质 3</strong>. <span class="math inline">\((A\otimes B)\otimes C=A\otimes(B\otimes C)\)</span>.</p><div class="note note-secondary">            <p>证明思路：证明 <span class="math inline">\(a_{ij}b_{kl}c_{uv}\)</span> 在同一个位置。</p>          </div><p><strong>性质 4</strong>. <span class="math inline">\((A\otimes B)(C\otimes D)=AC\otimes BD\)</span>.</p><div class="note note-secondary">            <p>证明思路：直接展开。</p>          </div><p><strong>推论 1</strong>. <span class="math inline">\((A_1\otimes\cdots\otimes A_l)(B_1\otimes\cdots\otimes B_l)=A_1B_1\otimes\cdots\otimes A_lB_l\)</span>.</p><p><strong>推论 2</strong>. <span class="math inline">\((A_1\otimes B_1)\cdots(A_l\otimes B_l)=(A_1\cdots A_l)\otimes(B_1\cdots B_l)\)</span>.</p><p><strong>性质 5</strong>. <span class="math inline">\((A\otimes B)^{-1}=A^{-1}\otimes B^{-1}\)</span>.</p><div class="note note-secondary">            <p>证明：<span class="math inline">\((A\otimes B)(A^{-1}\otimes B^{-1})=(AA^{-1})\otimes(BB^{-1})=I\)</span>.</p>          </div><p><strong>性质 6</strong>. 若 <span class="math inline">\(A,B\)</span> 为三角矩阵，则 <span class="math inline">\(A\otimes B\)</span> 也是三角矩阵。</p><p><strong>性质 7</strong>. <span class="math inline">\((A\otimes B)^H=A^H\otimes B^H\)</span>.</p><p><strong>性质 8</strong>. 设 <span class="math inline">\(A_{m\times m}\)</span> 和 <span class="math inline">\(B_{n\times n}\)</span> 都是酉矩阵，则 <span class="math inline">\(A\otimes B\)</span> 也是酉矩阵。</p><p><strong>性质 9</strong>. <span class="math inline">\(\text{rank}(A\otimes B)=\text{rank}(A)\cdot\text{rank}(B)\)</span>.</p><div class="note note-secondary">            <p>证明：对 <span class="math inline">\(A,B\)</span> 作奇异值分解 <span class="math inline">\(A=U_A\Sigma_AV_A^H,\,B=U_B\Sigma_BV_B^H\)</span>，则： <span class="math display">\[A\otimes B=(U_A\otimes U_B)(\Sigma_A\otimes\Sigma_B)(V_A\otimes V_B)^H\]</span> 按定义易知 <span class="math inline">\(\Sigma_A\otimes \Sigma_B\)</span> 有 <span class="math inline">\(\text{rank}(A)\cdot\text{rank}(B)\)</span> 个非零元。证毕。</p>          </div><p><strong>性质 10</strong>. <span class="math inline">\(\text{tr}(A\otimes B)=\text{tr}(A)\cdot\text{tr}(B)\)</span>.</p><div class="note note-secondary">            <p>证明思路：直接展开。</p>          </div><p><strong>性质 11</strong>. 对于<strong>方阵</strong> <span class="math inline">\(A\in\mathbb C^{m\times m},\,B\in\mathbb C^{n\times n}\)</span>，有 <span class="math inline">\(A\otimes B\)</span> 相似于 <span class="math inline">\(B\otimes A\)</span>.</p><h3 id="拉直算子">拉直算子</h3><p><strong>定义</strong>：设 <span class="math inline">\(A=(a_1,a_2,\ldots,a_n)\)</span>，定义<strong>列拉直</strong>为： <span class="math display">\[\text{vec}(A)=\begin{bmatrix}a_1\\a_2\\\vdots\\a_n\end{bmatrix}=\sum_{j=1}^ne_j\otimes (Ae_j)\]</span> 类似地，设 <span class="math inline">\(A=\begin{bmatrix}a_1^T\\a_2^T\\\vdots\\a_m^T\end{bmatrix}\)</span>，定义<strong>行拉直</strong>为： <span class="math display">\[\overline{\text{vec}}(A)=\begin{bmatrix}a_1\\a_2\\\vdots\\a_m\end{bmatrix}=\sum_{i=1}^me_i\otimes(A^Te_i)\]</span> 显然有：<span class="math inline">\(\overline{\text{vec}}(A)=\text{vec}(A^T)\)</span>.</p><p><strong>定理</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n},X\in\mathbb C^{n\times p},B\in\mathbb C^{p\times q}\)</span>，则： <span class="math display">\[\begin{align}&amp;\text{vec}(AXB)=(B^T\otimes A)\text{vec}(X)\\&amp;\overline{\text{vec}}(AXB)=(A\otimes B^T)\overline{\text{vec}}(X)\end{align}\]</span> <div class="note note-secondary">            <p>证明（这里采用代数形式，采用展开形式也能证）： <span class="math display">\[\begin{align}\text{vec}(AXB)&amp;=\sum_{j=1}^qe_j\otimes(AXBe_j)=\sum_{j=1}^qe_j\otimes\left(AX\left(\sum_{k=1}^pe_ke_k^T\right)Be_j\right)\\&amp;=\sum_{j=1}^q\sum_{k=1}^pe_j\otimes (AXe_k{\color{purple}e_k^TBe_j})=\sum_{k=1}^p\sum_{j=1}^q(e_j{\color{purple}e_j^TB^Te_k})\otimes (AXe_k)\\&amp;=\sum_{k=1}^p(B^Te_k)\otimes (AXe_k)=\sum_{k=1}^p(B^T\otimes A)(e_k\otimes Xe_k)=(B^T\otimes A)\text{vec}(X)\end{align}\]</span> 推导过程中注意紫色部分是一个数，可以转置并移动到直积的另一边去。</p><p>行展开类似，或者直接取转置即可。证毕。</p>          </div></p><h3 id="以矩阵直积定义方幂">以矩阵直积定义方幂</h3><p><strong>定义</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，定义 <span class="math inline">\(A^{[1]}=A,\,A^{[k+1]}=A^{[k]}\otimes A,\,k=1,2,\ldots\)</span>.</p><p><strong>性质</strong>：<span class="math inline">\((AB)^{[k]}=A^{[k]}B^{[k]}\)</span>. 利用直积的性质 4 容易证明。</p><h3 id="二元函数-fab-的特征值">二元函数 <span class="math inline">\(f(A,B)\)</span> 的特征值</h3><p>对二元函数 <span class="math inline">\(f(x,y)=\displaystyle\sum_{i,j=0}^lc_{ij}x^iy^j\)</span> 及矩阵 <span class="math inline">\(A\in\mathbb C^{m\times m},\,B\in\mathbb C^{n\times n}\)</span>，定义： <span class="math display">\[f(A,B)=\sum_{i,j=0}^lc_{ij}A^{i}\otimes B^{j}\]</span> <strong>定理</strong>：设 <span class="math inline">\(A_{m\times m}\)</span> 的特征值为 <span class="math inline">\(\lambda_1,\lambda_2,\ldots,\lambda_m\)</span>，<span class="math inline">\(B_{n\times n}\)</span> 的特征值为 <span class="math inline">\(\mu_1,\mu_2,\ldots,\mu_n\)</span>，则 <span class="math inline">\(f(A,B)\)</span> 的全体特征值为 <span class="math inline">\(f(\lambda_i,\mu_j),\,i=1,2,\ldots,m,j=1,2,\ldots,n\)</span>.</p><div class="note note-secondary">            <p>证明：将 <span class="math inline">\(A,B\)</span> 相似上三角化 <span class="math inline">\(A=P_AR_AP_A^{-1},\,B=P_BR_BP_B^{-1}\)</span>，其中 <span class="math inline">\(R_A,R_B\)</span> 为上三角矩阵。那么 <span class="math inline">\(A^i=P_AR_A^iP_A^{-1},\,B=P_BR_B^iP_B^{-1}\)</span>，于是： <span class="math display">\[\begin{align}f(A,B)&amp;=\sum_{i,j=0}^lc_{ij}(P_AR_A^iP_A^{-1})\otimes(P_BR_B^jP_B^{-1})\\&amp;=\sum_{i,j=0}^lc_{ij}(P_A\otimes P_B)(R_A^i\otimes R_B^j)(P_A^{-1}\otimes P_B^{-1})\\&amp;=(P_A\otimes P_B)\left(\sum_{i,j=0}^lc_{ij}R_A^i\otimes R_B^j\right)(P_A\otimes P_B)^{-1}\\\end{align}\]</span> 由于 <span class="math inline">\(R_A^i\otimes R_B^j\)</span> 是上三角矩阵，其对角元 <span class="math inline">\(\{\lambda_r^i\mu_s^j\mid r=1,\ldots,m,s=1,\ldots,n\}\)</span> 就是所有特征值，故 <span class="math inline">\(f(A,B)\)</span> 全体特征值为： <span class="math display">\[\text{diag}\left(\sum_{i,j=0}^{l}c_{ij}R_A^i\otimes R_B^j\right)=\left\{\sum_{i,j=0}^lc_{ij}\lambda_r^i\mu_s^j\mid r,s\right\}=\{f(\lambda_r,\mu_s)\mid r,s\}\]</span></p><p>证毕。</p>          </div><p><strong>推论 1</strong>：<span class="math inline">\(A\otimes B\)</span> 的全体特征值为 <span class="math inline">\(\{\lambda_i\mu_j\mid i=1,\ldots,m,j=1,\ldots,n\}\)</span>.</p><p><strong>推论 2</strong>：<span class="math inline">\(\det(A\otimes B)=(\det A)^n(\det B)^m\)</span>.</p><div class="note note-secondary">            <p>证明：<span class="math inline">\(\det(A\otimes B)=\prod_{i=1}^m\prod_{j=1}^n\lambda_i\mu_j=(\prod_{i=1}^m\lambda_i^n)(\prod_{j=1}^n\mu_j^m)=(\det A)^n(\det B)^m\)</span>.</p>          </div><p><strong>推论 3</strong>：<span class="math inline">\(\text{tr}(A\otimes B)=(\text{tr}A)(\text{tr}B)\)</span>.</p><h3 id="矩阵方程及相关定理">矩阵方程及相关定理</h3><div class="note note-success">            <p>遇到形如 <span class="math inline">\(\displaystyle\sum_{i=1}^lA_iXB_i=F\)</span> 的矩阵方程，其中 <span class="math inline">\(X\)</span> 为未知量，考虑<strong>两边同时行拉直</strong>。</p>          </div><p><strong>定理</strong>： <span class="math display">\[\sum_{i=1}^lA_iXB_i=F\quad\text{有解}\quad\iff\quad\overline{\text{vec}}(F)\in R\left(\sum_{i=1}^lA_i\otimes B_i^T\right)\]</span> <div class="note note-secondary">            <p>证明：两边同时进行行拉直，得： <span class="math display">\[\sum_{i=1}^l(A_i\otimes B_i^T)\overline{\text{vec}}(X)=\overline{\text{vec}}(F)\]</span> 于是结论显然成立。证毕。</p>          </div></p><p><strong>定理</strong>：设 <span class="math inline">\(A_{m\times m}\)</span> 的特征值为 <span class="math inline">\(\lambda_1,\ldots,\lambda_m\)</span>，<span class="math inline">\(B_{n\times n}\)</span> 的特征值为 <span class="math inline">\(\mu_1,\ldots,\mu_n\)</span>，则方程 <span class="math inline">\(AX+XB=F\)</span> 有唯一解的充要条件是 <span class="math display">\[\lambda_i+\mu_j\neq 0,\,(i=1,\ldots,m,\,j=1,\ldots,n)\]</span> <div class="note note-secondary">            <p>证明：两边同时进行行拉直，得： <span class="math display">\[(A\otimes I+I\otimes B^T)\overline{\text{vec}}(X)=\overline{\text{vec}}(F)\]</span> 要使该方程有唯一解，则系数矩阵特征值非零。根据上文的定理，<span class="math inline">\(A\otimes I+I\otimes B^T\)</span> 的特征值为 <span class="math inline">\(\lambda_i+\mu_j\)</span>，因此命题成立。证毕。</p>          </div></p><p><strong>推论</strong>：<span class="math inline">\(AX+XB=O\)</span> 有非零解的充要条件是存在 <span class="math inline">\(i_0,j_0\)</span> 使得 <span class="math inline">\(\lambda_{i_0}+\mu_{j_0}=0\)</span>.</p><p><strong>推论</strong>：<span class="math inline">\(AX-XA=O\)</span> 一定有非零解。</p><p><strong>定理</strong>：设 <span class="math inline">\(A_{m\times m}\)</span> 的特征值为 <span class="math inline">\(\lambda_1,\ldots,\lambda_m\)</span>，<span class="math inline">\(B_{n\times n}\)</span> 的特征值为 <span class="math inline">\(\mu_1,\ldots,\mu_n\)</span>，则方程 <span class="math inline">\(\displaystyle\sum_{k=0}^lA^kXB^k=F\)</span> 有唯一解的充要条件是 <span class="math display">\[1+(\lambda_i\mu_j)+\cdots+(\lambda_i\mu_j)^l\neq0\]</span> <div class="note note-secondary">            <p>证明：两边同时进行行拉直，得： <span class="math display">\[\sum_{k=0}^l\left(A^k\otimes (B^k)^T\right)\overline{\text{vec}}(X)=\overline{\text{vec}}(F)\]</span> 要使该方程有唯一解，则系数矩阵特征值非零。取 <span class="math inline">\(f(x,y)=\displaystyle\sum_{k=0}^lx^ky^k\)</span>，则系数矩阵为 <span class="math inline">\(f(A,B^T)\)</span>，因此特征值为 <span class="math inline">\(f(\lambda_i,\mu_j)=1+(\lambda_i\mu_j)+\cdots+(\lambda_i\mu_j)^l\)</span>. 证毕。</p>          </div></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]5.3对称矩阵特征值的极性</title>
    <link href="/blog-main/2023/12/12/%E7%9F%A9%E9%98%B5%E8%AE%BA-5-3%E5%AF%B9%E7%A7%B0%E7%9F%A9%E9%98%B5%E7%89%B9%E5%BE%81%E5%80%BC%E7%9A%84%E6%9E%81%E6%80%A7/"/>
    <url>/blog-main/2023/12/12/%E7%9F%A9%E9%98%B5%E8%AE%BA-5-3%E5%AF%B9%E7%A7%B0%E7%9F%A9%E9%98%B5%E7%89%B9%E5%BE%81%E5%80%BC%E7%9A%84%E6%9E%81%E6%80%A7/</url>
    
    <content type="html"><![CDATA[<h2 id="实对称矩阵特征值的极性与-rayleigh-商">实对称矩阵特征值的极性与 Rayleigh 商</h2><p>本节中记实对称矩阵 <span class="math inline">\(A\)</span> 的特征值按大小排列为 <span class="math inline">\(\lambda_1\leq\lambda_2\leq\cdots\leq\lambda_n\)</span>，对应<strong>标准正交</strong>特征向量系为 <span class="math inline">\(p_1,p_2,\ldots,p_n\)</span>.</p><div class="note note-success">            <p>将 Rayleigh 商定义在实对称矩阵上是为了保证特征值一定是实数，这样才可以比较。</p>          </div><p><strong>定义</strong>：设 <span class="math inline">\(A\)</span> 是 <span class="math inline">\(n\)</span> 阶实对称矩阵，<span class="math inline">\(x\in\mathbb R^n\)</span>，定义矩阵 <span class="math inline">\(A\)</span> 的 Rayleigh 商为： <span class="math display">\[R(x)=\frac{x^TAx}{x^Tx},\quad x\neq 0\]</span> <strong>定理</strong>：设 <span class="math inline">\(A\)</span> 为实对称矩阵，则： <span class="math display">\[\min_{x\neq 0}R(x)=\lambda_1,\quad\max_{x\neq 0}R(x)=\lambda_n\]</span> <div class="note note-secondary">            <p>证明：任取 <span class="math inline">\(x\neq 0\)</span>，依标准正交特征向量系分解 <span class="math inline">\(x=c_1p_1+\cdots+c_np_n\)</span>，则： <span class="math display">\[x^Tx=c_1^2+\cdots+c_n^2,\quad x^TAx=c_1^2\lambda_1+\cdots+c_n^2\lambda_n\]</span> 故： <span class="math display">\[R(x)=k_1\lambda_1+\cdots+k_n\lambda_n,\quad\text{where}\; k_i=\frac{c_i^2}{c_1^2+\cdots+c_n^2}\]</span> 容易推出 <span class="math inline">\(\lambda_1\leq R(x)\leq\lambda_n\)</span>，且 <span class="math inline">\(R(p_1)=\lambda_1,\,R(p_n)=\lambda_n\)</span>. 证毕。</p>          </div></p><div class="note note-secondary">            <p>证明 2（梯度法）：取对数 <span class="math inline">\(\ln R(x)=\ln (x^TAx)-\ln(x^Tx)\)</span>，求导并令为零： <span class="math display">\[\frac{\mathrm d\ln R(x)}{\mathrm dx}=\frac{2Ax}{x^TAx}-\frac{2x}{x^Tx}=0\implies Ax=R(x)x\]</span> 因此极值点处 <span class="math inline">\(R(x)\)</span> 为 <span class="math inline">\(A\)</span> 的特征值，<span class="math inline">\(x\)</span> 为对应特征向量。于是最小值为最小特征值 <span class="math inline">\(\lambda_1\)</span>，最大值为最大特征值 <span class="math inline">\(\lambda_n\)</span>. 证毕。</p><p>（或者转换成约束 <span class="math inline">\(x^Tx=1\)</span> 下的优化问题，引入拉格朗日乘子求解）</p>          </div><div class="note note-success">            <p>由于 <span class="math inline">\(x\)</span> 模长不影响 <span class="math inline">\(R(x)\)</span>，所以上述定理也常常写作： <span class="math display">\[\min_{\Vert x\Vert_2=1}x^TAx=\lambda_1,\quad\max_{\Vert x\Vert_2=1}x^TAx=\lambda_n\]</span> 下文视情况两种写法都可能出现。</p>          </div><p><strong>定理</strong>：设 <span class="math inline">\(A\)</span> 为实对称矩阵，则对于任意的 <span class="math inline">\(1\leq r\leq n\)</span> 有： <span class="math display">\[\min_{X^TX=I_r}\text{tr}(X^TAX)=\lambda_1+\cdots+\lambda_r\]</span> <div class="note note-info">            <p>将 Rayleigh 商中的向量扩展为矩阵。</p>          </div></p><div class="note note-secondary">            <p>证明：对 <span class="math inline">\(A\)</span> 作谱分解 <span class="math inline">\(A=P\Lambda P^T\)</span>，其中 <span class="math inline">\(P\)</span> 为正交矩阵，<span class="math inline">\(\Lambda=\text{diag}(\lambda_1,\ldots,\lambda_n)\)</span>. 记 <span class="math inline">\(B=P^TX\)</span>，则问题转化为对角矩阵情形，即求证： <span class="math display">\[\min_{B^TB=I_r}\text{tr}(B^T\Lambda B)=\lambda_1+\cdots+\lambda_r\]</span> 记 <span class="math inline">\(BB^T\)</span> 的对角元素为 <span class="math inline">\(B_{11},\ldots,B_{nn}\)</span>，则可以证明在 <span class="math inline">\(B^TB=I\)</span> 的条件下，<span class="math inline">\(0\leq B_{ii}\leq 1\)</span>. 因此： <span class="math display">\[\text{tr}(B^T\Lambda B)=\text{tr}(\Lambda BB^T)=\lambda_1B_{11}+\cdots+\lambda_n B_{nn}\]</span> 所以这是一个关于 <span class="math inline">\(B_{11},\ldots,B_{nn}\)</span> 的线性约束下的线性问题，最小值必然在约束区域的顶点处取得。显然这个顶点应该是 <span class="math inline">\(B_{11}=\cdots=B_{rr}=1\)</span>，<span class="math inline">\(B_{r+1,r+1}=\cdots=B_{nn}=0\)</span>，此时取到最小值 <span class="math inline">\(\lambda_1+\cdots+\lambda_r\)</span>. 证毕。</p>          </div><p><strong>定理</strong>：设 <span class="math inline">\(x\in L(p_r,p_{r+1},\ldots,p_s),\,1\leq r\leq s\leq n\)</span>，则： <span class="math display">\[\min_{x\neq 0}R(x)=\lambda_r,\quad\max_{x\neq 0}R(x)=\lambda_s\]</span> <div class="note note-secondary">            <p>证明与前面的定理证明类似。</p>          </div></p><div class="note note-info">            <p>这个定理表明 Rayleigh 商在特征向量张成的子空间中的极值就是对应最大最小特征值。但是在实际应用中，特征向量张成的子空间并不好找，所以下面的 Courant-Fischer 定理只限制子空间维数为 <span class="math inline">\(k\)</span>，再对所有 <span class="math inline">\(k\)</span> 维子空间求极值。</p>          </div><p><strong>定理（Courant-Fischer）</strong>：设 <span class="math inline">\(V_k\)</span> 表示 <span class="math inline">\(\mathbb R^n\)</span> 中的任意一个 <span class="math inline">\(k\)</span> 维子空间，则： <span class="math display">\[\lambda_k=\min_{V_k}\max_{x\in V_k,x\neq0}R(x)\]</span> 或写作： <span class="math display">\[\lambda_k=\min_{V_k}\max_{x\in V_k,\Vert x\Vert_2=1}x^TAx\]</span> <div class="note note-secondary">            <p>证明：取 <span class="math inline">\(V_k=L(p_1,\ldots,p_k)\)</span>，根据前一个定理有： <span class="math display">\[\max_{x\neq 0}R(x)=\lambda_k\]</span> 因此下面只需要证明对所有 <span class="math inline">\(V_k\)</span>，<span class="math inline">\(\max\limits_{x\neq 0}R(x)\geq \lambda_k\)</span>.</p><p>构造 <span class="math inline">\(R^n\)</span> 的 <span class="math inline">\(n-k+1\)</span> 维子空间 <span class="math inline">\(W_k=L(p_k,p_{k+1},\ldots,p_n)\)</span>，则任一 <span class="math inline">\(V_k\)</span> 与 <span class="math inline">\(W_k\)</span> 必有交集。取非零向量 <span class="math inline">\(x\in V_k\cap W_k\)</span>，设 <span class="math inline">\(x=c_kp_k+\cdots+c_np_n\)</span>，则： <span class="math display">\[R(x)=\frac{c_k^2\lambda_k+\cdots+c_n^2\lambda_n}{c_k^2+\cdots c_n^2}\geq \lambda_k\]</span> 证毕。</p>          </div></p><div class="note note-info">            <p>将 <span class="math inline">\(A\)</span> 替换成 <span class="math inline">\(-A\)</span>，能立刻得到如下定理： <span class="math display">\[\lambda_{n-k+1}=\max_{V_k}\min_{x\in V_k,x\neq0}R(x)\]</span></p>          </div><p><strong>定理</strong>：设实对称矩阵 <span class="math inline">\(A\)</span> 和 <span class="math inline">\(A+Q\)</span> 的特征值分别为 <span class="math inline">\(\lambda_1\leq\cdots\leq\lambda_n\)</span> 和 <span class="math inline">\(\mu_1\leq\cdots\leq\mu_n\)</span>，则： <span class="math display">\[|\lambda_i-\mu_i|\leq \Vert Q\Vert_2\]</span> <div class="note note-secondary">            <p>证明：利用 Courant-Fischer 定理， <span class="math display">\[\begin{align}\mu_i+\Vert Q\Vert_2&amp;=\min_{V_i}\max_{x\in V_i,\Vert x\Vert_2=1}x^T(A+Q)x+\Vert Q\Vert_2I\\&amp;=\min_{V_i}\max_{x\in V_i,\Vert x\Vert_2=1}x^T(A+Q+\Vert Q\Vert_2I)x\\&amp;\geq\min_{V_i}\max_{x\in V_i,\Vert x\Vert_2=1}x^TAx\\&amp;=\lambda_i\end{align}\]</span> 不等式是因为 <span class="math inline">\(Q+\Vert Q\Vert_2I\)</span> 为半正定矩阵。另一个方向可以进行类似的证明（用 <span class="math inline">\(Q-\Vert Q\Vert_2I\)</span> 为半负定矩阵）。</p>          </div></p><div class="note note-secondary">            <p>为什么 <span class="math inline">\(Q+\Vert Q\Vert_2I\)</span> 是半正定矩阵？对 <span class="math inline">\(Q\)</span> 进行谱分解 <span class="math inline">\(Q=P\Lambda P^T\)</span>，则： <span class="math display">\[Q+\Vert Q\Vert_2I=P\Lambda P^T+\Vert\Lambda\Vert_2I=P(\Lambda +\Vert\Lambda\Vert_2I)P^T\]</span> 因此只需要证明对角矩阵 <span class="math inline">\(\Lambda +\Vert\Lambda\Vert_2I\)</span> 是半正定矩阵。由于 <span class="math inline">\(\Vert\Lambda\Vert_2=\rho(\Lambda)\)</span>，因此其对角元素一定非负，这样就完成了证明。</p>          </div><h2 id="广义特征值的极性与广义-rayleigh-商">广义特征值的极性与广义 Rayleigh 商</h2><p><strong>定义</strong>：设 <span class="math inline">\(A,B\)</span> 为 <span class="math inline">\(n\)</span> 阶实对称矩阵，<strong>且 <span class="math inline">\(B\)</span> 正定</strong>，定义矩阵 <span class="math inline">\(A\)</span> 相对于矩阵 <span class="math inline">\(B\)</span> 的广义 Rayleigh 商为： <span class="math display">\[R(x)=\frac{x^TAx}{x^TBx},\quad x\neq 0\]</span> <strong>定理</strong>：非零向量 <span class="math inline">\(x_0\)</span> 是 <span class="math inline">\(R(x)\)</span> 的驻点的充要条件是 <span class="math inline">\(x_0\)</span> 为 <span class="math inline">\(Ax=\lambda Bx\)</span> 的属于特征值 <span class="math inline">\(\lambda\)</span> 的特征向量。</p><p><strong>推论</strong>：若 <span class="math inline">\(x_0\)</span> 是 <span class="math inline">\(A\)</span> 相对于 <span class="math inline">\(B\)</span> 的特征向量，则 <span class="math inline">\(R(x_0)\)</span> 是与之对应的特征值。</p><p><strong>定理（Courant-Fischer 定理在广义特征值上的扩展）</strong>：广义特征值问题有如下极小极大性质： <span class="math display">\[\begin{align}\lambda_k&amp;=\min_{V_k}\max_{x\in V_k,x\neq 0}R(x)\\\lambda_{n-k+1}&amp;=\max_{V_k}\min_{x\in V_k,x\neq0}R(x)\end{align}\]</span></p><h2 id="矩阵奇异值的极性">矩阵奇异值的极性</h2><p><strong>定理</strong>：设 <span class="math inline">\(A\in\mathbb R^{m\times n}_r\)</span> 的奇异值为： <span class="math display">\[0=\sigma_1=\sigma_2=\cdots=\sigma_{n-r}&lt;\sigma_{n-r+1}\leq\cdots\leq\sigma_n\]</span> <span class="math inline">\(A+Q\in\mathbb R^{m\times n}_{r&#39;}\)</span> 的奇异值为： <span class="math display">\[0=\tau_1=\tau_2=\cdots=\tau_{n-r&#39;}&lt;\tau_{n-r&#39;+1}\leq\cdots\leq\tau_n\]</span> 则有： <span class="math display">\[|\sigma_i-\tau_i|\leq \Vert Q\Vert_2\quad(i=1,2,\ldots,n)\]</span> <div class="note note-info">            <p>对比第一节最后一个定理。</p>          </div></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]5.2广义特征值问题</title>
    <link href="/blog-main/2023/12/11/%E7%9F%A9%E9%98%B5%E8%AE%BA-5-2%E5%B9%BF%E4%B9%89%E7%89%B9%E5%BE%81%E5%80%BC%E9%97%AE%E9%A2%98/"/>
    <url>/blog-main/2023/12/11/%E7%9F%A9%E9%98%B5%E8%AE%BA-5-2%E5%B9%BF%E4%B9%89%E7%89%B9%E5%BE%81%E5%80%BC%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<p><strong>广义特征值定义</strong>：设 <span class="math inline">\(A,B\)</span> 为 <span class="math inline">\(n\)</span> 阶方阵，若存在数 <span class="math inline">\(\lambda\)</span>，使得方程 <span class="math inline">\(Ax=\lambda Bx\)</span> 存在非零解，则称 <span class="math inline">\(\lambda\)</span> 为 <span class="math inline">\(A\)</span> 相对于 <span class="math inline">\(B\)</span> 的广义特征值，<span class="math inline">\(x\)</span> 为 <span class="math inline">\(A\)</span> 相对于 <span class="math inline">\(B\)</span> 的属于广义特征值 <span class="math inline">\(\lambda\)</span> 的特征向量。</p><p><strong>广义特征值求解</strong>：解 <span class="math inline">\((\lambda B-A)x=0\iff\det(\lambda B-A)=0\)</span>. 注意这里的特征多项式 <span class="math inline">\(\det(\lambda B-A)\)</span> 不一定是 <span class="math inline">\(n\)</span> 阶多项式，从而不一定有 <span class="math inline">\(n\)</span> 个根。</p><hr /><p><strong>以下仅考虑 <span class="math inline">\(A,B\)</span> 是正定 Hermite 矩阵的情形</strong>。</p><p><strong>按 <span class="math inline">\(B\)</span> 标准正交化</strong>：若向量系 <span class="math inline">\((x_1,\ldots,x_n)\)</span> 满足： <span class="math display">\[(x_i,Bx_j)=\delta_{ij}\]</span> 称该向量系为按 <span class="math inline">\(B\)</span> 标准正交化的向量系。</p><p><strong>广义特征值的等价表述 1</strong>：由于 <span class="math inline">\(B\)</span> 正定，因此可逆，故： <span class="math display">\[Ax=\lambda Bx\implies B^{-1}Ax=\lambda x\]</span> 转化为了标准特征值问题。但一般而言 <span class="math inline">\(B^{-1}A\)</span> 不再是 Hermite 矩阵。</p><p><strong>广义特征值的等价表述 2</strong>：由于 <span class="math inline">\(B\)</span> 为 Hermite 矩阵，因此存在 Cholesky 分解，<span class="math inline">\(B=GG^H\)</span>，其中 <span class="math inline">\(G\)</span> 满秩，于是： <span class="math display">\[Ax=\lambda Bx\implies Ax=\lambda GG^Hx\]</span> 令 <span class="math inline">\(y=G^Hx\)</span>，则： <span class="math display">\[G^{-1}A(G^H)^{-1}y=\lambda y\]</span> 转化为了标准特征值问题，且 <span class="math inline">\(G^{-1}A(G^H)^{-1}\)</span> 仍然是 Hermite 矩阵，因此广义特征值为实数，设为 <span class="math inline">\(\lambda_1\leq\cdots\leq\lambda_n\)</span>，且存在一组正交归一化的特征向量 <span class="math inline">\((y_1,\ldots,y_n)\)</span>，即： <span class="math display">\[\begin{align}&amp;G^{-1}A(G^H)^{-1}y_i=\lambda y_i\\&amp;y_i^Hy_j=\delta_{ij}\end{align}\]</span> 将 <span class="math inline">\(y\)</span> 还原为 <span class="math inline">\(x\)</span>，则： <span class="math display">\[y_i^Hy_j=(G^Hx_i)^H(G^Hx_j)=x_i^HGG^Hx_j=x_i^HBx_j=\delta_{ij}\]</span> 即 <span class="math inline">\((x_1,\ldots,x_n)\)</span> 按 <span class="math inline">\(B\)</span> 带权正交。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]5.1特征值的估计</title>
    <link href="/blog-main/2023/12/05/%E7%9F%A9%E9%98%B5%E8%AE%BA-5-1%E7%89%B9%E5%BE%81%E5%80%BC%E7%9A%84%E4%BC%B0%E8%AE%A1/"/>
    <url>/blog-main/2023/12/05/%E7%9F%A9%E9%98%B5%E8%AE%BA-5-1%E7%89%B9%E5%BE%81%E5%80%BC%E7%9A%84%E4%BC%B0%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<h2 id="特征值的界">特征值的界</h2><h3 id="特征值的上界">特征值的上界</h3><p><strong>定理</strong>：设 <span class="math inline">\(A\in \mathbb R^{n\times n}\)</span>，令： <span class="math display">\[M=\max_{1\leq r,s\leq n}\frac{1}{2}|a_{rs}-a_{sr}|\]</span> 若 <span class="math inline">\(\lambda\)</span> 表示 <span class="math inline">\(A\)</span> 的任一特征值，则 <span class="math inline">\(\lambda\)</span> 的虚部 <span class="math inline">\(\text{Im}(\lambda)\)</span> 满足不等式： <span class="math display">\[\begin{align}&amp;|\text{Im}(\lambda)|\leq M\sqrt{\frac{n(n-1)}{2}}\\&amp;|\text{Im}(\lambda)|\leq\frac{\Vert A-A^T\Vert_2}{2}\\&amp;|\text{Im}(\lambda)|\leq\frac{\Vert A-A^T\Vert_1\cdot\sqrt{n}}{2}\end{align}\]</span> <div class="note note-success">            <p>基本思想：任何一个实方阵都可以拆分为实对称阵和实反对称阵的和： <span class="math display">\[A=\frac{1}{2}(A+A^T)+\frac{1}{2}(A-A^T)\]</span> 我们知道实对称阵的特征值一定是实数，而实反对称阵的特征值一定是 0 或虚数。因此，可以认为上面的两部分分别决定了特征值的实部和虚部，这就是为什么我们关注 <span class="math inline">\(M=\max\limits_{1\leq r,s\leq n}\frac{1}{2}|a_{rs}-a_{sr}|\)</span>.</p>          </div></p><div class="note note-secondary">            <p>证明：暂略。</p>          </div><h3 id="行列式的界">行列式的界</h3><p><strong>行对角占优</strong>：设 <span class="math inline">\(A\in\mathbb C^{n\times n}\)</span>，令： <span class="math display">\[R_r(A)=\sum_{s=1,s\neq r}^n|a_{rs}|,\quad r=1,\ldots,n\]</span> 若 <span class="math inline">\(|a_{rr}|&gt;R_{r}(A),\ \forall\ r=1,\ldots,n\)</span>，则称矩阵 <span class="math inline">\(A\)</span> 按行对角占优。</p><p><strong>定理</strong>：行对角占优阵一定可逆。</p><div class="note note-secondary">            <p>证明：假设 <span class="math inline">\(A\)</span> 不可逆，则其列线性相关，即存在不全为零的 <span class="math inline">\(k_1,\ldots,k_n\)</span> 使得 <span class="math inline">\(k_1a_1+\cdots+k_na_n=0\)</span>. 设 <span class="math inline">\(k_r\)</span> 是其中模长最大的，考虑第 <span class="math inline">\(r\)</span> 个分量： <span class="math display">\[k_1a_{r1}+\cdots+k_na_{rn}=0\implies a_{rr}=-\sum_{s=1,s\neq r}^n\frac{k_s}{k_r}a_{rs}\]</span> 于是： <span class="math display">\[|a_{rr}|\leq \sum_{s=1,s\neq r}^n\frac{|k_s|}{|k_r|}|a_{rs}|\leq \sum_{s=1,s\neq r}^n|a_{rs}|\]</span> 与 <span class="math inline">\(A\)</span> 行对角占优矛盾，故假设不成立。证毕。</p>          </div><p><strong>定理（行对角占优阵的行列式的上下界）</strong>：设 <span class="math inline">\(A\in\mathbb C^{n\times n}\)</span> 且按行对角占优，令： <span class="math display">\[M_r=|a_{rr}|+\sum_{s=r+1}^n|a_{rs}|,\quad m_r=|a_{rr}|-\sum_{s=r+1}^n|a_{rs}|,\quad r=1,\ldots,n\]</span> 则： <span class="math display">\[0&lt;\prod_{r=1}^n m_r\leq |\det A|=\prod_{r=1}^n|\lambda_r(A)|\leq\prod_{r=1}^n M_r\]</span> 当 <span class="math inline">\(a_{rs}=0\ (s&gt;r)\)</span> 时等号成立。</p><div class="note note-secondary">            <p>证明：对 <span class="math inline">\(A\)</span> 分块： <span class="math display">\[A=\left[\begin{array}{c:ccc}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\\hdashline a_{21}&amp;&amp;&amp;\\\vdots&amp;&amp;A_1&amp;\\a_{n1}&amp;&amp;&amp;\\\end{array}\right]\]</span> 设 <span class="math inline">\(x=(\xi_2,\ldots,\xi_n)^T\)</span> 且满足： <span class="math display">\[\begin{bmatrix}a_{21}\\\vdots\\a_{n1}\end{bmatrix}+A_1\begin{bmatrix}\xi_2\\\vdots\\\xi_n\end{bmatrix}=0\]</span> 则有 <span class="math inline">\(|\xi_k|=\max\{|\xi_2|,\ldots,|\xi_n|\}&lt;1\)</span>，证明方式与上面证明行对角占优矩阵一定可逆的方式类似。</p><p>于是，利用行列式的性质，有： <span class="math display">\[\det(A)=\det\left(A\begin{bmatrix}1&amp;0\\x&amp;I_{n-1}\end{bmatrix}\right)=\det\left(\left[\begin{array}{c:ccc}b_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\\hdashline&amp;&amp;&amp;\\0&amp;&amp;A_1&amp;\\&amp;&amp;&amp;\end{array}\right]\right)=|b_{11}|\cdot\det(A_1)\]</span> 其中 <span class="math display">\[b_{11}=a_{11}+\sum_{s=2}^na_{1s}\xi_s,\quad|\xi_s|&lt;1\]</span> 因此 <span class="math inline">\(m_1\leq|b_{11}|\leq M_1\)</span>. 然后递归地对 <span class="math inline">\(A_1\)</span> 做类似推导即可。</p>          </div><p><strong>定理（Hadamard 不等式）</strong>：设 <span class="math inline">\(A\in\mathbb C^{n\times n}\)</span>，则有： <span class="math display">\[|\det(A)|=\prod_{r=1}^n|\lambda_r(A)|\leq\left[\prod_{s=1}^n\left(\sum_{r=1}^n|a_{rs}|^2\right)\right]^{1/2}\]</span> <div class="note note-info">            <p>直观解释：平行多面体的体积小于等于将其拉成正立方体的体积。</p>          </div></p><div class="note note-secondary">            <p>证明：若 <span class="math inline">\(a_1,\ldots,a_n\)</span> 线性无关，则显然成立；否则，进行 Gram-Schmidt 正交化过程，写作矩阵形式： <span class="math display">\[(a_1,a_2,\ldots,a_n)=(b_1,b_2,\ldots,b_n)\begin{bmatrix}1&amp;k_{21}&amp;\cdots&amp;k_{n1}\\&amp;1&amp;\cdots&amp;k_{n2}\\&amp;&amp;\ddots&amp;\vdots\\&amp;&amp;&amp;1\end{bmatrix}\]</span> 易知 <span class="math inline">\(\Vert a_i\Vert^2\geq\Vert b_i\Vert^2\)</span>，于是： <span class="math display">\[|\det(A)|^2=|\det(B)|^2=\det(B^HB)=\Vert b_1\Vert^2\cdots\Vert b_n\Vert^2\leq\Vert a_1\Vert^2\cdots\Vert a_n\Vert^2\]</span> 证毕。</p>          </div><h3 id="特征值模长之和的界">特征值模长之和的界</h3><p><strong>定理（Schur 不等式）</strong>：设 <span class="math inline">\(A\in\mathbb C^{n\times n}\)</span> 的特征值为 <span class="math inline">\(\lambda_1,\ldots,\lambda_n\)</span>，则： <span class="math display">\[\sum_{r=1}^n|\lambda_r|^2\leq\sum_{r=1}^n\sum_{s=1}^n|a_{rs}|^2=\Vert A\Vert_F^2\]</span> <div class="note note-secondary">            <p>证明：根据 Schur 定理，存在酉矩阵 <span class="math inline">\(U\)</span> 使得 <span class="math inline">\(A=UTU^H\)</span>，其中 <span class="math inline">\(T\)</span> 为上三角矩阵，对角元素为 <span class="math inline">\(A\)</span> 的特征值，且： <span class="math display">\[\sum_{r=1}^n|\lambda_r|^2=\sum_{k=1}^n|t_{kk}|^2\leq\sum_{r=1}^n\sum_{s=1}^n|t_{rs}|^2=\text{tr}(T^HT)=\text{tr}(A^HA)\]</span> 证毕。</p>          </div></p><h2 id="特征值的包含区域圆盘定理">特征值的包含区域（圆盘定理）</h2><p><strong>行盖尔圆</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，记第 <span class="math inline">\(i\)</span> 个行盖尔圆为： <span class="math display">\[S_i=\{z\mid |z-a_{ii}|\leq R_i,\,z\in\mathbb C\}\]</span> 其中 <span class="math inline">\(R_i=R_i(A)=\sum_{j\neq i}|a_{ij}|\)</span>.</p><p><strong>圆盘定理 1</strong>：<span class="math inline">\(A\)</span> 的所有特征值在其 <span class="math inline">\(n\)</span> 个盖尔圆的并集之内，即： <span class="math display">\[\lambda\in S=\bigcup_{i=1}^nS_i=\bigcup_{i=1}^n\{z\mid |z-a_{ii}|\leq R_i,\,z\in\mathbb C\}\]</span> <div class="note note-secondary">            <p>证明：设 <span class="math inline">\(Ax=\lambda x\)</span>，写作分量形式： <span class="math display">\[\sum_{j=1}^na_{ij}x_j=\lambda x_i\implies (\lambda-a_{ii})x_i=\sum_{j\neq i}a_{ij}x_j\]</span> 设 <span class="math inline">\(x_t\)</span> 为 <span class="math inline">\(x\)</span> 各分量中模最大的那个，则 <span class="math inline">\(x_t\neq 0\)</span>，上式取 <span class="math inline">\(i=t\)</span> 后两边同时除以 <span class="math inline">\(x_t\)</span> 并取模得： <span class="math display">\[|\lambda-a_{tt}|=\sum_{j\neq t}|a_{tj}|\left|\frac{x_j}{x_t}\right|\leq\sum_{j\neq t}|a_{tj}|=R_t(A)\]</span> 因此 <span class="math inline">\(\lambda\in S_t\)</span>. 于是任意特征值 <span class="math inline">\(\lambda\in\bigcup_{i=1}^n S_i\)</span>. 证毕。</p>          </div></p><div class="note note-info">            <p>圆盘定理 1 说明，特征值 <span class="math inline">\(\lambda\)</span> 属于特征向量最大分量的下标对应的盖尔圆。</p>          </div><p><strong>圆盘定理 2</strong>：设矩阵 <span class="math inline">\(A\)</span> 的 <span class="math inline">\(n\)</span> 个盖尔圆有 <span class="math inline">\(k\)</span> 个互相连通并且与其余 <span class="math inline">\(n-k\)</span> 个不相交，则这个连通区域中恰有 <span class="math inline">\(k\)</span> 个特征值。</p><div class="note note-success">            <p>说明：设 <span class="math inline">\(A=D+B\)</span>，其中 <span class="math inline">\(D\)</span> 为对角矩阵。设 <span class="math inline">\(A(t)=D+tB\)</span>，则当 <span class="math inline">\(t=0\)</span> 时，所有盖尔圆都是点，也就是特征值；当 <span class="math inline">\(t\)</span> 从 <span class="math inline">\(0\)</span> 到 <span class="math inline">\(1\)</span> 连续变化时，盖尔圆越来越大，而特征值也是<strong>连续</strong>变化的，所以 <span class="math inline">\(t=1\)</span> 时特征值只能在其连通区域内，不可能跳跃到不连通的另一个区域中。从这个角度也可以知道，当两个盖尔圆相切时，其特征值最多到达切点处。</p>          </div><p><strong>推论</strong>：若 <span class="math inline">\(n\)</span> 阶矩阵 <span class="math inline">\(A\)</span> 的 <span class="math inline">\(n\)</span> 个盖尔圆两两互不相交（都是孤立的），则 <span class="math inline">\(A\)</span> 相似于对角矩阵。</p><p><strong>推论</strong>：设 <span class="math inline">\(n\)</span> 阶实矩阵 <span class="math inline">\(A\)</span> 的 <span class="math inline">\(n\)</span> 个盖尔圆两两互不相交，则 <span class="math inline">\(A\)</span> 的特征值全为实数。</p><div class="note note-secondary">            <p>证明：由于 <span class="math inline">\(A\)</span> 为实矩阵，故所有盖尔圆的圆心都在实轴上。若 <span class="math inline">\(A\)</span> 有复特征值，则必有与之关于实轴对称的共轭复对称值，与其在同一个盖尔圆中。由于盖尔圆两两互不相交，且一个孤立盖尔圆内只能有一个特征值，所以 <span class="math inline">\(A\)</span> 的特征值只能都是实数。证毕。</p>          </div><p><strong>列盖尔圆</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，记第 <span class="math inline">\(j\)</span> 个列盖尔圆为： <span class="math display">\[G_j=\{z\mid |z-a_{jj}|\leq R&#39;_j,\,z\in\mathbb C\}\]</span> 其中 <span class="math inline">\(R&#39;_j=R&#39;_j(A)=\sum_{i\neq j}|a_{ij}|\)</span>.</p><p>列盖尔圆有与行盖尔圆类似的圆盘定理。结合二者可知，<span class="math inline">\(A\)</span> 的所有特征值都在以下平面区域之中： <span class="math display">\[T=\left(\bigcup_{i=1}^nS_i\right)\cap\left(\bigcup_{j=1}^nG_j\right)\]</span> <strong>定理</strong>：相似矩阵的盖尔圆的圆心相同、半径不同。</p><div class="note note-secondary">            <p>证明：由于 <span class="math inline">\(B=P^{-1}AP\)</span> 的第 <span class="math inline">\(i\)</span> 行第 <span class="math inline">\(j\)</span> 列元素为 <span class="math inline">\(a_{ij}p_i/p_j\)</span>，所以对于 <span class="math inline">\(B\)</span>： <span class="math display">\[R_i=\sum_{j\neq i}\left\vert a_{ij}\cdot\frac{p_i}{p_j}\right\vert\]</span> 而圆心依旧是 <span class="math inline">\(a_{ii}\cdot p_i/p_i=a_{ii}\)</span>.</p>          </div><div class="note note-info">            <p>这个定理可以用于隔离特征值：取 <span class="math inline">\(P=\text{diag}(p_1,\ldots,p_n),\,p_i&gt;0\)</span>，若想要缩小半径，则将对应元素设置得大一些，这样原本相交的盖尔圆就能被隔离开了。</p>          </div><h2 id="矩阵谱半径的估计">矩阵谱半径的估计</h2><p><strong>定理</strong>：设 <span class="math inline">\(A\)</span> 为 <span class="math inline">\(n\)</span> 阶正规矩阵，则： <span class="math display">\[\rho(A)=\Vert A\Vert_2=\sqrt{\lambda_{A^HA}}\]</span> 其中 <span class="math inline">\(\lambda_{A^HA}\)</span> 为 <span class="math inline">\(A^HA\)</span> 的最大特征值。</p><div class="note note-info">            <p>换句话说，正规矩阵的最大奇异值等于最大特征值的模。</p>          </div><div class="note note-secondary">            <p>证明：设 <span class="math inline">\(A\)</span> 的特征值为 <span class="math inline">\(\lambda_1,\ldots,\lambda_n\)</span>，存在酉矩阵 <span class="math inline">\(U\)</span> 使得： <span class="math display">\[U^HAU=\text{diag}(\lambda_1,\ldots,\lambda_n)\]</span> 于是： <span class="math display">\[U^HA^HU=\text{diag}(\bar\lambda_1,\ldots,\bar\lambda_n)\]</span> 因此： <span class="math display">\[U^HA^HAU=\text{diag}(|\lambda_1|^2,\ldots,|\lambda_n|^2)\]</span> 故 <span class="math inline">\(A^HA\)</span> 的特征值为 <span class="math inline">\(|\lambda_1|^2,\ldots,|\lambda_n|^2\)</span>，记 <span class="math inline">\(\lambda_{A^HA}=\max_{1\leq i\leq n}|\lambda_i|^2\)</span>，则： <span class="math display">\[\Vert A\Vert_2=\sqrt{\lambda_{A^HA}}=\max_{1\leq i\leq n}|\lambda_i|=\rho(A)\]</span> 证毕。</p>          </div><div class="note note-success">            <p>由于实对称矩阵、实反对称矩阵、正交矩阵、酉矩阵、Hermite 矩阵、反 Hermite 矩阵、对角矩阵都是正规矩阵，所以它们都有性质 <span class="math inline">\(\rho(A)=\Vert A\Vert_2\)</span>.</p>          </div>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]4.4矩阵的奇异值分解</title>
    <link href="/blog-main/2023/12/03/%E7%9F%A9%E9%98%B5%E8%AE%BA-4-4%E7%9F%A9%E9%98%B5%E7%9A%84%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/"/>
    <url>/blog-main/2023/12/03/%E7%9F%A9%E9%98%B5%E8%AE%BA-4-4%E7%9F%A9%E9%98%B5%E7%9A%84%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="预备知识">预备知识</h2><p><strong>定理</strong>：对任意 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，<span class="math inline">\(A^HA\)</span> 是 Hermite 半正定矩阵。</p><div class="note note-secondary">            <p>证明：<span class="math inline">\(\forall x\neq 0\)</span>，<span class="math inline">\(x^HA^HAx=(Ax)^H(Ax)=\Vert Ax\Vert^2\geq 0\)</span>. 证毕。</p>          </div><p><strong>定理</strong>：齐次方程组 <span class="math inline">\(Ax=0\)</span> 与 <span class="math inline">\(A^HAx=0\)</span> 同解。</p><div class="note note-secondary">            <p>证明：若 <span class="math inline">\(Ax=0\)</span>，则显然 <span class="math inline">\(A^HAx=0\)</span>；另一方面， <span class="math display">\[A^HAx=0\implies x^HA^HAx=0\implies\Vert Ax\Vert=0\implies Ax=0\]</span> 证毕。</p>          </div><p><strong>定理</strong>：<span class="math inline">\(\text{rank}(A)=\text{rank}(A^HA)\)</span>.</p><div class="note note-secondary">            <p>证明：根据上一个定理，<span class="math inline">\(A\)</span> 与 <span class="math inline">\(A^HA\)</span> 的零空间相同，因此维数相同，即 <span class="math inline">\(n-\text{rank}(A)=n-\text{rank}(A^HA)\)</span>，故 <span class="math inline">\(\text{rank}(A)=\text{rank}(A^HA)\)</span>. 证毕。</p>          </div><p><strong>定理</strong>：<span class="math inline">\(A=O_{m\times n}\iff A^HA=O_{m\times n}\)</span>.</p><div class="note note-secondary">            <p>证明：必要性显然；充分性：根据上一个定理，<span class="math inline">\(\text{rank}(A)=\text{rank}(A^HA)=0\)</span>，所以 <span class="math inline">\(A=O\)</span>. 证毕。</p>          </div><h2 id="hermite-矩阵的谱分解">Hermite 矩阵的谱分解</h2><p>Hermite 矩阵 <span class="math inline">\(A\)</span> 酉相似于对角阵： <span class="math display">\[U^HAU=\Lambda=\begin{bmatrix}\lambda_1&amp;&amp;&amp;\\&amp;\lambda_2&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\lambda_n\end{bmatrix}\]</span> 或写作谱分解的形式： <span class="math display">\[A=U\Lambda U^H=\begin{bmatrix}u_1&amp;u_2&amp;\cdots&amp;u_n\end{bmatrix}\begin{bmatrix}\lambda_1&amp;&amp;&amp;\\&amp;\lambda_2&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\lambda_n\end{bmatrix}\begin{bmatrix}u_1^H\\u_2^H\\\vdots\\u_n^H\end{bmatrix}=\sum_{i=1}^n\lambda_iu_iu_i^H\]</span> 其中 <span class="math inline">\(U\)</span> 为酉矩阵，列向量为 <span class="math inline">\(A\)</span> 的特征向量，<span class="math inline">\(\Lambda\)</span> 对角元为 <span class="math inline">\(A\)</span> 的特征值。</p><h2 id="非奇异方阵的酉对角分解">非奇异方阵的酉对角分解</h2><p>设 <span class="math inline">\(A\)</span> 为 <span class="math inline">\(n\)</span> 阶非奇异矩阵，则存在 <span class="math inline">\(n\)</span> 阶酉矩阵 <span class="math inline">\(U\)</span> 和 <span class="math inline">\(V\)</span>，使得： <span class="math display">\[U^HAV=\Sigma=\begin{bmatrix}\sigma_1&amp;&amp;&amp;\\&amp;\sigma_2&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\sigma_n\end{bmatrix},\quad\sigma_i&gt;0\]</span> 或写作矩阵分解形式： <span class="math display">\[A=U\Sigma V^H=\begin{bmatrix}u_1&amp;u_2&amp;\cdots&amp;u_n\end{bmatrix}\begin{bmatrix}\sigma_1&amp;&amp;&amp;\\&amp;\sigma_2&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\sigma_n\end{bmatrix}\begin{bmatrix}v_1^H\\v_2^H\\\vdots\\v_n^H\end{bmatrix}=\sum_{i=1}^n\sigma_iu_iv_i^H\]</span> <div class="note note-secondary">            <p>证明：由于 <span class="math inline">\(A^HA\)</span> 是非奇异 Hermite 半正定矩阵，因此其谱分解为： <span class="math display">\[A^HA=V\Sigma^2V^H=V\begin{bmatrix}\sigma_1^2&amp;&amp;&amp;\\&amp;\sigma_2^2&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\sigma_n^2\end{bmatrix}V^H\]</span> 其中 <span class="math inline">\(\sigma_i^2\)</span> 是 <span class="math inline">\(A^HA\)</span> 的特征值。令 <span class="math inline">\(U=AV\Sigma^{-1}\)</span>，则： <span class="math display">\[U^HU=\Sigma^{-1}V^HA^HAV\Sigma^{-1}=\Sigma^{-1}V^H(V\Sigma^2V^H)V\Sigma^{-1}=I\]</span> 因此 <span class="math inline">\(U\)</span> 也是酉矩阵。并且： <span class="math display">\[U\Sigma V^H=AV\Sigma^{-1}\Sigma V^{H}=A\]</span> 证毕。</p>          </div></p><p>酉对角分解的计算方法就是证明中的构造方法：先将 <span class="math inline">\(A^HA\)</span> 酉对角化，求解 <span class="math inline">\(V\)</span> 和 <span class="math inline">\(\Sigma\)</span>，再令 <span class="math inline">\(U=AV\Sigma^{-1}\)</span> 即可。</p><div class="note note-info">            <p>将 <span class="math inline">\(A\)</span> 视作 <span class="math inline">\(\mathbb C^n\)</span> 上的线性变换： <span class="math display">\[A=U\Sigma V^H\iff AV=U\Sigma\]</span> 因此酉对角分解相当于<strong><span class="math inline">\(A\)</span> 在基 <span class="math inline">\(U,V\)</span> 下的表示矩阵为 <span class="math inline">\(\Sigma\)</span></strong>.</p>          </div><h2 id="一般矩阵的奇异值分解">一般矩阵的奇异值分解</h2><p>设 <span class="math inline">\(A\in\mathbb C^{m\times n}_r\)</span>，则存在 <span class="math inline">\(m\)</span> 阶酉矩阵 <span class="math inline">\(U\)</span> 和 <span class="math inline">\(n\)</span> 阶酉矩阵 <span class="math inline">\(V\)</span> 使得： <span class="math display">\[U^HAV=\Sigma=\begin{bmatrix}\sigma_1&amp;&amp;&amp;&amp;O\\&amp;\sigma_2&amp;&amp;&amp;\\&amp;&amp;\ddots&amp;&amp;\\&amp;&amp;&amp;\sigma_r&amp;\\O&amp;&amp;&amp;&amp;O\end{bmatrix},\quad\sigma_i&gt;0\]</span> 或写作矩阵分解形式： <span class="math display">\[A=U\Sigma V^H=\begin{bmatrix}u_1&amp;u_2&amp;\cdots&amp;u_m\end{bmatrix}\begin{bmatrix}\sigma_1&amp;&amp;&amp;&amp;O\\&amp;\sigma_2&amp;&amp;&amp;\\&amp;&amp;\ddots&amp;&amp;\\&amp;&amp;&amp;\sigma_r&amp;\\O&amp;&amp;&amp;&amp;O\end{bmatrix}\begin{bmatrix}v_1^H\\v_2^H\\\vdots\\v_n^H\end{bmatrix}\]</span> <div class="note note-secondary">            <p>证明：由于 <span class="math inline">\(A^HA\)</span> 是 Hermite 半正定矩阵，因此其谱分解为： <span class="math display">\[A^HA=V\begin{bmatrix}\Sigma^2_r&amp;O\\O&amp;O\end{bmatrix}V^H\]</span> 划分 <span class="math inline">\(V=(V_1\mid V_2)\)</span>，其中 <span class="math inline">\(V_1\)</span> 是 <span class="math inline">\(V\)</span> 的前 <span class="math inline">\(r\)</span> 列，<span class="math inline">\(V_2\)</span> 是 <span class="math inline">\(V\)</span> 的后 <span class="math inline">\(n-r\)</span> 列，于是： <span class="math display">\[A^HAV=V\begin{bmatrix}\Sigma^2_r&amp;O\\O&amp;O\end{bmatrix}\implies\left(A^HAV_1\mid A^HAV_2\right)=\left(V_1\Sigma^2_r\mid O\right)\]</span> 也即：</p><ul><li><span class="math inline">\(A^HAV_1=V_1\Sigma^2_r\implies V_1^HA^HAV_1=\Sigma^2_r\implies(AV_1\Sigma_r^{-1})^H(AV_1\Sigma_r^{-1})=I\)</span></li><li><span class="math inline">\(A^HAV_2=O\implies V_2^HA^HAV_2=O\implies(AV_2)^H(AV_2)=O\implies AV_2=O\)</span></li></ul><p>令 <span class="math inline">\(U_1=AV_1\Sigma^{-1}_r\in\mathbb C^{m\times r}\)</span>，容易验证 <span class="math inline">\(U_1^HU_1=I\)</span>. 将 <span class="math inline">\(U_1\)</span> 的列扩充为 <span class="math inline">\(\mathbb C^m\)</span> 的标准正交基构成 <span class="math inline">\(U=\left[\begin{array}{c:c}U_1&amp;U_2\end{array}\right]\)</span>，那么： <span class="math display">\[U^HAV=U^H\left[\begin{array}{c:c}AV_1&amp;AV_2\end{array}\right]=\begin{bmatrix}U_1^H\\U_2^H\end{bmatrix}\left[\begin{array}{c:c}U_1\Sigma_r&amp;O\end{array}\right]=\begin{bmatrix}U_1^HU_1\Sigma_r&amp;O\\U_2^HU_1\Sigma_r&amp;O\end{bmatrix}=\begin{bmatrix}\Sigma_r&amp;O\\O&amp;O\end{bmatrix}\]</span> 证毕。</p>          </div></p><p>奇异值分解的计算方法就是证明中的构造方法：先求解 <span class="math inline">\(A^HA\)</span> 的特征向量 <span class="math inline">\(V\)</span> 和特征值 <span class="math inline">\(\Sigma^2\)</span>，再计算 <span class="math inline">\(U_1=AV_1\Sigma_r^{-1}\)</span>，然后将其列扩充为标准正交基构成 <span class="math inline">\(U\)</span>.</p><p>当然换一个方向也可以：先求解 <span class="math inline">\(AA^H\)</span> 的特征向量 <span class="math inline">\(U\)</span> 和特征值 <span class="math inline">\(\Sigma^2_r\)</span>，再计算 <span class="math inline">\(V_1=A^{-1}U_1\Sigma_r\)</span>，然后将其列扩充为标准正交基构成 <span class="math inline">\(V\)</span>. 一般可以根据 <span class="math inline">\(A^HA\)</span> 和 <span class="math inline">\(AA^H\)</span> 哪个矩阵更小更好算决定选择哪一种方法。</p><div class="note note-warning">            <p>注意：不能计算 <span class="math inline">\(A^HA\)</span> 的特征向量作为 <span class="math inline">\(V\)</span>，<strong>同时</strong>计算 <span class="math inline">\(AA^H\)</span> 的特征向量作为 <span class="math inline">\(U\)</span>.</p>          </div><div class="note note-info">            <p>将 <span class="math inline">\(A\)</span> 视作 <span class="math inline">\(\mathbb C^n\)</span> 到 <span class="math inline">\(\mathbb C^m\)</span> 的线性映射： <span class="math display">\[A=U\Sigma V^H\iff AV=U\Sigma\]</span> 因此奇异值分解相当于<strong><span class="math inline">\(A\)</span> 在基 <span class="math inline">\(U,V\)</span> 下的表示矩阵为 <span class="math inline">\(\Sigma\)</span></strong>.</p>          </div><p><strong>秩 <span class="math inline">\(k\)</span> 最佳逼近</strong>：设 <span class="math inline">\(A=U\Sigma V^H\)</span>，定义 <span class="math inline">\(A_k\)</span> 为： <span class="math display">\[A_k=\sum_{i=1}^k\sigma_iu_iv_i^H,\quad k&lt;r\]</span> 则： <span class="math display">\[\begin{align}&amp;\min_{\text{rank}(B)=k}\Vert A-B\Vert_1=\Vert A-A_k\Vert_1=\sigma_{k+1}\\&amp;\min_{\text{rank}(B)=k}\Vert A-B\Vert_F^2=\Vert A-A_k\Vert_F^2=\sigma_{k+1}^2+\cdots\sigma_{r}^2\end{align}\]</span> <strong>定理</strong>：设 <span class="math inline">\(A=U\Sigma V^H\)</span>，则： <span class="math display">\[\begin{align}&amp;N(A)=\text{span}\{v_{r+1},v_{r+2},\ldots,v_{n}\}\\&amp;R(A)=\text{span}\{u_1,u_2,\ldots,u_r\}\end{align}\]</span> <div class="note note-secondary">            <p>证明：设 <span class="math display">\[A=U\Sigma V^H=\begin{bmatrix}U_1&amp;U_2\end{bmatrix}\begin{bmatrix}\Sigma_r&amp;O\\O&amp;O\end{bmatrix}\begin{bmatrix}V_1^H\\V_2^H\end{bmatrix}=U_1\Sigma_rV_1^H\]</span></p><p>容易验证： <span class="math display">\[U_1\Sigma_rV_1^Hx=0\iff V_1^Hx=0\]</span> (1). <span class="math display">\[\begin{align}N(A)&amp;=\{x\mid Ax=0\}=\{x\mid U_1\Sigma V_1^Hx=0\}\\&amp;=\{x\mid V_1^Hx=0\}=N(V_1^H)=R^{\perp}(V_1)\\&amp;=R(V_2)=\text{span}\{v_{r+1},\ldots,v_n\}\end{align}\]</span> (2). <span class="math display">\[\begin{align}&amp;R(A)=\{y\mid y=Ax\}=\{y\mid y=U_1(\Sigma V_1^Hx)\}\subset\{y\mid y=U_1z\}=R(U_1)\\&amp;R(U_1)=\{y\mid y=U_1z\}=\{y\mid y=A(V_1\Sigma_1^{-1}z)\}\subset\{y\mid y=Ax\}=R(A)\\\implies&amp;R(A)=R(U_1)=\text{span}\{u_1,u_2,\ldots,u_r\}\end{align}\]</span></p>          </div></p><div class="note note-success">            <p>该定理可以形象化地绘制作下图：</p><p><img src="SVDmap.png" width=80% /></p>          </div><p><strong>补充（正规矩阵的奇异值分解）</strong>：设 <span class="math inline">\(A\)</span> 是正规矩阵（即 <span class="math inline">\(A^HA=AA^H\)</span>），则 <span class="math inline">\(A\)</span> 酉相似于对角矩阵，即存在酉矩阵 <span class="math inline">\(U\)</span> 使得： <span class="math display">\[A=U\begin{bmatrix}\lambda_1&amp;&amp;&amp;&amp;&amp;\\&amp;\ddots&amp;&amp;&amp;&amp;\\&amp;&amp;\lambda_r&amp;&amp;&amp;\\&amp;&amp;&amp;0&amp;&amp;\\&amp;&amp;&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;&amp;&amp;0\\\end{bmatrix}U^H\]</span> 其中 <span class="math inline">\(\lambda_i\)</span> 是 <span class="math inline">\(A\)</span> 的特征值（注意可能为负）。那么可以构造奇异值分解如下： <span class="math display">\[A=U\underbrace{\begin{bmatrix}|\lambda_1|&amp;&amp;&amp;&amp;&amp;\\&amp;\ddots&amp;&amp;&amp;&amp;\\&amp;&amp;|\lambda_r|&amp;&amp;&amp;\\&amp;&amp;&amp;0&amp;&amp;\\&amp;&amp;&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;&amp;&amp;0\\\end{bmatrix}}_{\Sigma}\underbrace{\begin{bmatrix}\lambda_1/|\lambda_1|&amp;&amp;&amp;&amp;&amp;\\&amp;\ddots&amp;&amp;&amp;&amp;\\&amp;&amp;\lambda_r/|\lambda_r|&amp;&amp;&amp;\\&amp;&amp;&amp;1&amp;&amp;\\&amp;&amp;&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;&amp;&amp;1\\\end{bmatrix}U^H}_{V^H}\]</span></p><h2 id="正交相抵">正交相抵</h2><p><strong>定义</strong>：设有矩阵 <span class="math inline">\(A_{m\times n},\,B_{m\times n}\)</span>，若存在酉矩阵 <span class="math inline">\(U_{m\times m},\,V_{n\times n}\)</span> 使得 <span class="math inline">\(U^HAV=B\)</span>，则称 <span class="math inline">\(A\)</span> 与 <span class="math inline">\(B\)</span> 正交相抵。</p><p><strong>性质</strong>：正交相抵是一个等价关系，即满足自反性、对称性和传递性。</p><p><strong>定理</strong>：若 <span class="math inline">\(A\)</span> 与 <span class="math inline">\(B\)</span> 正交相抵，则 <span class="math inline">\(\sigma_A=\sigma_B\)</span>.</p><div class="note note-secondary">            <p>证明：<span class="math inline">\(B=U^HAV\implies B^HB=V^HA^HUU^HAV=V^HA^HAV\implies\lambda_{B^HB}=\lambda_{A^HA}\implies \sigma_B=\sigma_A\)</span>. 证毕。</p>          </div>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]4.3矩阵的满秩分解</title>
    <link href="/blog-main/2023/12/03/%E7%9F%A9%E9%98%B5%E8%AE%BA-4-3%E7%9F%A9%E9%98%B5%E7%9A%84%E6%BB%A1%E7%A7%A9%E5%88%86%E8%A7%A3/"/>
    <url>/blog-main/2023/12/03/%E7%9F%A9%E9%98%B5%E8%AE%BA-4-3%E7%9F%A9%E9%98%B5%E7%9A%84%E6%BB%A1%E7%A7%A9%E5%88%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="概念">概念</h2><p><strong>满秩分解</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n}_r\)</span>，若存在矩阵 <span class="math inline">\(F\in\mathbb C^{m\times r}_r\)</span> 和 <span class="math inline">\(G\in\mathbb C^{r\times n}_r\)</span>，使得 <span class="math inline">\(A=FG\)</span>，则称为 <span class="math inline">\(A\)</span> 的满秩分解。</p><p>当 <span class="math inline">\(A\)</span> 列满秩或行满秩时，<span class="math inline">\(A\)</span> 可分解为单位矩阵及其本身，称作平凡分解。</p><p><strong>定理</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n}_r\)</span>，则 <span class="math inline">\(A\)</span> 的满秩分解存在。</p><div class="note note-secondary">            <p>证明过程就是构造过程。</p><p>设 <span class="math inline">\(A=(a_1,\ldots,a_n)\)</span>，取其列向量的一个极大线性无关组，设为 <span class="math inline">\(\{a_{i1},\ldots,a_{ir}\}\)</span>，则 <span class="math inline">\(A\)</span> 的各列都可以表示为该线性无关组的线性组合，即： <span class="math display">\[a_i=g_{1i}a_{i1}+g_{2i}a_{i2}+\cdots+g_{ri}a_{ir},\quad\forall\ i=1,\ldots,n\]</span> 写作矩阵形式： <span class="math display">\[A=FG=\begin{bmatrix}|&amp;|&amp;&amp;|\\a_{i1}&amp;a_{i2}&amp;\ldots&amp;a_{ir}\\|&amp;|&amp;&amp;|\\\end{bmatrix}\begin{bmatrix}g_{11}&amp;g_{12}&amp;\cdots&amp;g_{1n}\\g_{21}&amp;g_{22}&amp;\cdots&amp;g_{2n}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\g_{r1}&amp;g_{r2}&amp;\cdots&amp;g_{rn}\end{bmatrix}\]</span> 这样就构造出了 <span class="math inline">\(A\)</span> 的满秩分解。</p>          </div><p><strong>注意</strong>：满秩分解并不唯一。设 <span class="math inline">\(A=FG\)</span>，<span class="math inline">\(D\in\mathbb C^{r\times r}_r\)</span>，则 <span class="math inline">\(A=FDD^{-1}G=(FD)(D^{-1}G)\)</span> 也是一个合法的满秩分解。</p><h2 id="计算方法">计算方法</h2><p>上文中我们通过寻找极大线性无关组并将 <span class="math inline">\(A\)</span> 的各列表示为其线性组合的方式构造出了满秩分解，但这种计算方法太麻烦了。有没有更简单的计算方法呢？</p><h3 id="化行阶梯形">化行阶梯形</h3><p>设 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span> 经初等行变换后化作行阶梯形： <span class="math display">\[A\xrightarrow{\text{行}} B\iff PA=B\iff A=P^{-1}B\]</span> 将 <span class="math inline">\(B\)</span> 与 <span class="math inline">\(P^{-1}\)</span> 写作如下分块形式： <span class="math display">\[P^{-1}=\left[\begin{array}{c:c}F_{m\times r}&amp;S_{m\times(m-r)}\end{array}\right],\quadB=\begin{bmatrix}G_{r\times n}\\\hdashline O_{(m-r)\times n}\end{bmatrix}\]</span> 那么： <span class="math display">\[A=P^{-1}B=\left[\begin{array}{c:c}F_{m\times r}&amp;S_{m\times(m-r)}\end{array}\right]\begin{bmatrix}G_{r\times n}\\\hdashline O_{(m-r)\times n}\end{bmatrix}=FG\]</span> 也就是说<strong>满秩分解中的 <span class="math inline">\(F\)</span> 就是 <span class="math inline">\(P^{-1}\)</span> 的前 <span class="math inline">\(r\)</span> 列，<span class="math inline">\(G\)</span> 就是 <span class="math inline">\(B\)</span> 的前 <span class="math inline">\(r\)</span> 行</strong>。</p><p>然而，这个方法需要求解 <span class="math inline">\(P^{-1}\)</span>，依然不是很友好，因此我们有下面的方法。</p><h3 id="化-hermite-标准形">化 Hermite 标准形</h3><p><strong>Hermite 标准形</strong>：所谓的 Hermite 标准形其实就是行最简阶梯形： <span class="math display">\[\begin{bmatrix}0&amp;1&amp;\times&amp;\times&amp;0&amp;\times&amp;\times\\0&amp;0&amp;0&amp;0&amp;1&amp;\times&amp;\times\\0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\end{bmatrix}\]</span> <strong>定理</strong>：任意非零矩阵 <span class="math inline">\(A\in\mathbb C^{m\times n}_r\)</span> 都可以通过初等行变换化为 Hermite 标准形 <span class="math inline">\(B\)</span>，且 <span class="math inline">\(B\)</span> 的前 <span class="math inline">\(r\)</span> 行线性无关。</p><p><strong>列置换矩阵</strong>：<span class="math inline">\(P=(e_{j_1},e_{j_2},\ldots,e_{j_n})\)</span>，其中 <span class="math inline">\(j_1,\ldots,j_n\)</span> 是 <span class="math inline">\(1,\ldots,n\)</span> 的一个排列。<span class="math inline">\(AP\)</span> 的作用就是对 <span class="math inline">\(A\)</span> 进行列置换。</p><p>根据以上的知识，我们可以首先通过初等行变换将 <span class="math inline">\(A\)</span> 化作 Hermite 标准形 <span class="math inline">\(B\)</span>： <span class="math display">\[A\xrightarrow{\text{行}}B\iff PA=B\iff A=P^{-1}B\]</span> 与上一节不同的是，由于 <span class="math inline">\(B\)</span> 现在是 Hermite 标准形，因此存在一个置换矩阵 <span class="math inline">\(P_1\)</span>，使得列置换后可以写作如下分块形式： <span class="math display">\[BP_1=\left[\begin{array}{c:c}I_r&amp;B_{12}\\\hdashline O&amp;O\end{array}\right]\]</span> 因此： <span class="math display">\[AP_1=P^{-1}BP_1=\left[\begin{array}{c:c}F&amp;S\end{array}\right]\left[\begin{array}{c:c}I_r&amp;B_{12}\\\hdashline O&amp;O\end{array}\right]=\left[\begin{array}{c:c}F&amp;FB_{12}\end{array}\right]\]</span> 也就是说，上一节中为了求解 <span class="math inline">\(F\)</span> 我们要算 <span class="math inline">\(P^{-1}\)</span>，但这里我们发现 <span class="math inline">\(F\)</span> 就是 <span class="math inline">\(A\)</span> 经过同样的列置换后的前 <span class="math inline">\(r\)</span> 列！</p><p>并且实际计算中我们不必真的置换，<strong>只需要观察 <span class="math inline">\(B\)</span> 中哪些列构成单位阵，那么拿出 <span class="math inline">\(A\)</span> 中对应的列构成 <span class="math inline">\(F\)</span> 即可</strong>。</p><div class="note note-secondary">            <p>例：设 <span class="math inline">\(A=\begin{bmatrix}1&amp;0&amp;1&amp;1\\2&amp;1&amp;2&amp;1\\2&amp;0&amp;2&amp;2\\4&amp;2&amp;4&amp;2\end{bmatrix}\)</span>，求 <span class="math inline">\(A\)</span> 的满秩分解。</p><p>解：将 <span class="math inline">\(A\)</span> 化作 Hermite 标准形： <span class="math display">\[A\xrightarrow{\text{行}}\begin{bmatrix}1&amp;0&amp;1&amp;1\\0&amp;1&amp;0&amp;-1\\0&amp;0&amp;0&amp;0\\0&amp;0&amp;0&amp;0\end{bmatrix}=B\]</span> 那么 <span class="math inline">\(G\)</span> 就是 <span class="math inline">\(B\)</span> 的前两行： <span class="math display">\[G=\begin{bmatrix}1&amp;0&amp;1&amp;1\\0&amp;1&amp;0&amp;-1\end{bmatrix}\]</span> 又因为 <span class="math inline">\(B\)</span> 的前两列构成了单位阵，所以取出 <span class="math inline">\(A\)</span> 的前两列构成 <span class="math inline">\(F\)</span>： <span class="math display">\[F=\begin{bmatrix}1&amp;0\\2&amp;1\\2&amp;0\\4&amp;2\end{bmatrix}\]</span> 这样就得到了满秩分解：<span class="math inline">\(A=FG\)</span>.</p>          </div><h2 id="qr-分解的扩展">QR 分解的扩展</h2><p>上一篇的 QR 分解是对非奇异方阵或列满秩矩阵讨论的，即：</p><ul><li>若 <span class="math inline">\(A\)</span> 是非奇异方阵，则可分解为正交/酉矩阵 <span class="math inline">\(Q\)</span> 与非奇异上三角矩阵 <span class="math inline">\(R\)</span> 的乘积，且在除去相差一个对角元素的绝对值/模全等于 1 的对角矩阵外，分解唯一。</li><li>若 <span class="math inline">\(A\)</span> 是列满秩矩阵，则可分解为 <span class="math inline">\(Q\)</span> 与非奇异上三角矩阵 <span class="math inline">\(R\)</span> 的乘积，其中 <span class="math inline">\(Q^HQ=I\)</span>（即各列正交），且在除去相差一个对角元素的绝对值/模全等于 1 的对角矩阵外，分解唯一。</li></ul><p>那么如果 <span class="math inline">\(A\)</span> 不是列满秩的呢，这种情况下我们依旧可以保持 <span class="math inline">\(Q\)</span> 各列的正交性，但是 <span class="math inline">\(R\)</span> 不再是上三角矩阵。</p><p><strong>定理</strong>：设矩阵 <span class="math inline">\(A\in\mathbb C^{m\times n}_r\)</span>，则有分解： <span class="math display">\[A=QR\]</span> 其中 <span class="math inline">\(Q\in\mathbb C^{m\times r}_r\)</span> 且 <span class="math inline">\(Q^HQ=I\)</span>，<span class="math inline">\(R\in\mathbb C^{r\times n}\)</span> 且行线性无关。</p><div class="note note-secondary">            <p>证明：对 <span class="math inline">\(A\)</span> 进行满秩分解： <span class="math display">\[A=FG\]</span> 其中 <span class="math inline">\(F\in\mathbb C^{m\times r}_r\)</span>，<span class="math inline">\(G\in\mathbb C^{r\times n}_r\)</span>，对 <span class="math inline">\(F\)</span> 作 QR 分解： <span class="math display">\[F=QR_1\]</span> 其中 <span class="math inline">\(Q\in\mathbb C^{m\times r}_r\)</span> 且 <span class="math inline">\(Q^HQ=I\)</span>，<span class="math inline">\(R_1\in\mathbb C^{r\times r}_r\)</span> 为非奇异上三角矩阵。</p><p>因此， <span class="math display">\[A=Q(R_1G)=QR\]</span> 其中 <span class="math inline">\(R=R_1G\in\mathbb C^{r\times n}_r\)</span> 为行线性无关的矩阵。</p><p>证毕。</p>          </div>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Stable Diffusion]ControlNet代码分析</title>
    <link href="/blog-main/2023/12/01/Stable-Diffusion-ControlNet%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/"/>
    <url>/blog-main/2023/12/01/Stable-Diffusion-ControlNet%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>ControlNet 1.0:</p><ul><li>GitHub: <a href="https://github.com/lllyasviel/ControlNet/" class="uri">https://github.com/lllyasviel/ControlNet/</a></li><li>HuggingFace: <a href="https://huggingface.co/lllyasviel/ControlNet/" class="uri">https://huggingface.co/lllyasviel/ControlNet/</a></li></ul><p>ControlNet 1.1:</p><ul><li>GitHub: <a href="https://github.com/lllyasviel/ControlNet-v1-1-nightly/" class="uri">https://github.com/lllyasviel/ControlNet-v1-1-nightly/</a></li><li>HuggingFace: <a href="https://huggingface.co/lllyasviel/ControlNet-v1-1/" class="uri">https://huggingface.co/lllyasviel/ControlNet-v1-1/</a></li></ul><h2 id="overview">Overview</h2><p>ControlNet 是基于 Stable Diffusion 的代码库开发的，以下为整个仓库的文件组织结构（排除了非代码文件，如文档、字体、测试图片等）：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs text">.<br>├── annotator<br>│   ├── canny<br>│   ├── ckpts<br>│   ├── hed<br>│   ├── midas<br>│   ├── mlsd<br>│   ├── openpose<br>│   ├── uniformer<br>│   └── util.py<br>├── cldm<br>│   ├── cldm.py<br>│   ├── ddim_hacked.py<br>│   ├── hack.py<br>│   ├── logger.py<br>│   └── model.py<br>├── config.py<br>├── gradio_annotator.py<br>├── gradio_canny2image.py<br>├── gradio_depth2image.py<br>├── gradio_fake_scribble2image.py<br>├── gradio_hed2image.py<br>├── gradio_hough2image.py<br>├── gradio_normal2image.py<br>├── gradio_pose2image.py<br>├── gradio_scribble2image_interactive.py<br>├── gradio_scribble2image.py<br>├── gradio_seg2image.py<br>├── ldm<br>│   ├── data<br>│   ├── models<br>│   ├── modules<br>│   └── util.py<br>├── models<br>│   ├── cldm_v15.yaml<br>│   └── cldm_v21.yaml<br>├── share.py<br>├── tool_add_control.py<br>├── tool_add_control_sd21.py<br>├── tool_transfer_control.py<br>├── tutorial_dataset.py<br>├── tutorial_dataset_test.py<br>├── tutorial_train.py<br>└── tutorial_train_sd21.py<br></code></pre></td></tr></table></figure><h2 id="details">Details</h2><h3 id="annotator">annotator</h3><p>Annotator 指各种条件提取器，如 canny 边缘检测器、midas 深度和法线估计模型、openpose 人体姿态识别器等，存放在 <code>annotator</code> 下。这些 annotator 既有传统方法，也有基于深度学习的方法，因此有些 annotator 需要模型权重文件，权重文件应放置在 <code>annotator/ckpts</code> 下。</p><p>作者将每一种 annotator 分别放在对应子目录下，并且在各自的 <code>__init__.py</code> 中封装了 <code>xxxDetector</code> 类，例如 <code>CannyDetector</code>、<code>MidasDetector</code>、<code>OpenposeDetector</code> 等。这些 <code>xxxDetector</code> 有 <code>__call__()</code> 方法，接受输入图像（numpy uint8 HWC），返回检测图像（numpy uint8 HWC），因此调用起来非常的方便。</p><p>作者还在 <code>annotator/util.py</code> 中提供了两个工具方法：<code>resize_image()</code> 将图像按小边成比例缩放到指定大小附近的 64 倍率处，<code>HWC3()</code> 将不同通道数的图像统一为 3 通道。</p><p>下面是一个简单的代码片段示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">from</span> annotator.canny <span class="hljs-keyword">import</span> CannyDetector<br><span class="hljs-keyword">from</span> annotator.midas <span class="hljs-keyword">import</span> MidasDetector<br><span class="hljs-keyword">from</span> annotator.uniformer <span class="hljs-keyword">import</span> UniformerDetector<br><span class="hljs-keyword">from</span> annotator.util <span class="hljs-keyword">import</span> resize_image, HWC3<br><br><span class="hljs-comment"># read image</span><br>img_path = <span class="hljs-string">&#x27;./test_imgs/building.png&#x27;</span><br>img = HWC3(np.array(Image.<span class="hljs-built_in">open</span>(img_path)))  <span class="hljs-comment"># uint8, (848, 564, 3)</span><br><br><span class="hljs-comment"># apply canny</span><br>apply_canny = CannyDetector()<br>img_canny = apply_canny(img, low_threshold=<span class="hljs-number">128</span>, high_threshold=<span class="hljs-number">200</span>)<br>img_canny = HWC3(img_canny)<br>Image.fromarray(img_canny).save(<span class="hljs-string">&#x27;img_canny.png&#x27;</span>)<br><br><span class="hljs-comment"># apply midas</span><br>apply_midas = MidasDetector()<br>img_r = resize_image(img, <span class="hljs-number">768</span>)  <span class="hljs-comment"># uint8, (1152, 768, 3)</span><br>img_midas_depth, img_midas_normal = apply_midas(img_r)<br>img_midas_depth = HWC3(img_midas_depth)<br>img_midas_normal = HWC3(img_midas_normal)<br>Image.fromarray(img_midas_depth).save(<span class="hljs-string">&#x27;img_midas_depth.png&#x27;</span>)<br>Image.fromarray(img_midas_normal).save(<span class="hljs-string">&#x27;img_midas_normal.png&#x27;</span>)<br><br><span class="hljs-comment"># apply uniformer</span><br>apply_uniformer = UniformerDetector()<br>img_uniformer = apply_uniformer(img)<br>img_uniformer = HWC3(img_uniformer)<br>Image.fromarray(img_uniformer).save(<span class="hljs-string">&#x27;img_uniformer.png&#x27;</span>)<br></code></pre></td></tr></table></figure><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="building.png" width=100% /></div><div class="group-image-wrap"><img src="img_canny.png" width=100% /></div><div class="group-image-wrap"><img src="img_midas_depth.png" width=100% /></div><div class="group-image-wrap"><img src="img_midas_normal.png" width=100% /></div><div class="group-image-wrap"><img src="img_uniformer.png" width=100% /></div></div></div><h3 id="ldm">ldm</h3><p>ldm 摘取自 Stable Diffusion 项目，是 SD 的核心。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs text">ldm<br>├── models<br>│   ├── autoencoder.py<br>│   └── diffusion<br>│       ├── __init__.py<br>│       ├── ddim.py<br>│       ├── ddpm.py<br>│       ├── dpm_solver<br>│       ├── plms.py<br>│       └── sampling_util.py<br>├── modules<br>│   ├── attention.py<br>│   ├── diffusionmodules<br>│   │   ├── __init__.py<br>│   │   ├── model.py<br>│   │   ├── openaimodel.py<br>│   │   ├── upscaling.py<br>│   │   └── util.py<br>│   ├── distributions<br>│   │   ├── __init__.py<br>│   │   └── distributions.py<br>│   ├── ema.py<br>│   ├── encoders<br>│   │   ├── __init__.py<br>│   │   └── modules.py<br>│   ├── image_degradation<br>│   │   ├── __init__.py<br>│   │   ├── bsrgan_light.py<br>│   │   ├── bsrgan.py<br>│   │   ├── utils<br>│   │   └── utils_image.py<br>│   └── midas<br>│       ├── __init__.py<br>│       ├── api.py<br>│       ├── midas<br>│       └── utils.py<br>└── util.py<br></code></pre></td></tr></table></figure><h3 id="ldmutil.py">ldm/util.py</h3><p><code>ldm/util.py</code> 定义了一些工具函数和工具类，其中值得提及的是如下函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">instantiate_from_config</span>(<span class="hljs-params">config</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-string">&quot;target&quot;</span> <span class="hljs-keyword">in</span> config:<br>        <span class="hljs-keyword">if</span> config == <span class="hljs-string">&#x27;__is_first_stage__&#x27;</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">elif</span> config == <span class="hljs-string">&quot;__is_unconditional__&quot;</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">raise</span> KeyError(<span class="hljs-string">&quot;Expected key `target` to instantiate.&quot;</span>)<br>    <span class="hljs-keyword">return</span> get_obj_from_str(config[<span class="hljs-string">&quot;target&quot;</span>])(**config.get(<span class="hljs-string">&quot;params&quot;</span>, <span class="hljs-built_in">dict</span>()))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_obj_from_str</span>(<span class="hljs-params">string, reload=<span class="hljs-literal">False</span></span>):<br>    module, cls = string.rsplit(<span class="hljs-string">&quot;.&quot;</span>, <span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">if</span> reload:<br>        module_imp = importlib.import_module(module)<br>        importlib.reload(module_imp)<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">getattr</span>(importlib.import_module(module, package=<span class="hljs-literal">None</span>), cls)<br></code></pre></td></tr></table></figure><p>在 Stable Diffusion 项目（以及 ControlNet 项目）中，模型的实例化都是通过调用 <code>instantiate_from_config()</code> 实现的。其中 <code>config</code> 是 <code>OmegaConf</code> 包从配置文件读入的配置字典，字典中 <code>target</code> 字段写要实例化的类，<code>params</code> 字段写类的参数，例如 <code>configs/cldm_v15.yaml</code> 中有这么一段：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">unet_config:</span><br>  <span class="hljs-attr">target:</span> <span class="hljs-string">cldm.cldm.ControlledUnetModel</span><br>  <span class="hljs-attr">params:</span><br>    <span class="hljs-attr">image_size:</span> <span class="hljs-number">32</span> <span class="hljs-comment"># unused</span><br>    <span class="hljs-attr">in_channels:</span> <span class="hljs-number">4</span><br>    <span class="hljs-attr">out_channels:</span> <span class="hljs-number">4</span><br>    <span class="hljs-attr">model_channels:</span> <span class="hljs-number">320</span><br>    <span class="hljs-attr">attention_resolutions:</span> [ <span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span> ]<br>    <span class="hljs-attr">num_res_blocks:</span> <span class="hljs-number">2</span><br>    <span class="hljs-attr">channel_mult:</span> [ <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span> ]<br>    <span class="hljs-attr">num_heads:</span> <span class="hljs-number">8</span><br>    <span class="hljs-attr">use_spatial_transformer:</span> <span class="hljs-literal">True</span><br>    <span class="hljs-attr">transformer_depth:</span> <span class="hljs-number">1</span><br>    <span class="hljs-attr">context_dim:</span> <span class="hljs-number">768</span><br>    <span class="hljs-attr">use_checkpoint:</span> <span class="hljs-literal">True</span><br>    <span class="hljs-attr">legacy:</span> <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><p>那么代码就会从 <code>cldm.cldm</code> 模块中 import <code>ControlledUNetModel</code> 这个类，并用上述参数实例化。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> omegaconf <span class="hljs-keyword">import</span> OmegaConf<br><span class="hljs-keyword">from</span> ldm.util <span class="hljs-keyword">import</span> instantiate_from_config<br><br>conf = OmegaConf.load(<span class="hljs-string">&#x27;./models/cldm_v15.yaml&#x27;</span>)<br>unet = instantiate_from_config(conf.model.params.unet_config)<br><span class="hljs-built_in">print</span>(unet)<br></code></pre></td></tr></table></figure><p>执行上述代码后就能看到我们成功实例化了一个 UNet.</p><p>这种实例化方式非常灵活，适合快速修改参数和尝试不同的模型架构，同时保持代码简洁；缺点是看代码的时候必须要找到对应配置文件才知道到底实例化的是什么，以及在 IDE 中不能支持快速跳转和类型提示。</p><h3 id="ldmmodules">ldm/modules</h3><p><code>ldm/modules</code> 下是各种网络模块的定义（即继承 <code>torch.nn.Module</code> 的类）。</p><h3 id="ldmmodulesema.py">ldm/modules/ema.py</h3><p>该文件定义了 <code>LitEMA</code> 类，方便在训练过程中维护 EMA 权重。</p><ul><li>初始化时传入 <code>model</code>，则 <code>LitEMA</code> 会将其参数存入自己的 buffer；</li><li>每次调用 <code>LitEMA</code> 时传入新的 <code>model</code>，则 <code>LitEMA</code> 更新自己的 buffer；</li><li><code>copy_to()</code> 函数把自己的 buffer 复制给传入的 <code>model</code>；</li><li><code>store()</code> 函数将传入的参数暂存起来；</li><li><code>restore()</code> 函数将暂存的参数赋值给传入的参数。</li></ul><h3 id="ldmmodulesattention.py">ldm/modules/attention.py</h3><p>该文件定义了一些注意力模块，包括：</p><ul><li><code>CrossAttention</code>：交叉注意力模块。输入特征 <code>x</code>、条件 <code>context</code> 和掩码 <code>mask</code>，输出与 <code>x</code> 维度相同。如果 <code>context=None</code>，则等价于自注意力。</li><li><code>MemoryEfficientCrossAttention</code>：利用 <code>xformers</code> 包做交叉注意力，功能与 <code>CrossAttention</code> 相同，但更省内存。</li><li><code>BasicTransformerBlock</code>：SelfAttention + CrossAttention + FF 串起来。输入特征 <code>x</code> 和条件 <code>context</code>，输出与 <code>x</code> 维度相同。</li><li><code>SpatialTransformer</code>：支持图片形式输入输出的 <code>BasicTransformerBlock</code> 列表。输入特征图 <code>x</code> 和条件 <code>context</code>，输出与 <code>x</code> 维度相同。</li></ul><h3 id="ldmmodulesencoders">ldm/modules/encoders</h3><p>这里的 encoders 指各种文本编码器。<code>ldm/modules/encoders/modules.py</code> 在 <code>transformer</code> 包和 <code>openclip</code> 包的基础上进行了进一步封装，包括：</p><ul><li><code>FrozenT5Embedder</code></li><li><code>FrozenCLIPEmbedder</code></li><li><code>FrozenOpenCLIPEmbedder</code></li><li><code>FrozenCLIPT5Encoder</code></li></ul><p>它们都有 <code>encode()</code> 方法，调用即可获取输入文本的 embeddings.</p><p>Stable Diffusion v1.5 使用的是 <code>FrozenCLIPEmbedder</code>，加载的权重是 <code>openai/clip-vit-large-patch14</code>.</p><h3 id="ldmmodulesdistributions">ldm/modules/distributions</h3><p><code>ldm/modules/distributions/distributions.py</code> 定义了各种概率分布，包括：</p><ul><li><code>DiracDistribution</code>：狄拉克分布</li><li><code>DiagonalGaussianDistribution</code>：各分量独立高斯分布</li></ul><p>它们都有方法 <code>sample()</code> 和 <code>mode()</code> 用于采样以及返回均值。高斯分布还有 <code>kl()</code> 和 <code>nll()</code> 计算 KL 散度和负对数似然。</p><h3 id="ldmmodulesdiffusionmodules">ldm/modules/diffusionmodules</h3><p><code>ldm/modules/diffusionmodules</code> 下是搭建 VAE 和 UNet 的各种模块。</p><ul><li><code>model.py</code>：仿照 UNet 设计的各种 <code>Encoder</code> 和 <code>Decoder</code>；</li><li><code>openaimodel.py</code>：参考自 openai 代码的 <code>UNetModel</code>. 其 <code>forward()</code> 接受噪声图 <code>x</code>、时间步 <code>timestep</code>、以交叉注意力融入的条件 <code>context</code> 和类别标签 <code>y</code>（即以 adm 的方式融入模型的条件，不一定真的表示类别）。</li></ul><p>Stable Diffusion v1.5 使用的 UNet 就是 <code>openaimodel.py</code> 中定义的 <code>UNetModel</code>；使用的 VAE 就是 <code>model.py</code> 中定义的 <code>Encoder</code> 和 <code>Decoder</code>.</p><h3 id="ldmmodels">ldm/models</h3><p>上文中 <code>ldm/modules</code> 目录下定义了所有 SD 需要用到的网络模块，但由于 Stable Diffusion 项目（包括 ControlNet 项目）采用的是 PyTorch Lightning 训练，因此还需要对接 PL，这就是 <code>ldm/models</code> 的作用。</p><p>PL 要求定义一个继承 <code>pl.LightningModule</code> 的类，其中不仅要在 <code>__init__()</code> 中实例化各个 <code>nn.Module</code> 模块组件，还要重写一些方法。在 Stable Diffusion 项目中，重写的方法包括：</p><ul><li><code>configure_optimizers(self)</code>：返回一个元组，第一个元素为 optimizers 列表，第二个元素为 schedulers 列表。</li><li><code>training_step(self, batch, batch_idx)</code>：参数为一个 batch 的数据和当前 batch 的编号，返回一个 loss 张量或一个含有 <code>loss</code> 字段的字典。</li><li><code>validation_step(self, batch, batch_idx)</code>：参数为一个 batch 的数据和当前 batch 的编号，返回一个 loss 张量或一个含有 <code>loss</code> 字段的字典。</li><li><code>on_train_batch_end(self, outputs, batch, batch_idx)</code>：这是一个 hook，用于在一个 batch 训练结束后执行里面的内容。在 Stable Diffusion 项目中用于更新 EMA 权重。</li></ul><h3 id="ldmmodelsautoencoders.py">ldm/models/autoencoders.py</h3><p>该文件定义了 SD 中的 VAE 模块，主要实现了 <code>AutoencoderKL</code> 类（继承 <code>pl.LightningModule</code>）。</p><p>该类包含四个 <code>nn.Module</code> 组件：</p><ul><li><code>encoder</code>：<code>Encoder</code> 类的实例（上文提及），即编码器；</li><li><code>decoder</code>：<code>Decoder</code> 类的实例（上文提及），即解码器；</li><li><code>quant_conv</code>：一层 1x1 卷积 <code>nn.Conv2d</code>，将编码器隐空间维度 (=512) 映射到 embedding 维度 (=4)；</li><li><code>post_quant_conv</code>：一层 1x1 卷积 <code>nn.Conv2d</code>，将 embedding 维度 (=4) 映射回解码器隐空间维度 (=512)。</li></ul><p><code>forward()</code> 流程为：<code>encoder</code> 编码均值方差 → <code>quant_conv</code> 降维 → 从服从该均值方差的高斯分布中采样 → <code>post_quant_conv</code> 升维 → <code>decoder</code> 解码输出图像。</p><h3 id="ldmmodelsdiffusion">ldm/models/diffusion</h3><p>该目录下定义了扩散模型的 PL 接口以及各种采样器。</p><p>首先来看采样器。项目实现的采样器包括：</p><ul><li><code>DDIMSampler</code>：在 <code>ldm/models/diffusion/ddim.py</code> 中；</li><li><code>PLMSSampler</code>：在 <code>ldm/models/diffusion/plms.py</code> 中；</li><li><code>DPMSolverSampler</code>：在 <code>ldm/models/diffusion/dpm_solver</code> 目录下。</li></ul><p>这些采样器都是普通的类，有 <code>sample()</code> 方法，给定采样步数、batch size、图像大小以及其他参数，就能迭代地从高斯噪声生成最终的图像。</p><p><br/></p><p>还剩下一个 <code>ddpm.py</code> 是整个仓库里最长的文件，有 1800 行，其中定义了 <code>DDPM</code> 类、<code>LatentDiffusion</code> 类、<code>DiffusionWrapper</code> 类以及一系列继承自 <code>LatentDiffusion</code> 来做微调的类。我们着重看前三个。</p><p><code>DiffusionWrapper</code> 类继承自 <code>pl.LightningModule</code>，其主要作用是将不同种类的条件以不同的方式融入扩散模型，包含一个组件 <code>diffusion_model</code>，是上文提到的 <code>UNetModel</code> 的实例。</p><p>其 <code>forward()</code> 函数接受带噪图像 <code>x</code>、时间步 <code>t</code>、条件 <code>c_concat</code>、条件 <code>c_crossattn</code>和条件 <code>c_adm</code>. 顾名思义，后面三个条件的区别在于融入模型的方式不同，融入哪些条件由参数 <code>conditioning_key</code> 决定。回忆上文提到 <code>UNetModel</code> 的 <code>forward()</code> 本身支持两种条件——<code>context</code> 表示交叉注意力、<code>y</code> 表示 adm 方式，因此 <code>DiffusionWrapper</code> 的 <code>forward()</code> 就是基于 <code>UNetModel</code> 的 <code>forward()</code> 上的进一步封装。这个类应该是为了适应更多种类的条件而后来添加的，否则直接在 UNet 的 <code>forward()</code> 函数中处理更好。</p><p><code>DDPM</code> 类也继承自 <code>pl.LightningModule</code>，包含一个组件 <code>model</code>，是 <code>DiffusionWrapper</code> 的实例。</p><p><code>LatentDiffusion</code> 类是 SD 最主要的类。它继承自 <code>DDPM</code> 类，在其基础上增加了两个组件：</p><ul><li><code>cond_stage_model</code>：<code>FrozenCLIPEmbedder</code> 类的实例，即文本编码器；</li><li><code>first_stage_model</code>：<code>AutoencoderKL</code> 类的实例，即 VAE.</li></ul><h3 id="cldm">cldm</h3><p>上面 <code>ldm</code> 里的一大堆其实都是 Stable Diffusion 的工作，而 <code>cldm</code> 里才是 ControlNet 相关的代码。让我们看看作者是怎么把 ControlNet hack 进 Stable Diffusion 中的。</p><h3 id="cldmcldm.py">cldm/cldm.py</h3><p>这里是 ControlNet 的核心，定义了三个类：<code>ControlledUNetModel</code>、<code>ControlNet</code> 和 <code>ControlLDM</code>.</p><p><code>ControlledUNetModel</code> 继承自 <code>UNetModel</code>（即 SD 的去噪 UNet），重写了 <code>forward()</code> 方法，从而把 ControlNet 的输出接入 SD 之中。实现上也没有什么 tricky 的地方，就是老老实实抄过来改写。</p><p><code>ControlNet</code> 就是 ControlNet 本体了，是一个 <code>nn.Module</code> 模块。其大部分内容就是把 <code>UNetModel</code> 的 Encoder 部分抄过来，但是需要新添加一个将条件输入压缩到隐空间维度的 8 层卷积，称作 <code>input_hint_block</code>. 另外还加了一些 <code>zero_convs</code>（其中有一个 <code>middle_block_out</code> 也是零卷积）。</p><p><code>ControlLDM</code> 则继承自 <code>LatentDiffusion</code>，提供 PyTorch Lightning 的接口。其新增了组件 <code>control_model</code>，即 <code>ControlNet</code> 的实例化。</p><h3 id="cldmddim_hacked.py">cldm/ddim_hacked.py</h3><p>该文件主要修复了 <code>ldm/models/diffusion/ddim.py</code> 在采样时的一个 bug，可以直接平替原文件使用。</p><h3 id="cldmlogger.py">cldm/logger.py</h3><p>这里定义了 <code>ImageLogger</code> 类，它是 PL 的一个 Callback，用于在训练过程中采样图片，方便监视训练过程。</p><h3 id="cldmmodel.py">cldm/model.py</h3><p>这里定义了一些加载模型权重的帮助函数：</p><ul><li><code>get_state_dict(d)</code>：取出字典 <code>d</code> 中 key 为 <code>state_dict</code> 的项；如果没有 <code>state_dict</code> 这个 key，返回字典本身；</li><li><code>load_state_dict(ckpt_path, location='cpu')</code>：从 <code>ckpt_path</code> 加载模型权重，支持 <code>safetensors</code> 格式或普通的 <code>torch</code> 格式；</li><li><code>create_model(config_path)</code>：利用 <code>instantiate_from_config()</code> 从配置文件实例化整个模型。</li></ul><h3 id="gradio_xxx.py">gradio_xxx.py</h3><p>基于 <code>gradio</code> 包制作 webui，每种条件一个 webui. 下面只以 <code>gradio_canny2image.py</code> 为例进行分析。</p><p>首先实例化 <code>CannyDetector</code>、创建模型并加载权重，以及定义采样器 DDIMSampler：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">apply_canny = CannyDetector()<br><br>model = create_model(<span class="hljs-string">&#x27;./models/cldm_v15.yaml&#x27;</span>).cpu()<br>model.load_state_dict(load_state_dict(<span class="hljs-string">&#x27;./models/control_sd15_canny.pth&#x27;</span>, location=<span class="hljs-string">&#x27;cuda&#x27;</span>))<br>model = model.cuda()<br>ddim_sampler = DDIMSampler(model)<br></code></pre></td></tr></table></figure><p>然后按照 <code>gradio</code> 的语法格式创建 ui，这个不是重点所以略去，主要看处理函数 <code>process()</code>.</p><p>处理函数接收以下输入：</p><ul><li><code>input_image</code>：numpy uint8 HWC 格式</li><li><code>prompt</code>：文本</li><li><code>a_prompt</code>：额外文本</li><li><code>n_prompt</code>：负向文本</li><li><code>num_samples</code>：采样数量</li><li><code>image_resolution</code>：图像分辨率</li><li><code>ddim_steps</code>：DDIM 采样步数</li><li><code>guess_mode</code>：是否使用 guess mode</li><li><code>strength</code>：ControlNet 加入的强度</li><li><code>scale</code>：CFG scale</li><li><code>seed</code>：随机种子</li><li><code>eta</code>：DDIM eta</li><li><code>low_threshold</code>：canny low threshold</li><li><code>high_threshold</code>：canny high threshold</li></ul><p>处理函数的处理流程在代码里清晰明了，这里就不多赘述了。值得一提的是，作者没有给出非 webui 的推理脚本，而我在使用的时候发现 <code>gradio</code> 的版本似乎存在一些问题。不过根据 gradio 代码，稍微改改写一个非 webui 的推理脚本也并不难。</p><h2 id="model-weights">Model weights</h2><p>有了上面的基础之后，我们可以深入看一下 ControlNet 发布的权重。</p><h3 id="controlnet-1.0">ControlNet 1.0</h3><p>在 1.0 版本中，ControlNet 权重是和 SD1.5 的权重<strong>放一起发布</strong>的，所以一个文件就有 5.4GB.</p><p>以 <code>control_sd15_canny.pth</code> 为例，它就是 <code>ControlLDM</code> 类的 state_dict，其所有的 keys 包括：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs text">alphas_cumprod <br>alphas_cumprod_prev <br>betas <br>log_one_minus_alphas_cumprod <br>logvar <br>posterior_log_variance_clipped <br>posterior_mean_coef1 <br>posterior_mean_coef2 <br>posterior_variance <br>sqrt_alphas_cumprod<br>sqrt_one_minus_alphas_cumprod <br>sqrt_recip_alphas_cumprod <br>sqrt_recipm1_alphas_cumprod<br><br># Below are FrozenCLIPEmbedder&#x27;s state_dict<br>cond_stage_model.transformer.text_model.embeddings.[xxx]<br>cond_stage_model.transformer.text_model.encoder.[xxx]<br>cond_stage_model.transformer.text_model.final_layer_norm.[xxx]<br><br># Below are AutoencoderKL&#x27;s state_dict<br>first_stage_model.encoder.[xxx]<br>first_stage_model.decoder.[xxx]<br>first_stage_model.quant_conv.[xxx]<br>first_stage_model.post_quant_conv.[xxx]<br><br># Below are DiffusionWrapper&#x27;s state_dict<br>model.diffusion_model.time_embed.[xxx]<br>model.diffusion_model.input_blocks.[xxx]<br>model.diffusion_model.middle_block.[xxx]<br>model.diffusion_model.output_blocks.[xxx]<br>model.diffusion_model.out.[xxx]<br><br># Below are ControlNet&#x27;s state_dict<br>control_model.time_embed.[xxx]<br>control_model.input_blocks.[xxx]<br>control_model.middle_block.[xxx]<br>control_model.middle_block_out.0.bias<br>control_model.middle_block_out.0.weight<br>control_model.input_hint_block.[xxx]<br>control_model.zero_convs.[xxx]<br></code></pre></td></tr></table></figure><ul><li>第一部分是扩散模型的参数，并不是网络的权重</li><li>第二部分是文本编码器，即 <code>FrozenCLIPEmbedder</code> 的权重</li><li>第三部分是自编码器，即 <code>AutoencoderKL</code> 的权重</li><li>第四部分是扩散模型 UNet，即 <code>DiffusionWrapper</code> 的权重（前文提及，<code>DiffusionWrapper</code> 包裹了 <code>UNetModel</code> 类）</li><li>第五部分是 <code>ControlNet</code> 的权重，可以看到确实是在 SD UNet Encoder 的基础上增加了 <code>input_hint_block</code>、<code>zero_convs</code> 和 <code>middle_block_out</code>.（注意 <code>middle_block_out</code> 本身也是一层 zero convolution，这个名字有迷惑性）</li></ul><p>:warning: 论文里说 SD 是固定的，但是深入探究后发现不同条件的模型的 <code>model.diffusion_model.output_blocks</code> 和 <code>model.diffusion_model.out</code> 竟然不一样，这在代码里对应配置了 <code>sd_locked=False</code>，即让 SD UNet decoder 可训练。</p><h3 id="controlnet-1.1">ControlNet 1.1</h3><p>在 1.1 版本中，ControlNet 权重和 SD1.5 的权重是<strong>分开发布</strong>的，所以一个文件只有 1.4GB.</p><p>以 <code>control_v11p_sd15_canny.pth</code> 为例，它只是 <code>ControlNet</code> 类的 state_dict，其所有的 keys 包括：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs text">control_model.time_embed.[xxx]<br>control_model.input_blocks.[xxx]<br>control_model.middle_block.[xxx]<br>control_model.middle_block_out.0.bias<br>control_model.middle_block_out.0.weight<br>control_model.input_hint_block.[xxx]<br>control_model.zero_convs.[xxx]<br></code></pre></td></tr></table></figure><h3 id="control-loras-by-stabilityai">Control-LoRAs by StabilityAI</h3><p>StabilityAI 的 Control-LoRAs 在为 ControlNet 加入了 LoRA，一个 rank128 的文件有 396MB，一个 rank256 的文件有 774 MB.</p><p>以 <code>control-lora-canny-rank128.safetensors</code> 为例，它包含的 keys 有：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs text">lora_controlnet  (an empty tensor: Tensor([]))<br><br>label_emb.0.0.bias<br>label_emb.0.0.up<br>label_emb.0.0.down<br>label_emb.0.2.bias<br>label_emb.0.2.up<br>label_emb.0.2.down<br><br>time_embed.0.up<br>time_embed.0.down<br>time_embed.0.bias<br>time_embed.2.up<br>time_embed.2.down<br>time_embed.2.bias<br><br>input_blocks.[xxx].up<br>input_blocks.[xxx].down<br>input_blocks.[xxx].bias<br><br>middle_block.[xxx].up<br>middle_block.[xxx].down<br>middle_block.[xxx].bias<br><br>middle_block_out.0.bias<br>middle_block_out.0.weight<br>zero_convs.[xxx].weight<br>zero_convs.[xxx].bias<br></code></pre></td></tr></table></figure><p>对比 ControlNet 的 keys，最明显的是多了两项：<code>label_emb</code> 和 <code>lora_controlnet</code>，后者只是一个空 Tensor，用于方便判断加载的是否是 Control-LoRAs 的权重；前者的作用目前并不清楚（因为代码也没有开源）。其他 keys 就是在 ControlNet 的基础上为 Linear 层添加了 <code>.up</code> 和 <code>.down</code>（即 LoRA 的权重），有些层（如归一化层）还调整了 <code>.bias</code>。</p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>AIGC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
      <tag>AIGC</tag>
      
      <tag>stable diffusion</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]4.2矩阵的QR分解</title>
    <link href="/blog-main/2023/11/26/%E7%9F%A9%E9%98%B5%E8%AE%BA-4-2%E7%9F%A9%E9%98%B5%E7%9A%84QR%E5%88%86%E8%A7%A3/"/>
    <url>/blog-main/2023/11/26/%E7%9F%A9%E9%98%B5%E8%AE%BA-4-2%E7%9F%A9%E9%98%B5%E7%9A%84QR%E5%88%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="givens-变换与-householder-变换">Givens 变换与 Householder 变换</h2><h3 id="givens-矩阵与-givens-变换">Givens 矩阵与 Givens 变换</h3><p><strong>Givens 矩阵（初等旋转矩阵）</strong>：设 <span class="math inline">\(c,s\in\mathbb R\)</span> 且 <span class="math inline">\(c^2+s^2=1\)</span>，定义 Givens 矩阵为： <span class="math display">\[T_{ij}=T_{ij}(c,s)=\begin{bmatrix}1&amp;      &amp; &amp;  &amp; &amp;      &amp; &amp; &amp; &amp;      &amp; \\ &amp;\ddots&amp; &amp;  &amp; &amp;      &amp; &amp; &amp; &amp;      &amp; \\ &amp;      &amp;1&amp;  &amp; &amp;      &amp; &amp; &amp; &amp;      &amp; \\ &amp;      &amp; &amp;c &amp; &amp;      &amp; &amp;s&amp; &amp;      &amp; \\ &amp;      &amp; &amp;  &amp;1&amp;      &amp; &amp; &amp; &amp;      &amp; \\ &amp;      &amp; &amp;  &amp; &amp;\ddots&amp; &amp; &amp; &amp;      &amp; \\ &amp;      &amp; &amp;  &amp; &amp;      &amp;1&amp; &amp; &amp;      &amp; \\ &amp;      &amp; &amp;-s&amp; &amp;      &amp; &amp;c&amp; &amp;      &amp; \\ &amp;      &amp; &amp;  &amp; &amp;      &amp; &amp; &amp;1&amp;      &amp; \\ &amp;      &amp; &amp;  &amp; &amp;      &amp; &amp; &amp; &amp;\ddots&amp; \\ &amp;      &amp; &amp;  &amp; &amp;      &amp; &amp; &amp; &amp;      &amp;1\\\end{bmatrix}\]</span> 由 Givens 矩阵确定的线性变换称作 <strong>Givens 变换（初等旋转变换）</strong>。</p><div class="note note-info">            <p>顾名思义，Givens 变换的作用是旋转。具体而言，是在第 <span class="math inline">\(i\)</span> 和第 <span class="math inline">\(j\)</span> 维的平面上绕原点<strong>顺时针</strong>旋转 <span class="math inline">\(\theta\)</span>，其中 <span class="math inline">\(c=\cos\theta,\,s=\sin\theta\)</span>.</p>          </div><p><strong>性质</strong>：Givens 矩阵是正交矩阵，且有： <span class="math display">\[[T_{ij}(c,s)]^{-1}=[T_{ij}(c,s)]^T=T_{ij}(c,-s),\quad\det(T_{ij}(c,s))=1\]</span> <strong>定理</strong>：设 <span class="math inline">\(x=(\xi_1,\ldots,\xi_n)^T\neq0\)</span>，则存在有限个 Givens 矩阵的乘积，记作 <span class="math inline">\(T\)</span>，使得 <span class="math inline">\(Tx=|x|e_1\)</span>.</p><div class="note note-info">            <p>直观理解：依次把第 <span class="math inline">\(2,3,\ldots,n\)</span> 维转到 0 即可。</p>          </div><div class="note note-secondary">            <p>证明（构造方法）：构造 Givens 矩阵 <span class="math inline">\(T_{12}(c,s)\)</span>： <span class="math display">\[c=\frac{\xi_1}{\sqrt{\xi_1^2+\xi_2^2}}\quad s=\frac{\xi_2}{\sqrt{\xi_1^2+\xi_2^2}}\]</span> 则： <span class="math display">\[T_{12}x=\left(\sqrt{\xi_1^2+\xi_2^2},0,\xi_3,\ldots,\xi_n\right)^T\]</span> 再对 <span class="math inline">\(T_{12}x\)</span> 构造 Givens 矩阵 <span class="math inline">\(T_{13}(c,s)\)</span>： <span class="math display">\[c=\frac{\sqrt{\xi_1^2+\xi_2^2}}{\sqrt{\xi_1^2+\xi_2^2+\xi_3^2}}\quad s=\frac{\xi_3}{\sqrt{\xi_1^2+\xi_2^2+\xi_3^2}}\]</span> 则： <span class="math display">\[T_{13}(T_{12}x)=\left(\sqrt{\xi_1^2+\xi_2^2+\xi_3^2},0,0,\xi_4,\ldots,\xi_n\right)^T\]</span> 如此继续下去，最后对 <span class="math inline">\(T_{1,n-1}\cdots T_{12}x\)</span> 构造 Givens 矩阵 <span class="math inline">\(T_{1n}(c,s)\)</span>： <span class="math display">\[c=\frac{\sqrt{\xi_1^2+\cdots\xi_{n-1}^2}}{\sqrt{\xi_1^2+\cdots\xi_{n-1}^2+\xi_n^2}}\quad s=\frac{\xi_n}{\sqrt{\xi_1^2+\cdots\xi_{n-1}^2+\xi_n^2}}\]</span> 则： <span class="math display">\[T_{1n}(T_{1,n-1}\cdots T_{12}x)=\left(\sqrt{\xi_1^2+\cdots\xi_n^2},0,\ldots,0\right)^T\]</span> 令 <span class="math inline">\(T=T_{1n}T_{1,n-1}\cdots T_{12}\)</span>，则 <span class="math inline">\(Tx=|x|e_1\)</span>.</p>          </div><p><strong>推论</strong>：任给非零列向量 <span class="math inline">\(x\in\mathbb R^n\)</span> 及单位列向量 <span class="math inline">\(z\in\mathbb R^n\)</span>，则存在有限个 Givens 矩阵的乘积，记作 <span class="math inline">\(T\)</span>，使得 <span class="math inline">\(Tx=|x|z\)</span>.</p><p><strong>快速 Givens 变换</strong>：暂略。</p><h3 id="householder-矩阵与-householder-变换">Householder 矩阵与 Householder 变换</h3><p><strong>Householder 矩阵（初等反射矩阵）</strong>：设 <span class="math inline">\(u\in\mathbb R^n\)</span> 是单位列向量，定义 Householder 矩阵为： <span class="math display">\[H=I-2uu^T\]</span> 由 Householder 矩阵确定的线性变换称作 <strong>Householder 变换（初等反射变换）</strong>。</p><div class="note note-info">            <p>顾名思义，Householder 变换的作用是反射。具体而言，是对以 <span class="math inline">\(u\)</span> 为法向量的平面做反射。</p>          </div><p><strong>性质</strong>：Householder 矩阵对称、正交、对合、自逆，且行列式为 <span class="math inline">\(-1\)</span>： <span class="math display">\[H^T=H,\quad H^TH=I,\quad H^2=I,\quad H^{-1}=H,\quad \det(H)=-1\]</span> <strong>定理</strong>：任给非零列向量 <span class="math inline">\(x\in\mathbb R^n\)</span> 及单位列向量 <span class="math inline">\(z\in\mathbb R^n\)</span>，则存在 Householder 矩阵 <span class="math inline">\(H\)</span>，使得 <span class="math inline">\(Hx=|x|z\)</span>.</p><div class="note note-info">            <p>直观理解：找到 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(z\)</span> 之间对称平面，取 <span class="math inline">\(u\)</span> 为该平面的单位法向量即可。</p>          </div><div class="note note-secondary">            <p>证明（构造方法）：若 <span class="math inline">\(x=|x|z\)</span>，取单位向量 <span class="math inline">\(u\)</span> 使得 <span class="math inline">\(u\perp x\)</span>，则 <span class="math inline">\(H_u=I-2uu^T\)</span>： <span class="math display">\[H_ux=(I-2uu^T)x=x-2uu^Tx=x=|x|z\]</span> 否则，<span class="math inline">\(x\neq|x|z\)</span>，取 <span class="math inline">\(u=\frac{x-|x|z}{\left|x-|x|z\right|}\)</span>，则： <span class="math display">\[\begin{align}H_ux&amp;=\left[I-2\frac{(x-|x|z)(x-|x|z)^T}{|x-|x|z|^2}\right]x\\&amp;=x-\frac{2(x-|x|z)^Tx}{|x-|x|z|^2}(x-|x|z)\\&amp;=x-(x-|x|z)=|x|z\end{align}\]</span></p>          </div><p><strong>定理</strong>：Givens 变换是两个 Householder 变换的乘积。</p><div class="note note-info">            <p>换句话说，一次旋转操作可以分解为两次反射操作。</p>          </div><div class="note note-secondary">            <p>证明：取 <span class="math display">\[\begin{align}&amp;u=(0,\ldots,0,\sin(\theta/4),0,\ldots,0,\cos(\theta/4),0,\ldots,0)^T\\&amp;v=(0,\ldots,0,\sin(3\theta/4),0,\ldots,0,\cos(3\theta/4),0,\ldots,0)^T\end{align}\]</span> 可以验证 <span class="math inline">\(T_{ij}=H_vH_u\)</span>. 证毕。</p>          </div><div class="note note-warning">            <p>Householder 矩阵并不能由若干个 Givens 矩阵的乘积表示，因为 <span class="math inline">\(\det(H)=-1,\,\det(G)=1\)</span>.</p>          </div><h2 id="矩阵的-qr-分解">矩阵的 QR 分解</h2><h3 id="qr-分解的定义">QR 分解的定义</h3><p><strong>定义</strong>：若实/复矩阵 <span class="math inline">\(A\)</span> 能分解成正交/酉矩阵 <span class="math inline">\(Q\)</span> 与<strong>非奇异</strong>上三角矩阵 <span class="math inline">\(R\)</span> 的乘积，即 <span class="math inline">\(A=QR\)</span>，则称为 <span class="math inline">\(A\)</span> 的 QR 分解。</p><p><strong>定理</strong>：对 <span class="math inline">\(n\)</span> 阶<strong>非奇异</strong>矩阵 <span class="math inline">\(A\)</span>，QR 分解在除去相差一个对角元素的绝对值/模全等于 1 的对角矩阵外，分解唯一。</p><div class="note note-info">            <p>证明（实数情形）：设 <span class="math inline">\(A=Q_1R_1=Q_2R_2\)</span>，则 <span class="math inline">\(P=Q_2^TQ_1=R_2R_1^{-1}\)</span>.</p><p>注意到 <span class="math inline">\(P^TP=I\)</span>，所以 <span class="math inline">\(P^T=P^{-1}\)</span>. 由于 <span class="math inline">\(P\)</span> 是上三角矩阵，所以 <span class="math inline">\(P^{-1}\)</span> 是上三角矩阵，<span class="math inline">\(P^T\)</span> 是下三角矩阵，二者又相等，因此只能是对角矩阵。又 <span class="math inline">\(P^2=I\)</span>，所以对角元只能是 <span class="math inline">\(\pm1\)</span>. 那么 <span class="math inline">\(Q_1=Q_2P,\,R_1=PR_2\)</span>. 证毕。</p>          </div><p><strong>定理</strong>：对 <span class="math inline">\(m\times n\)</span> 实/复矩阵 <span class="math inline">\(A\)</span>，其 <span class="math inline">\(n\)</span> 列线性无关，则 <span class="math inline">\(A\)</span> 有分解 <span class="math inline">\(A=QR\)</span>，其中 <span class="math inline">\(Q\)</span> 是 <span class="math inline">\(m\times n\)</span> 实/复矩阵，且 <span class="math inline">\(Q^HQ=I\)</span>（即 <span class="math inline">\(Q\)</span> 是正交/酉矩阵的一部分），<span class="math inline">\(R\)</span> 是实/复非奇异上三角矩阵，且除去相差一个对角元素的绝对值/模全等于 1 的对角矩阵外，分解唯一。</p><h3 id="计算方法基于-gram-schmidt-正交化过程">计算方法：基于 Gram-Schmidt 正交化过程</h3><p>设 <span class="math inline">\(A=(a_1,\ldots,a_n)\)</span> 非奇异，对其各列实施 Gram-Schmidt 正交化过程： <span class="math display">\[\begin{cases}b_1=a_1\\b_2=a_2-k_{21}b_1\\\quad\vdots\\b_n=a_n-k_{n,n-1}b_{n-1}-\cdots-k_{n1}b_1\end{cases}\implies\begin{cases}a_1=b_1\\a_2=b_2+k_{21}b_1\\\quad\vdots\\a_n=b_n+k_{n,n-1}b_{n-1}+\cdots+k_{n1}b_1\end{cases}\]</span> 写作矩阵形式： <span class="math display">\[\begin{align}A=BK&amp;\implies(a_1,a_2,\ldots,a_n)=(b_1,b_2,\ldots,b_n)\begin{bmatrix}1&amp;k_{21}&amp;\cdots&amp;k_{n1}\\&amp;1&amp;\cdots&amp;k_{n2}\\&amp;&amp;\ddots&amp;\vdots\\&amp;&amp;&amp;1\end{bmatrix}\\&amp;\implies(a_1,a_2,\ldots,a_n)=\underbrace{\begin{bmatrix}\dfrac{b_1}{|b_1|},\dfrac{b_2}{|b_2|}\ldots,\dfrac{b_n}{|b_n|}\end{bmatrix}}_Q\underbrace{\begin{bmatrix}|b_1|&amp;&amp;&amp;\\&amp;|b_2|&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;|b_n|\end{bmatrix}\begin{bmatrix}1&amp;k_{21}&amp;\cdots&amp;k_{n1}\\&amp;1&amp;\cdots&amp;k_{n2}\\&amp;&amp;\ddots&amp;\vdots\\&amp;&amp;&amp;1\end{bmatrix}}_R\\&amp;\implies A=QR\end{align}\]</span> <div class="note note-warning">            <p>基于 Gram-Schmidt 正交化过程的 QR 分解计算方法在高阶时容易出现数值不稳定，需要用下文的基于 Givens 变换或 Householder 变换的方法计算。</p>          </div></p><h3 id="计算方法基于-givens-变换">计算方法：基于 Givens 变换</h3><p>任何 <span class="math inline">\(n\)</span> 阶<strong>实非奇异</strong>矩阵 <span class="math inline">\(A\)</span> 都可以通过左连乘 Givens 初等旋转矩阵化为上三角矩阵。</p><ol type="1"><li><p>由于 <span class="math inline">\(\det(A)\neq0\)</span>，因此 <span class="math inline">\(A\)</span> 的第一列 <span class="math inline">\((a_{11},a_{21},\ldots,a_{n1})^T\neq 0\)</span>，根据前文的定理，存在有限个 Givens 矩阵的乘积 <span class="math inline">\(T_1\)</span>，使得 <span class="math display">\[T_1(a_{11},a_{21},\ldots,a_{n1})^T=\left|(a_{11},a_{21},\ldots,a_{n1})^T\right|\cdot e_1\]</span> 将 <span class="math inline">\(T_1\)</span> 作用在 <span class="math inline">\(A\)</span> 上得： <span class="math display">\[T_1A=\left[\begin{array}{c:ccc}a_{11}^{(1)}&amp;a_{12}^{(1)}&amp;\cdots&amp;a_{1n}^{(1)}\\\hdashline0&amp;&amp;&amp;\\\vdots&amp;&amp;A^{(1)}&amp;\\0&amp;&amp;&amp;\end{array}\right]\]</span></p></li><li><p>由于 <span class="math inline">\(\det(A^{(1)})\neq0\)</span>，因此 <span class="math inline">\(A^{(1)}\)</span> 的第一列 <span class="math inline">\(\left(a_{22}^{(1)},a_{32}^{(1)},\ldots,a_{n2}^{(1)}\right)^T\neq0\)</span>，于是存在有限个 Givens 矩阵的乘积 <span class="math inline">\(T_2\)</span>，使得 <span class="math display">\[T_2\left(a_{22}^{(1)},a_{32}^{(1)},\ldots,a_{n2}^{(1)}\right)^T=\left|\left(a_{22}^{(1)},a_{32}^{(1)},\ldots,a_{n2}^{(1)}\right)^T\right|\cdot e_1\]</span> 将 <span class="math inline">\(T_2\)</span> 作用在 <span class="math inline">\(A\)</span> 上得： <span class="math display">\[T_2A^{(1)}=\left[\begin{array}{c:ccc}a_{22}^{(2)}&amp;a_{23}^{(2)}&amp;\cdots&amp;a_{2n}^{(2)}\\\hdashline0&amp;&amp;&amp;\\\vdots&amp;&amp;A^{(2)}&amp;\\0&amp;&amp;&amp;\end{array}\right]\]</span></p></li><li><p>重复上述步骤，直到第 <span class="math inline">\(n-1\)</span> 步： <span class="math display">\[T_{n-1}A^{(n-2)}=\begin{bmatrix}a_{n-1,n-1}^{(n-1)}&amp;a_{n-1,n}^{(n-1)}\\0&amp;a_{nn}^{(n-1)}\end{bmatrix}\]</span></p></li></ol><p>最后，令： <span class="math display">\[T=\begin{bmatrix}I_{n-2}&amp;O\\O&amp;T_{n-1}\end{bmatrix}\cdots\begin{bmatrix}I_{2}&amp;O\\O&amp;T_{3}\end{bmatrix}\begin{bmatrix}I_{1}&amp;O\\O&amp;T_{2}\end{bmatrix}T_1\]</span> 则： <span class="math display">\[TA=\begin{bmatrix}a_{11}^{(1)}&amp;a_{12}^{(1)}&amp;\cdots&amp;a_{1,n-1}^{(1)}&amp;a_{1n}^{(1)}\\&amp;a_{22}^{(2)}&amp;\cdots&amp;a_{2,n-1}^{(2)}&amp;a_{2n}^{(2)}\\&amp;&amp;\ddots&amp;\vdots&amp;\vdots\\&amp;&amp;&amp;a_{n-1,n-1}^{(n-1)}&amp;a_{n-1,n}^{(n-1)}\\&amp;&amp;&amp;&amp;a_{nn}^{(n-1)}\end{bmatrix}\]</span> 即 <span class="math inline">\(A=QR\)</span>，其中 <span class="math inline">\(Q=T^{-1}=T^T\)</span>，<span class="math inline">\(R\)</span> 就是上面化出来的那一大坨上三角矩阵。</p><div class="note note-info">            <p>该过程与高斯消元法有异曲同工之妙。</p>          </div><h3 id="计算方法基于-householder-变换">计算方法：基于 Householder 变换</h3><p>任何 <span class="math inline">\(n\)</span> 阶<strong>实非奇异</strong>矩阵 <span class="math inline">\(A\)</span> 都可以通过左连乘 Householder 初等反射矩阵化为上三角矩阵。</p><ol type="1"><li><p>由于 <span class="math inline">\(\det(A)\neq0\)</span>，因此 <span class="math inline">\(A\)</span> 的第一列 <span class="math inline">\((a_{11},a_{21},\ldots,a_{n1})^T\neq 0\)</span>，根据前文的定理，存在一个 Householder 矩阵 <span class="math inline">\(H_1\)</span>，使得 <span class="math display">\[H_1(a_{11},a_{21},\ldots,a_{n1})^T=\left|(a_{11},a_{21},\ldots,a_{n1})^T\right|\cdot e_1\]</span> 将 <span class="math inline">\(H_1\)</span> 作用在 <span class="math inline">\(A\)</span> 上得： <span class="math display">\[H_1A=\left[\begin{array}{c:ccc}a_{11}^{(1)}&amp;a_{12}^{(1)}&amp;\cdots&amp;a_{1n}^{(1)}\\\hdashline0&amp;&amp;&amp;\\\vdots&amp;&amp;A^{(1)}&amp;\\0&amp;&amp;&amp;\end{array}\right]\]</span></p></li><li><p>由于 <span class="math inline">\(\det(A^{(1)})\neq0\)</span>，因此 <span class="math inline">\(A^{(1)}\)</span> 的第一列 <span class="math inline">\(\left(a_{22}^{(1)},a_{32}^{(1)},\ldots,a_{n2}^{(1)}\right)^T\neq0\)</span>，于是存在一个 Householder 矩阵 <span class="math inline">\(H_2\)</span>，使得 <span class="math display">\[H_2\left(a_{22}^{(1)},a_{32}^{(1)},\ldots,a_{n2}^{(1)}\right)^T=\left|\left(a_{22}^{(1)},a_{32}^{(1)},\ldots,a_{n2}^{(1)}\right)^T\right|\cdot e_1\]</span> 将 <span class="math inline">\(H_2\)</span> 作用在 <span class="math inline">\(A\)</span> 上得： <span class="math display">\[H_2A^{(1)}=\left[\begin{array}{c:ccc}a_{22}^{(2)}&amp;a_{23}^{(2)}&amp;\cdots&amp;a_{2n}^{(2)}\\\hdashline0&amp;&amp;&amp;\\\vdots&amp;&amp;A^{(2)}&amp;\\0&amp;&amp;&amp;\end{array}\right]\]</span></p></li><li><p>重复上述步骤，直到第 <span class="math inline">\(n-1\)</span> 步： <span class="math display">\[H_{n-1}A^{(n-2)}=\begin{bmatrix}a_{n-1,n-1}^{(n-1)}&amp;a_{n-1,n}^{(n-1)}\\0&amp;a_{nn}^{(n-1)}\end{bmatrix}\]</span></p></li></ol><p>最后，令： <span class="math display">\[H=\begin{bmatrix}I_{n-2}&amp;O\\O&amp;H_{n-1}\end{bmatrix}\cdots\begin{bmatrix}I_{2}&amp;O\\O&amp;H_{3}\end{bmatrix}\begin{bmatrix}I_{1}&amp;O\\O&amp;H_{2}\end{bmatrix}H_1\]</span> 则： <span class="math display">\[HA=\begin{bmatrix}a_{11}^{(1)}&amp;a_{12}^{(1)}&amp;\cdots&amp;a_{1,n-1}^{(1)}&amp;a_{1n}^{(1)}\\&amp;a_{22}^{(2)}&amp;\cdots&amp;a_{2,n-1}^{(2)}&amp;a_{2n}^{(2)}\\&amp;&amp;\ddots&amp;\vdots&amp;\vdots\\&amp;&amp;&amp;a_{n-1,n-1}^{(n-1)}&amp;a_{n-1,n}^{(n-1)}\\&amp;&amp;&amp;&amp;a_{nn}^{(n-1)}\end{bmatrix}\]</span> 即 <span class="math inline">\(A=QR\)</span>，其中 <span class="math inline">\(Q=H^{-1}=H^T\)</span>，<span class="math inline">\(R\)</span> 就是上面化出来的那一大坨上三角矩阵。</p><h2 id="hessenberg-矩阵的正交相似">Hessenberg 矩阵的正交相似</h2><p>暂略。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]4.1矩阵的三角分解</title>
    <link href="/blog-main/2023/11/21/%E7%9F%A9%E9%98%B5%E8%AE%BA-4-1%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%89%E8%A7%92%E5%88%86%E8%A7%A3/"/>
    <url>/blog-main/2023/11/21/%E7%9F%A9%E9%98%B5%E8%AE%BA-4-1%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%89%E8%A7%92%E5%88%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h2 id="高斯消元法">高斯消元法</h2><p><strong>初等矩阵</strong>：初等行变换可以用左乘初等矩阵表示：</p><ul><li><p>交换两行：设 <span class="math inline">\(u_{i,j}=(e_i-e_j)/\sqrt{2}\)</span>， <span class="math display">\[I-2u_{i,j}u_{i,j}^T\]</span></p></li><li><p>第 <span class="math inline">\(i\)</span> 行乘以 <span class="math inline">\(\alpha\)</span> 加到第 <span class="math inline">\(j\)</span> 行： <span class="math display">\[I+\alpha e_je_i^T\]</span></p></li><li><p>第 <span class="math inline">\(i\)</span> 行乘以 <span class="math inline">\(\alpha\)</span>： <span class="math display">\[I+(\alpha-1)e_ie_i^T\]</span></p></li></ul><p><strong>消元因子</strong>：第 <span class="math inline">\(k\)</span> 步消元时，若 <span class="math inline">\(a_{kk}^{(k)}\neq 0\)</span>，则乘上消元因子 <span class="math inline">\(m_{ik}=a_{ik}^{(k)}/a_{kk}^{(k)}\)</span> 减到第 <span class="math inline">\(i\)</span> 行，其中 <span class="math inline">\(i=k+1,\ldots,n\)</span>. 这相当于左乘矩阵： <span class="math display">\[L_k=I-\left(\sum_{i=k+1}^nm_{ik}e_i\right)e_k^T=I-l_ke_k^T\]</span> 其中 <span class="math inline">\(l_k=(0,\ldots,0,m_{k+1,k},m_{k+2,k},\ldots,m_{nk})^T\)</span>.</p><p><strong>定理</strong>：消元过程不改变矩阵的顺序主子式。</p><p><strong>引理</strong>：约化的主元素 <span class="math inline">\(a_{ii}^{(i)}\neq 0\)</span> 的充要条件是矩阵 <span class="math inline">\(A\)</span> 的顺序主子式 <span class="math inline">\(D_i\neq 0\,(i=1,\ldots,k)\)</span>.</p><p><strong>推论</strong>：若矩阵 <span class="math inline">\(A\)</span> 的顺序主子式 <span class="math inline">\(D_i\neq 0\,(i=1,\ldots,k)\)</span>，则： <span class="math display">\[a_{11}^{(1)}=D_1,\quad a_{ii}^{(i)}=D_i/D_{i-1}\]</span></p><p><strong>定理</strong>：若 <span class="math inline">\(A\)</span> 是正定矩阵，则一次消元后剩下的矩阵依旧是正定的。</p><p><strong>定理</strong>：若 <span class="math inline">\(A\)</span> 严格对角占优，则一次消元后剩下的矩阵依旧是严格对角占优的。</p><h2 id="lu-分解和-ldu-分解">LU 分解和 LDU 分解</h2><p><strong>LU 分解</strong>：高斯消元的过程可以用 LU 分解表示： <span class="math display">\[\underbrace{L_{n-1}L_{n-2}\cdots L_2L_1}_{L^{-1}} A=U\implies A=LU\]</span> 其中 <span class="math inline">\(L\)</span> 为<strong>单位</strong>下三角矩阵，<span class="math inline">\(U\)</span> 为上三角矩阵。</p><p><strong>LDU 分解</strong>：<span class="math inline">\(A=LDU\)</span>，其中 <span class="math inline">\(L\)</span> 为<strong>单位</strong>下三角矩阵，<span class="math inline">\(U\)</span> 为<strong>单位</strong>上三角矩阵，<span class="math inline">\(D\)</span> 为对角矩阵。</p><p><strong>定理</strong>：设 <span class="math inline">\(A\)</span> 是 <span class="math inline">\(n\)</span> 阶方阵，当且仅当 <span class="math inline">\(A\)</span> 的顺序主子式 <span class="math inline">\(\Delta_i\neq 0\)</span> 时，<span class="math inline">\(A\)</span> 可唯一分解为 <span class="math inline">\(A=LDU\)</span>，其中： <span class="math display">\[D=\text{diag}(d_1,\ldots,d_n),\quad d_k=\frac{\Delta_k}{\Delta_{k-1}}\]</span> <strong>推论</strong>：<span class="math inline">\(n\)</span> 阶非奇异方阵 <span class="math inline">\(A\)</span> 有 LU 分解 <span class="math inline">\(A=LU\)</span> 的充要条件是顺序主子式 <span class="math inline">\(\Delta_i\neq 0\)</span>.</p><p><strong>带有置换矩阵的三角分解</strong>：设 <span class="math inline">\(A\)</span> 是 <span class="math inline">\(n\)</span> 阶非奇异方阵，则存在置换矩阵 <span class="math inline">\(P\)</span>，使得： <span class="math display">\[PA=L\hat U=LDU\]</span> 其中 <span class="math inline">\(L\)</span> 为<strong>单位</strong>下三角矩阵，<span class="math inline">\(\hat U\)</span> 为上三角矩阵，<span class="math inline">\(U\)</span> 为<strong>单位</strong>上三角矩阵，<span class="math inline">\(D\)</span> 为对角矩阵。</p><h2 id="doolittle-分解和-crout-分解">Doolittle 分解和 Crout 分解</h2><p><strong>Doolittle 分解</strong>：把 LDU 分解中的 <span class="math inline">\(DU\)</span> 结合起来用 <span class="math inline">\(\hat U\)</span> 表示，得到唯一的分解： <span class="math display">\[A=L(DU)=L\hat U\]</span> <strong>Crout 分解</strong>：把 LDU 分解中的 <span class="math inline">\(LD\)</span> 结合起来用 <span class="math inline">\(\hat L\)</span> 表示，得到唯一的分解： <span class="math display">\[A=(LD)U=\hat LU\]</span> <strong>计算方法（以 Doolittle 分解为例）</strong>：比较等式两边的元素逐行逐列求解 <span class="math inline">\(L,U\)</span> 各元素： <span class="math display">\[\begin{bmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\a_{21}&amp;a_{22}&amp;\cdots&amp;a_{2n}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\a_{n1}&amp;a_{n2}&amp;\cdots&amp;a_{nn}\\\end{bmatrix}=\begin{bmatrix}1&amp;&amp;&amp;\\l_{21}&amp;1&amp;&amp;\\\vdots&amp;\vdots&amp;\ddots&amp;\\l_{n1}&amp;l_{n2}&amp;\cdots&amp;1\\\end{bmatrix}\begin{bmatrix}u_{11}&amp;u_{12}&amp;\cdots&amp;u_{1n}\\&amp;u_{22}&amp;\cdots&amp;u_{2n}\\&amp;&amp;\ddots&amp;\vdots\\&amp;&amp;&amp;u_{nn}\\\end{bmatrix}\]</span> 计算顺序：</p><p><img src="calc.png" width=50% /></p><h2 id="对称正定矩阵的-cholesky-分解">对称正定矩阵的 Cholesky 分解</h2><p><strong>Cholesky 分解</strong>： <span class="math display">\[A=L\tilde D^2L^T\quad A=LDL^T\quad A=LL^T\]</span> 第二种表示中，<span class="math inline">\(D\)</span> 的对角元都大于零；第三种表示中，<span class="math inline">\(L\)</span> 的对角元都大于零。</p><p><strong>计算方法</strong>：与上一节 Doolittle 分解的计算类似，逐行逐列计算即可： <span class="math display">\[\begin{bmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\a_{21}&amp;a_{22}&amp;\cdots&amp;a_{2n}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\a_{n1}&amp;a_{n2}&amp;\cdots&amp;a_{nn}\\\end{bmatrix}=\begin{bmatrix}l_{11}&amp;&amp;&amp;\\l_{21}&amp;l_{22}&amp;&amp;\\\vdots&amp;\vdots&amp;\ddots&amp;\\l_{n1}&amp;l_{n2}&amp;\cdots&amp;l_{nn}\\\end{bmatrix}\begin{bmatrix}l_{11}&amp;l_{12}&amp;\cdots&amp;l_{1n}\\&amp;l_{22}&amp;\cdots&amp;l_{2n}\\&amp;&amp;\ddots&amp;\vdots\\&amp;&amp;&amp;l_{nn}\\\end{bmatrix}\]</span> 可得： <span class="math display">\[l_{jj}=\left(a_{jj}-\sum_{k=1}^{j-1}l_{jk}^2\right)^{1/2},\quad l_{ij}=\left(a_{ij}-\sum_{k=1}^{j-1}l_{ik}l_{jk}\right)/l_{jj}\]</span> 该计算方法的缺点是有开方运算，在计算机中可能出现根号下负数。</p><p><strong>改进的 Cholesky 分解</strong>：所谓改进的 Cholesky 分解，其实就是指 <span class="math inline">\(A=LDL^T\)</span>，其计算过程不涉及开方： <span class="math display">\[\begin{bmatrix}a_{11}&amp;a_{12}&amp;\cdots&amp;a_{1n}\\a_{21}&amp;a_{22}&amp;\cdots&amp;a_{2n}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\a_{n1}&amp;a_{n2}&amp;\cdots&amp;a_{nn}\\\end{bmatrix}=\begin{bmatrix}1&amp;&amp;&amp;\\l_{21}&amp;1&amp;&amp;\\\vdots&amp;\vdots&amp;\ddots&amp;\\l_{n1}&amp;l_{n2}&amp;\cdots&amp;1\\\end{bmatrix}\begin{bmatrix}d_1&amp;d_1l_{12}&amp;\cdots&amp;d_1l_{1n}\\&amp;d_2&amp;\cdots&amp;d_2l_{2n}\\&amp;&amp;\ddots&amp;\vdots\\&amp;&amp;&amp;d_n\\\end{bmatrix}\]</span> 逐行逐列计算可得： <span class="math display">\[l_{ij}=\left(a_{ij}-\sum_{k=1}^{j-1}l_{ik}d_kl_{jk}\right)/d_j,\quad d_i=a_{ii}-\sum_{k=1}^{j-1}l_{ik}^2d_k\]</span></p><h2 id="分块矩阵的拟-lu-分解和拟-ldu-分解">分块矩阵的拟 LU 分解和拟 LDU 分解</h2><p>只考虑 2 阶的分块矩阵： <span class="math display">\[A=\begin{bmatrix}A_{11}&amp;A_{12}\\A_{21}&amp;A_{22}\end{bmatrix}\]</span> 若 <span class="math inline">\(A_{11}\)</span> 可逆，设： <span class="math display">\[L=\begin{bmatrix}I_{n_1}&amp;0\\-A_{21}A_{11}^{-1}&amp;I_{n_2}\end{bmatrix}\]</span> 则： <span class="math display">\[LA=\begin{bmatrix}A_{11}&amp;A_{12}\\0&amp;A_{22}-A_{21}A_{11}^{-1}A_{12}\end{bmatrix}\]</span> 若 <span class="math inline">\(A_{22}\)</span> 可逆，可右乘矩阵（作列变换）得到类似的结果。</p><p><strong>求逆引理（Woodbury 公式）</strong>： <span class="math display">\[(A+BC)^{-1}=A^{-1}-A^{-1}B(I+CA^{-1}B)^{-1}CA^{-1}\]</span> <div class="note note-secondary">            <p>证明：考虑方程 <span class="math inline">\((A+BC)x=b\)</span>，令 <span class="math inline">\(y=Cx\)</span>，则： <span class="math display">\[\begin{bmatrix}A&amp;B\\-C&amp;0\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}b\\0\end{bmatrix}\]</span> 利用高斯消元法：</p><p><img src="gauss.png" width=50% /></p><p>故解得： <span class="math display">\[x=(A^{-1}-A^{-1}B(I+CA^{-1}B)^{-1}CA^{-1})b\]</span> 又 <span class="math inline">\(x=(A+BC)^{-1}b\)</span>，由 <span class="math inline">\(b\)</span> 的任意性可知命题成立。证毕。</p>          </div></p><p><strong>推论</strong>： <span class="math display">\[(A+BD^{-1}C)^{-1}=A^{-1}-A^{-1}B(D+CA^{-1}B)^{-1}CA^{-1}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]3.3矩阵的微分和积分</title>
    <link href="/blog-main/2023/11/15/%E7%9F%A9%E9%98%B5%E8%AE%BA-3-3%E7%9F%A9%E9%98%B5%E7%9A%84%E5%BE%AE%E5%88%86%E5%92%8C%E7%A7%AF%E5%88%86/"/>
    <url>/blog-main/2023/11/15/%E7%9F%A9%E9%98%B5%E8%AE%BA-3-3%E7%9F%A9%E9%98%B5%E7%9A%84%E5%BE%AE%E5%88%86%E5%92%8C%E7%A7%AF%E5%88%86/</url>
    
    <content type="html"><![CDATA[<h2 id="矩阵的微分">矩阵的微分</h2><p><strong>定义</strong>：若 <span class="math inline">\(A(t)=\left(a_{ij}(t)\right)\in\mathbb C^{m\times n}\)</span> 的每个元素 <span class="math inline">\(a_{ij}(t)\)</span> 都是 <span class="math inline">\(t\)</span> 的可微函数，则 <span class="math inline">\(A(t)\)</span> 关于 <span class="math inline">\(t\)</span> 的导数定义为： <span class="math display">\[\frac{\mathrm d}{\mathrm dt}A(t)=A&#39;(t)=\left(a&#39;_{ij}(t)\right)_{m\times n}\]</span> <strong>性质 1</strong>. <span class="math display">\[\frac{\mathrm d}{\mathrm dt}(A(t)+B(t))=\frac{\mathrm d}{\mathrm dt}A(t)+\frac{\mathrm d}{\mathrm dt}B(t)\]</span> <strong>性质 2</strong>. <span class="math display">\[\frac{\mathrm d}{\mathrm dt}(A(t)B(t))=\frac{\mathrm d}{\mathrm dt}A(t)\cdot B(t)+A(t)\cdot\frac{\mathrm d}{\mathrm dt}B(t)\]</span> <strong>推论</strong>. 若 <span class="math inline">\(C(t)=A(t)B(t)\)</span>，则： <span class="math display">\[\mathrm dC=\mathrm dA\cdot B+A\cdot\mathrm dB\]</span> 其中，<span class="math inline">\(\mathrm dA=(\mathrm dA/\mathrm dt)\cdot dt\)</span>，<span class="math inline">\(\mathrm dB=(\mathrm dB/\mathrm dt)\cdot dt\)</span>.</p><p><strong>例（逆矩阵的导数）</strong>： <span class="math display">\[\frac{\mathrm d}{\mathrm dt}\left(A(t)^{-1}\right)=-A(t)^{-1}\frac{\mathrm dA(t)}{\mathrm dt}A(t)^{-1}\]</span> <div class="note note-secondary">            <p>证明：由于 <span class="math inline">\(A(t)\cdot A(t)^{-1}=I\)</span>，两边求导得： <span class="math display">\[\frac{\mathrm d}{\mathrm dt}A(t)\cdot A(t)^{-1}+A(t)\cdot\frac{\mathrm d}{\mathrm dt}\left(A(t)^{-1}\right)=0\]</span> 故： <span class="math display">\[\frac{\mathrm d}{\mathrm dt}\left(A(t)^{-1}\right)=-A(t)^{-1}\frac{\mathrm dA(t)}{\mathrm dt}A(t)^{-1}\]</span></p>          </div></p><p><strong>性质 3</strong>. <span class="math display">\[\frac{\mathrm d}{\mathrm dt}(\alpha(t)A(t))=\alpha&#39;(t)A(t)+\alpha(t)\cdot\frac{\mathrm d}{\mathrm dt}A(t)\]</span> <strong>性质 4</strong>. 若 <span class="math inline">\(A(t)\)</span> 和 <span class="math inline">\(\frac{\mathrm d}{\mathrm dt}A(t)\)</span> <strong>可交换</strong>，<span class="math inline">\(f(z)\)</span> 是与 <span class="math inline">\(t\)</span> 无关的一元解析函数，则： <span class="math display">\[\frac{\mathrm d}{\mathrm dt}f(A(t))=f&#39;(A(t))\cdot\frac{\mathrm d}{\mathrm dt}A(t)\]</span> <div class="note note-secondary">            <p>形式证明（不严格）：若将 <span class="math inline">\(f(z)\)</span> 展开为幂级数的形式 <span class="math inline">\(f(z)=\sum_{k=0}^\infty c_kz^k\)</span>，则根据可交换条件有： <span class="math display">\[\frac{\mathrm d}{\mathrm dt}A^k(t)=\sum_{i=0}^{k-1}A^i(t)\cdot\frac{\mathrm d}{\mathrm dt}A(t)\cdot A^{k-1-i}(t)=kA^{k-1}(t)\cdot\frac{\mathrm d}{\mathrm dt}A(t)\]</span> 代入矩阵函数得： <span class="math display">\[\frac{\mathrm d}{\mathrm dt}f(A(t))=\sum_{k=0}^\infty c_k\left(kA^{k-1}(t)\cdot\frac{\mathrm d}{\mathrm dt}A(t)\right)=f&#39;(A(t))\cdot\frac{\mathrm d}{\mathrm dt}A(t)\]</span></p>          </div></p><p><strong>性质 5（迹函数求导基本定理）</strong>. 若 <span class="math inline">\(A(t)\)</span> 和 <span class="math inline">\(B(t)\)</span> 可交换，<span class="math inline">\(f(z)\)</span> 是与 <span class="math inline">\(t\)</span> 无关的一元解析函数，则： <span class="math display">\[\frac{\mathrm d}{\mathrm dt}\text{tr}\big(f(A(t))\cdot B(t)\big)=\text{tr}\left(f&#39;(A(t))\cdot\frac{\mathrm d}{\mathrm dt}A(t)B(t)\right)+\text{tr}\left(f(A(t))\cdot\frac{\mathrm d}{\mathrm dt}B(t)\right)\]</span> <strong>推论</strong>：取 <span class="math inline">\(B(t)=I\)</span>，则有： <span class="math display">\[\frac{\mathrm d}{\mathrm dt}\text{tr}(f(A(t)))=\text{tr}\left(f&#39;(A(t))\cdot\frac{\mathrm d}{\mathrm dt}A(t)\right)\]</span></p><h2 id="矩阵的积分">矩阵的积分</h2><p><strong>定义</strong>：若矩阵 <span class="math inline">\(A(t)=(a_{ij}(t))\in\mathbb C^{m\times n}\)</span> 的每个元素 <span class="math inline">\(a_{ij}(t)\)</span> 都在 <span class="math inline">\([t_0,t]\)</span> 上可积，则称 <span class="math inline">\(A(t)\)</span> 可积，记为： <span class="math display">\[\int_{t_0}^t A(\tau)\mathrm d\tau=\left(\int_{t_0}^ta_{ij}(\tau)\mathrm d\tau\right)_{m\times n}\]</span> <strong>性质 1</strong>. <span class="math display">\[\int_{t_0}^t[A(\tau)+B(\tau)]\mathrm d\tau=\int_{t_0}^tA(\tau)\mathrm d\tau+\int_{t_0}^tB(\tau)\mathrm d\tau\]</span> <strong>性质 2</strong>. <span class="math display">\[\begin{align}&amp;\int_{t_0}^t[A\cdot B(\tau)]\mathrm d\tau=A\cdot\left[\int_{t_0}^tB(\tau)\mathrm d\tau\right]\\&amp;\int_{t_0}^t[A(\tau)\cdot B]\mathrm d\tau=\left[\int_{t_0}^tA(\tau)\mathrm d\tau\right]\cdot B\end{align}\]</span> <strong>性质 3</strong>. <span class="math display">\[\frac{\mathrm d}{\mathrm dt}\int_{t_0}^tA(\tau)\mathrm d\tau=A(t)\]</span> <strong>性质 4</strong>. <span class="math display">\[\int_{t_0}^{t_1}A&#39;(\tau)\mathrm d\tau=A(t_1)-A(t_0)\]</span></p><h2 id="其他微分概念">其他微分概念</h2><h3 id="函数对矩阵的导数">函数对矩阵的导数</h3><div class="note note-warning">            <p>前文涉及到的矩阵函数是从矩阵到矩阵的一个映射，本节探讨的函数是关于矩阵各个元素的函数，即从矩阵映射到数，与矩阵函数不同。</p>          </div><div class="note note-info">            <p><strong>矩阵元素与矩阵符号的代数关系</strong></p><p>为了方便后续推导，这里列出矩阵元素与矩阵符号的代数关系式，可以避免将矩阵的各个分量列出来的繁琐。</p><p>我们约定 <span class="math inline">\(e_i\)</span> 表示第 <span class="math inline">\(i\)</span> 个分量为 1、其余分量为 0 的单位列向量，其维数由上下文确定，即同一个式子中出现的 <span class="math inline">\(e_i,e_j\)</span> 可能是不同维数的。</p><ul><li><p>用分量表示矩阵： <span class="math display">\[A=\sum_{i=1}^m\sum_{j=1}^na_{ij}e_ie_j^T\]</span></p></li><li><p>用矩阵表示分量： <span class="math display">\[a_{ij}=e_i^TAe_j\]</span></p></li><li><p>矩阵逐列拉成向量： <span class="math display">\[\text{Vec}(A)=\sum_{i=1}^m\sum_{j=1}^na_{ij}e_j\otimes e_i=\sum_{i=1}^mA^T e_i\otimes e_i=\sum_{j=1}^ne_j\otimes Ae_j\]</span></p></li><li><p>用拉成向量的矩阵表示分量： <span class="math display">\[a_{ij}=(e_j\otimes e_i)^T\text{Vec}(A)\]</span></p></li></ul>          </div><p><strong>定义</strong>：设 <span class="math inline">\(X=(\xi_{ij})_{m\times n}\)</span>，<span class="math inline">\(f(X)\)</span> 为 <span class="math inline">\(mn\)</span> 元函数： <span class="math display">\[f(X)=f(\xi_{11},\ldots,\xi_{1n},\xi_{21},\ldots,\xi_{2n},\ldots,\xi_{m1},\ldots,\xi_{mn})\]</span> 定义 <span class="math inline">\(f(X)\)</span> 对矩阵 <span class="math inline">\(X\)</span> 的导数为： <span class="math display">\[\frac{\mathrm df}{\mathrm dX}=\left(\frac{\partial f}{\partial \xi_{ij}}\right)_{m\times n}=\begin{bmatrix}\frac{\partial f}{\partial \xi_{11}}&amp;\cdots&amp;\frac{\partial f}{\partial \xi_{1n}}\\\vdots&amp;\ddots&amp;\vdots\\\frac{\partial f}{\partial \xi_{m1}}&amp;\cdots&amp;\frac{\partial f}{\partial \xi_{mn}}\\\end{bmatrix}=\sum_{i=1}^m\sum_{j=1}^n\frac{\partial f}{\partial \xi_{ij}}e_ie_j^T\]</span> <strong>微分形式</strong>： <span class="math display">\[\frac{\mathrm df}{\mathrm dX}=A\iff \mathrm df=\text{tr}(A^T\cdot\mathrm dX)=\text{tr}(A\cdot \mathrm dX^T)\]</span> <div class="note note-secondary">            <p>证明： <span class="math display">\[\begin{align}\mathrm df&amp;=\sum_{i=1}^m\sum_{j=1}^n\frac{\partial f}{\partial\xi_{ij}}\mathrm d\xi_{ij}\\&amp;=\sum_{i=1}^m\sum_{j=1}^n\left(e_i^T\frac{\mathrm df}{\mathrm dX}e_j\right)\left(e_i^T\cdot \mathrm dX\cdot e_j\right)\\&amp;=\sum_{i=1}^m\sum_{j=1}^ne_j^T\left(\frac{\mathrm df}{\mathrm dX}\right)^Te_ie_i^T\cdot\mathrm dX\cdot e_j\\&amp;=\sum_{j=1}^ne_j^T\left(\frac{\mathrm df}{\mathrm dX}\right)^T\left(\sum_{i=1}^me_ie_i^T\right)\cdot\mathrm dX\cdot e_j\\&amp;=\sum_{j=1}^ne_j^T\left(\frac{\mathrm df}{\mathrm dX}\right)^T\mathrm dX\cdot e_j\\&amp;=\text{tr}\left(\left(\frac{\mathrm df}{\mathrm dX}\right)^T\mathrm dX\right)\\&amp;=\text{tr}(A^T\cdot dX)\end{align}\]</span> 证毕。</p>          </div></p><div class="note note-success">            <p>非常重要！！！这个定理告诉我们：<strong>要求 <span class="math inline">\(\mathrm df/\mathrm dX\)</span>，只需要将 <span class="math inline">\(\mathrm df\)</span> 写作形如 <span class="math inline">\(\text{tr}(A^T\mathrm dX)\)</span> 的形式，那么 <span class="math inline">\(A\)</span> 就是结果</strong>。以下所有内容都围绕这一点展开。</p>          </div><p><strong>性质 1（链式法则）</strong>：设 <span class="math inline">\(f(x)\)</span> 是向量 <span class="math inline">\(x\)</span> 的函数，<span class="math inline">\(x\)</span> 是标量 <span class="math inline">\(t\)</span> 的函数 <span class="math inline">\(x=x(t)\)</span>，则： <span class="math display">\[\frac{\mathrm df}{\mathrm dt}=\frac{\mathrm dx^T}{\mathrm dt}\cdot\frac{\mathrm df}{\mathrm dx}\]</span> <strong>性质 2（链式法则）</strong>：设 <span class="math inline">\(f(X)\)</span> 是 <span class="math inline">\(m\times n\)</span> 矩阵 <span class="math inline">\(X\)</span> 各元素的函数，而 <span class="math inline">\(X\)</span> 又是标量 <span class="math inline">\(t\)</span> 的函数，则： <span class="math display">\[\frac{\mathrm df}{\mathrm dt}=\text{tr}\left(\frac{\mathrm dX^T}{\mathrm dt}\cdot\frac{\mathrm df}{\mathrm dX}\right)\]</span> <div class="note note-secondary">            <p>证明：根据微分形式： <span class="math display">\[\mathrm df=\text{tr}\left(\left(\frac{\mathrm df}{\mathrm dX}\right)^T\mathrm dX\right),\quad \mathrm dX=\frac{\mathrm dX}{\mathrm dt}\mathrm dt\]</span> 有： <span class="math display">\[\mathrm df=\text{tr}\left(\left(\frac{\mathrm df}{\mathrm dX}\right)^T\frac{\mathrm dX}{\mathrm dt}\mathrm dt\right)=\text{tr}\left(\left(\frac{\mathrm df}{\mathrm dX}\right)^T\frac{\mathrm dX}{\mathrm dt}\right)\mathrm dt=\text{tr}\left(\frac{\mathrm dX^T}{\mathrm dt}\cdot\frac{\mathrm df}{\mathrm dX}\right)\mathrm dt\]</span> 证毕。</p>          </div></p><p><strong>例</strong>：关于迹函数的导数： <span class="math display">\[\begin{align}&amp;\frac{\mathrm d}{\mathrm dX}\text{tr}(X^TA)=A\\&amp;\frac{\mathrm d}{\mathrm dX}\text{tr}(A^TX)=A\\&amp;\frac{\mathrm d}{\mathrm dX}\text{tr}(AX)=A^T\end{align}\]</span> <div class="note note-secondary">            <p>证明：微分是线性算子，可以放入 <span class="math inline">\(\text{tr}(\cdot)\)</span>，因此： <span class="math display">\[\mathrm d(\text{tr}(X^TA))=\text{tr}(\mathrm d(X^TA))=\text{tr}(\mathrm dX^T A)=\text{tr}(A^T\mathrm dX)\]</span> 根据微分形式，<span class="math inline">\(\mathrm d(\text{tr}(X^TA))/\mathrm dX=A\)</span>. 其他两式类似。证毕。</p>          </div></p><p><strong>例</strong>：设 <span class="math inline">\(X\)</span> 为可逆矩阵，<span class="math inline">\(f(X)=\text{tr}(AX^{-1})\)</span>，则： <span class="math display">\[\frac{\mathrm d}{\mathrm dX}f(X)=-(X^{-1}AX^{-1})^T\]</span> <div class="note note-secondary">            <p>证明： <span class="math display">\[\mathrm df(X)=\text{tr}(\mathrm d(AX^{-1}))=\text{tr}(A\cdot\mathrm dX^{-1})\]</span> 由于 <span class="math inline">\(XX^{-1}=I\)</span>，两边取微分得：<span class="math inline">\(\mathrm dX\cdot X^{-1}+X\cdot\mathrm dX^{-1}=0\)</span>，故 <span class="math inline">\(\mathrm dX^{-1}=-X^{-1}\cdot\mathrm dX\cdot X^{-1}\)</span>，因此： <span class="math display">\[\mathrm df(X)=\text{tr}\left(A\cdot(-X^{-1}\mathrm dX X^{-1})\right)=\text{tr}\left((-X^{-1}AX^{-1})\mathrm dX\right)\]</span> 根据微分形式，<span class="math inline">\(\mathrm df(X)/\mathrm dX=(-X^{-1}AX^{-1})^T\)</span>. 证毕。</p>          </div></p><p><strong>例</strong>：设 <span class="math inline">\(f(x)=x^TAx\)</span>，则： <span class="math display">\[\frac{\mathrm df}{\mathrm dx}=(A+A^T)x\]</span> <div class="note note-secondary">            <p>证明： <span class="math display">\[\mathrm df=\mathrm d(x^TAx)=\mathrm dx^TAx+x^TA\mathrm dx=x^T A^T\mathrm dx+x^TA\mathrm dx=x^T(A^T+A)\mathrm dx=\text{tr}\left(((A+A^T)x)^T\cdot\mathrm dx\right)\]</span> 根据微分形式，<span class="math inline">\(\mathrm df/\mathrm dx=(A+A^T)x\)</span>. 证毕。</p>          </div></p><p><img src="table1.png" width=60% /></p><p><strong>例（行列式求导）</strong>：设矩阵 <span class="math inline">\(X\)</span> 非奇异，则： <span class="math display">\[\frac{\mathrm d(|X|)}{\mathrm dX}=|X|(X^{-1})^T\]</span></p><p>或有微分形式： <span class="math display">\[\mathrm d|X|=|X|\text{tr}(X^{-1}\mathrm dX)=\text{tr}(|X|X^{-1}\mathrm dX)\]</span> <div class="note note-secondary">            <p>证明：设 <span class="math inline">\(c_{ij}\)</span> 为 <span class="math inline">\(x_{ij}\)</span> 的代数余子式，根据行列式定义知： <span class="math display">\[|X|=\sum_{i=1}^nc_{ij}x_{ij},\quad\forall j\]</span> 由于 <span class="math inline">\(c_{ij}\)</span> 与 <span class="math inline">\(x_{ij}\)</span> 无关，所以有 <span class="math inline">\(\partial|X|/\partial x_{ij}=c_{ij}\)</span>，于是有微分： <span class="math display">\[\mathrm d|X|=\sum_{i=1}^n\sum_{j=1}^n c_{ij}\mathrm dx_{ij}=\text{tr}(X^\ast\mathrm dX)\]</span> 其中 <span class="math inline">\(X^\ast\)</span> 是 <span class="math inline">\(X\)</span> 的伴随矩阵，由于 <span class="math inline">\(X^\ast=|X|X^{-1}\)</span>，因此： <span class="math display">\[\mathrm d|X|=\text{tr}(|X|X^{-1}\mathrm dX)\]</span> 证毕。</p>          </div></p><p><strong>例</strong>：对于任意非奇异矩阵 <span class="math inline">\(X\)</span>，有： <span class="math display">\[\frac{\mathrm d(\ln|X|)}{\mathrm dX}=(X^T)^{-1}\]</span> <strong>例</strong>：对于任意 <span class="math inline">\(X\in\mathbb R^{m\times n}\)</span>， <span class="math display">\[\frac{\mathrm d(\ln|\delta I+X^TX|)}{\mathrm dX}=2X(\delta I+X^TX)\]</span> <img src="table2.png" width=70% /></p><h3 id="矩阵对矩阵的导数">矩阵对矩阵的导数</h3><p><strong>直积（Kronecker 积）</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n},\,B\in\mathbb C^{p\times q}\)</span>，称如下分块矩阵为 <span class="math inline">\(A\)</span> 与 <span class="math inline">\(B\)</span> 的直积： <span class="math display">\[A\otimes B=\begin{bmatrix}a_{11}B&amp;a_{12}B&amp;\cdots&amp;a_{1n}B\\a_{21}B&amp;a_{22}B&amp;\cdots&amp;a_{2n}B\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\a_{m1}B&amp;a_{m2}B&amp;\cdots&amp;a_{mn}B\\\end{bmatrix}\in\mathbb C^{mp\times nq}\]</span> 特别地，若 <span class="math inline">\(A\in\mathbb C^{m\times 1},\,B\in\mathbb C^{n\times 1}\)</span>，则 <span class="math inline">\(A\otimes B^T=AB^T\)</span>.</p><p><strong>定义</strong>：设有关于 <span class="math inline">\(X=(\xi_{ij})_{m\times n}\)</span> 的 <span class="math inline">\(mn\)</span> 元函数： <span class="math display">\[f_{ij}(X)=f_{ij}(\xi_{11},\ldots,\xi_{1n},\ldots,\xi_{m1},\ldots,\xi_{mn}),\quad i=1,\ldots,r,\,j=1,\ldots,s\]</span> 定义矩阵： <span class="math display">\[F(X)=\begin{bmatrix}f_{11}&amp;\cdots&amp;f_{1s}\\\vdots&amp;\ddots&amp;\vdots\\f_{r1}&amp;\cdots&amp;f_{rs}\end{bmatrix}\]</span> 定义 <span class="math inline">\(F(X)\)</span> 的导数如下： <span class="math display">\[\frac{\mathrm dF}{\mathrm dX}=\begin{bmatrix}\frac{\partial F}{\partial \xi_{11}}&amp;\cdots&amp;\frac{\partial F}{\partial \xi_{1n}}\\\vdots&amp;\ddots&amp;\vdots\\\frac{\partial F}{\partial \xi_{m1}}&amp;\cdots&amp;\frac{\partial F}{\partial \xi_{mn}}\end{bmatrix}=\sum_{i=1}^m\sum_{j=1}^n(e_ie_j^T)\otimes\frac{\partial F}{\partial \xi_{ij}}\]</span> 其中， <span class="math display">\[\frac{\partial F}{\partial \xi_{ij}}=\begin{bmatrix}\frac{\partial f_{11}}{\partial \xi_{ij}}&amp;\cdots&amp;\frac{\partial f_{1s}}{\partial \xi_{ij}}\\\vdots&amp;\ddots&amp;\vdots\\\frac{\partial f_{r1}}{\partial \xi_{ij}}&amp;\cdots&amp;\frac{\partial f_{rs}}{\partial \xi_{ij}}\end{bmatrix}=\sum_{k=1}^r\sum_{l=1}^s\frac{\partial f_{kl}}{\partial \xi_{ij}}e_ke_l^T\]</span> 特别地，当 <span class="math inline">\(F\in\mathbb C^{m\times 1}\)</span> 且 <span class="math inline">\(X\in\mathbb C^{n\times 1}\)</span> 时， <span class="math display">\[\frac{\mathrm dF}{\mathrm dX^T}=\begin{bmatrix}\frac{\partial f_{1}}{\partial \xi_{1}}&amp;\cdots&amp;\frac{\partial f_{1}}{\partial \xi_{n}}\\\vdots&amp;\ddots&amp;\vdots\\\frac{\partial f_{m}}{\partial \xi_{1}}&amp;\cdots&amp;\frac{\partial f_{m}}{\partial \xi_{n}}\end{bmatrix}\]</span> <strong>性质（链式法则）</strong>：设 <span class="math inline">\(f(x)\)</span> 是向量 <span class="math inline">\(x\)</span> 的函数，<span class="math inline">\(x\)</span> 又是 <span class="math inline">\(u\)</span> 的函数，则： <span class="math display">\[\frac{\mathrm df}{\mathrm du}=\frac{\mathrm dx^T}{\mathrm du}\frac{\mathrm df}{\mathrm dx}\]</span> <strong>微分形式</strong>：设 <span class="math inline">\(y=F(x)\)</span> 是向量 <span class="math inline">\(x\)</span> 的向量值函数，则： <span class="math display">\[\frac{\mathrm dy}{\mathrm dx^T}=A\iff\mathrm dy=A\mathrm dx\]</span> <div class="note note-warning">            <p>一些论文中可能会把 <span class="math inline">\(\mathrm dy/\mathrm dx^T\)</span> 不严谨地直接写作 <span class="math inline">\(\mathrm dy/\mathrm dx\)</span>.</p>          </div></p><h3 id="在微分方程中的应用">在微分方程中的应用</h3><p>微分方程： <span class="math display">\[\begin{cases}x_1&#39;(t)=a_{11}x_1(t)+a_{12}x_2(t)+\cdots+a_{1n}x_n(t)+b_1(t)\\x_2&#39;(t)=a_{21}x_1(t)+a_{22}x_2(t)+\cdots+a_{2n}x_n(t)+b_2(t)\\\quad\vdots\\x_n&#39;(t)=a_{n1}x_1(t)+a_{n2}x_2(t)+\cdots+a_{nn}x_n(t)+b_n(t)\\\end{cases}\]</span> 令： <span class="math display">\[A=\begin{bmatrix}a_{11}&amp;\cdots&amp;a_{1n}\\\vdots&amp;\ddots&amp;\vdots\\a_{n1}&amp;\cdots&amp;a_{nn}\end{bmatrix},\quadx(t)=\begin{bmatrix}x_1(t)\\\vdots\\x_n(t)\end{bmatrix},\quadb(t)=\begin{bmatrix}b_1(t)\\\vdots\\b_n(t)\end{bmatrix}\]</span> 则可写作矩阵形式： <span class="math display">\[x&#39;(t)=A\cdot x(t)+b(t)\]</span> <strong>定理</strong>：齐次微分方程 <span class="math inline">\(x&#39;(t)=Ax(t)\)</span> 的通解为： <span class="math display">\[x(t)=e^{tA}c\]</span> 其中 <span class="math inline">\(c\)</span> 为任意常向量。若再加上初始条件 <span class="math inline">\(x(t_0)=x_0\)</span>，则解为： <span class="math display">\[x(t)=e^{(t-t_0)A}x_0\]</span> <strong>定理</strong>：非齐次微分方程 <span class="math inline">\(x&#39;(t)=Ax(t)+b(t)\)</span> 的通解为： <span class="math display">\[x(t)=x_1(t)+x_2(t)\]</span> 其中 <span class="math inline">\(x_1(t)=e^{tA}c\)</span> 是对应齐次微分方程的通解，<span class="math inline">\(x_2(t)\)</span> 是非齐次微分方程的一个特解。常向量 <span class="math inline">\(c\)</span> 由初始条件确定。</p><p>使用<strong>常数变易法</strong>计算特解：设 <span class="math inline">\(x_2(t)=e^{tA}c(t)\)</span>，代入原非齐次微分方程有： <span class="math display">\[e^{tA}c&#39;(t)=b(t)\]</span> 由此可以解出一个 <span class="math inline">\(c(t)\)</span>，从而得到一个特解。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]3.2矩阵函数</title>
    <link href="/blog-main/2023/11/11/%E7%9F%A9%E9%98%B5%E8%AE%BA-3-2%E7%9F%A9%E9%98%B5%E5%87%BD%E6%95%B0/"/>
    <url>/blog-main/2023/11/11/%E7%9F%A9%E9%98%B5%E8%AE%BA-3-2%E7%9F%A9%E9%98%B5%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand\norm[1]{\Vert#1\Vert}\]</span></p><h2 id="矩阵函数的概念">矩阵函数的概念</h2><p><strong>定义</strong>：设一元函数 <span class="math inline">\(f(z)\)</span> 能展开为 <span class="math inline">\(z\)</span> 的幂级数： <span class="math display">\[f(z)=\sum_{k=0}^\infty c_kz^k,\quad |z|&lt;r\]</span> 则当 <span class="math inline">\(n\)</span> 阶方阵 <span class="math inline">\(A\)</span> 满足 <span class="math inline">\(\rho(A)&lt;r\)</span> 时，矩阵级数 <span class="math inline">\(\sum_{k=0}^\infty c_kA^k\)</span> 收敛，其和称为矩阵函数，记作： <span class="math display">\[f(A)=\sum_{k=0}^\infty c_k A^k\]</span> <strong>代入规则</strong>：若 <span class="math inline">\(f(z)\)</span> 能展开为 <span class="math inline">\(z\)</span> 的幂级数且 <span class="math inline">\(f(z)=g(z)\)</span> 对 <span class="math inline">\(|z|&lt;r\)</span> 成立，则当 <span class="math inline">\(\rho(A)&lt;r\)</span> 时，<span class="math inline">\(f(A)=g(A)\)</span>.</p><p><strong>二元函数的代入规则</strong>：若 <span class="math inline">\(f(x,y)\)</span> 能展开为 <span class="math inline">\(x,y\)</span> 的幂级数且 <span class="math inline">\(f(x,y)=g(x,y)\)</span>. <strong>若 <span class="math inline">\(AB=BA\)</span></strong>，则 <span class="math inline">\(f(A,B)=g(A,B)\)</span>.</p><div class="note note-info">            <p>为什么要求 <span class="math inline">\(AB=BA\)</span>？因为 <span class="math inline">\(f(x,y)=\sum_{i=0}^\infty\sum_{j=0}^\infty c_{ij}x^iy^j\)</span>，中间项要求交换律才能合并。</p>          </div><p><strong>举例</strong>： <span class="math display">\[\begin{align}&amp;\sin(A)=A-\frac{A^3}{3!}+\frac{A^5}{5!}-\cdots\\&amp;\cos(A)=I-\frac{A^2}{2!}+\frac{A^4}{4!}-\cdots\\&amp;e^A=I+A+\frac{A^2}{2!}+\frac{A^3}{3!}+\cdots\end{align}\]</span></p><h2 id="矩阵函数的求法">矩阵函数的求法</h2><h3 id="待定系数法">待定系数法</h3><p>给定 <span class="math inline">\(A\)</span>，确定首一零化多项式 <span class="math inline">\(g(\lambda)\)</span>，使得 <span class="math inline">\(g(A)=0\)</span>，例如特征多项式或最小多项式均可。设 <span class="math display">\[f(\lambda)=g(\lambda)q(\lambda)+r(\lambda)\]</span> 其中 <span class="math inline">\(\deg r(\lambda)&lt;\deg g(\lambda)\)</span>. 那么只要确定了 <span class="math inline">\(r(\lambda)\)</span>，就有 <span class="math inline">\(f(A)=r(A)\)</span>.</p><p>所以问题的关键在于如何确定 <span class="math inline">\(r(\lambda)\)</span>. 我们并不需要解 <span class="math inline">\(q(\lambda)\)</span>，只需要找 <span class="math inline">\(\deg r(\lambda)\)</span> 个函数值或导数值（<span class="math inline">\(k\)</span> 重根就求 <span class="math inline">\(k-1\)</span> 阶导）形成线性方程组，就能解系数方程（其实就是带有导数约束的插值问题）。具体而言，设 <span class="math inline">\(g(\lambda)\)</span> 的互异零点为 <span class="math inline">\(\lambda_1,\ldots,\lambda_s\)</span>，对应重数为 <span class="math inline">\(r_1,\ldots,r_s\)</span>，那么易知： <span class="math display">\[g^{(l)}(\lambda_i)=0,\quad l=0,\ldots,r_i-1;\,i=1,\ldots,s\]</span> 因此： <span class="math display">\[r^{(l)}(\lambda_i)=f^{(l)}(\lambda_i),\quad l=0,\ldots,r_i-1;\,i=1,\ldots,s\label{prob}\tag{1}\]</span> 解该方程组即可确定 <span class="math inline">\(r(\lambda)\)</span>.</p><h4 id="方法-0.-直接求解">方法 0. 直接求解</h4><p>对于规模较小的问题，直接求解方程组 <span class="math inline">\(\eqref{prob}\)</span> 式即可。</p><div class="note note-secondary">            <p>例：设 <span class="math inline">\(A=\begin{bmatrix}2&amp;0&amp;0\\1&amp;1&amp;1\\1&amp;-1&amp;3\end{bmatrix}\)</span>，求 <span class="math inline">\(e^A\)</span>.</p><p>解：特征方程为： <span class="math display">\[\varphi(\lambda)=\begin{vmatrix}\lambda-2&amp;0&amp;0\\-1&amp;\lambda-1&amp;-1\\-1&amp;1&amp;\lambda-3\end{vmatrix}=(\lambda-2)^3\]</span> 设 <span class="math inline">\(f(\lambda)=e^\lambda=\varphi(\lambda)q(\lambda)+r(\lambda)\)</span>，其中 <span class="math inline">\(r(\lambda)=a\lambda^2+b\lambda+c\)</span>，则： <span class="math display">\[\begin{cases}r(2)=f(2)=e^2\\r&#39;(2)=f&#39;(2)=e^2\\r&#39;&#39;(2)=f&#39;&#39;(2)=e^2\end{cases}\implies\begin{cases}4a+2b+c=e^2\\4a+b=e^2\\2a=e^2\end{cases}\implies\begin{cases}a=e^2/2\\b=-e^2\\c=e^2\end{cases}\]</span> 故 <span class="math inline">\(r(\lambda)=e^2/2\cdot(\lambda^2-2\lambda+2)\)</span>，故： <span class="math display">\[f(A)=r(A)=\frac{e^2}{2}(A^2-2A+2I)=\begin{bmatrix}\cdots\end{bmatrix}\]</span></p>          </div><h4 id="方法-1.-sylvester-插值公式">方法 1. Sylvester 插值公式</h4><p>暂略。</p><h4 id="方法2.-广义-newton-插值公式">方法2. 广义 Newton 插值公式</h4><div class="note note-success">            <p>思想：正如上文所说，解方程组 <span class="math inline">\(\eqref{prob}\)</span> 本质就是解带有导数约束的插值问题。我们知道 Taylor 展开式满足在<strong>一个点</strong>处的 <span class="math inline">\(n\)</span> 阶<strong>导数值</strong>相等，而 Newton 展开式满足在<strong>多个点</strong>处的<strong>函数值</strong>相等，所以我们想解决的插值问题其实是二者的结合，遂称作广义 Newton 插值。</p>          </div><p><strong>Taylor 展开式</strong>：若已知 <span class="math inline">\(f(x)\)</span> 在 <span class="math inline">\(x_0\)</span> 的函数值和直到 <span class="math inline">\(n\)</span> 阶导数值，则： <span class="math display">\[Tf(x)=f(x_0)+f&#39;(x_0)(x-x_0)+\frac{f&#39;&#39;(x_0)}{2!}(x-x_0)^2+\cdots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n\]</span> 使得 <span class="math inline">\(Tf^{(l)}(x_0)=f^{(l)}(x_0),\,l=0,\ldots,n\)</span> 成立。</p><p><strong>Newton 展开式</strong>：若已知 <span class="math inline">\(f(x)\)</span> 在互异点 <span class="math inline">\(x_0,x_1,\ldots,x_n\)</span> 处的函数值，则有 Newton 插值公式： <span class="math display">\[\begin{align}Nf(x)&amp;=f(x_0)\\&amp;+f[x_0,x_1](x-x_0)\\&amp;+f[x_0,x_1,x_2](x-x_0)(x-x_1)\\&amp;+\cdots\\&amp;+f[x_0,x_1,\ldots,x_n](x-x_0)(x-x_1)\cdots(x-x_{n-1})\end{align}\]</span> 使得 <span class="math inline">\(Nf(x_i)=f(x_i),\,i=0,\ldots,n\)</span> 成立。</p><p>其中 <span class="math inline">\(f[x_0,x_1]\)</span> 称作一阶均差： <span class="math display">\[f[x_0,x_1]=\frac{f(x_1)-f(x_0)}{x_1-x_0}\]</span> <span class="math inline">\(f[x_0,x_1,x_2]\)</span> 称作二阶均差： <span class="math display">\[f[x_0,x_1,x_2]=\frac{f[x_1,x_2]-f[x_0,x_1]}{x_2-x_0}\]</span> <span class="math inline">\(f[x_0,x_1,\ldots,x_k]\)</span> 称作 <span class="math inline">\(k\)</span> 阶均差： <span class="math display">\[f[x_0,x_1,\ldots,x_k]=\frac{f[x_1,\ldots,x_{k-1},x_k]-f[x_0,x_1,\ldots,x_{k-1}]}{x_k-x_0}\]</span> 可以借助下面这个三角形表格来计算均差：</p><p><img src="diff.png" width=70% /></p><p><strong>广义 Newton 展开式</strong></p><p>从均差的定义可以看出，它的极限与导数有密切关系。事实上，利用罗尔定理可以证明：若 <span class="math inline">\(f(x)\)</span> 在 <span class="math inline">\([a,b]\)</span> 上存在 <span class="math inline">\(n\)</span> 阶导数，且节点 <span class="math inline">\(x_0,x_1,\ldots,x_n\in[a,b]\)</span>，则存在 <span class="math inline">\(\xi\in[a,b]\)</span> 使得： <span class="math display">\[f([x_0,x_1,\ldots,x_n])=\frac{f^{(n)}(\xi)}{n!}\]</span> 因此，我们可以定义在相同点处的“广义”均差为： <span class="math display">\[f([c,c,\ldots,c])=\frac{f^{(n)}(c)}{n!}\]</span> 那么，Taylor 展开就可以看作是广义 Newton 展开在插值节点重合的特殊情形。</p><p>广义的均差也可以借助均差表计算，例如：</p><p><img src="diff2.png" width=70% /></p><div class="note note-secondary">            <p>例：设 <span class="math inline">\(A=\begin{bmatrix}2&amp;0&amp;0\\1&amp;1&amp;2\\1&amp;-1&amp;3\end{bmatrix}\)</span>，求 <span class="math inline">\(e^{At}\)</span>.</p><p>解：特征方程为： <span class="math display">\[\varphi(\lambda)=\begin{vmatrix}\lambda-2&amp;0&amp;0\\-1&amp;\lambda-1&amp;-2\\-1&amp;1&amp;\lambda-3\end{vmatrix}=(\lambda-2)(\lambda-5)(\lambda+1)\]</span> 画均差表：</p><p><img src="diff-ex.png" width=70% /></p><p>于是： <span class="math display">\[r(\lambda)=e^{2t}+\frac{e^{5t}-e^{2t}}{3}(\lambda-2)+\frac{e^{5t}-2e^{2t}+e^{-t}}{18}(\lambda-2)(\lambda-5)\]</span> 故： <span class="math display">\[e^{At}=r(A)=\cdots\]</span></p>          </div><h3 id="数项级数求和法">数项级数求和法</h3><p>暂略。</p><h3 id="对角形法">对角形法</h3><div class="note note-danger">            <p>对角化的计算量较大（要计算 <span class="math inline">\(P\)</span> 和 <span class="math inline">\(P^{-1}\)</span>，即解特征值和特征向量）</p>          </div><p>若 <span class="math inline">\(A\)</span> 可对角化，即存在非奇异矩阵 <span class="math inline">\(P\)</span>，使得： <span class="math display">\[P^{-1}AP=\begin{bmatrix}\lambda_1&amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;\lambda_n\end{bmatrix}\]</span> 则： <span class="math display">\[f(A)=P\begin{bmatrix}f(\lambda_1)&amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;f(\lambda_n)\end{bmatrix}P^{-1}\]</span></p><h3 id="jordan-标准形法">Jordan 标准形法</h3><p>在第一章中我们已经推导过了 Jordan 标准形的多项式，而对于一般函数，根据代入规则容易知道其结论有类似的形式。</p><p>设 <span class="math inline">\(A\)</span> 的 Jordan 标准形为 <span class="math inline">\(J\)</span>，则存在可逆矩阵 <span class="math inline">\(P\)</span> 使得： <span class="math display">\[P^{-1}AP=J=\begin{bmatrix}J_1&amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;J_s\end{bmatrix},\quad J_i=\begin{bmatrix}\lambda_i&amp;1&amp;&amp;\\&amp;\ddots&amp;\ddots&amp;\\&amp;&amp;\lambda_i&amp;1\\&amp;&amp;&amp;\lambda_i\end{bmatrix}_{m_i\times m_i}\]</span> 那么： <span class="math display">\[f(J_i)=\begin{bmatrix}f(\lambda_i)&amp;\frac{1}{1!}f&#39;(\lambda_i)&amp;\frac{1}{2!}f&#39;&#39;(\lambda_i)&amp;\cdots&amp;\frac{1}{(m_i-1)!}f^{(m_i-1)}(\lambda_i)\\&amp;f(\lambda_i)&amp;\frac{1}{1!}f&#39;(\lambda_i)&amp;\cdots&amp;\frac{1}{(m_i-2)!}f^{(m_i-2)}(\lambda_i)\\&amp;&amp;\ddots&amp;\ddots&amp;\vdots\\&amp;&amp;&amp;f(\lambda_i)&amp;\frac{1}{1!}f&#39;(\lambda_i)\\&amp;&amp;&amp;&amp;f(\lambda_i)\end{bmatrix}_{m_i\times m_i}\]</span> 于是原问题： <span class="math display">\[f(A)=Pf(J)P^{-1}=P\begin{bmatrix}f(J_1)&amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;f(J_s)\end{bmatrix}P^{-1}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]3.1矩阵序列与矩阵级数</title>
    <link href="/blog-main/2023/11/09/%E7%9F%A9%E9%98%B5%E8%AE%BA-3-1%E7%9F%A9%E9%98%B5%E5%BA%8F%E5%88%97%E4%B8%8E%E7%9F%A9%E9%98%B5%E7%BA%A7%E6%95%B0/"/>
    <url>/blog-main/2023/11/09/%E7%9F%A9%E9%98%B5%E8%AE%BA-3-1%E7%9F%A9%E9%98%B5%E5%BA%8F%E5%88%97%E4%B8%8E%E7%9F%A9%E9%98%B5%E7%BA%A7%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand\norm[1]{\Vert#1\Vert}\]</span></p><h2 id="矩阵序列">矩阵序列</h2><h3 id="收敛">收敛</h3><p><strong>定义</strong>：设有矩阵序列 <span class="math inline">\(\{A^{(k)}=(a_{ij}^{(k)})\}\)</span>，若各分量分别收敛，则称 <span class="math inline">\(\{A^{(k)}\}\)</span> 收敛；否则，若存在一组 <span class="math inline">\(i,j\)</span> 使得 <span class="math inline">\(a_{ij}^{(k)}\)</span> 发散，则称 <span class="math inline">\(\{A^{(k)}\}\)</span> 发散。</p><p><strong>定理（各分量收敛等价于范数收敛）</strong>：设 <span class="math inline">\(A^{(k)}\in\mathbb C^{m\times n}\)</span>，<span class="math inline">\(\norm{\cdot}\)</span> 为任一<strong>广义矩阵范数</strong>，则： <span class="math display">\[\begin{align}&amp;A^{(k)}\to 0\iff \norm{A^{(k)}}\to 0\\&amp;A^{(k)}\to A\iff \norm{A^{(k)}-A}\to 0\end{align}\]</span> <div class="note note-secondary">            <p>证明：根据广义矩阵范数的等价性定理，仅需对 <span class="math inline">\(m_\infty\)</span> 范数证明。由于： <span class="math display">\[a^{(k)}\to 0\wedge b^{(k)}\to 0\iff\max(a^{(k)},b^{(k)})\to0\]</span> 所以： <span class="math display">\[A^{(k)}\to 0\iff |a_{ij}^{(k)}|\to 0\iff\norm{A}_{m_\infty}=\max_{i,j}|a_{ij}^{(k)}|\to 0\]</span> 证毕。</p>          </div></p><div class="note note-info">            <p>按各元素收敛需要验证 <span class="math inline">\(m\times n\)</span> 个数列是否收敛，比较麻烦；按范数收敛只需要验证 1 个数列是否收敛，更加方便。</p>          </div><div class="note note-success">            <p>尽管上述定理对任意<strong>广义矩阵范数</strong>都成立，但在证明时我们通常会选取一个<strong>矩阵范数</strong>，这样可以用上<strong>相容性条件</strong>，使得很多证明与高等数学中证明数列收敛没有什么区别。</p>          </div><p><strong>Cauchy 收敛</strong>：矩阵序列 <span class="math inline">\(\{A^{(k)}\}\)</span> 收敛的充要条件是：对任意 <span class="math inline">\(\epsilon&gt;0\)</span>，存在 <span class="math inline">\(N(\epsilon)\)</span>，当 <span class="math inline">\(k,l\geq N(\epsilon)\)</span> 时，有： <span class="math display">\[\norm{A^{(k)}-A^{(l)}}&lt;\epsilon\]</span> 其中 <span class="math inline">\(\norm\cdot\)</span> 为任一广义矩阵范数。</p><h3 id="性质">性质</h3><p><strong>性质 0</strong>. 若 <span class="math inline">\(A^{(k)}\to A\)</span>，则 <span class="math inline">\(\norm{A^{(k)}}\to\norm{A}\)</span>.</p><div class="note note-secondary">            <p>证明： <span class="math display">\[\norm{A^{(k)}}-\norm{A}\leq\norm{A^{(k)}-A}\to0\implies\norm{A^{(k)}}\to\norm{A}\]</span> 证毕。</p>          </div><p><strong>性质 1</strong>. 设 <span class="math inline">\(A^{(k)}\to A,\,B^{(k)}\to B\)</span>，则： <span class="math display">\[\begin{align}&amp;\lim_{k\to\infty}\alpha A^{(k)}+\beta B^{(k)}=\alpha A+\beta B\\&amp;\lim_{k\to\infty}A^{(k)}B^{(k)}=AB\end{align}\]</span> <div class="note note-secondary">            <p>证明：只考虑相容的矩阵范数。 <span class="math display">\[\begin{align}\norm{A^{(k)}B^{(k)}-AB}&amp;=\norm{A^{(k)}B^{(k)}-AB^{(k)}+AB^{(k)}-AB}\\&amp;\leq\norm{A^{(k)}B^{(k)}-AB^{(k)}}+\norm{AB^{(k)}-AB}\\&amp;\leq\norm{A^{(k)}-A}\norm{B^{(k)}}+\norm{A}\norm{B^{(k)}-B}\end{align}\]</span> 由于 <span class="math inline">\(\norm{A^{(k)}-A}\to0,\,\norm{B^{(k)}}\to\norm B,\,\norm{B^{(k)}-B}\to0\)</span>，所以 <span class="math inline">\(\norm{A^{(k)}B^{(k)}-AB}\to0\)</span>，故 <span class="math inline">\(A^{(k)}B^{(k)}\to AB\)</span>.</p>          </div></p><p><strong>定理</strong>：<span class="math inline">\(A^{(k)}\to A\)</span> 的充要条件是对任意 <span class="math inline">\(x\)</span> 有 <span class="math inline">\(A^{(k)}x\to Ax\)</span>，或者对任意 <span class="math inline">\(x,y\)</span> 有 <span class="math inline">\(y^HA^{(k)}x\to y^HAx\)</span>.</p><p><strong>定理</strong>：若 <span class="math inline">\(A^{(k)}\)</span> 和 <span class="math inline">\(A\)</span> 都为 Hermite 矩阵，那么 <span class="math inline">\(A^{(k)}\to A\)</span> 的充要条件是 <span class="math inline">\(x^HA^{(k)}x=x^HAx,\forall x\)</span>.</p><p><strong>推论【类比单调有界定理】</strong>：若 <span class="math inline">\(A^{(k)}\)</span> 为半正定 Hermite 矩阵且单调减少（即 <span class="math inline">\(A^{(k)}-A^{(k+1)}\)</span> 为半正定 Hermite 矩阵），那么 <span class="math inline">\(A^{(k)}\)</span> 有极限。</p><p><strong>性质 2</strong>. 设 <span class="math inline">\(A^{(k)}\to A\)</span>，且 <span class="math inline">\(A^{(k)}\)</span> 和 <span class="math inline">\(A\)</span> 都可逆，则： <span class="math display">\[\lim_{k\to\infty}\left(A^{(k)}\right)^{-1}=A^{-1}\]</span></p><h3 id="有界">有界</h3><p><strong>定义</strong>：如果存在常数 <span class="math inline">\(M&gt;0\)</span>，使得对所有 <span class="math inline">\(k\)</span> 都有： <span class="math display">\[|a_{ij}^{(k)}|&lt;M\]</span> 或等价地： <span class="math display">\[\norm{A^{(k)}}&lt;M^\ast\]</span> 则称矩阵序列 <span class="math inline">\(\{A^{(k)}\}\)</span> 为有界的。</p><p><strong>定理</strong>：有界矩阵序列 <span class="math inline">\(\{A^{(k)}\}\)</span> 一定有收敛的子列。</p><h3 id="收敛矩阵">收敛矩阵</h3><p><strong>定义</strong>：设 <span class="math inline">\(A\)</span> 为方阵且 <span class="math inline">\(A^k\to 0\,(k\to\infty)\)</span>，则称 <span class="math inline">\(A\)</span> 为收敛矩阵。</p><p><strong>定理（迭代法基本定理）</strong>：<span class="math inline">\(A\)</span> 是收敛矩阵的充要条件是 <span class="math inline">\(\rho(A)&lt;1\)</span>.</p><div class="note note-secondary">            <p>证明：</p><p>必要性：设 <span class="math inline">\(\lambda,x\)</span> 为 <span class="math inline">\(A\)</span> 的特征值和特征向量，<span class="math inline">\(\lambda x=Ax\)</span>，则 <span class="math inline">\(\lambda^kx=A^kx\)</span>. 两边取范数： <span class="math display">\[|\lambda|^k\norm{x}=\norm{A^kx}\leq\norm{A^k}\norm{x}   \]</span> 由于 <span class="math inline">\(x\neq 0\)</span>，故 <span class="math inline">\(|\lambda|^k\leq\norm{A^k}\to0\,(k\to\infty)\)</span>，于是 <span class="math inline">\(|\lambda|&lt;1\)</span>，故 <span class="math inline">\(\rho(A)=\max_i|\lambda_i|&lt;1\)</span>.</p><p>充分性：取 <span class="math inline">\(\epsilon=(1-\rho(A))/2\)</span>，根据上一章的结论，存在某种矩阵范数使得： <span class="math display">\[\norm{A}&lt;\rho(A)+\epsilon&lt;1\]</span> 故： <span class="math display">\[\norm{A^k}\leq\norm{A}^k&lt;(\rho(A)+\epsilon)^k\to 0\quad(k\to\infty)\]</span> 从而 <span class="math inline">\(A^k\to0\)</span>.</p>          </div><p><strong>定理</strong>：<span class="math inline">\(A\)</span> 是收敛矩阵的充要条件是存在某种矩阵范数满足 <span class="math inline">\(\norm{A}&lt;1\)</span>.</p><div class="note note-secondary">            <p>证明：根据迭代法基本定理和谱半径与矩阵范数的关系易证。</p>          </div><div class="note note-info">            <p>实际应用中不会去算 <span class="math inline">\(\rho(A)\)</span>（解特征值很麻烦），而是计算 <span class="math inline">\(\norm{A}\)</span>.</p>          </div><div class="note note-info">            <p><strong>迭代法解线性方程组</strong>中，设迭代格式为 <span class="math inline">\(x_{n+1}=Ax_n+f\)</span>，则： <span class="math display">\[\begin{align}x_{n}&amp;=Ax_{n-1}+f\\&amp;=A^2x_{n-2}+Af+f\\&amp;=\cdots\\&amp;=A^n x_0+(A^{n-1}+\cdots+A+I)f\\&amp;=A^n x_0+(I-A^n)(I-A)^{-1}f\\&amp;=A^nx_0+(I-A)^{-1}f-A^n(I-A)^{-1}f\\\end{align}\]</span> 若迭代收敛，则解满足 <span class="math inline">\(x=Ax+f\implies x=(I-A)^{-1}f\)</span>. 记 <span class="math inline">\(\epsilon_n=x_n-x\)</span> 表示第 <span class="math inline">\(n\)</span> 次迭代后的误差，<span class="math inline">\(\epsilon_0=x_0-x\)</span> 表示初始误差，那么： <span class="math display">\[\epsilon_n=A^n\epsilon_0\]</span> 因此，当 <span class="math inline">\(A\)</span> 是收敛矩阵时，<span class="math inline">\(\epsilon_n\to0\)</span>，迭代法收敛。</p>          </div><h2 id="矩阵级数">矩阵级数</h2><h3 id="收敛与绝对收敛">收敛与绝对收敛</h3><p><strong>定义</strong>：称矩阵级数 <span class="math inline">\(\sum_{k=0}^\infty A^{(k)}\)</span> （绝对）收敛到 <span class="math inline">\(S\)</span>，若其部分和序列 <span class="math inline">\(\left\{S_N=\sum_{k=0}^N A^{(k)}\right\}\)</span> （绝对）收敛，且极限为 <span class="math inline">\(S\)</span>.</p><p><strong>性质</strong>：矩阵级数 <span class="math inline">\(\sum_{k=0}^\infty A^{(k)}\)</span> 收敛的充要条件是对任意向量 <span class="math inline">\(x\)</span>，向量级数 <span class="math inline">\(\sum_{k=0}^\infty A^{(k)}x\)</span> 收敛。</p><p><strong>性质</strong>：若矩阵级数是绝对收敛的，则它一定是收敛的，并且任意调换其项的顺序所得到的级数仍然是收敛的，且其和不变。</p><p><strong>性质</strong>：矩阵级数 <span class="math inline">\(\sum_{k=0}^\infty A^{(k)}\)</span> 绝对收敛的充要条件是 <span class="math inline">\(\sum_{k=0}^\infty\norm{A^{(k)}}\)</span> 收敛。</p><p><strong>性质</strong>：若矩阵级数 <span class="math inline">\(\sum_{k=0}^\infty A^{(k)}\)</span> （绝对）收敛，则 <span class="math inline">\(\sum_{k=0}^\infty PA^{(k)}Q\)</span> 也（绝对）收敛，且： <span class="math display">\[\sum_{k=0}^\infty PA^{(k)}Q=P\left(\sum_{k=0}^\infty A^{(k)}\right)Q\]</span> <strong>性质</strong>：若两个矩阵级数都绝对收敛，且分别收敛到 <span class="math inline">\(A,B\)</span>，则其 Cauchy 乘积 <span class="math inline">\(\sum_{k=0}^\infty\sum_{i+j=k}A^{(i)}B^{(j)}\)</span> 绝对收敛，且收敛到 <span class="math inline">\(AB\)</span>.</p><h3 id="关于幂级数的三个定理">关于幂级数的三个定理</h3><p><strong>定理</strong>：方阵 <span class="math inline">\(A\)</span> 的幂级数 <span class="math inline">\(\sum_{k=0}^\infty A^k\)</span> 收敛的充要条件是 <span class="math inline">\(\rho(A)&lt;1\)</span>，且收敛时的和为 <span class="math inline">\((I-A)^{-1}\)</span>.</p><div class="note note-secondary">            <p>证明：</p><p>必要性。由于级数 <span class="math inline">\(I+A+A^2+\cdots\)</span> 收敛，所以部分和序列 <span class="math inline">\(S^{(k)}=I+A+\cdots+A^k\)</span> 收敛。记 <span class="math inline">\(T^{(k)}=I+A+\cdots+A^{k+1}\)</span>，则 <span class="math inline">\(A^{k+1}=T^k-S^k\to0\)</span>，即 <span class="math inline">\(A\)</span> 是收敛矩阵。根据前文的定理，有 <span class="math inline">\(\rho(A)&lt;1\)</span>.</p><p>充分性。已知 <span class="math inline">\(\rho(A)&lt;1\)</span>，故 <span class="math inline">\(A\)</span> 为收敛矩阵，<span class="math inline">\(A^k\to 0\)</span>. 于是 <span class="math inline">\(S^{(k)}=I+A+\cdots+A^k=(I-A)^{-1}-(I-A)^{-1}A^{k+1}\to(I-A)^{-1}\)</span>.</p><p>证毕。</p>          </div><p><strong>定理</strong>：若方阵 <span class="math inline">\(A\)</span> 的某一范数满足 <span class="math inline">\(\norm{A}&lt;1\)</span>，则部分和 <span class="math inline">\(I+A+\cdots+A^N\)</span> 与 <span class="math inline">\((I-A)^{-1}\)</span> 之间的误差为： <span class="math display">\[\left\Vert(I-A)^{-1}-\sum_{k=0}^NA^k\right\Vert\leq\frac{\norm{A}^{N+1}}{1-\norm{A}}\]</span> <div class="note note-secondary">            <p>证明：设 <span class="math inline">\(B=(I-A)^{-1}-\sum_{k=0}^NA^k=(I-A)^{-1}A^{N+1}\)</span>，则 <span class="math inline">\((I-A)B=A^{N+1}\)</span>，即 <span class="math inline">\(B=AB+A^{N+1}\)</span>，从而： <span class="math display">\[\norm{B}=\norm{AB+A^{N+1}}\leq\norm{A}\norm{B}+\norm{A}^{N+1}\implies\norm{B}\leq\frac{\norm{A}^{N+1}}{1-\norm{A}}\]</span> 证毕。</p>          </div></p><p><strong>定理</strong>：设幂级数 <span class="math inline">\(\sum_{k=0}^\infty c_kz^k\)</span> 的收敛半径为 <span class="math inline">\(r\)</span>，如果方阵 <span class="math inline">\(A\)</span> 满足 <span class="math inline">\(\rho(A)&lt;r\)</span>，则矩阵幂级数 <span class="math inline">\(\sum_{k=0}^\infty c_kA^k\)</span> 绝对收敛；若 <span class="math inline">\(\rho(A)&gt;r\)</span>，则矩阵幂级数发散。</p><div class="note note-secondary">            <p>证明：</p><p>根据上一章的结论，存在某个矩阵范数 <span class="math inline">\(\norm{A}\)</span> 使得 <span class="math inline">\(\rho(A)\leq\norm{A}&lt;r\)</span>，于是： <span class="math display">\[\sum_{k=0}^{\infty}\norm{c_kA^k}=\sum_{k=0}^{\infty}|c_k|\norm{A^k}\leq\sum_{k=0}^\infty |c_k|\norm{A}^k\]</span> 右侧数项级数收敛，因此左侧收敛，即矩阵幂级数绝对收敛。</p><p>设 <span class="math inline">\(A\)</span> 的特征值 <span class="math inline">\(\lambda=\rho(A)\)</span>，<span class="math inline">\(x\)</span> 为对应特征向量，则： <span class="math display">\[\sum_{k=0}^\infty c_k(A^kx)=\sum_{k=0}^\infty c_k(\lambda^k x)=\left(\sum_{k=0}^\infty c_k\lambda^k\right)x\]</span> 当 <span class="math inline">\(\rho(A)&gt;r\)</span> 时，幂级数 <span class="math inline">\(\sum_{k=0}^\infty c_k\lambda^k\)</span> 发散，因此矩阵幂级数发散；反之同理。证毕。</p>          </div>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]2.2矩阵范数</title>
    <link href="/blog-main/2023/11/05/%E7%9F%A9%E9%98%B5%E8%AE%BA-2-2%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0/"/>
    <url>/blog-main/2023/11/05/%E7%9F%A9%E9%98%B5%E8%AE%BA-2-2%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand\norm[1]{\Vert#1\Vert}\]</span></p><h2 id="矩阵范数">矩阵范数</h2><p><strong>广义矩阵范数</strong>：与向量范数相同，满足 3 条性质：</p><ol type="1"><li><strong>非负性</strong>：<span class="math inline">\(\norm{A}\geq 0\)</span> 且 <span class="math inline">\(\norm{A}=0\iff A=0\)</span></li><li><strong>齐次性</strong>：<span class="math inline">\(\norm{kA}=|k|\norm{A},\,k\in K\)</span></li><li><strong>三角不等式</strong>：<span class="math inline">\(\norm{A+B}\leq\norm{A}+\norm{B}\)</span></li></ol><p><strong>矩阵范数</strong>：在广义矩阵范数的基础上增加<strong>相容条件</strong>：</p><ol start="4" type="1"><li><strong>相容性</strong>：<span class="math inline">\(\norm{AB}\leq\norm{A}\norm{B}\)</span></li></ol><div class="note note-success">            <p>对于涉及到矩阵范数的不等式放缩，一般就是考虑三角不等式和相容性。</p>          </div><h2 id="相容范数">相容范数</h2><p>对 <span class="math inline">\(\mathbb C^{m\times n}\)</span> 上的矩阵范数 <span class="math inline">\(\norm{A}_M\)</span> 和 <span class="math inline">\(\mathbb C^m,\mathbb C^n\)</span> 上的同类向量范数 <span class="math inline">\(\norm{x}_v\)</span>，若： <span class="math display">\[\norm{Ax}_v\leq \norm{A}_M\norm{x}_v,\quad \forall A\in\mathbb C^{m\times n},\,\forall x\in\mathbb C^n\]</span> 则称矩阵范数 <span class="math inline">\(\norm{A}_M\)</span> 与向量范数 <span class="math inline">\(\norm{x}_v\)</span> 是相容的。</p><div class="note note-info">            <p>简单地说，把矩阵视作线性映射，则相容范数表示，映射后的向量与原向量的长度的相对变化量被矩阵范数所控制。</p>          </div><h3 id="m_1-范数"><span class="math inline">\(m_1\)</span> 范数</h3><p>设 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，矩阵的 <span class="math inline">\(m_1\)</span> 范数与向量的 1-范数定义类似： <span class="math display">\[\norm{A}_{m_1}=\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|\]</span> 即所有元素的绝对值（模）之和。可以证明 <strong><span class="math inline">\(m_1\)</span> 范数是矩阵范数，并且与向量的 1-范数相容</strong>。</p><div class="note note-secondary">            <p>证明：非负性、齐次性和三角不等式与向量的 1-范数完全相同，必然成立，只需证明矩阵的相容性以及与向量 1-范数的相容性。</p><p>相容性： <span class="math display">\[\begin{align}\norm{AB}_{m_1}&amp;=\sum_{i=1}^m\sum_{j=1}^l\left|\sum_{k=1}^na_{ik}b_{kj}\right|\\&amp;\leq\sum_{i=1}^m\sum_{j=1}^l\sum_{k=1}^n|a_{ik}||b_{kj}|\\&amp;\leq\sum_{k=1}^n\left[\left(\sum_{i=1}^m|a_{ik}|\right)\left(\sum_{j=1}^l|b_{kj}|\right)\right]\\&amp;\leq\left(\sum_{k=1}^n\sum_{i=1}^m|a_{ik}|\right)\left(\sum_{k=1}^n\sum_{j=1}^l|b_{kj}|\right)\\&amp;=\norm{A}_{m_1}\norm{B}_{m_1}\end{align}\]</span> 与向量的 1-范数相容： <span class="math display">\[\begin{align}\norm{Ax}_1&amp;=\sum_{i=1}^m\left|\sum_{j=1}^na_{ij}x_j\right|\\&amp;\leq\sum_{i=1}^m\sum_{j=1}^n|a_{ij}||x_j|\\&amp;\leq\left(\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|\right)\left(\sum_{j=1}^n|x_j|\right)\\&amp;=\norm{A}_{m_1}\norm{x}_1\end{align}\]</span> 证毕。</p>          </div><h3 id="m_infty-范数"><span class="math inline">\(m_\infty\)</span> 范数</h3><p>设 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，矩阵的 <span class="math inline">\(m_\infty\)</span> 范数与向量的无穷范数定义类似，<strong>但是要乘以 <span class="math inline">\(n\)</span></strong>，否则不满足相容性： <span class="math display">\[\norm{A}_{m_\infty}=n\cdot\max_{i,j}|a_{ij}|\]</span> 可以证明 <strong><span class="math inline">\(m_\infty\)</span> 范数是矩阵范数，且与向量的无穷范数相容</strong>。</p><div class="note note-secondary">            <p>证明：相容性： <span class="math display">\[\begin{align}\norm{AB}_{m_\infty}&amp;=n\cdot\max_{i,j}\left|\sum_{k=1}^n a_{ik}b_{kj}\right|\\&amp;\leq n\cdot\max_{i,j}\sum_{k=1}^n |a_{ik}||b_{kj}|\\&amp;\leq n\cdot\max_{i,j}\left[n\cdot(\max_k |a_{ik}|)(\max_k |b_{kj}|)\right]\\&amp;=\max_{i,j}\left[(n\cdot \max_k |a_{ik}|)(n\cdot\max_k |b_{kj}|)\right]\\&amp;=(n\cdot \max_{i,k} |a_{ik}|)(n\cdot\max_{k,j} |b_{kj}|)\\&amp;=\norm{A}_{m_\infty}\norm{B}_{m_\infty}\end{align}\]</span> 与向量的无穷范数相容： <span class="math display">\[\begin{align}\norm{Ax}_{\infty}&amp;=\max_i\left|\sum_{j=1}^na_{ij}x_j\right|\\&amp;\leq\max_i\sum_{j=1}^n|a_{ij}||x_j|\\&amp;\leq\max_i\left[n\cdot(\max_j |a_{ij}|)(\max_j |x_j|)\right]\\&amp;=n\cdot\max_{i,j}|a_{ij}|\cdot\max_j |x_j|\\&amp;=\norm{A}_{m_\infty}\norm{x}_{\infty}\end{align}\]</span> 证毕。</p>          </div><h3 id="frobenius-范数">Frobenius 范数</h3><p>设 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，矩阵的 F-范数与向量的 2-范数定义类似： <span class="math display">\[\norm{A}_{F}=\norm{A}_{m_2}=\left(\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^2\right)^{1/2}=\left(\text{tr}(A^HA)\right)^{1/2}\]</span> 可以证明 <strong>F-范数是矩阵范数，并且与向量的 2-范数相容</strong>。</p><div class="note note-secondary">            <p>证明：相容性： <span class="math display">\[\begin{align}\norm{AB}_F^2&amp;=\sum_{i=1}^m\sum_{j=1}^l\left|\sum_{k=1}^na_{ik}b_{kj}\right|^2\\&amp;\leq\sum_{i=1}^m\sum_{j=1}^l\left(\sum_{k=1}^n|a_{ik}||b_{kj}|\right)^2\\&amp;\leq\sum_{i=1}^m\sum_{j=1}^l\left(\sum_{k=1}^n|a_{ik}|\right)^2\left(\sum_{k=1}^n|b_{kj}|\right)^2\\&amp;\leq\left(\sum_{i=1}^m\sum_{k=1}^n|a_{ik}|\right)^2\left(\sum_{j=1}^l\sum_{k=1}^n|b_{kj}|\right)^2\\&amp;=\norm{A}_F^2\norm{B}_F^2\end{align}\]</span> 与向量的 2-范数相容： <span class="math display">\[\begin{align}\norm{Ax}_2^2&amp;=\sum_{i=1}^m\left|\sum_{j=1}^n a_{ij}x_j\right|^2\\&amp;\leq\sum_{i=1}^m\left(\sum_{j=1}^n |a_{ij}||x_j|\right)^2\\&amp;\leq\sum_{i=1}^m\left(\sum_{j=1}^n |a_{ij}|\right)^2\left(\sum_{j=1}^n|x_j|\right)^2\\&amp;\leq\left(\sum_{i=1}^m\sum_{j=1}^n |a_{ij}|\right)^2\left(\sum_{j=1}^n|x_j|\right)^2\\&amp;=\norm{A}_F^2\norm{x}_2^2\end{align}\]</span> 证毕。</p>          </div><p><strong>定理（Frobenius 范数的酉不变性）</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，酉矩阵 <span class="math inline">\(P\in\mathbb C^{m\times n},\,Q\in\mathbb C^{n\times n}\)</span>，则： <span class="math display">\[\norm{PA}_F=\norm{A}_F=\norm{AQ}_F\]</span> <div class="note note-secondary">            <p>证明： <span class="math display">\[\begin{align}&amp;\norm{PA}_F^2=\text{tr}\left((PA)^H(PA)\right)=\text{tr}\left(A^HP^HPA\right)=\text{tr}\left(A^HA\right)=\norm{A}_F^2\\&amp;\norm{AQ}_F^2=\text{tr}\left((AQ)^H(AQ)\right)=\text{tr}\left(Q^HA^HAQ\right)=\text{tr}\left(A^HQ^HQA\right)=\text{tr}\left(A^HA\right)=\norm{A}_F^2\end{align}\]</span> 证毕。</p><p>注：用到了 <span class="math inline">\(\text{tr}(AB)=\text{tr}(BA)\)</span>.</p>          </div></p><p><strong>推论</strong>：矩阵的 F-范数等于所有奇异值平方和开根，即： <span class="math display">\[\norm{A}_F=\left(\sigma_1^2+\cdots+\sigma_r^2\right)^{1/2}\]</span> <div class="note note-secondary">            <p>根据奇异值分解和酉不变性易证。</p>          </div></p><p><strong>性质</strong>：转置和共轭都不改变矩阵的 F-范数，即： <span class="math display">\[\norm{A}_F=\norm{A^T}_F=\norm{\bar A}_F=\norm{A^H}_F\]</span></p><h3 id="定理存在相容的向量范数">定理：存在相容的向量范数</h3><p>对于 <span class="math inline">\(\mathbb C^{m\times n}\)</span> 上的矩阵范数 <span class="math inline">\(\norm{A}\)</span>，存在向量范数 <span class="math inline">\(\norm{x}_v\)</span>，使得 <span class="math inline">\(\norm{Ax}_v\leq\norm{A}\norm{x}_v\)</span>.</p><div class="note note-secondary">            <p>证明：任取非零向量 <span class="math inline">\(y\in\mathbb C^n\)</span>，按如下方式构造向量范数即可： <span class="math display">\[\norm{x}_v=\norm{xy^H}\]</span> 首先由于它是由矩阵范数定义的，所以必然满足向量范数的 3 条性质，即确实是一个向量范数。</p><p>其次证明相容性： <span class="math display">\[\norm{Ax}_v=\norm{Axy^H}\leq\norm{A}\norm{xy^H}=\norm{A}\norm{x}_v\]</span> 证毕。</p>          </div><h2 id="从属范数">从属范数</h2><h3 id="定义">定义</h3><p>已知 <span class="math inline">\(\mathbb C^m\)</span> 和 <span class="math inline">\(\mathbb C^n\)</span> 上的同类向量范数 <span class="math inline">\(\norm{x}_v\)</span>，设 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，定义函数： <span class="math display">\[\norm{A}=\max_{\norm{x}_v=1}\norm{Ax}_v=\max_{x\neq 0}\frac{\norm{Ax}_v}{\norm{x}_v}\]</span> 则 <span class="math inline">\(\norm{A}\)</span> 是矩阵范数，且与 <span class="math inline">\(\norm{x}_v\)</span> 相容。</p><div class="note note-secondary">            <p>证明：</p><ol type="1"><li><p>非负性：若 <span class="math inline">\(A\neq0\)</span>，则存在 <span class="math inline">\(x_0\)</span> 满足 <span class="math inline">\(\norm{x_0}_v=1\)</span> 且 <span class="math inline">\(Ax_0\neq 0\)</span>，于是： <span class="math display">\[\norm{A}\geq\norm{Ax_0}_v&gt;0\]</span> 若 <span class="math inline">\(A=0\)</span>，则： <span class="math display">\[\norm{A}=\max_{\norm{x}_v=1}\norm{Ax}_v=\max_{\norm{x}_v=1}\norm{0}_v=0\]</span></p></li><li><p>齐次性： <span class="math display">\[\norm{kA}=\max_{\norm{x}_v=1}\norm{kAx}_v=k\cdot\max_{\norm{x}_v=1}\norm{Ax}_v=k\norm{A}\]</span></p></li><li><p>三角不等式： <span class="math display">\[\begin{align}\norm{A+B}&amp;=\max_{\norm{x}_v=1}\norm{(A+B)x}_v\leq\max_{\norm{x}_v=1}(\norm{Ax}_v+\norm{Bx}_v)\\&amp;\leq\max_{\norm{x}_v=1}\norm{Ax}_v+\max_{\norm{x}_v=1}\norm{Bx}_v=\norm{A}+\norm{B}\\\end{align}\]</span></p></li><li><p>与 <span class="math inline">\(\norm{x}_v\)</span> 的相容：若 <span class="math inline">\(x=0\)</span> 显然成立；若 <span class="math inline">\(x\neq 0\)</span>，设 <span class="math inline">\(x_0=x/\norm{x}_v\)</span>，则： <span class="math display">\[\norm{Ax_0}_v\leq\max_{\norm{x}_v=1}\norm{Ax}_v=\norm{A}\]</span> 故： <span class="math display">\[\norm{Ax}_v=\big\Vert{A(\norm{x}_v\cdot x_0)}\big\Vert=\norm{Ax_0}\norm{x}_v\leq\norm{A}\norm{x}_v\]</span></p></li><li><p>相容性： <span class="math display">\[\norm{AB}=\max_{\norm{x}_v=1}\norm{ABx}_v=\norm{A(Bx^\ast)}_v\leq\norm{A}\norm{Bx^\ast}_v\leq\norm{A}\norm{B}\]</span> 其中，<span class="math inline">\(x^\ast\)</span> 是使得 <span class="math inline">\(\norm{ABx}_v\)</span> 取到最大值的单位向量。</p></li></ol>          </div><p><strong>特点</strong>：从属范数一定有 <span class="math inline">\(\norm{I}=1\)</span>，但一般的矩阵范数并不一定 ，例如 <span class="math inline">\(\norm{I}_{m_1}=n,\,\norm{I}_F=\sqrt{n}\)</span>.</p><h3 id="范数列范数">1-范数（列范数）</h3><p>由向量的 1-范数诱导出的矩阵范数： <span class="math display">\[\norm{A}_1=\max_{j}\sum_{i=1}^m|a_{ij}|\]</span> 即对各列的元素求绝对值（模）的和，选最大的和。</p><div class="note note-secondary">            <p>证明：记 <span class="math inline">\(t=\max_{j}\sum_{i=1}^m|a_{ij}|\)</span>，我们只需要证明 <span class="math inline">\(\norm{A}_1\leq t\)</span> 且 <span class="math inline">\(\norm{A}_1\geq t\)</span> 即可。</p><p>首先，设 <span class="math inline">\(x\in\mathbb C^n\)</span> 且 <span class="math inline">\(\norm{x}_1=1\)</span>，则： <span class="math display">\[\begin{align}\norm{Ax}_1&amp;=\sum_{i=1}^m\left|\sum_{j=1}^na_{ij}x_j\right|\\&amp;\leq\sum_{i=1}^m\sum_{j=1}^n|a_{ij}||x_j|\\&amp;=\sum_{j=1}^n\left(|x_j|\cdot\sum_{i=1}^m|a_{ij}|\right)\\&amp;\leq\sum_{j=1}^n\left(|x_j|\cdot t\right)=t\cdot\norm{x}_1=t\end{align}\]</span> 由于上式对任意长度为 1 的 <span class="math inline">\(x\)</span> 都成立，因此 <span class="math inline">\(\norm{A}_1\leq t\)</span>.</p><p>其次，设 <span class="math inline">\(k=\mathop{\text{argmax }}_{j}\sum_{i=1}^m|a_{ij}|\)</span>，即 <span class="math inline">\(t=\sum_{i=1}^m|a_{ik}|\)</span>；再设 <span class="math inline">\(e_k=(0,\ldots,1,\ldots,0)^T\)</span>，则： <span class="math display">\[\norm{A}_1\geq\norm{Ae_k}_1=\sum_{i=1}^m|a_{ik}|=t\]</span> 综上，<span class="math inline">\(\norm{A}_1=t\)</span>. 证毕。</p>          </div><h3 id="无穷范数行范数">无穷范数（行范数）</h3><p>由向量的无穷范数诱导出的矩阵范数： <span class="math display">\[\norm{A}_\infty=\max_i\sum_{j=1}^n|a_{ij}|\]</span> 即对各行的元素求绝对值（模）的和，选最大的和。</p><div class="note note-secondary">            <p>证明：记 <span class="math inline">\(t=\max_{i}\sum_{j=1}^n|a_{ij}|\)</span>，我们只需要证明 <span class="math inline">\(\norm{A}_\infty\leq t\)</span> 且 <span class="math inline">\(\norm{A}_\infty\geq t\)</span> 即可。</p><p>首先，设 <span class="math inline">\(x\in\mathbb C^n\)</span> 且 <span class="math inline">\(\norm{x}_\infty=1\)</span>，则 <span class="math inline">\(|x_j|\leq 1\)</span>，于是： <span class="math display">\[\begin{align}\norm{Ax}_\infty&amp;=\max_{i=1}^m\left|\sum_{j=1}^na_{ij}x_j\right|\\&amp;\leq\max_{i=1}^m\sum_{j=1}^n|a_{ij}||x_j|\\&amp;\leq\max_{i=1}^m\sum_{j=1}^n|a_{ij}|=t\end{align}\]</span> 由于上式对任意长度为 1 的 <span class="math inline">\(x\)</span> 都成立，因此 <span class="math inline">\(\norm{A}_\infty\leq t\)</span>.</p><p>其次，设 <span class="math inline">\(\mathbf1=(1,\ldots,1)^T\)</span>，则： <span class="math display">\[\norm{A}_\infty\geq\norm{A\mathbf1}_\infty=\max_{i=1}^m\sum_{j=1}^n|a_{ij}|=t\]</span> 综上 <span class="math inline">\(\norm{A}_\infty=t\)</span>. 证毕。</p>          </div><h3 id="范数谱范数">2-范数（谱范数）</h3><p>由向量的 2-范数诱导出的矩阵范数： <span class="math display">\[\norm{A}_2=\max_j\sqrt{\lambda_j(A^HA)}=\sigma_1\]</span> 即 <span class="math inline">\(A\)</span> 的最大奇异值。</p><div class="note note-secondary">            <p>证明：设 <span class="math inline">\(A\)</span> 的奇异值分解为 <span class="math inline">\(A=U\Sigma V^H\)</span>，其中 <span class="math inline">\(U,V\)</span> 都是酉矩阵。则： <span class="math display">\[\begin{align}\norm{A}_2&amp;=\max_{x\neq 0}\frac{\norm{Ax}_2}{\norm{x}_2}=\max_{x\neq 0}\frac{\norm{U\Sigma V^Hx}_2}{\norm{x}_2}=\max_{x\neq 0}\frac{\norm{\Sigma V^Hx}_2}{\norm{V^Hx}_2}\\&amp;=\max_{y\neq 0}\frac{\norm{\Sigma y}_2}{\norm{y}_2}=\max_{\norm{y}_2=1}\norm{\Sigma y}_2=\max_{\norm{y}_2=1}\left(\sum_{i=1}^ny_i^2\sigma_i^2\right)^{1/2}\end{align}\]</span> 显然当 <span class="math inline">\(y=(1,0,\ldots,0)^T\)</span> 时上式取到最大值 <span class="math inline">\(\sigma_1\)</span>. 证毕。</p>          </div><p><strong>定理（谱范数的酉不变性）</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，酉矩阵 <span class="math inline">\(P\in\mathbb C^{m\times n},\,Q\in\mathbb C^{n\times n}\)</span>，则： <span class="math display">\[\norm{PA}_2=\norm{A}_2=\norm{AQ}_2\]</span> <div class="note note-secondary">            <p>由于酉变换不改变矩阵的奇异值，因此谱范数是酉不变范数。</p>          </div></p><h2 id="范数的应用">范数的应用</h2><h3 id="逆矩阵的扰动分析">逆矩阵的扰动分析</h3><p><strong>定理</strong>：设有 <span class="math inline">\(\mathbb C^{n\times n}\)</span> 上的矩阵范数 <span class="math inline">\(\norm\cdot\)</span>，若矩阵 <span class="math inline">\(A\in\mathbb C^{n\times n}\)</span> 满足： <span class="math display">\[\norm{A}&lt;1\]</span> 则 <span class="math inline">\(I-A\)</span> 非奇异，且： <span class="math display">\[\begin{align}&amp;\norm{(I-A)^{-1}}\leq\frac{\norm{I}}{1-\norm{A}}\\&amp;\norm{I-(I-A)^{-1}}\leq\frac{\norm{A}}{1-\norm{A}}\end{align}\]</span> <div class="note note-secondary">            <p>证明：</p><p>(1). 选取与 <span class="math inline">\(\norm{A}\)</span> 相容的向量范数 <span class="math inline">\(\norm{x}_v\)</span>（根据前文的定理，一定存在这样的向量范数）。假设 <span class="math inline">\(\det(I-A)=0\)</span>，则取 <span class="math inline">\(x_0\in N(I-A)\)</span>，有： <span class="math display">\[(I-A)x_0=0\implies x_0=Ax_0\implies\norm{x_0}_v=\norm{Ax_0}_v\leq\norm{A}\norm{x_0}_v&lt;\norm{x_0}_v\]</span> 因此假设不成立，故 <span class="math inline">\(I-A\)</span> 非奇异。</p><p>(2). <span class="math display">\[\begin{align}(I-A)^{-1}(I-A)=I&amp;\implies(I-A)^{-1}=I+(I-A)^{-1}A\\&amp;\implies\norm{(I-A)^{-1}}\leq\norm{I}+\norm{(I-A)^{-1}A}\\&amp;\implies\norm{(I-A)^{-1}}\leq\norm{I}+\norm{(I-A)^{-1}}\norm{A}\\&amp;\implies\norm{(I-A)^{-1}}\leq\frac{\norm{I}}{1-\norm{A}}\\\end{align}\]</span> (3). <span class="math display">\[\begin{align}(I-A)-I=-A&amp;\implies I-(I-A)^{-1}=-A(I-A)^{-1}\\&amp;\implies A-A(I-A)^{-1}=-A^2(I-A)^{-1}\\&amp;\implies A(I-A)^{-1}=A+A^2(I-A)^{-1}\\&amp;\implies\norm{A(I-A)^{-1}}\leq\norm{A}+\norm{A}\norm{A(I-A)^{-1}}\\&amp;\implies\norm{A(I-A)^{-1}}\leq\frac{\norm{A}}{1-\norm{A}}\\&amp;\implies\norm{I-(I-A)^{-1}}=\norm{-A(I-A)^{-1}}\leq\frac{\norm{A}}{1-\norm{A}}\end{align}\]</span> 证毕。</p><p>注：(2)、(3) 的证明思路可以从后往前推。</p>          </div></p><p><strong>条件数</strong>：设 <span class="math inline">\(A\in\mathbb C^{n\times n}\)</span> 可逆，其条件数定义为： <span class="math display">\[\text{cond}(A)=\norm{A}\norm{A^{-1}}\]</span> 条件数反映了 <span class="math inline">\(A^{-1}\)</span> 对扰动的敏感程度。例如，设有线性方程组 <span class="math inline">\(Ax=b\)</span>，其扰动解满足： <span class="math display">\[(A+\delta A)(x+\delta x)=b+\delta b\]</span> 那么： <span class="math display">\[Ax+\delta A\cdot x+A\cdot \delta x+\delta A\cdot \delta x=b+\delta b\implies \delta A\cdot x+A\cdot \delta x\approx\delta b\]</span> 故： <span class="math display">\[\delta x=A^{-1}\delta b-A^{-1}\delta A\cdot x\]</span> 两边取范数并放大： <span class="math display">\[\begin{align}\norm{\delta x}&amp;=\norm{A^{-1}\delta b-A^{-1}\delta A\cdot x}\\&amp;\leq\norm{A^{-1}\delta b}+\norm{A^{-1}\delta A\cdot x}\\&amp;\leq\norm{A^{-1}}\norm{\delta b}+\norm{A^{-1}}\norm{\delta A}\norm{x}\end{align}\]</span> 再根据 <span class="math inline">\(\norm{A}\norm{x}\geq\norm{b}\)</span>，于是： <span class="math display">\[\begin{align}\frac{\norm{\delta x}}{\norm{x}}&amp;\leq\frac{\norm{A^{-1}}\norm{\delta b}}{\norm{x}}+\norm{A^{-1}}\norm{\delta A}\\&amp;\leq\norm{A}\norm{A^{-1}}\frac{\norm{\delta b}}{\norm{b}}+\norm{A}\norm{A^{-1}}\frac{\norm{\delta A}}{\norm{A}}\\&amp;=\text{cond}(A)\left(\frac{\norm{\delta b}}{\norm{b}}+\frac{\norm{\delta A}}{\norm{A}}\right)\end{align}\]</span> 也就是说，解的相对扰动被输入 <span class="math inline">\(A,b\)</span> 的相对扰动和条件数控制。如果 <span class="math inline">\(A\)</span> 的条件数很大，那么解的误差就会很大。</p><h3 id="谱半径及其性质">谱半径及其性质</h3><p><strong>谱半径</strong>：设 <span class="math inline">\(A\in\mathbb C^{n\times n}\)</span> 的特征值为 <span class="math inline">\(\lambda_1,\ldots,\lambda_n\)</span>，则谱半径定义为： <span class="math display">\[\rho(A)=\max_i|\lambda_i|\]</span> <strong>谱半径是矩阵范数的下确界</strong>：设 <span class="math inline">\(A\in\mathbb C^{n\times n}\)</span>，则对任意一种矩阵范数都有： <span class="math display">\[\rho(A)\leq\norm{A}\]</span> 而对任意 <span class="math inline">\(\epsilon&gt;0\)</span>，必然存在某种矩阵范数，使得： <span class="math display">\[\norm{A}\leq \rho(A)+\epsilon\]</span> <div class="note note-secondary">            <p>证明：</p><p>首先证明第一个关系式。设 <span class="math inline">\(\lambda\)</span> 为 <span class="math inline">\(A\)</span> 的特征值，<span class="math inline">\(x\)</span> 为对应特征向量，<span class="math inline">\(\norm{\cdot}_v\)</span> 为与 <span class="math inline">\(\norm{A}\)</span> 相容的向量范数，则： <span class="math display">\[|\lambda|\norm{x}_v=\norm{\lambda x}_v=\norm{Ax}_v\leq\norm{A}\norm{x}_v\implies |\lambda|\leq\norm{A}\]</span> 于是： <span class="math display">\[\rho(A)=\max_i|\lambda_i|\leq\norm{A}\]</span> 然后证明第二个关系式。将 <span class="math inline">\(A\)</span> 相似变换为 Jordan 标准形，即存在 <span class="math inline">\(P\in\mathbb C^{n\times n}\)</span>，使得： <span class="math display">\[P^{-1}AP=\Lambda+\tilde I=\text{diag}(\lambda_1,\ldots,\lambda_n)+\begin{bmatrix}0&amp;\delta_1&amp;&amp;&amp;\\&amp;0&amp;\delta_2&amp;&amp;\\&amp;&amp;\ddots&amp;\ddots&amp;\\&amp;&amp;&amp;0&amp;\delta_{n-1}\\&amp;&amp;&amp;&amp;0\end{bmatrix}\]</span> 其中 <span class="math inline">\(\delta_i\)</span> 为 0 或 1. 构造对角矩阵 <span class="math inline">\(D=\text{diag}(1,\epsilon,\ldots,\epsilon^{n-1})\)</span>，设 <span class="math inline">\(S=PD\)</span>，则： <span class="math display">\[S^{-1}AS=D^{-1}P^{-1}APD=D^{-1}(\Lambda+\tilde I)D=\Lambda+D^{-1}\tilde ID=\Lambda+\epsilon\tilde I\]</span> 于是： <span class="math display">\[\norm{S^{-1}AS}_1\leq\norm{\Lambda}_1+\epsilon\norm{\tilde I}_1\leq\rho(A)+\epsilon\]</span> 因此，构造矩阵范数： <span class="math display">\[\norm{A}_M=\norm{S^{-1}AS}_1\]</span> 即可满足 <span class="math inline">\(\norm{A}_M\leq\rho(A)+\epsilon\)</span>. 证毕。</p><p>值得注意的是，<strong>上述构造的范数与矩阵 <span class="math inline">\(A\)</span> 有关，实际上不存在这样的矩阵范数对任意矩阵都成立</strong>。</p>          </div></p><p>上述关系也可以表述为： <span class="math display">\[\rho(A)=\inf_\alpha\norm{A}_\alpha\]</span> 可视化：</p><p><img src="rho.png" width=40% /></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[模式分类]非参数方法</title>
    <link href="/blog-main/2023/11/01/%E6%A8%A1%E5%BC%8F%E5%88%86%E7%B1%BB-%E9%9D%9E%E5%8F%82%E6%95%B0%E6%96%B9%E6%B3%95/"/>
    <url>/blog-main/2023/11/01/%E6%A8%A1%E5%BC%8F%E5%88%86%E7%B1%BB-%E9%9D%9E%E5%8F%82%E6%95%B0%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文对应《模式分类》的第 4 章。</p></blockquote><h2 id="核心思想">核心思想</h2><p>给定样本集 <span class="math inline">\(D=\{\mathbf x_1,\ldots,\mathbf x_n\}\)</span>，假定这些样本独立采样自 <span class="math inline">\(p(\mathbf x)\)</span>，我们希望得到 <span class="math inline">\(p(\mathbf x)\)</span> 的一个估计。</p><p>考虑样本空间中的一个小区域 <span class="math inline">\(R\)</span>。一方面，若 <span class="math inline">\(p(\mathbf x)\)</span> 是连续的，且 <span class="math inline">\(R\)</span> 足够小使得 <span class="math inline">\(p(\mathbf x)\)</span> 在 <span class="math inline">\(R\)</span> 中几乎不变，那么向量 <span class="math inline">\(\mathbf x\)</span> 落入 <span class="math inline">\(R\)</span> 的概率为： <span class="math display">\[P=\int_R p(\mathbf x&#39;)\mathrm d\mathbf x&#39;\approx p(\mathbf x)V\]</span> 其中 <span class="math inline">\(V\)</span> 是区域 <span class="math inline">\(R\)</span> 的体积。另一方面，当数据量足够大时，如果有 <span class="math inline">\(k\)</span> 个样本落入 <span class="math inline">\(R\)</span>，那么： <span class="math display">\[P\approx k/n\]</span> 因此，联立上述两式，得： <span class="math display">\[p(\mathbf x)\approx\frac{k/n}{V}\]</span> 然而，为了让这个“约等于”尽可能准确，<span class="math inline">\(V\)</span> 需要趋近于零，<span class="math inline">\(n\)</span> 需要趋近于无穷。但是在现实中我们能获得的样本量肯定是有限的。因此，如果 <span class="math inline">\(V\)</span> 设置得太小，那么落入 <span class="math inline">\(R\)</span> 的样本太少，甚至没有，导致对 <span class="math inline">\(p(\mathbf x)\)</span> 的估计不连续；如果 <span class="math inline">\(V\)</span> 设置得太大，那么对 <span class="math inline">\(p(\mathbf x)\)</span> 的估计将太平滑。</p><p>为了解决这个问题，我们考虑如下过程：为了估计 <span class="math inline">\(\mathbf x\)</span> 处的概率密度，构造一系列包含点 <span class="math inline">\(\mathbf x\)</span> 的区域 <span class="math inline">\(R_1,R_2,\ldots\)</span>，其中 <span class="math inline">\(R_n\)</span> 将使用 <span class="math inline">\(n\)</span> 个样本做密度估计。记 <span class="math inline">\(V_n\)</span> 为 <span class="math inline">\(R_n\)</span> 的体积，<span class="math inline">\(k_n\)</span> 为落入 <span class="math inline">\(R_n\)</span> 的样本数，那么可以得到序列： <span class="math display">\[p_n(\mathbf x)=\frac{k_n/n}{V_n},\quad n=1,2,\ldots\label{1}\tag{1}\]</span> 当以下三个条件满足时，<span class="math inline">\(p_n(\mathbf x)\)</span> 能收敛到 <span class="math inline">\(p(\mathbf x)\)</span>：</p><ol type="1"><li><span class="math inline">\(\lim_{n\to\infty}V_n=0\)</span></li><li><span class="math inline">\(\lim_{n\to\infty} k_n=\infty\)</span></li><li><span class="math inline">\(\lim_{n\to\infty} k_n/n=0\)</span></li></ol><p>为了构造这样的序列，我们有两种方法——<strong>Parzen 窗</strong>和 <strong>k 近邻</strong>。前者取 <span class="math inline">\(V_n\)</span> 为某个关于 <span class="math inline">\(n\)</span> 的函数（例如 <span class="math inline">\(V_n=1/\sqrt{n}\)</span>），而后者取 <span class="math inline">\(k_n\)</span> 为某个关于 <span class="math inline">\(n\)</span> 的函数（例如 <span class="math inline">\(k_n=\sqrt{n}\)</span>）。在 <span class="math inline">\(n\to\infty\)</span> 时二者都能够收敛，但在有限样本情况下很难预测它们的效果。</p><p><img src="two-methods.png" width=80% /></p><h2 id="parzen-窗">Parzen 窗</h2><h3 id="基本原理">基本原理</h3><p>为了方便，首先假设区域 <span class="math inline">\(R_n\)</span> 是以 <span class="math inline">\(\mathbf x\)</span>（待求密度处）为中心、边长为 <span class="math inline">\(h_n\)</span> 的 <span class="math inline">\(d\)</span> 维超立方体，则其体积为： <span class="math display">\[V_n=h_n^d\]</span> 为了解析地表达 <span class="math inline">\(k_n\)</span>，定义<strong>窗函数</strong>如下： <span class="math display">\[\varphi(\mathbf u)=\begin{cases}1,&amp;|u_j|\leq 1/2,\,j=1,\ldots,d\\0,&amp;\text{otherwise}\end{cases}\]</span> 即一个以原点为中心、边长为 <span class="math inline">\(1\)</span> 的超立方体。那么： <span class="math display">\[k_n=\sum_{i=1}^n\varphi\left(\frac{\mathbf x-\mathbf x_i}{h_n}\right)\]</span> 代入 <span class="math inline">\(\eqref{1}\)</span> 式得： <span class="math display">\[p_n(\mathbf x)=\frac{1}{n}\sum_{i=1}^n\frac{1}{V_n}\varphi\left(\frac{\mathbf x-\mathbf x_i}{h_n}\right)\label{parzen}\tag{2}\]</span> 可以验证这的确是一个概率分布。</p><div class="note note-info">            <p>非负性显然，只需验证归一性： <span class="math display">\[\begin{align}\int p_n(\mathbf x)\mathrm d\mathbf x&amp;=\frac{1}{n}\sum_{i=1}^n\frac{1}{V_n}\int\varphi\left(\frac{\mathbf x-\mathbf x_i}{h_n}\right)\mathrm d\mathbf x\\&amp;=\frac{1}{n}\sum_{i=1}^n\frac{1}{V_n}\int\varphi(\mathbf u_i)h_n^d\mathrm d\mathbf u_i&amp;&amp;\mathbf u_i=\frac{\mathbf x-\mathbf x_i}{h_n}\\&amp;=\frac{1}{n}\sum_{i=1}^n\int\varphi(\mathbf u_i)\mathrm d\mathbf u_i\\&amp;=\frac{1}{n}\sum_{i=1}^n1\\&amp;=1\end{align}\]</span> 因此 <span class="math inline">\(\eqref{parzen}\)</span> 式的确是一个概率分布。</p>          </div><h3 id="核函数角度">核函数角度</h3><p>从核函数的角度理解，定义： <span class="math display">\[K(\mathbf x,\mathbf x_i)=\frac{1}{V_n}\varphi\left(\frac{\mathbf x-\mathbf x_i}{h_n}\right)\]</span> 满足： <span class="math display">\[K(\mathbf x,\mathbf x_i)\geq 0,\quad\int K(\mathbf x,\mathbf x_i)=1\]</span> 那么 <span class="math inline">\(\eqref{parzen}\)</span> 式可以写作： <span class="math display">\[p_n(\mathbf x)=\frac{1}{n}\sum_{i=1}^n K(\mathbf x,\mathbf x_i)\tag{3}\label{parzen-kernel}\]</span> 因此 Parzen 窗方法也被称作<strong>核密度估计 (KDE)</strong>。<span class="math inline">\(\eqref{parzen-kernel}\)</span> 式意味着 Parzen 窗估计也可以视作<strong>用核函数对样本在取值空间中进行插值</strong>。</p><h3 id="窗函数核函数的选择">窗函数/核函数的选择</h3><p>上面为了推导方便，我们假定了窗函数是单位超立方体，但这只是一种选择而已，我们还可以使用其他形式：</p><ul><li><p><strong>正态窗</strong>： <span class="math display">\[K(\mathbf x,\mathbf x_i)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(\mathbf x-\mathbf x_i)^T\Sigma^{-1}(\mathbf x-\mathbf x_i)\right)\]</span></p></li><li><p><strong>球窗</strong>： <span class="math display">\[K(\mathbf x,\mathbf x_i)=\begin{cases}\frac{1}{V},&amp;\Vert\mathbf x-\mathbf x_i\Vert\leq r\\0,&amp;\text{otherwise}\end{cases}\]</span> 其中 <span class="math inline">\(r\)</span> 是超球体的半径，<span class="math inline">\(V\)</span> 是超球体的体积。</p></li></ul><h3 id="窗宽的影响">窗宽的影响</h3><p>显然，如果 <span class="math inline">\(h_n\)</span>（或 <span class="math inline">\(V_n\)</span>）选取太大，那么估计不够精确，可以理解为欠拟合；如果太小，那么不够稳定，可以理解为过拟合。下图展示了不同情况下用正态窗做估计的例子，窗宽设置为 <span class="math inline">\(h_n=h_1/\sqrt{n}\)</span>，其中 <span class="math inline">\(h_1\)</span> 是可以调整的参数。</p><p><img src="parzen-ex.png" /></p><h2 id="k-近邻">k 近邻</h2><h3 id="基本原理-1">基本原理</h3><p>Parzen 窗方法是人为设置 <span class="math inline">\(V_n\)</span>，再计算 <span class="math inline">\(k_n\)</span>；k 近邻方法则相反——人为设置 <span class="math inline">\(k_n\)</span>，再调整 <span class="math inline">\(V_n\)</span> 使得区域内正好落入 <span class="math inline">\(k_n\)</span> 个样本。这样窗宽将与训练样本有关，避免了如何选取合适窗宽的问题。</p><p>值得注意的是，尽管 k 近邻估计出的 <span class="math inline">\(p_n(\mathbf x)\)</span> 是连续的，但其往往不可导，会有非常多的尖峰，且这些不可导点与原数据点几乎都是不同的，如下图所示：</p><p><img src="knn-ex1.png" width=100% /></p><p>另外，与 Parzen 窗不同的是，k 近邻得到的概率密度估计<strong>并不是一个合法的概率密度函数</strong>。例如，在一维情形下，记第 <span class="math inline">\(k_n\)</span> 个正好落入区域内的样本为 <span class="math inline">\(\mathbf x_\text{kNN}\)</span>，那么 <span class="math inline">\(V_n=2|\mathbf x-\mathbf x_\text{kNN}|\)</span>，于是代入 <span class="math inline">\(\eqref{1}\)</span> 式得： <span class="math display">\[p_n(\mathbf x)=\frac{k_n}{2n|\mathbf x-\mathbf x_\text{kNN}|}\]</span> 由于 <span class="math inline">\(\frac{1}{x}\)</span> 的积分是发散的，所以 <span class="math inline">\(p_n(\mathbf x)\)</span> 的积分是无穷大，如下图所示：</p><p><img src="knn-ex2.png" width=60% /></p><p>虽然积分是发散的，但 k 近邻密度估计的一个优点是 <span class="math inline">\(p_n(\mathbf x)\)</span> 永远不会为零，这在高维情况下非常有用。</p><h3 id="用于估计后验概率">用于估计后验概率</h3><p>我们可以用 k 近邻方法估计每一个类别的概率分布，然后使用<strong>最大后验准则</strong>进行分类。具体而言，设 <span class="math inline">\(\mathbf x\)</span> 周围包含 <span class="math inline">\(k\)</span> 个样本的区域中，有 <span class="math inline">\(k_i\)</span> 个样本属于 <span class="math inline">\(\omega_i\)</span> 类，那么： <span class="math display">\[p_n(\mathbf x\vert\omega_i)=\frac{k_i/n_i}{V},\quad p_n(\mathbf x,\omega_i)=\frac{k_i/n}{V}\]</span> 其中 <span class="math inline">\(n_i\)</span> 表示属于 <span class="math inline">\(\omega_i\)</span> 类的样本数量。于是对后验概率的估计为： <span class="math display">\[P_n(\omega_i\vert\mathbf x)=\frac{p_n(\mathbf x,\omega_i)}{p(\mathbf x)}=\frac{k_i/n}{V}\cdot\frac{V}{k/n}=\frac{k_i}{k}\]</span> 即区域中属于 <span class="math inline">\(\omega_i\)</span> 类的样本数量占区域中所有样本数量的比例。</p><p>根据最大后验准则，有了后验概率，就可以得到一个分类器： <span class="math display">\[\omega_m=\mathop{\text{argmax}}_i\{P_n(\omega_i\vert\mathbf x)\}\]</span></p><h3 id="最近邻分类器">最近邻分类器</h3><p>上面提到我们可以用 k 近邻方法估计后验概率，再根据最大后验准则就可以进行分类。但事实上，我们只依赖最近邻就能达到足够好的性能。</p><p>最近邻分类器的基本思想非常简单，即对于一个新样本，将其与已知样本逐一比较，找出距离最近的已知样本，以该样本的类别作为新样本的类别。如此，特征空间可以被分成一个个小单元（称作 Voronoi 网格），如图所示：</p><p><img src="knn-partition.png" width=70% /></p><p>最近邻分类器有多好呢？可以证明，<strong>在无限训练样本的情形下，其误差率最多不会超过贝叶斯误差率的两倍</strong>。具体而言，设 <span class="math inline">\(P_n(e)\)</span> 为 <span class="math inline">\(n\)</span> 个样本下最近邻分类器的误差率，当 <span class="math inline">\(n\)</span> 趋近无穷时该误差收敛到 <span class="math inline">\(P\)</span>，记 <span class="math inline">\(P^\ast\)</span> 为贝叶斯分类器的误差率，<span class="math inline">\(c\)</span> 为类别数量，那么有： <span class="math display">\[P^\ast\leq P\leq P^\ast\left(2-\frac{c}{c-1}P^\ast\right)\]</span> <img src="knn-error.png" width=30% /></p><p>证明比较复杂，暂时略去，以后有时间再看。</p><h3 id="k-近邻分类器及其改进">k 近邻分类器及其改进</h3><p>将最近邻分类器进行推广，选择前若干个离测试样本最近的样本，取其中出现最多的类别作为新样本的类别，这就是 k 近邻分类器。对 k 近邻分类器的分析比最近邻更加复杂，这里略去。结论是，当样本量无限时，随着 <span class="math inline">\(k\)</span> 的增加，k 近邻分类器的误差率逐渐逼近下界贝叶斯误差率；当 <span class="math inline">\(k\)</span> 趋近无穷大时二者相等。</p><p><img src="knn-error2.png" width=40% /></p><p>一些改进方法：</p><ul><li><p><strong>剪辑近邻法</strong>。考虑到分类时最容易分类错误的地方就是交界区域处，因此可以设法将交界区域的样本去掉。因此我们需要识别出那些位于交界区域的样本。一种做法是：将已知样本集划分为训练集和测试集，采用近邻法利用训练集中的样本对测试样本进行分类，从中去掉被错分类的样本，剩余样本构成剪辑样本集，用于对新来的样本进行分类。</p><p><strong>多重剪辑</strong>：</p><ol type="1"><li>划分。将样本集随机划分为 <span class="math inline">\(s\)</span> 个子集 <span class="math inline">\(X_1,X_2,\ldots, X_s\)</span>；</li><li>分类。轮流地以其中一个作为训练样本集，对其邻近编号的样本进行测试；</li><li>剪辑。从各个子集中去掉在步骤 2 中被分错的样本；</li><li>混合。将剩下的样本合在一起，形成新的样本集；</li><li>迭代。转步骤 1，如果没有新的样本被剪辑掉，则停止迭代。</li></ol></li><li><p><strong>压缩近邻法</strong>。考虑近邻法的分类原理，那些远离分类边界的样本对于最后的分类决策没有贡献，因此可以去掉。</p><p>将样本集为两个活动的子集：储存集 <span class="math inline">\(X_S\)</span> 和备选集 <span class="math inline">\(X_G\)</span>.</p><p>首先，在算法开始时，<span class="math inline">\(X_S\)</span> 中只有一个样本，其余样本均在 <span class="math inline">\(X_G\)</span> 中；</p><p>然后，考查 <span class="math inline">\(X_G\)</span> 中的每一个样本，如果采用 <span class="math inline">\(X_S\)</span> 中的样本能够对其正确分类，则该样本仍然保留在 <span class="math inline">\(X_G\)</span> 中， 否则移动到 <span class="math inline">\(X_S\)</span> 中，从而扩大代表集合。依次重复进行上述操作，直到没有样本需要搬移为止。</p><p>最后，用 <span class="math inline">\(X_S\)</span> 中的样本作为代表样本，对新来的样本进行分类。</p></li></ul><h2 id="距离度量">距离度量</h2><p>合法的距离度量应满足：</p><ol type="1"><li>非负性：<span class="math inline">\(D(\mathbf x,\mathbf y)\geq 0\)</span></li><li>自反性：<span class="math inline">\(D(\mathbf x,\mathbf y)=0\iff\mathbf x=\mathbf y\)</span></li><li>对称性：<span class="math inline">\(D(\mathbf x,\mathbf y)=D(\mathbf y,\mathbf x)\)</span></li><li>三角不等式：<span class="math inline">\(D(\mathbf x,\mathbf y)+D(\mathbf y,\mathbf z)\geq D(\mathbf x,\mathbf z)\)</span></li></ol><p>常见距离度量：</p><ul><li><p>Minkowski 距离： <span class="math display">\[D(\mathbf x,\mathbf y)=\left(\sum_{i=1}^d|x_i-y_i|^q\right)^{1/q}\]</span></p></li><li><p>Manhattan 距离： <span class="math display">\[D(\mathbf x,\mathbf y)=\sum_{i=1}^d|x_i-y_i|\]</span></p></li><li><p>Euclidean 距离： <span class="math display">\[D(\mathbf x,\mathbf y)=\sqrt{\sum_{i=1}^d(x_i-y_i)^2}\]</span></p></li><li><p>Chebyshev 距离： <span class="math display">\[D(\mathbf x,\mathbf y)=\max_{i=1}^d|x_i-y_i|\]</span></p></li><li><p>Mahalanobis 距离： <span class="math display">\[D(\mathbf x,\mathbf y)=\sqrt{(\mathbf x-\mathbf y)^TM(\mathbf x-\mathbf y)}\]</span> 其中 <span class="math inline">\(M\)</span> 为半正定矩阵。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>模式分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]2.1向量范数</title>
    <link href="/blog-main/2023/10/26/%E7%9F%A9%E9%98%B5%E8%AE%BA-2-1%E5%90%91%E9%87%8F%E8%8C%83%E6%95%B0/"/>
    <url>/blog-main/2023/10/26/%E7%9F%A9%E9%98%B5%E8%AE%BA-2-1%E5%90%91%E9%87%8F%E8%8C%83%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand\norm[1]{\Vert#1\Vert}\]</span></p><h2 id="概念与基本性质">概念与基本性质</h2><h3 id="向量范数的定义">向量范数的定义</h3><p>设 <span class="math inline">\(V\)</span> 是数域 <span class="math inline">\(K\)</span> 上的线性空间，对 <span class="math inline">\(V\)</span> 的任一向量 <span class="math inline">\(x\)</span>，定义实值函数 <span class="math inline">\(\norm x\)</span>，满足：</p><ol type="1"><li><strong>非负性</strong>：<span class="math inline">\(\norm x\geq0\)</span>，且 <span class="math inline">\(\norm{x}=0\iff x=0\)</span></li><li><strong>齐次性</strong>：<span class="math inline">\(\norm{kx}=|k|\norm{x},\,k\in K,\,x\in V\)</span></li><li><strong>三角不等式</strong>：<span class="math inline">\(\norm{x+y}\leq\norm{x}+\norm{y},\,x,y\in V\)</span></li></ol><blockquote><p>注：<span class="math inline">\(|k|\)</span> 表示 <span class="math inline">\(k\)</span> 的绝对值（<span class="math inline">\(K=\mathbb R\)</span>）或模（<span class="math inline">\(K=\mathbb C\)</span>）。</p></blockquote><h3 id="向量范数的性质">向量范数的性质</h3><p><strong>性质 1（范数是凸函数）</strong>： <span class="math display">\[\norm{(1-\lambda)x+\lambda y}\leq(1-\lambda)\norm x+\lambda\norm y,\quad 0\leq\lambda\leq1\]</span> <strong>性质 2（范数的乘法）</strong>：若 <span class="math inline">\(\norm{\cdot}\)</span> 是 <span class="math inline">\(V\)</span> 上的向量范数，则 <span class="math inline">\(k\norm{\cdot}\)</span> 仍然为向量范数，其中 <span class="math inline">\(k&gt;0\)</span>.</p><p><strong>性质 3（范数的复合）</strong>：设 <span class="math inline">\(\norm{\cdot}_\text{comp}\)</span> 是 <span class="math inline">\(\mathbb R^m\)</span> 上的范数，且对 <span class="math inline">\(x\in\mathbb (R^+)^m\)</span> 为单调增加的。那么，给定 <span class="math inline">\(m\)</span> 个 <span class="math inline">\(n\)</span> 维线性空间 <span class="math inline">\(V\)</span> 上的范数 <span class="math inline">\(\norm{\cdot}_i\,(i=1,\ldots,m)\)</span>，可以定义复合范数为： <span class="math display">\[\norm{x}=\norm{U(x)}_\text{comp},\quad \text{where }\ U(x)=(\norm{x}_1,\ldots,\norm{x}_m)^T\]</span> <div class="note note-secondary">            <p>证明：非负性和齐次性是显然的，下面证明三角不等式。 <span class="math display">\[\begin{align}\norm{x+y}&amp;=\norm{U(x+y)}_\text{comp}\\&amp;\leq\norm{U(x)+U(y)}_\text{comp}&amp;&amp;U(x+y)\leq U(x)+U(y)\\&amp;\leq\norm{U(x)}_\text{comp}+\norm{U(y)}_\text{comp}\\&amp;=\norm{x}+\norm{y}\end{align}\]</span> 证毕。</p>          </div></p><p>例如：设 <span class="math inline">\(\norm\cdot_f\)</span> 和 <span class="math inline">\(\norm\cdot_g\)</span> 是线性空间 <span class="math inline">\(V\)</span> 上的两个向量范数，则：</p><ol type="1"><li><span class="math inline">\(\norm\cdot_f+\norm\cdot_g\)</span> 是 <span class="math inline">\(V\)</span> 上的向量范数</li><li><span class="math inline">\(\max\{\norm\cdot_f,\norm\cdot_g\}\)</span> 是 <span class="math inline">\(V\)</span> 上的向量范数</li><li><span class="math inline">\(\left[(\norm\cdot_f)^2+(\norm\cdot_g)^2\right]^{1/2}\)</span> 是 <span class="math inline">\(V\)</span> 上的向量范数</li></ol><p><strong>性质 4（范数的合成）</strong>：设 <span class="math inline">\(n\)</span> 维线性空间 <span class="math inline">\(V=V_1\oplus V_2\oplus\cdots\oplus V_m\)</span>，且 <span class="math inline">\(\norm\cdot_i\,(i=1,\ldots,m)\)</span> 为线性子空间 <span class="math inline">\(V_i\)</span> 上的范数。设 <span class="math inline">\(\norm{\cdot}_\text{comp}\)</span> 是 <span class="math inline">\(\mathbb R^m\)</span> 上的范数，且对 <span class="math inline">\(x\in\mathbb (R^+)^m\)</span> 为单调增加的，则对任意 <span class="math inline">\(x\in V\)</span>，存在唯一分解 <span class="math inline">\(x=x_1+\cdots+x_n\)</span>，其中 <span class="math inline">\(x_i\in V_i\)</span>. 定义合成范数为： <span class="math display">\[\norm x=\norm{U(x)}_\text{comp},\quad\text{where }\ U(x)=(\norm{x_1}_1,\ldots,\norm{x_m}_m)^T\]</span> <div class="note note-secondary">            <p>证明与性质 3 是类似的。</p>          </div></p><p><strong>定义（均衡闭凸集）</strong>：线性空间 <span class="math inline">\(V\)</span> 的闭凸集 <span class="math inline">\(\Omega\)</span> 若满足：<span class="math inline">\(x\in\Omega\implies \lambda x\in\Omega\,(|\lambda|\leq1)\)</span>，那么 <span class="math inline">\(\Omega\)</span> 为均衡闭凸集。</p><p><strong>性质 5（范数的几何性质：范数与均衡闭凸集一一对应）</strong>：若 <span class="math inline">\(\norm\cdot\)</span> 为 <span class="math inline">\(V\)</span> 上的向量范数，则 <span class="math inline">\(\Omega=\{x\mid\norm x\leq 1\}\)</span> 是 <span class="math inline">\(V\)</span> 上的均衡闭凸集；反之，若 <span class="math inline">\(\Omega\)</span> 是 <span class="math inline">\(V\)</span> 上的均衡闭凸集，且 <span class="math inline">\(\Omega\)</span> 含有内点，即包含一个小单位球，则可以定义函数 <span class="math inline">\(P(x)\)</span> 如下： <span class="math display">\[P(x)=\begin{cases}\min\{\lambda&gt;0\mid x/\lambda\in\Omega\}&amp;&amp;x\neq0\\0&amp;&amp;x=0\end{cases}\]</span> 那么 <span class="math inline">\(P(x)\)</span> 为 <span class="math inline">\(V\)</span> 上的范数。</p><h3 id="补充绝对范数与单调范数">补充：绝对范数与单调范数</h3><p><strong>绝对范数</strong>：设 <span class="math inline">\(x=(\xi_1,\ldots,\xi_n)^T\in\mathbb C^n\)</span>，记 <span class="math inline">\(|x|=(|\xi_1|,\ldots,|\xi_n|)^T\in\mathbb R^n\)</span>，若 <span class="math inline">\(\mathbb C^n\)</span> 上的范数 <span class="math inline">\(v\)</span> 满足条件： <span class="math display">\[v(x)=v(|x|),\quad \forall x\in\mathbb C^n\]</span> 则称 <span class="math inline">\(v\)</span> 为绝对范数。</p><p><strong>单调范数</strong>：若对于任意 <span class="math inline">\(x,y\in\mathbb C^n\)</span>，范数 <span class="math inline">\(v\)</span> 满足： <span class="math display">\[|x|\leq |y|\implies v(x)\leq v(y)\]</span> 则称 <span class="math inline">\(v\)</span> 为单调范数。</p><p><strong>定理</strong>：<span class="math inline">\(v\)</span> 为绝对范数的充要条件是 <span class="math inline">\(v\)</span> 为单调范数。</p><h2 id="具体实例">具体实例</h2><h3 id="范数">2-范数</h3><p><span class="math display">\[\norm x_2=\sqrt{|x_1|^2+|x_2|^2+\cdots+|x_n|^2}\]</span></p><h3 id="范数-1">1-范数</h3><p><span class="math display">\[\norm x_1=|x_1|+|x_2|+\cdots+|x_n|\]</span></p><h3 id="infty-范数"><span class="math inline">\(\infty\)</span>-范数</h3><p><span class="math display">\[\norm x_\infty=\max_{i=1}^n |x_i|\]</span></p><h3 id="p-范数"><span class="math inline">\(p\)</span>-范数</h3><p><span class="math display">\[\norm x_p=\left(|x_1|^p+|x_2|^p+\cdots+|x_n|^p\right)^{1/p}\quad\quad p\geq 1\]</span></p><blockquote><p>当 <span class="math inline">\(0\leq p&lt;1\)</span> 时并不是范数，因为不满足三角不等式，但是在实际应用中仍然有重要应用。</p></blockquote><p><img src="pnorm.png" width=50% /></p><p>为了证明 <span class="math inline">\(p\)</span>-范数满足三角不等式，首先需要证明一个引理和 Hölder 不等式。</p><div class="note note-secondary">            <p><strong>引理</strong>：对任意实数 <span class="math inline">\(\alpha&gt;0,\beta&gt;0\)</span>，都有 <span class="math inline">\(\alpha\beta\leq \frac{\alpha^p}{p}+\frac{\beta^q}{q}\)</span>，其中 <span class="math inline">\(p&gt;1,q&gt;1\)</span> 且 <span class="math inline">\(\frac{1}{p}+\frac{1}{q}=1\)</span>.</p><p>证： <span class="math display">\[\begin{align}\frac{\alpha^p}{p}+\frac{\beta^q}{q}&amp;\geq\frac{q\alpha^p+p\beta^q}{pq}=\frac{q\alpha^p+p\beta^q}{p+q}\\&amp;=\frac{(\alpha^p+\cdots+\alpha^p)+(\beta^q+\cdots+\beta^q)}{p+q}\\&amp;\geq\sqrt[p+q]{\alpha^{pq}\beta^{pq}}=\sqrt[pq]{\alpha^{pq}\beta^{pq}}=\alpha\beta\end{align}\]</span> 证毕。</p>          </div><div class="note note-secondary">            <p><strong>Hölder 不等式</strong>：对任意 <span class="math inline">\(\xi_k,\eta_k\in\mathbb C\,(k=1,\ldots,n)\)</span>，有： <span class="math display">\[\sum_{k=1}^n|\xi_k||\eta_k|\leq\left(\sum_{k=1}^n|\xi_k|^p\right)^{1/p}\left(\sum_{k=1}^n|\eta_k|^q\right)^{1/q}\]</span> 其中 <span class="math inline">\(p&gt;1,q&gt;1\)</span> 且 <span class="math inline">\(\frac{1}{p}+\frac{1}{q}=1\)</span>.</p><p>证：令 <span class="math inline">\(\alpha={|\xi_i|}/{\left(\sum_{k=1}^n|\xi_k|^p\right)^{1/p}}\)</span>，<span class="math inline">\(\beta={|\eta_i|}/{\left(\sum_{k=1}^n|\eta_k|^q\right)^{1/q}}\)</span>，由引理得： <span class="math display">\[\frac{|\xi_i|}{\left(\sum_{k=1}^n|\xi_k|^p\right)^{1/p}}\cdot\frac{|\eta_i|}{\left(\sum_{k=1}^n|\eta_k|^q\right)^{1/q}}\leq\frac{|\xi_i|^p}{p\sum_{k=1}^n|\xi_k|^p}+\frac{|\eta_i|^q}{q\sum_{k=1}^n|\eta_k|^q}\]</span> 对 <span class="math inline">\(i\)</span> 求和： <span class="math display">\[\frac{\sum_{i=1}^n|\xi_i||\eta_i|}{\left(\sum_{k=1}^n|\xi_k|^p\right)^{1/p}\left(\sum_{k=1}^n|\eta_k|^q\right)^{1/q}}\leq\frac{1}{p}+\frac{1}{q}=1\]</span> 证毕。</p>          </div><p>接下来利用 Hölder 不等式就可以证明 <span class="math inline">\(p\)</span>-范数满足三角不等式了。</p><div class="note note-secondary">            <p>设 <span class="math inline">\(x,y\in\mathbb C^n\)</span>，求证 <span class="math inline">\(\norm{x+y}_p\leq\norm{x}_p+\norm{y}_p\)</span>，其中 <span class="math inline">\(p\geq 1\)</span>.</p><p>证明： <span class="math display">\[\begin{align}\norm{x+y}_p^p&amp;=\sum_{k=1}^n|x_k+y_k|^p\\&amp;\leq\sum_{k=1}^n|x_k||x_k+y_k|^{p-1}+\sum_{k=1}^n|y_k||x_k+y_k|^{p-1}\\&amp;\leq\left(\sum_{k=1}^n|x_k|^p\right)^{1/p}\left(\sum_{k=1}^n|x_k+y_k|^{q(p-1)}\right)^{1/q}+\left(\sum_{k=1}^n|x_k|^p\right)^{1/p}\left(\sum_{k=1}^n|x_k+y_k|^{q(p-1)}\right)^{1/q}\\&amp;=\left(\norm{x}_p+\norm{y}_p\right)\left(\sum_{k=1}^n|x_k+y_k|^{p}\right)^{1/q}\\&amp;=\left(\norm{x}_p+\norm{y}_p\right)\norm{x+y}_p^{p/q}\\&amp;=\left(\norm{x}_p+\norm{y}_p\right)\norm{x+y}_p^{p-1}\end{align}\]</span> 于是： <span class="math display">\[\norm{x+y}_p\leq\left(\norm{x}_p+\norm{y}_p\right)\]</span> 证毕。</p>          </div><h3 id="补充对偶范数">补充：对偶范数</h3><p><strong>定义</strong>：令 <span class="math inline">\(\norm\cdot\)</span> 为 <span class="math inline">\(\mathbb R^n\)</span> 上的范数，其对偶范数 <span class="math inline">\(\norm\cdot_\ast\)</span> 定义为： <span class="math display">\[\norm{z}_\ast=\sup_{\norm x\leq 1}\{z^Tx\}\]</span> <strong>性质</strong>：<span class="math inline">\(\norm{x}_{\ast\ast}=\norm x\)</span>.</p><p><strong>例子</strong>：<span class="math inline">\(l_p\)</span> 和 <span class="math inline">\(l_q\)</span> 互为对偶范数，其中 <span class="math inline">\(\frac{1}{p}+\frac{1}{q}=1\)</span>.</p><h2 id="范数等价">范数等价</h2><p>对有限维空间 <span class="math inline">\(V^n\)</span> 中任意两个向量范数 <span class="math inline">\(\norm x_\alpha,\norm x_\beta\)</span>，若存在正常数 <span class="math inline">\(c_1,c_2\)</span>，使得： <span class="math display">\[c_1\norm{x}_\beta\leq\norm{x}_\alpha\leq c_2\norm{x}_\beta,\quad\forall x\in V^n\]</span> 则称范数 <span class="math inline">\(\norm{x}_\alpha\)</span> 与 <span class="math inline">\(\norm{x}_\beta\)</span> 等价。</p><p>范数等价是一个<strong>等价关系</strong>，满足<strong>自反性、对称性、传递性</strong>。</p><p><strong>定理</strong>：有限维空间中任意两个向量范数都等价。</p><div class="note note-secondary">            <p>证明：由于等价关系具有传递性，我们只需要证明任意一个向量范数都等价于 2-范数即可。</p><p>令 <span class="math inline">\(f(x)=\norm{x}\)</span> 为 <span class="math inline">\(V^n\)</span> 上的任一向量范数，由于当 <span class="math inline">\(x\to y\)</span> 时，<span class="math inline">\(\left|\norm x-\norm y\right|\leq \norm{x-y}\to 0\)</span>，因此范数是连续函数。于是 <span class="math inline">\(f(x)\)</span> 在单位超球面上有大于零的最小值和最大值： <span class="math display">\[0&lt;\min_{\norm{x}_2=1}f(x)\leq \max_{\norm{x}_2=1}f(x)\]</span> 记上述最小值为 <span class="math inline">\(c_1\)</span>，最大值为 <span class="math inline">\(c_2\)</span>，于是： <span class="math display">\[c_1\leq \left\Vert\frac{x}{\norm{x}_2}\right\Vert\leq c_2\implies c_1\norm{x}_2\leq\norm{x}\leq c_2\norm{x}_2\]</span> 故 <span class="math inline">\(\norm\cdot\)</span> 与 2-范数等价。证毕。</p>          </div><h2 id="基于向量范数的收敛性">基于向量范数的收敛性</h2><p><strong>定义</strong>：设 <span class="math inline">\(\{x^{(k)}\}\)</span> 是 <span class="math inline">\(V^n\)</span> 中的向量序列，若存在 <span class="math inline">\(x\in V^n\)</span>，使得： <span class="math display">\[\lim_{k\to\infty} \norm{x^{(k)}-x}_\alpha=0\]</span> 则称序列 <span class="math inline">\(\{x^{(k)}\}\)</span> 按 <span class="math inline">\(\alpha\)</span> 范数收敛到 <span class="math inline">\(x\)</span>.</p><p><strong>定理</strong>：向量各分量收敛等价于范数收敛，即： <span class="math display">\[\lim_{k\to\infty} x^{(k)}=x\iff\lim_{k\to\infty} \norm{x^{(k)}-x}=0\]</span> <div class="note note-secondary">            <p>由于向量范数的等价性，只需要对 1-范数证明即可。 <span class="math display">\[\begin{align}x^{(k)}\to x&amp;\iff \xi_i^{(k)}\to\xi_i&amp;&amp;(i=1,\ldots,n)\\&amp;\iff |\xi_i^{(k)}-\xi_i|\to 0&amp;&amp;(i=1,\ldots,n)\\&amp;\iff \sum_{i=1}^n|\xi_i^{(k)}-\xi_i|\to 0\\&amp;\iff \norm{x^{(k)}-x}_1\to0\end{align}\]</span> 证毕。</p>          </div></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]1.3欧氏空间与酉空间</title>
    <link href="/blog-main/2023/10/17/%E7%9F%A9%E9%98%B5%E8%AE%BA-1-3%E6%AC%A7%E6%B0%8F%E7%A9%BA%E9%97%B4%E4%B8%8E%E9%85%89%E7%A9%BA%E9%97%B4/"/>
    <url>/blog-main/2023/10/17/%E7%9F%A9%E9%98%B5%E8%AE%BA-1-3%E6%AC%A7%E6%B0%8F%E7%A9%BA%E9%97%B4%E4%B8%8E%E9%85%89%E7%A9%BA%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<h2 id="欧氏空间">欧氏空间</h2><p>欧氏空间（Euclid 空间、内积空间）是定义了<strong>内积</strong>运算的<strong>实数域</strong> <span class="math inline">\(\mathbb R\)</span> 上线性空间。</p><h3 id="内积的定义与基本性质">内积的定义与基本性质</h3><p>满足 4 条性质：</p><ol type="1"><li>交换律：<span class="math inline">\((x,y)=(y,x)\)</span></li><li>分配律：<span class="math inline">\((x,y+z)=(x,y)+(x,z)\)</span></li><li>齐次性：<span class="math inline">\((kx,y)=k(x,y),\,\forall k\in \mathbb R\)</span></li><li>非负性：<span class="math inline">\((x,x)\geq 0\)</span>，当且仅当 <span class="math inline">\(x=0\)</span> 时 <span class="math inline">\((x,x)=0\)</span></li></ol><p>任意线性空间上都可以定义内积，但是<strong>不唯一</strong>。一种较为简单的定义方式是根据坐标定义内积（见下文）。</p><p><strong>内积的基本性质</strong>：</p><ol type="1"><li><span class="math inline">\((x,ky)=k(x,y)\)</span></li><li><span class="math inline">\((x,0)=(0,x)=0\)</span></li><li>线性性：<span class="math inline">\(\left(\sum_{i=1}^n\xi_ix_i,\sum_{j=1}^n\eta_jy_j\right)=\sum_{i=1}^n\sum_{j=1}^n\xi_i\eta_j(x_i,y_j)\)</span></li></ol><h3 id="根据坐标定义内积">根据坐标定义内积</h3><p>设 <span class="math inline">\(X\)</span> 为 <span class="math inline">\(V\)</span> 上的一个基，向量 <span class="math inline">\(x,y\in V\)</span> 在该基下的坐标分别为 <span class="math inline">\(\alpha=(\alpha_1,\ldots,\alpha_n)^T,\beta=(\beta_1,\ldots,\beta_n)^T\)</span>，则可以定义内积为： <span class="math display">\[(x,y)=\alpha_1\beta_1+\cdots+\alpha_n\beta_n=\alpha^T\beta\]</span> 容易验证这确实满足内积的 4 个条件。</p><div class="note note-success">            <p>这门课上内积是一个抽象的概念，只有在上述定义下可以写作 <span class="math inline">\(\alpha^T\beta\)</span> 的形式，否则只能写成 <span class="math inline">\((x,y)\)</span> 的形式。</p>          </div><p>这种定义方式与基的选取有关，可以推导不同基下这样定义的内积之间的关系。设 <span class="math inline">\(X&#39;=XC\)</span>，<span class="math inline">\(x,y\)</span> 在 <span class="math inline">\(X&#39;\)</span> 下的坐标为 <span class="math inline">\(\alpha&#39;,\beta&#39;\)</span>，那么有：<span class="math inline">\(\alpha&#39;=C^{-1}\alpha,\,\beta&#39;=C^{-1}\beta\)</span>，于是： <span class="math display">\[(x,y)&#39;=(\alpha&#39;)^T\beta&#39;=\alpha^T(C^{-1})^TC^{-1}\beta=\alpha^T A^{-1}\beta\]</span> 其中 <span class="math inline">\(A=CC^T\)</span> 为正定矩阵。</p><h3 id="长度模-范数和夹角">长度（模 / 范数）和夹角</h3><p>长度（模 / 范数）：<span class="math inline">\(\Vert x\Vert=\sqrt{(x,x)}\)</span></p><p>夹角：<span class="math inline">\(\langle x,y\rangle=\arccos\dfrac{(x,y)}{\Vert x\Vert\Vert y\Vert}\)</span></p><h3 id="gram-矩阵与合同">Gram 矩阵与合同</h3><p><strong>定义</strong>：向量组 <span class="math inline">\(x_1,\ldots,x_n\)</span> 组成的 Gram 矩阵为： <span class="math display">\[\text{Gram}(x_1,\ldots,x_n)=[(x_i,x_j)]_{ij}\]</span> <strong>基于 Gram 矩阵的线性无关判别定理</strong></p><p><span class="math inline">\(x_1,\ldots,x_n\)</span> 线性无关的充要条件是它们组成的 Gram 矩阵非奇异。</p><div class="note note-secondary">            <p>证明：设 <span class="math inline">\(a_1x_1+\cdots+a_nx_n=0\)</span>，则与 <span class="math inline">\(x_k\)</span> 做内积得： <span class="math display">\[a_1(x_k,x_1)+\cdots+a_n(x_k,x_n)=0,\quad k=1,\ldots,n\]</span> 写作矩阵形式： <span class="math display">\[\text{Gram}(x_1,\ldots,x_n)\begin{bmatrix}a_1\\\vdots\\a_n\end{bmatrix}=0\]</span> 这是一个关于 <span class="math inline">\(a_1,\ldots,a_n\)</span> 的齐次线性方程，Gram 矩阵非奇异 <span class="math inline">\(\iff\)</span> <span class="math inline">\((a_1,\ldots,a_n)^T\)</span> 只有零解 <span class="math inline">\(\iff\)</span> <span class="math inline">\(x_1,\ldots,x_n\)</span> 线性无关。</p><p>证毕。</p>          </div><p><strong>合同</strong></p><p>设向量组 <span class="math inline">\(X=(x_1,\ldots,x_n)\)</span> 与向量组 <span class="math inline">\(Y=(y_1,\ldots,y_n)\)</span> 的 Gram 矩阵分别是 <span class="math inline">\(A=\text{Gram}(X),\,B=\text{Gram}(Y)\)</span>，且 <span class="math inline">\(Y=XC\)</span>（即 <span class="math inline">\(C\)</span> 是 <span class="math inline">\(Y\)</span> 在向量组 <span class="math inline">\(X\)</span> 下的表示矩阵），那么： <span class="math display">\[B=C^TAC\]</span> 称 <span class="math inline">\(A\)</span> 与 <span class="math inline">\(B\)</span> 合同。</p><div class="note note-secondary">            <p>证明： <span class="math display">\[B_{ij}=(y_i,y_j)=\left(\sum_{k=1}^nc_{ki}x_k,\sum_{l=1}^nc_{lj}x_l\right)=\sum_{k=1}^n\sum_{l=1}^nc_{ki}c_{lj}(x_k,x_l)=\sum_{k=1}^n\sum_{l=1}^nc_{ki}A_{kl}c_{lj}\]</span> 写作矩阵形式就是 <span class="math inline">\(B=C^TAC\)</span>. 证毕。</p>          </div><h3 id="schwarz-不等式">Schwarz 不等式</h3><p><span class="math display">\[|(x,y)|\leq \Vert x\Vert\Vert y\Vert\]</span></p><div class="note note-secondary">            <p>证明：设有向量组 <span class="math inline">\(X=(x_1,\ldots,x_m)\)</span>，设 <span class="math inline">\(y\)</span> 可由它们线性表示：<span class="math inline">\(y=\sum_{i=1}^m\lambda_ix_i\)</span>，则： <span class="math display">\[F(\lambda)=\Vert y\Vert^2=(y,y)=\left(\sum_{i=1}^m\lambda_ix_i,\sum_{j=1}^m\lambda_jx_j\right)=\sum_{i=1}^m\sum_{j=1}^m\lambda_i\lambda_j(x_i,x_j)=\lambda^T \text{Gram}(X)\lambda\geq0\]</span> 二次型 <span class="math inline">\(F(\lambda)\)</span> 非负，故 <span class="math inline">\(\text{Gram}(X)\)</span> 半正定，那么 <span class="math inline">\(\det(\text{Gram}(X))\geq 0\)</span>.</p><p>当 <span class="math inline">\(m=2\)</span> 时，<span class="math inline">\(X=(x,y)\)</span>，那么： <span class="math display">\[\det\left(\begin{bmatrix}(x,x)&amp;(x,y)\\(y,x)&amp;(y,y)\end{bmatrix}\right)=(x,x)(y,y)-(x,y)(y,x)\geq 0\]</span> 化简即得 Schwarz 不等式。</p>          </div><h3 id="三角不等式">三角不等式</h3><p><span class="math display">\[\Vert x+y\Vert\leq \Vert x\Vert+\Vert y\Vert\]</span></p><div class="note note-secondary">            <p>证明： <span class="math display">\[\Vert x+y\Vert^2=(x+y,x+y)=(x,x)+2(x,y)+(y,x)\leq \Vert x\Vert^2+2\Vert x\Vert\Vert y\Vert+\Vert y\Vert^2=(\Vert x\Vert+\Vert y\Vert)^2\]</span> 证毕。</p>          </div><h3 id="reize-表示定理">Reize 表示定理</h3><p>欧氏空间 <span class="math inline">\(V^n\)</span> 中所有的线性函数都可以表示为内积的形式。</p><p>即：设 <span class="math inline">\(l(x)\)</span> 为 <span class="math inline">\(V^n\)</span> 的一个线性函数，则存在一个向量 <span class="math inline">\(u_l\in V^n\)</span>，使得对任一 <span class="math inline">\(x\in V^n\)</span> 都有 <span class="math inline">\(l(x)=(u_l,x)\)</span>.</p><div class="note note-secondary">            <p>证明：取 <span class="math inline">\(V^n\)</span> 中的一个基 <span class="math inline">\(X=(x_1,\ldots,x_n)\)</span>，设 <span class="math inline">\(x=\sum_{i=1}^n\alpha_ix_i\)</span>，则： <span class="math display">\[l(x)=l\left(\sum_{i=1}^n\alpha_ix_i\right)=\sum_{i=1}^n\alpha_i l(x_i)\]</span> 定义内积为基 <span class="math inline">\(X\)</span> 下坐标的内积，那么构造 <span class="math inline">\(u_l\)</span> 为对应坐标 <span class="math inline">\((l(x_1),\ldots,l(x_n))\)</span> 的向量，即： <span class="math display">\[u_l=X\big(l(x_1),\ldots,l(x_n)\big)=\sum_{i=1}^nl(x_i)x_i\]</span> 那么就有 <span class="math inline">\(l(x)=(u_l,x)\)</span>. 证毕。</p>          </div><h3 id="正交性正交向量组标准正交基">正交性、正交向量组、标准正交基</h3><p><strong>正交</strong>：<span class="math inline">\((x,y)=0\)</span>，记作 <span class="math inline">\(x\perp y\)</span>.</p><p><strong>正交向量组</strong>：两两正交的一组非零向量（注：必定线性无关）</p><p><strong>标准正交基</strong>：单位向量组成的正交基</p><h3 id="gram-schmidt-正交化过程">Gram-Schmidt 正交化过程</h3><p>正交化： <span class="math display">\[\begin{align}&amp;y_1=x_1\\&amp;y_2=x_2-\frac{(x_2,y_1)}{(y_1,y_1)}y_1\\&amp;y_3=x_3-\frac{(x_3,y_1)}{(y_1,y_1)}y_1-\frac{(x_3,y_2)}{(y_1,y_2)}y_2\\&amp;\vdots\\&amp;y_i=x_i-\sum_{k=1}^{i-1}\frac{(x_i,y_k)}{(y_k,y_k)}y_k\\&amp;\vdots\end{align}\]</span> 归一化： <span class="math display">\[z_i=\frac{y_i}{\Vert y_i\Vert},\quad i=1,\ldots,n\]</span> <div class="note note-secondary">            <p>证明正交性：数学归纳法，假设前 <span class="math inline">\(y_1,\ldots,y_{i-1}\)</span> 两两正交，那么对于 <span class="math inline">\(j=1,\ldots,i-1\)</span>，有： <span class="math display">\[\begin{align}(y_i,y_j)&amp;=\left(x_i-\sum_{k=1}^{i-1}\frac{(x_i,y_k)}{(y_k,y_k)}y_k,y_j\right)\\&amp;=(x_i,y_j)-\left(\sum_{k=1}^{i-1}\frac{(x_i,y_k)}{(y_k,y_k)}y_k,y_j\right)\\&amp;=(x_i,y_j)-\sum_{k=1}^{i-1}\frac{(x_i,y_k)}{(y_k,y_k)}(y_k,y_j)\\&amp;=(x_i,y_j)-\frac{(x_i,y_j)}{(y_j,y_j)}(y_j,y_j)\\&amp;=0\end{align}\]</span> 即 <span class="math inline">\(y_i\perp y_j\)</span>. 根据归纳法，<span class="math inline">\(y_1,\ldots,y_n\)</span> 两两正交。</p>          </div></p><h3 id="子空间的正交性">子空间的正交性</h3><p>设 <span class="math inline">\(V^n\)</span> 的两个子空间 <span class="math inline">\(V_1,V_2\)</span> 满足：<span class="math inline">\(\forall x\in V_1,\forall y\in V_2\)</span>，<span class="math inline">\((x,y)=0\)</span>，称 <span class="math inline">\(V_1\)</span> 与 <span class="math inline">\(V_2\)</span> 正交。</p><p><strong>正交补</strong>：<span class="math inline">\(V_1^{\perp}=\{x\mid(x,y)=0,\forall y\in V_1,x\in V^n\}\)</span>.</p><p><strong>定理</strong>：<span class="math inline">\(V_1\oplus V_1^{\perp}=V^n\)</span>.</p><div class="note note-secondary">            <p>证明：<span class="math inline">\(V_1\cap V_1^{\perp}=\{0\}\)</span> 显然，<span class="math inline">\(V_1+V_1^{\perp}\subset V^n\)</span> 显然，故只需证明 <span class="math inline">\(V_1+V_1^{\perp}\supset V^n\)</span>.</p><p>设 <span class="math inline">\(V_1\)</span> 的一个正交基为 <span class="math inline">\((x_1,\ldots,x_r)\)</span>，任取 <span class="math inline">\(z\in V^n\)</span>，设 <span class="math inline">\(x=\sum_{i=1}^r(z,x_i)x_i\)</span>，则只需证 <span class="math inline">\(y=z-x\in V_1^{\perp}\)</span>，即证 <span class="math inline">\((y,x_i)=0\)</span>. <span class="math display">\[(y,x_i)=(z-x,x_i)=\left(z-\sum_{j=1}^n(z,x_j)x_j,x_i\right)=(z,x_i)-\sum_{i=1}^n(z,x_j)(x_j,x_i)=(z,x_i)-(z,x_i)=0\]</span> 证毕。</p>          </div><p><strong>定理</strong>：对任意矩阵 <span class="math inline">\(A\in\mathbb R^{m\times n}\)</span>，有： <span class="math display">\[\begin{align}&amp;R^{\perp}(A)=N(A^T),\quad R(A)\oplus N(A^T)=\mathbb R^m\\&amp;R^{\perp}(A^T)=N(A),\quad R(A^T)\oplus N(A)=\mathbb R^n\end{align}\]</span> <div class="note note-info">            <p>也就是说，<span class="math inline">\(R(A)\)</span> 与 <span class="math inline">\(N(A^T)\)</span> 互为正交补、<span class="math inline">\(R(A^T)\)</span> 与 <span class="math inline">\(N(A)\)</span> 互为正交补，即 Gilbert Strang 的四个基本子空间图中垂直符号的意义：</p><p><img src="spaces.png" width=50% /></p>          </div></p><h3 id="正交变换与正交矩阵">正交变换与正交矩阵</h3><p><strong>正交变换</strong>：保持长度不变，即 <span class="math inline">\(\forall x\in V,\,(Tx,Tx)=(x,x)\)</span>.</p><p><strong>定理</strong>：<span class="math inline">\(T\)</span> 正交的充要条件是 <span class="math inline">\(\forall x,y\in V,\,(Tx,Ty)=(x,y)\)</span>，即保持长度不变等价于保持内积不变。</p><p><strong>正交矩阵</strong>：方阵 <span class="math inline">\(Q\)</span> 满足：<span class="math inline">\(Q^TQ=I\)</span> 或 <span class="math inline">\(Q^{-1}=Q^T\)</span>. 即 <span class="math inline">\(Q\)</span> 各列向量标准正交。</p><p><strong>正交变换与正交矩阵</strong>：<span class="math inline">\(T\)</span> 为正交变换的充要条件是其在<strong>标准正交基</strong>下的矩阵表示是正交矩阵。</p><div class="note note-secondary">            <p>证明：设 <span class="math inline">\(X\)</span> 为一个标准正交基，<span class="math inline">\(TX=XA\)</span>，任取 <span class="math inline">\(x=X\alpha\)</span>，则 <span class="math inline">\(Tx=TX\alpha=XA\alpha\)</span>，因此： <span class="math display">\[(Tx,Tx)=(A\alpha)^T(A\alpha)=\alpha^TA^TA\alpha=(x,x)=\alpha^T\alpha\iff\alpha^T(I-A^TA)\alpha=0\iff A^TA=I\]</span> 证毕。</p>          </div><div class="note note-warning">            <p>注意必须是在标准正交基下。</p>          </div><p><strong>正交矩阵的性质</strong>：</p><ol type="1"><li><p>正交矩阵非奇异。</p></li><li><p>正交矩阵的逆仍为正交矩阵。</p></li><li><p>正交矩阵的乘积仍为正交矩阵。</p></li><li><p>正交基变换矩阵为正交矩阵。</p><div class="note note-secondary">            <p>证明：设 <span class="math inline">\(X,Y\)</span> 为正交基，<span class="math inline">\(Y=XC\)</span>，任取 <span class="math inline">\(x=Y\alpha=XC\alpha,\,y=Y\beta=XC\beta\)</span>，则： <span class="math display">\[(x,y)=\alpha^T\beta=(C\alpha)^T(C\beta)=\alpha^T(C^TC)\beta\implies C^TC=I\]</span> 证毕。</p>          </div></li><li><p>正交矩阵的特征值位于复平面的单位圆上。</p><div class="note note-secondary">            <p>证明：设 <span class="math inline">\(A\)</span> 为正交矩阵，<span class="math inline">\(Ax=\lambda x\ (x\neq 0)\)</span>，则两边取共轭转置得 <span class="math inline">\(x^HA^T=\bar\lambda x^H\)</span>（注意 <span class="math inline">\(A\)</span> 是实矩阵，但其特征值和特征向量可能是复数）。于是： <span class="math display">\[x^Hx=x^HA^TAx=\lambda\bar\lambda x^Hx=|\lambda|^2x^Hx\implies |\lambda|^2=1\]</span> 证毕。</p>          </div></li></ol><h3 id="线性映射和线性变换的共轭">线性映射和线性变换的共轭</h3><p><img src="conj.png" width=50% /></p><p><strong>线性映射的共轭</strong>：设 <span class="math inline">\(P\)</span> 是欧氏空间 <span class="math inline">\(W\)</span> 到欧氏空间 <span class="math inline">\(V\)</span> 的一个线性映射，<span class="math inline">\(Q\)</span> 是欧氏空间 <span class="math inline">\(V\)</span> 到欧氏空间 <span class="math inline">\(W\)</span> 的一个线性映射，若对 <span class="math inline">\(\forall x\in W,y\in V\)</span>，有：<span class="math inline">\((Px,y)=(x,Qy)\)</span>，则称 <span class="math inline">\(Q\)</span> 为 <span class="math inline">\(P\)</span> 的共轭。</p><p>定理：设 <span class="math inline">\(X,Y\)</span> 是 <span class="math inline">\(W,V\)</span> 的标准正交基，<span class="math inline">\(P,Q\)</span> 在 <span class="math inline">\(X,Y\)</span> 下的矩阵表示为 <span class="math inline">\(A,B\)</span>，则 <span class="math inline">\(B=A^T\)</span>.</p><div class="note note-secondary">            <p>证明：由于 <span class="math inline">\(PX=YA,\,QY=XB\)</span>，所以： <span class="math display">\[\begin{cases}(Px_j,y_i)=(\sum_{k=1}^m a_{kj}y_k,y_i)=a_{ij}\\(x_j,Qy_i)=(x_j,\sum_{k=1}^nb_{ki}x_k)=b_{ji}\end{cases}\quad\implies\quad a_{ij}=b_{ji}\]</span> 证毕。</p>          </div><p><strong>线性变换的共轭</strong>：设 <span class="math inline">\(T\)</span> 是欧氏空间 <span class="math inline">\(V\)</span> 上的一个线性变换，若对 <span class="math inline">\(\forall x,y\in V\)</span>，有 <span class="math inline">\((Tx,y)=(x,T^\ast y)\)</span> 成立，则称 <span class="math inline">\(T^\ast\)</span> 为 <span class="math inline">\(T\)</span> 的共轭。</p><p>性质：设 <span class="math inline">\(T\)</span> 在基 <span class="math inline">\(X=(x_1,\ldots,x_n)\)</span> 下的矩阵表示为 <span class="math inline">\(A\)</span>，<span class="math inline">\(X\)</span> 的 Gram 矩阵为 <span class="math inline">\(C\)</span>，那么 <span class="math inline">\(T^\ast\)</span> 在基 <span class="math inline">\(X\)</span> 下的矩阵表示为 <span class="math inline">\(B=C^{-1}A^TC\)</span>.</p><div class="note note-secondary">            <p>证明：由于 <span class="math inline">\(TX=XA,\,T^\ast X=XB\)</span>，所以： <span class="math display">\[\begin{cases}(Tx_i,x_j)=\left(\sum_{k=1}^na_{ki}x_k,x_j\right)=\sum_{k=1}^na_{ki}(x_k,x_j)=\sum_{k=1}^na_{ki}c_{kj}\\(x_i,T^\ast x_j)=\left(x_i,\sum_{k=1}^nb_{kj}x_k\right)=\sum_{k=1}^nb_{kj}(x_i,x_k)=\sum_{k=1}^nb_{kj}c_{ik}\\\end{cases}\implies \sum_{k=1}^na_{ki}c_{kj}=\sum_{k=1}^n b_{kj}c_{ik}\]</span> 即 <span class="math inline">\(A^TC=CB\)</span>. 证毕。</p>          </div><h3 id="对称变换与对称矩阵">对称变换与对称矩阵</h3><p><strong>（实）对称变换</strong>：<span class="math inline">\(\forall x,y\in V,\,(Tx,y)=(x,Ty)\)</span>.</p><p><strong>（实）对称矩阵</strong>：欧氏空间中的线性变换是实对称变换的充要条件是它在标准正交基下的矩阵为实对称矩阵。</p><div class="note note-secondary">            <p>证明：设 <span class="math inline">\(X\)</span> 为一个标准正交基，设 <span class="math inline">\(T\)</span> 在 <span class="math inline">\(X\)</span> 下的矩阵表示为 <span class="math inline">\(A\)</span>，即 <span class="math inline">\(TX=XA\)</span>.</p><p>必要性：由于 <span class="math inline">\(X\)</span> 为标准正交基，故其 Gram 矩阵为 <span class="math inline">\(I\)</span>，由于 <span class="math inline">\(T\)</span> 本身就是自己的共轭，所以 <span class="math inline">\(A=I^{-1}A^TI=A^T\)</span>.</p><p>充分性： <span class="math display">\[\begin{cases}(Tx_i,x_j)=(\sum_{k=1}^na_{ki}x_k,x_j)=a_{ji}\\(x_i,Tx_j)=(x_i,\sum_{k=1}^na_{kj}x_k)=a_{ij}\end{cases}\quad\implies\quad a_{ji}=a_{ij}\]</span> 证毕。</p>          </div><p><strong>性质</strong>：实对称矩阵特征值都为实数，属于不同特征值的特征向量相互正交。</p><div class="note note-secondary">            <p>证明：设 <span class="math inline">\(Ax=\lambda x\ (x\neq 0)\)</span>，则： <span class="math display">\[x^HAx=\lambda x^Hx=(A^Hx)^Hx=(Ax)^Hx=(\lambda x)^Hx=\bar\lambda x^Hx\implies \lambda=\bar\lambda\]</span> 故 <span class="math inline">\(\lambda\in\mathbb R\)</span>.</p><p>再设 <span class="math inline">\(Ay=\mu y\ (y\neq 0)\)</span> 且 <span class="math inline">\(\lambda\neq \mu\)</span>，则： <span class="math display">\[y^TAx=\lambda y^Tx=(A^Ty)^Tx=(Ay)^Tx=\mu y^Tx\implies y^Tx=0\]</span> 证毕。</p>          </div><h2 id="酉空间">酉空间</h2><p>酉空间将欧氏空间扩展到了<strong>复数域</strong> <span class="math inline">\(\mathbb C\)</span> 上。</p><h3 id="复内积">复内积</h3><p>满足 4 条性质：</p><ol type="1"><li>交换律：<span class="math inline">\((x,y)=\overline{(y,x)}\)</span></li><li>分配律：<span class="math inline">\((x,y+z)=(x,y)+(x,z)\)</span></li><li>齐次性：<span class="math inline">\((kx,y)=k(x,y),\,\forall k\in \mathbb C\)</span></li><li>非负性：<span class="math inline">\((x,x)\geq 0\)</span>，当且仅当 <span class="math inline">\(x=0\)</span> 时 <span class="math inline">\((x,x)=0\)</span></li></ol><h3 id="根据坐标定义复内积">根据坐标定义复内积</h3><p>设 <span class="math inline">\(X\)</span> 为 <span class="math inline">\(V\)</span> 上的一个基，向量 <span class="math inline">\(x,y\in V\)</span> 在该基下的坐标分别为 <span class="math inline">\(\alpha=(\alpha_1,\ldots,\alpha_n)^T,\beta=(\beta_1,\ldots,\beta_n)^T\)</span>，则可以定义内积为： <span class="math display">\[(x,y)=\alpha_1\bar\beta_1+\cdots+\alpha_n\bar\beta_n=\beta^H\alpha\]</span> 容易验证这确实满足内积的 4 个条件。</p><div class="note note-warning">            <p>注意共轭转置取在 <span class="math inline">\(\beta\)</span> 上，这是为了满足齐次性而导致的。在有些教材上，齐次性写作 <span class="math inline">\((x,ky)=k(x,y)\)</span>，则内积相应地会变成 <span class="math inline">\(\alpha^H\beta\)</span>，共轭转置取在 <span class="math inline">\(\alpha\)</span> 上。</p>          </div><p>欧氏空间上内积的性质平行推广到复内积空间中。</p><h3 id="酉变换和酉矩阵">酉变换和酉矩阵</h3><div class="note note-success">            <p>可以看作正交变换和正交矩阵在复内积空间中的推广。</p>          </div><p><strong>酉变换</strong>：保持长度不变，即 <span class="math inline">\(\forall x\in V,\,(Tx,Tx)=(x,x)\)</span>.</p><p><strong>定理</strong>：<span class="math inline">\(T\)</span> 是酉变换的充要条件是 <span class="math inline">\(\forall x,y\in V,\,(Tx,Ty)=(x,y)\)</span>，即保持长度不变等价于保持内积不变。</p><p><strong>酉矩阵</strong>：酉变换在酉空间的标准正交基下的矩阵是酉矩阵，满足 <span class="math inline">\(A^HA=AA^H=I\)</span>.</p><p><strong>复线性映射和复线性变换的共轭</strong>：类比欧氏空间上线性映射和线性变换的共轭。</p><h3 id="hermite-变换和-hermite-矩阵">Hermite 变换和 Hermite 矩阵</h3><div class="note note-success">            <p>可以看作对称变换和对称矩阵在复内积空间中的推广。</p>          </div><p><strong>Hermite 变换（酉对称变换）</strong>：<span class="math inline">\(\forall x,y\in V,\,(Tx,y)=(x,Ty)\)</span>.</p><p><strong>Hermite 矩阵</strong>：Hermite 变换在标准正交基下的矩阵 <span class="math inline">\(A\)</span> 为 Hermite 矩阵，即 <span class="math inline">\(A=A^H\)</span>.</p><p><strong>性质</strong>：Hermite 矩阵的特征值都是实数，属于不同特征值的特征向量相互正交。</p><h3 id="schur-定理">Schur 定理</h3><p>在 1.2 节中，我们证明了任意矩阵都相似于一个上三角矩阵。Schur 定理是这个定理的加强版，它限制用来相似化的矩阵是一个酉矩阵。</p><p><strong>定理</strong>：</p><p>(1). 设 <span class="math inline">\(A\in\mathbb C^{n\times n}\)</span>，则存在酉矩阵 <span class="math inline">\(P\)</span> 使得： <span class="math display">\[P^HAP=U\]</span> 其中 <span class="math inline">\(U\)</span> 为上三角矩阵。</p><p>(2). 设 <span class="math inline">\(A\in\mathbb R^{n\times n}\)</span> <strong>且所有特征值为实数</strong>，则存在正交矩阵 <span class="math inline">\(Q\)</span> 使得： <span class="math display">\[Q^TAQ=U\]</span> 其中 <span class="math inline">\(U\)</span> 为上三角矩阵。</p><div class="note note-secondary">            <p>证明过程与证明相似于一个上三角矩阵是类似的，只不过基扩充时需要扩充为标准正交基，所有的可逆矩阵换成酉矩阵。</p>          </div><h2 id="正规矩阵">正规矩阵</h2><h3 id="正规矩阵的定义">正规矩阵的定义</h3><p>设 <span class="math inline">\(A\in\mathbb C^{n\times n}\)</span> 且 <span class="math inline">\(A^HA=AA^H\)</span>，称 <span class="math inline">\(A\)</span> 为正规矩阵。</p><div class="note note-success">            <p>式 <span class="math inline">\(A^HA=AA^H\)</span> 意味着 <span class="math inline">\(A\)</span> 的 <span class="math inline">\(i,j\)</span> 行内积等于 <span class="math inline">\(i,j\)</span> 列内积。因此，前面提到的<strong>正交矩阵、对称矩阵、酉矩阵、Hermite 矩阵都是正规矩阵</strong>。</p>          </div><p><strong>定理</strong>：</p><p>(1). 设 <span class="math inline">\(A\in\mathbb C^{n\times n}\)</span>，则 <span class="math inline">\(A\)</span> 酉相似于对角矩阵的充要条件为 <span class="math inline">\(A\)</span> 为正规矩阵，即存在酉矩阵 <span class="math inline">\(P\)</span> 使得 <span class="math inline">\(P^HAP=D\)</span>，其中 <span class="math inline">\(D\)</span> 为对角矩阵。</p><p>(2). 设 <span class="math inline">\(A\in\mathbb R^{n\times n}\)</span> <strong>且所有特征值为实数</strong>，则 <span class="math inline">\(A\)</span> 正交相似于对角矩阵的充要条件为 <span class="math inline">\(A\)</span> 为正规矩阵，即存在正交矩阵 <span class="math inline">\(Q\)</span> 使得 <span class="math inline">\(Q^TAQ=D\)</span>，其中 <span class="math inline">\(D\)</span> 为对角矩阵。</p><div class="note note-secondary">            <p>根据 Schur 定理，<span class="math inline">\(A\)</span> 酉相似于一个上三角矩阵：<span class="math inline">\(P^HAP=U\)</span>. 容易证明，<span class="math inline">\(A\)</span> 正规 <span class="math inline">\(\iff\)</span> <span class="math inline">\(U\)</span> 正规，因此 <span class="math inline">\(U^HU=UU^H\)</span>，则 <span class="math inline">\(U\)</span> 只能是对角矩阵。证毕。</p>          </div><p><strong>推论</strong>：实对称矩阵正交相似于对角矩阵。</p><p><strong>推论</strong>：设 <span class="math inline">\(T\)</span> 是 <span class="math inline">\(V\)</span> 上的对称变换，则在 <span class="math inline">\(V\)</span> 中存在标准正交基 <span class="math inline">\(y_1,\ldots,y_n\)</span> 使得 <span class="math inline">\(T\)</span> 在该基下的矩阵为对角矩阵。</p><div class="note note-secondary">            <p>设 <span class="math inline">\(X\)</span> 是 <span class="math inline">\(V\)</span> 的一组标准正交基且 <span class="math inline">\(TX=XA\)</span>，则 <span class="math inline">\(A\)</span> 为对称矩阵，因此存在正交矩阵 <span class="math inline">\(P\)</span> 使得 <span class="math inline">\(A=PDP^T\)</span>，其中 <span class="math inline">\(D\)</span> 为对角矩阵. 取 <span class="math inline">\(Y=XP\)</span>，则 <span class="math inline">\(TY=TXP=XAP=XPD=YD\)</span>，故 <span class="math inline">\(D\)</span> 为 <span class="math inline">\(T\)</span> 在 <span class="math inline">\(Y\)</span> 下的矩阵表示。证毕。</p>          </div><h3 id="hermite-矩阵的谱分解">Hermite 矩阵的谱分解</h3><p>设 <span class="math inline">\(A\)</span> 为 Hermite 矩阵，<span class="math inline">\(\lambda_i,p_i\)</span> 是 <span class="math inline">\(A\)</span> 的特征值和特征向量，则： <span class="math display">\[A=\lambda_1p_1p_1^H+\cdots+\lambda_np_np_n^H=P\Lambda P^H\]</span></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]1.2线性变换及其矩阵表示</title>
    <link href="/blog-main/2023/10/12/%E7%9F%A9%E9%98%B5%E8%AE%BA-1-2%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E5%8F%8A%E5%85%B6%E7%9F%A9%E9%98%B5%E8%A1%A8%E7%A4%BA/"/>
    <url>/blog-main/2023/10/12/%E7%9F%A9%E9%98%B5%E8%AE%BA-1-2%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E5%8F%8A%E5%85%B6%E7%9F%A9%E9%98%B5%E8%A1%A8%E7%A4%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="线性映射">线性映射</h2><p>只需验证 2 条性质：</p><ol type="1"><li><span class="math inline">\(T(x+y)=T(x)+T(y)\)</span></li><li><span class="math inline">\(T(kx)=kT(x)\)</span></li></ol><h3 id="线性映射的矩阵表示">线性映射的矩阵表示</h3><p>设有 <span class="math inline">\(m\)</span> 维线性空间 <span class="math inline">\(W\)</span> 和 <span class="math inline">\(n\)</span> 维线性空间 <span class="math inline">\(V\)</span>，<span class="math inline">\(X=(x_1,\ldots,x_m),\,Y=(y_1,\ldots,y_n)\)</span> 分别是 <span class="math inline">\(W,V\)</span> 的基。<span class="math inline">\(X\)</span> 被 <span class="math inline">\(T\)</span> 映射到 <span class="math inline">\(V\)</span> 中后可以由 <span class="math inline">\(Y\)</span> 线性表示，即：</p><p><span class="math display">\[\begin{cases}Tx_1=a_{11}y_1+\cdots+a_{n1}y_n\\\quad\vdots\\Tx_m=a_{1m}y_1+\cdots+a_{nm}y_n\\\end{cases}\implies TX=Y\underbrace{\begin{bmatrix}a_{11}&amp;\cdots&amp;a_{1m}\\\vdots&amp;\ddots&amp;\vdots\\a_{n1}&amp;\cdots&amp;a_{nm}\\\end{bmatrix}}_A\]</span> 称 <span class="math inline">\(A\in\mathbb R^{n\times m}\)</span> 为 <span class="math inline">\(T\)</span> 在基 <span class="math inline">\(X,Y\)</span> 下的矩阵表示。</p><p>可以看见，<strong>线性映射的矩阵表示依赖于基的选取，即 <span class="math inline">\(A=\sigma(T;X,Y)\)</span></strong>.</p><div class="note note-success">            <p>式 <span class="math inline">\(TX=YA\)</span> 非常重要，日后将经常使用。</p>          </div><p>设向量 <span class="math inline">\(x\in W\)</span> 在 <span class="math inline">\(X\)</span> 下的坐标表示为 <span class="math inline">\(\xi\)</span>，<span class="math inline">\(Tx\in V\)</span> 在 <span class="math inline">\(Y\)</span> 下的坐标表示为 <span class="math inline">\(\eta\)</span>，那么： <span class="math display">\[Tx=T(X\xi)=(TX)\xi=(YA)\xi=Y(A\xi)=Y\eta\implies \eta=A\xi\]</span></p><h3 id="不同基下矩阵表示的关系">不同基下矩阵表示的关系</h3><p>既然线性映射的矩阵表示依赖于基的选取，那么同一个线性映射在不同基下的矩阵表示有什么关系呢？</p><p>设 <span class="math inline">\(W,V\)</span> 空间中的另一组基为 <span class="math inline">\(X&#39;,Y&#39;\)</span>，且 <span class="math inline">\(X&#39;=XC,\,Y&#39;=YD\)</span>，那么： <span class="math display">\[TX&#39;=T(XC)=(TX)C=(YA)C=Y&#39;D^{-1}AC=Y&#39;A&#39;\implies A&#39;=D^{-1}AC\]</span> 注意 <span class="math inline">\(D\in\mathbb R^{n\times n},\,A\in\mathbb R^{n\times m},\,C\in\mathbb R^{m\times m}\)</span>.</p><h3 id="复合线性映射的矩阵表示">复合线性映射的矩阵表示</h3><p>设 <span class="math inline">\(S:W\to V,\,T: V\to U\)</span>，定义它们的复合为 <span class="math inline">\((T\circ S)(x)=T(S(x))\)</span>.</p><p>设 <span class="math inline">\(W,V,U\)</span> 下各有基 <span class="math inline">\(X,Y,Z\)</span>，在这些基下 <span class="math inline">\(S,T\)</span> 的矩阵表示分别为：<span class="math inline">\(A=\sigma(S;X,Y)\)</span>，<span class="math inline">\(B=\sigma(T;Y,Z)\)</span>，则： <span class="math display">\[(T\circ S)(X)=T(S(X))=T(YA)=(TY)A=(ZB)A=Z(BA)\]</span> 即复合映射的矩阵表示为 <span class="math inline">\(BA\)</span>.</p><p>可以看见 <span class="math inline">\(BA\)</span> 只与 <span class="math inline">\(X,Z\)</span> 有关，与 <span class="math inline">\(Y\)</span> 无关，即 <span class="math inline">\(BA=\sigma(T\circ S;X,Z)\)</span>. 可以选取 <span class="math inline">\(V\)</span> 的另一组基证明这一点：设 <span class="math inline">\(Y&#39;\)</span> 也是 <span class="math inline">\(V\)</span> 的基且 <span class="math inline">\(Y=Y&#39;C\)</span>，那么： <span class="math display">\[\begin{align}&amp;SX=YA=(Y&#39;C)A=Y&#39;(CA)=Y&#39;A&#39;\implies CA=A&#39;\\&amp;TY=T(Y&#39;C)=(TY&#39;)C=(ZB&#39;)C=Z(B&#39;C)=ZB\implies B&#39;C=B\end{align}\]</span> 因此 <span class="math inline">\(BA=(B&#39;C)A=B&#39;(CA)=B&#39;A&#39;\)</span>.</p><h3 id="线性映射的维数公式">线性映射的维数公式</h3><p>与矩阵类似，也可以定义线性映射 <span class="math inline">\(T:W\to V\)</span> 的值域和核：</p><ul><li><strong>值域</strong>：<span class="math inline">\(R(T)=\{y\in V\vert y=Tx,\forall x\in W\}\)</span></li><li><strong>核</strong>：<span class="math inline">\(N(T)=\{x\vert Tx=0,x\in W\}\)</span></li><li><strong>秩</strong>：<span class="math inline">\(\dim(R(T))\)</span></li><li><strong>亏度</strong>：<span class="math inline">\(\dim(N(T))\)</span></li></ul><p>且有维数公式： <span class="math display">\[\dim(R(T))+\dim(N(T))=\dim(W)\]</span> 可以用基扩充的方式来证明，与矩阵的维数公式类似，此处略去。</p><h3 id="线性映射构成的空间">线性映射构成的空间</h3><p>线性映射本身在以下加法和数乘定义下也能构成一个线性空间：</p><ul><li>加法：<span class="math inline">\((T_1+T_2)(x)=T_1(x)+T_2(x)\)</span></li><li>数乘：<span class="math inline">\((kT_1)(x)=k(T_1(x))\)</span></li></ul><h3 id="线性映射与矩阵的关系">线性映射与矩阵的关系</h3><p>根据上文的讨论，我们知道线性映射在给定基后可以用矩阵表示。因此<strong>我们可以借助矩阵来研究线性映射的性质，或借助线性映射来研究矩阵的性质</strong>。例如下面的定理。</p><p><strong>定理</strong>：设 <span class="math inline">\(A\in\mathbb C^{m\times n}\)</span>，<span class="math inline">\(B\in\mathbb C^{n\times p}\)</span>，则： <span class="math display">\[\begin{align}&amp;\dim(N(AB))=\dim(N(B))+\dim(N(A)\cap R(B))\\&amp;\dim(R(AB))=\dim(R(B))-\dim(N(A)\cap R(B))\end{align}\]</span> <strong>推论</strong>： <span class="math display">\[\begin{align}&amp;\text{rank}(A)+\text{rank}(B)-n\leq \text{rank}(AB)\\&amp;\dim(R(AB))+\dim(R(BC))-\dim(R(B))\leq \dim(R(ABC))\end{align}\]</span> 将矩阵 <span class="math inline">\(A,B\)</span> 看作线性映射，那么这两条定理可以直观地按下图理解：</p><p><img src="proj.png" width=40% /></p><ul><li><span class="math inline">\(N(AB)\)</span> 包含被 <span class="math inline">\(B\)</span> 映射到了 <span class="math inline">\(0\)</span> 的部分和没被 <span class="math inline">\(B\)</span> 映射到 <span class="math inline">\(0\)</span>、但被 <span class="math inline">\(A\)</span> 映射到 <span class="math inline">\(0\)</span> 的部分。</li><li><span class="math inline">\(R(AB)\)</span> 是没有被 <span class="math inline">\(B\)</span> 映射到 <span class="math inline">\(0\)</span> 的部分中，也没有被 <span class="math inline">\(A\)</span> 映射到 <span class="math inline">\(0\)</span> 的部分。</li></ul><p>证明依旧可以采用基扩充的思路。</p><div class="note note-secondary">            <p>证明：存在一组线性无关的 <span class="math inline">\(x_1,\ldots,x_r\in\mathbb C^p\)</span> 使得 <span class="math inline">\((Bx_1,\ldots,Bx_r)\)</span> 为 <span class="math inline">\(N(A)\cap R(B)\)</span> 的一个基，再取 <span class="math inline">\(N(B)\)</span> 的一个基 <span class="math inline">\((y_1,\ldots,y_s)\)</span>，则只需要证明 <span class="math inline">\((x_1,\ldots,x_r,y_1,\ldots,y_s)\)</span> 构成 <span class="math inline">\(N(AB)\)</span> 的一个基即可。</p><p>首先证明线性无关。由于 <span class="math inline">\((x_1,\ldots,x_r)\)</span> 线性无关，<span class="math inline">\((y_1,\ldots,y_s)\)</span> 线性无关，因此只需要证明 <span class="math inline">\(y_j\ (j=1,\ldots,s)\)</span> 与 <span class="math inline">\((x_1,\ldots,x_r)\)</span> 线性无关即可。这是容易的，因为 <span class="math inline">\(y_j\in N(B),\,x_i\in R(B)\)</span>，而 <span class="math inline">\(N(B)\cap R(B)=\{0\}\)</span>.</p><p>其次，任取 <span class="math inline">\(z\in N(AB)\)</span>，那么 <span class="math inline">\(ABz=0\)</span>. 当 <span class="math inline">\(Bz=0\)</span> 时，<span class="math inline">\(z\in N(B)\)</span>，可以被 <span class="math inline">\((y_1,\ldots,y_s)\)</span> 线性表示；当 <span class="math inline">\(Bz\neq 0\)</span> 时，<span class="math inline">\(Bz\in N(A)\cap R(B)\)</span>，因此 <span class="math inline">\(Bz\)</span> 可以被 <span class="math inline">\((Bx_1,\ldots,Bx_r)\)</span> 线性表示，即： <span class="math display">\[Bz=\sum_{i=1}^r a_i Bx_i\implies B\left(z-\sum_{i=1}^ra_ix_i\right)=0\]</span> 但由于 <span class="math inline">\(z,x_i\notin N(B)\)</span>，所以只能是括号内为零，即 <span class="math inline">\(z\)</span> 可以被 <span class="math inline">\((x_1,\ldots,x_r)\)</span> 线性表示。证毕。</p>          </div><div class="note note-secondary">            <p>证明：可以类似地采用基扩充的思路证明，这里选择另一种方法。利用上一条定理的结论，结合： <span class="math display">\[\dim(R(AB))+\dim(N(AB))=\dim(R(B))+\dim(N(B))=p\]</span> 容易推出结论。</p>          </div><h2 id="线性变换">线性变换</h2><p>线性变换是特殊的线性映射。线性变换从一个线性空间映射到它本身，即 <span class="math inline">\(T:W\to W\)</span>.</p><h3 id="线性变换的矩阵表示">线性变换的矩阵表示</h3><p>由于线性变换只涉及一个空间，所以当我们讨论线性映射的矩阵表示时，只需选择一个基 <span class="math inline">\(X\)</span>，即： <span class="math display">\[TX=XA\]</span> 当然，我们也可以选择两个不同的基 <span class="math inline">\(X,Y\)</span>，这时相当于把线性变换依旧视作线性映射。本课程以后提到线性变换时都只选择一个基。</p><h3 id="相似矩阵不同基下的矩阵表示">相似矩阵——不同基下的矩阵表示</h3><p>与线性映射在不同基下有不同的矩阵表示类似，线性变换在不同基下也有着不同的矩阵表示。设线性变换 <span class="math inline">\(T\)</span> 在基 <span class="math inline">\(X\)</span> 下的矩阵表示为 <span class="math inline">\(A\)</span>，在基 <span class="math inline">\(X&#39;\)</span> 下的矩阵表示为 <span class="math inline">\(A&#39;\)</span>，且两个基之间的关系为：<span class="math inline">\(X&#39;=XC\)</span>，那么： <span class="math display">\[TX&#39;=T(XC)=(TX)C=(XA)C=X(AC)=X&#39;C^{-1}AC=X&#39;A&#39;\implies A&#39;=C^{-1}AC\]</span> 我们称 <span class="math inline">\(A\)</span> 与 <span class="math inline">\(A&#39;\)</span> 是相似的。</p><p>可以看见，<strong>相似矩阵本质上是同一个线性变换在不同基下的表示</strong>。因此，<strong>相似等价意义下具有的性质可以视作线性变换的性质</strong>。例如，相似矩阵的行列式相同，本质是因为行列式对应着线性变换对原空间的单位超立方体变换后的体积。又如，相似矩阵有着相同的特征多项式，所以我们可以定义线性变换的特征多项式。</p><h3 id="线性变换的多项式">线性变换的多项式</h3><p>定义 <span class="math inline">\(T^2\)</span> 表示复合变换 <span class="math inline">\(T\circ T\)</span>，定义 <span class="math inline">\(T^k=T^{k-1}\circ T\)</span>.</p><p>容易知道，若 <span class="math inline">\(T\)</span> 的矩阵表示为 <span class="math inline">\(A\)</span>，那么 <span class="math inline">\(T^k\)</span> 的矩阵表示为 <span class="math inline">\(A^k\)</span>.</p><p>进一步地，多项式 <span class="math inline">\(f(T)\)</span> 的矩阵表示就是 <span class="math inline">\(f(A)\)</span>.</p><h2 id="特征值与特征向量">特征值与特征向量</h2><h3 id="特征值特征向量特征子空间">特征值、特征向量、特征子空间</h3><p>线性变换的特征值和特征向量与矩阵的特征值和特征向量有着类似的定义： <span class="math display">\[Tx=\lambda x,\quad x\neq 0\]</span> 设 <span class="math inline">\(\lambda_0\)</span> 为 <span class="math inline">\(T\)</span> 的一个特征值，则称 <span class="math inline">\(\lambda_0I-T\)</span> 的核空间（<span class="math inline">\(I\)</span> 表示恒等变换）为 <span class="math inline">\(T\)</span> 属于 <span class="math inline">\(\lambda_0\)</span> 的特征子空间： <span class="math display">\[V_{\lambda_0}=N((\lambda_0I-T))=\{x\mid(\lambda_0 I-T)x=0\}\]</span></p><h3 id="线性变换与矩阵的特征值和特征向量的关系">线性变换与矩阵的特征值和特征向量的关系</h3><p>设 <span class="math inline">\(X\)</span> 为一个基，<span class="math inline">\(T\)</span> 在该基下的矩阵表示为 <span class="math inline">\(A\)</span>，<span class="math inline">\(\lambda\)</span> 为 <span class="math inline">\(T\)</span> 的一个特征值，对应特征向量 <span class="math inline">\(x\)</span> 在该基下的坐标表示为 <span class="math inline">\(\xi\)</span>，即： <span class="math display">\[x=X\xi\quad TX=XA\quad Tx=\lambda x\]</span> 那么： <span class="math display">\[\begin{align}Tx&amp;=T(X\xi)=(TX)\xi=XA\xi\\Tx&amp;=\lambda x=\lambda X\xi=X(\lambda\xi)\end{align}\impliesA\xi=\lambda\xi\]</span> 即 <span class="math inline">\(\lambda\)</span> 也是 <span class="math inline">\(A\)</span> 的特征值，<span class="math inline">\(\xi\)</span> 是对应的特征向量。</p><p>换句话说，<strong>线性变换的特征值和其矩阵表示的特征值是一样的，而特征向量的关系就是在选取的那个基下的坐标关系</strong>。</p><h3 id="ab-和-ba-有相同的非零特征值"><span class="math inline">\(AB\)</span> 和 <span class="math inline">\(BA\)</span> 有相同的非零特征值</h3><p>可以叙述为以下定理：设 <span class="math inline">\(A\in\mathbb R^{m\times n},B\in\mathbb R^{n\times m}\)</span>，<span class="math inline">\(AB\)</span> 的特征多项式为 <span class="math inline">\(\varphi_{AB}(\lambda)\)</span>，<span class="math inline">\(BA\)</span> 的特征多项式为 <span class="math inline">\(\varphi_{BA}(\lambda)\)</span>，则： <span class="math display">\[\lambda^n\varphi_{AB}(\lambda)=\lambda^m\varphi_{BA}(\lambda)\]</span></p><div class="note note-secondary">            <p>证明：由于 <span class="math display">\[\begin{bmatrix}I_m&amp;0\\-B&amp;I_n\end{bmatrix}\begin{bmatrix}I_m&amp;A\\0&amp;\lambda I_n\end{bmatrix}\begin{bmatrix}\lambda I_m-AB&amp;0\\B&amp;I_n\end{bmatrix}=\begin{bmatrix}I_m&amp;A\\0&amp;\lambda I_n-BA\end{bmatrix}\]</span> 等式两边取行列式即得证。</p>          </div><div class="note note-info">            <p>批注：上面的证明非常 tricky. 更直接的证明方式是：设 <span class="math inline">\(\lambda,x\)</span> 为 <span class="math inline">\(AB\)</span> 的特征值和特征向量，即 <span class="math inline">\(ABx=\lambda x\)</span>，那么左乘 <span class="math inline">\(B\)</span> 得到： <span class="math display">\[(BA)(Bx)=\lambda (Bx)\]</span> 也就是说 <span class="math inline">\(\lambda\)</span> 和 <span class="math inline">\(Bx\)</span> 是 <span class="math inline">\(BA\)</span> 的特征值和特征向量。</p>          </div><h3 id="属于不同特征值的特征向量线性无关">属于不同特征值的特征向量线性无关</h3><p>设 <span class="math inline">\(\lambda_1,\ldots,\lambda_s\)</span> 为 <span class="math inline">\(A\)</span> 的<strong>互不相同</strong>的特征值，<span class="math inline">\(x_1,\ldots,x_s\)</span> 是分别属于这些特征值的特征向量，那么 <span class="math inline">\(x_1,\ldots,x_s\)</span> 线性无关。</p><div class="note note-secondary">            <p>证明：数学归纳法。设： <span class="math display">\[\sum_{i=1}^s k_ix_i=0\]</span> 用 <span class="math inline">\(A\)</span> 左乘上式得： <span class="math display">\[\sum_{i=1}^sk_iAx_i=\sum_{i=1}^sk_i\lambda_ix_i=0\]</span> 根据上面两个式子消去 <span class="math inline">\(x_s\)</span>，得： <span class="math display">\[\sum_{i=1}^{s-1}k_i(\lambda_i-\lambda_s)x_i=0\]</span> 根据归纳假设，有 <span class="math inline">\(k_i(\lambda_i-\lambda_s)=0\)</span>；又特征值互不相同，故 <span class="math inline">\(k_i=0\ (i=1,\ldots,s-1)\)</span>. 进而 <span class="math inline">\(k_s=0\)</span>. 证毕。</p>          </div><h3 id="任意-n-阶矩阵都与一个上三角矩阵相似">任意 <span class="math inline">\(n\)</span> 阶矩阵都与一个上三角矩阵相似</h3><div class="note note-secondary">            <p>证明：数学归纳法。设 <span class="math inline">\(A\)</span> 是一个 <span class="math inline">\(n\)</span> 阶矩阵，<span class="math inline">\(x_1\)</span> 为 <span class="math inline">\(A\)</span> 的特征值 <span class="math inline">\(\lambda\)</span> 对应的特征向量。将 <span class="math inline">\(x_1\)</span> 扩充为 <span class="math inline">\(\mathbb C^n\)</span> 的一个基 <span class="math inline">\((x_1,\ldots,x_n)\)</span>，那么 <span class="math inline">\(Ax_i\)</span> 都可以被这个基线性表示： <span class="math display">\[\begin{cases}Ax_1=b_{11}x_1+b_{21}x_2+\cdots+b_{n1}x_n=\lambda x_1\\Ax_2=b_{12}x_1+b_{22}x_2+\cdots+b_{n2}x_n\\\quad\vdots\\Ax_n=b_{1n}x_1+b_{2n}x_2+\cdots+b_{nn}x_n\end{cases}\]</span> 写作矩阵形式： <span class="math display">\[AX=XB=X\begin{bmatrix}\lambda&amp;b_{12}&amp;\cdots&amp;b_{1n}\\0&amp;b_{22}&amp;\cdots&amp;b_{2n}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0&amp;b_{n2}&amp;\cdots&amp;b_{nn}\end{bmatrix}=X\begin{bmatrix}\lambda&amp;\alpha^T\\0&amp;B_1\end{bmatrix}\]</span> 根据归纳假设，设 <span class="math inline">\(B_1=QUQ^{-1}\)</span> 且 <span class="math inline">\(U\)</span> 是上三角矩阵，那么： <span class="math display">\[AX=X\begin{bmatrix}\lambda&amp;\alpha^T\\0&amp;QUQ^{-1}\end{bmatrix}=X\begin{bmatrix}1&amp;0\\0&amp;Q\end{bmatrix}\begin{bmatrix}\lambda&amp;\alpha^T\\0&amp;U\end{bmatrix}\begin{bmatrix}1&amp;0\\0&amp;Q^{-1}\end{bmatrix}\]</span> 所以 <span class="math inline">\(A\)</span> 相似于上三角矩阵 <span class="math inline">\(\begin{bmatrix}\lambda&amp;\alpha^T\\0&amp;U\end{bmatrix}\)</span>. 证毕。</p>          </div><h2 id="零化多项式与最小多项式">零化多项式与最小多项式</h2><h3 id="零化多项式">零化多项式</h3><p>设 <span class="math inline">\(A\)</span> 是一个 <span class="math inline">\(n\)</span> 阶矩阵，若多项式 <span class="math inline">\(f(x)\)</span> 使得 <span class="math inline">\(f(A)=0\)</span>，则称 <span class="math inline">\(f(x)\)</span> 为矩阵 <span class="math inline">\(A\)</span> 的一个<strong>零化多项式</strong>。</p><h3 id="hamilton-cayley-定理">Hamilton-Cayley 定理</h3><p>设 <span class="math inline">\(A\)</span> 是一个 <span class="math inline">\(n\)</span> 阶矩阵，则 <span class="math inline">\(A\)</span> 的特征多项式是其零化多项式。即设： <span class="math display">\[\varphi(\lambda)=\text{det}(\lambda I-A)=\lambda^n+a_1\lambda^{n-1}+\cdots+a_{n-1}\lambda+a_n\]</span> 则： <span class="math display">\[\varphi(A)=A^n+a_1A^{n-1}+\cdots+a_{n-1}A+a_nE_n=0\]</span> 可以用数学归纳法证明。</p><div class="note note-secondary">            <p>证：根据 Schur 定理的证明过程，设 <span class="math inline">\(A\)</span> 的 <span class="math inline">\(n\)</span> 个特征值为 <span class="math inline">\(\lambda_1,\ldots,\lambda_n\)</span>，那么 <span class="math inline">\(A\)</span> 相似于 <span class="math inline">\(R=\begin{bmatrix}\lambda_1&amp;\alpha^T\\0&amp;U\end{bmatrix}\)</span>，<span class="math inline">\(A=P^{-1}RP\)</span>.</p><p>由于相似矩阵有相同的特征值，所以 <span class="math inline">\(\lambda_1,\ldots,\lambda_n\)</span> 也是 <span class="math inline">\(R\)</span> 的特征值。容易知道 <span class="math inline">\(\lambda_2,\ldots,\lambda_n\)</span> 是 <span class="math inline">\(U\)</span> 的特征值。</p><p>又因为： <span class="math display">\[\varphi(A)=\varphi(P^{-1}RP)=\sum_{i=0}^n a_{n-i}A^i=\sum_{i=0}^n a_{n-i}(P^{-1}RP)^i=\sum_{i=0}^n a_{n-i}P^{-1}R^iP=P^{-1}\varphi(R)P\]</span> 所以要证明 <span class="math inline">\(\varphi(A)=0\)</span>，只需要证明 <span class="math inline">\(\varphi(R)=0\)</span>. <span class="math display">\[\begin{align}\varphi(R)&amp;=(R-\lambda_1E_n)\cdots(R-\lambda_n E_n)\\&amp;=\begin{bmatrix}\lambda_1-\lambda_1&amp;\alpha^T\\0&amp;U-\lambda_1E_{n-1}\end{bmatrix}\cdots\begin{bmatrix}\lambda_n-\lambda_1&amp;\alpha^T\\0&amp;U-\lambda_nE_{n-1}\end{bmatrix}\\&amp;=\begin{bmatrix}0&amp;\alpha^T\\0&amp;U-\lambda_1E_{n-1}\end{bmatrix}\begin{bmatrix}\prod_{j=2}^n(\lambda_j-\lambda_1)&amp;\beta^T\\0&amp;\prod_{j=2}^n(U-\lambda_jE_{n-1})\end{bmatrix}\\&amp;=\begin{bmatrix}0&amp;0\\0&amp;(U-\lambda_1 E_{n-1})\prod_{j=2}^n(U-\lambda_jE_{n-1})\end{bmatrix}\end{align}\]</span> 根据归纳假设，<span class="math inline">\(\prod_{j=2}^n(U-\lambda_jE_{n-1})=0\)</span>，因此上式为 <span class="math inline">\(0\)</span>，证毕。</p>          </div><p>Hamilton-Cayley 定理说明：<strong>对于 <span class="math inline">\(n\)</span> 阶矩阵 <span class="math inline">\(A\)</span>，<span class="math inline">\(\{A^n,A^{n-1},\ldots,A,E_n\}\)</span> 必然线性相关</strong>。</p><h3 id="最小多项式">最小多项式</h3><p>零化多项式中，次数最低的首项系数为 <span class="math inline">\(1\)</span> 的零化多项式 <span class="math inline">\(m(\lambda)\)</span> 称为<strong>最小多项式</strong>。（注意这里 <span class="math inline">\(\lambda\)</span> 只是一个变量符号，不是特征值的意思）</p><p><strong>定理</strong>：最小多项式可以整除任意其他首项系数为 <span class="math inline">\(1\)</span> 的零化多项式 <span class="math inline">\(\psi(\lambda)\)</span>，且是唯一的。</p><div class="note note-secondary">            <p>证：作多项式除法：<span class="math inline">\(\psi(\lambda)=m(\lambda)p(\lambda)+r(\lambda)\)</span>，其中 <span class="math inline">\(r(\lambda)\)</span> 次数小于 <span class="math inline">\(m(\lambda)\)</span> 的次数。由于 <span class="math inline">\(\psi(A)=m(A)=0\)</span>，故 <span class="math inline">\(r(A)=0\)</span>，但由于 <span class="math inline">\(m(\lambda)\)</span> 是次数最小的零化多项式，所以只能是 <span class="math inline">\(r(\lambda)=0\)</span>. 因此 <span class="math inline">\(m(\lambda)\mid\psi(\lambda)\)</span>.</p>          </div><p><strong>定理</strong>：矩阵 <span class="math inline">\(A\)</span> 的最小多项式 <span class="math inline">\(m(\lambda)\)</span> 和特征多项式 <span class="math inline">\(\varphi(\lambda)\)</span> 零点相同（重数可以不同）。<strong>换句话说，<span class="math inline">\(m(\lambda)\)</span> 的零点就是特征值，只是与 <span class="math inline">\(\varphi(\lambda)\)</span> 的次数不同</strong>。</p><div class="note note-secondary">            <p>证明：根据上一条定理，<span class="math inline">\(\varphi(\lambda)=m(\lambda)p(\lambda)\)</span>，所以 <span class="math inline">\(m(\lambda)=0\implies \varphi(\lambda)=0\)</span>. 所以现在只需证明 <span class="math inline">\(\varphi(\lambda)=0\implies m(\lambda)=0\)</span>.</p><p>设 <span class="math inline">\(\varphi(\lambda_0)=0\)</span>，<span class="math inline">\(A\mathbf x_0=\lambda_0\mathbf x_0\)</span>，则 <span class="math inline">\(m(A)\mathbf x_0=m(\lambda_0)\mathbf x_0=0\)</span>，由于 <span class="math inline">\(\mathbf x_0\neq \mathbf 0\)</span>，所以 <span class="math inline">\(m(\lambda_0)=0\)</span>. 证毕。</p>          </div><p>显然，如果矩阵 <span class="math inline">\(A\)</span> 的最小多项式次数为 <span class="math inline">\(m\)</span>，那么 <span class="math inline">\(\{A^{m-1},\ldots,A,E_n\}\)</span> 线性无关，但再加入一个 <span class="math inline">\(A^m\)</span> 就线性相关了。</p><h2 id="不变子空间与分块对角化">不变子空间与分块对角化</h2><h3 id="不变子空间">不变子空间</h3><p>若线性空间 <span class="math inline">\(V\)</span> 的线性子空间 <span class="math inline">\(V_1\)</span> 对线性变换 <span class="math inline">\(T\)</span> 保持不变，即：<span class="math inline">\(\forall \mathbf x\in V_1\)</span>，有 <span class="math inline">\(T\mathbf x\in V_1\)</span>，则称 <span class="math inline">\(V_1\)</span> 是 <span class="math inline">\(T\)</span> 的不变子空间。这时 <span class="math inline">\(T\)</span> 可以看作 <span class="math inline">\(V_1\)</span> 上的线性变换，称为 <span class="math inline">\(T\)</span> 在 <span class="math inline">\(V_1\)</span> 上的限制 <span class="math inline">\(T\vert V_1\)</span>. 但值得注意的是，<span class="math inline">\(T\)</span> 和 <span class="math inline">\(T\vert V_1\)</span> 是不同的线性变换（它们的输入维度都不同）。</p><p><img src="TV1.png" width=30% /></p><p><strong>性质 1</strong>：不变子空间的和与交也是不变子空间。【易证】</p><p><strong>性质 2</strong>：线性变换 <span class="math inline">\(T\)</span> 的值域 <span class="math inline">\(R(T)\)</span> 和核 <span class="math inline">\(N(T)\)</span> 都是 <span class="math inline">\(T\)</span> 的不变子空间。【易证】</p><p><strong>性质 3</strong>：设 <span class="math inline">\(f(t)\)</span> 为一多项式，则 <span class="math inline">\(T\)</span> 的不变子空间也是 <span class="math inline">\(f(T)\)</span> 的不变子空间。</p><div class="note note-secondary">            <p>证明：设 <span class="math inline">\(V_1\)</span> 是 <span class="math inline">\(T\)</span> 的不变子空间，即 <span class="math inline">\(\forall x\in V_1\)</span>，有 <span class="math inline">\(Tx\in V_1\)</span>. 那么 <span class="math inline">\(T^2x=T(Tx)\in V_1\)</span>，因此 <span class="math inline">\(f(T)(x)\in V_1\)</span>，即 <span class="math inline">\(V_1\)</span> 也是 <span class="math inline">\(f(T)\)</span> 的不变子空间。证毕。</p>          </div><p><strong>特征子空间为不变子空间</strong>：根据性质 2 和性质 3 可知，<span class="math inline">\(N(f(T))\)</span> 为 <span class="math inline">\(T\)</span> 的不变子空间，进而特征子空间 <span class="math inline">\(V_\lambda=N(\lambda I-T)\)</span> 为 <span class="math inline">\(T\)</span> 的不变子空间。</p><h3 id="分块对角化与对角化">分块对角化与对角化</h3><p>设 <span class="math inline">\(T\)</span> 是线性空间 <span class="math inline">\(V^n\)</span> 上的线性变换，若 <span class="math inline">\(V^n\)</span> 可以分解为 <span class="math inline">\(s\)</span> 个 <span class="math inline">\(T\)</span> 的不变子空间的直和： <span class="math display">\[V^n=V_1\oplus\cdots\oplus V_s\]</span> 在每个不变子空间 <span class="math inline">\(V_i\)</span> 中选取一个基 <span class="math inline">\(X_i=(x_{i1},\ldots,x_{in_i}),\,(i=1,\ldots,s)\)</span>，它们合并构成 <span class="math inline">\(V^n\)</span> 的基 <span class="math inline">\(X=(X_1,\ldots,X_s)\)</span>，则 <span class="math inline">\(T\)</span> 在这个基下的矩阵表示为<strong>分块对角矩阵</strong>： <span class="math display">\[A=\begin{bmatrix}A_1&amp;&amp;\\&amp;\ddots&amp;\\&amp;&amp;A_s\end{bmatrix}\]</span> 反过来也成立：若 <span class="math inline">\(T\)</span> 在基 <span class="math inline">\(X=(X_1,\ldots,X_s)\)</span> 下的矩阵表示为分块对角矩阵，那么 <span class="math inline">\(X_i\)</span> 张成的子空间 <span class="math inline">\(V_i\)</span> 是 <span class="math inline">\(T\)</span> 的不变子空间，且 <span class="math inline">\(V^n\)</span> 是它们的直和。</p><p><img src="subspaces.png" width=70% /></p><p>进一步地，如果我们想让 <span class="math inline">\(A\)</span> 是对角阵，那么各个 <span class="math inline">\(A_i\)</span> 都需要是对角阵，也就意味着 <span class="math inline">\(X_i\)</span> 是 <span class="math inline">\(T\)</span> 的特征向量。因此，<strong>线性变换可对角化的充要条件为存在一组特征向量构成的基</strong>。换句话说，<strong>有 <span class="math inline">\(n\)</span> 个线性无关的特征向量</strong>。再换句话说，<strong>各个特征值的代数重数和几何重数要相等</strong>。</p><div class="note note-info">            <p>一点补充：这里讨论的是线性变换在一组基下的矩阵表示是否是对角阵，但由于线性变换在不同基下的矩阵表示是相似的关系，所以其实和讨论一个矩阵是否可以<strong>相似对角化</strong>是一回事。</p>          </div><h3 id="基于不变特征子空间的直和分解">基于不变特征子空间的直和分解</h3><p><strong>定理</strong>：设 <span class="math inline">\(T\)</span> 是线性空间 <span class="math inline">\(V^n\)</span> 上的线性变换，任取 <span class="math inline">\(V^n\)</span> 的一个基，<span class="math inline">\(T\)</span> 在该基下的矩阵为 <span class="math inline">\(A\)</span>，<span class="math inline">\(T\)</span> 的特征多项式为： <span class="math display">\[\varphi(\lambda)=\det(\lambda I-A)=(\lambda-\lambda_1)^{m_1}(\lambda-\lambda_2)^{m_2}\cdots(\lambda-\lambda_s)^{m_s}\]</span> 其中 <span class="math inline">\(m_1+m_2+\cdots+m_s=n\)</span>，则 <span class="math inline">\(V^n\)</span> 可分解为不变子空间的直和： <span class="math display">\[V^n=N_1\oplus N_2\oplus\cdots\oplus N_s\]</span> 其中 <span class="math inline">\(N_i=\{x\mid (\lambda_i I-T)^{m_i}x=0\}\)</span> 是线性变换 <span class="math inline">\((\lambda_iI-T)^{m_i}\)</span> 的核空间。</p><div class="note note-info">            <p>进一步地，在每个 <span class="math inline">\(N((\lambda_i I-T)^{m_i})\)</span> 中取一个基，则 <span class="math inline">\(T\)</span> 在这些基下的矩阵表示是一个分块对角矩阵。这就是 Jordan 标准形的基础。</p>          </div><h2 id="jordan-标准形">Jordan 标准形</h2><p>我们已经看到，任意 <span class="math inline">\(n\)</span> 阶矩阵都能相似于一个上三角矩阵，但是上三角矩阵太多太复杂了，不便于研究；对角矩阵足够简单，但不是所有矩阵都能相似于一个对角矩阵（需要有 <span class="math inline">\(n\)</span> 个线性无关的特征向量）。所以，<strong>是否存在一个介于上三角与对角矩阵之间的矩阵形式，所有 <span class="math inline">\(n\)</span> 阶矩阵都能与之相似呢</strong>？这就是 Jordan 标准形。</p><h3 id="定义">定义</h3><p><strong>Jordan 块</strong>： <span class="math display">\[J(\lambda)=\begin{bmatrix}\lambda&amp;1&amp;&amp;&amp;\\&amp;\lambda&amp;1&amp;&amp;\\&amp;&amp;\ddots&amp;\ddots&amp;\\&amp;&amp;&amp;\lambda&amp;1\\&amp;&amp;&amp;&amp;\lambda\end{bmatrix}\]</span> <strong>Jordan 标准形</strong>： <span class="math display">\[J=\begin{bmatrix}J_1(\lambda_1)&amp;&amp;&amp;\\&amp;J_2(\lambda_2)&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;J_s(\lambda_s)\end{bmatrix}\]</span> <strong>定理</strong>：存在一种 <span class="math inline">\(A\)</span> 的特征多项式的分解： <span class="math display">\[\varphi(\lambda)=\det(\lambda I-A)=(\lambda-\lambda_1)^{m_1}(\lambda-\lambda_2)^{m_2}\cdots(\lambda-\lambda_s)^{m_s}\]</span> 其中 <span class="math inline">\(m_1+m_2+\cdots+m_s=n\)</span>，<strong>注意 <span class="math inline">\(\lambda_1,\lambda_2,\ldots,\lambda_s\)</span> 可以重复</strong>，使得 <span class="math inline">\(A\)</span> 相似于一个 Jordan 标准形： <span class="math display">\[P^{-1}AP=J\]</span> 且除了 Jordan 块的排列顺序以外 Jordan 标准形唯一。</p><h3 id="计算方法">计算方法</h3><p>由于特征多项式的分解不唯一，所以究竟怎么分解是一个问题。比如 <span class="math inline">\((\lambda-1)^4\)</span> 既可以分解成 <span class="math inline">\((\lambda-1)(\lambda-1)^3\)</span>，也可以分解成 <span class="math inline">\((\lambda -1)^2(\lambda -1)^2\)</span>. 下面的基于多项式矩阵（<span class="math inline">\(\lambda\)</span> 阵）的初等变换法给出了一种计算方法。</p><p><strong>Jordan 标准形的计算方法</strong>：</p><ol type="1"><li><p>写出 <span class="math inline">\(A\)</span> 的特征矩阵 <span class="math inline">\(\lambda I-A\)</span>；</p></li><li><p>计算特征矩阵的<strong>行列式因子</strong>：<span class="math inline">\(D_i(\lambda)\)</span> 表示所有 <span class="math inline">\(i\)</span> 阶子式的最大公因式；</p></li><li><p>计算<strong>不变因子</strong>：<span class="math inline">\(d_i(\lambda)=D_i(\lambda)/D_{i-1}(\lambda)\)</span>；其中 <span class="math inline">\(D_0(\lambda)=1\)</span>；</p></li><li><p>计算<strong>初等因子组</strong>：将每个不变因子化为不可约因式，这些不可约因式称为初等因子，全体初等因子称为初等因子组。</p><p>例如：设 <span class="math inline">\(d_1(\lambda)=(\lambda-2)^2(\lambda-3),\,d_2(\lambda)=(\lambda-2)^2(\lambda-3)^5\)</span>，则初等因子组为 <span class="math inline">\(\{(\lambda-2)^2,(\lambda-3),(\lambda-2)^2,(\lambda-3)^5\}\)</span>. 注意其中第一个 <span class="math inline">\((\lambda-2)^2\)</span> 来自 <span class="math inline">\(d_1(\lambda)\)</span>，第二个 <span class="math inline">\((\lambda-2)^2\)</span> 来自 <span class="math inline">\(d_2(\lambda)\)</span>.</p></li><li><p>写出 Jordan 标准形：一个初等因子对应一个 Jordan 块，初等因子的次数就是 Jordan 块的阶数。</p></li></ol><p><strong>Jordan 标准形变换矩阵的计算方法</strong>：上面求出了 Jordan 标准形 <span class="math inline">\(J\)</span>，现在求解变换矩阵 <span class="math inline">\(P\)</span>.</p><p>由于 <span class="math inline">\(P^{-1}AP=J\)</span>，所以 <span class="math inline">\(AP=PJ\)</span>. 鉴于 <span class="math inline">\(J\)</span> 是分块对角矩阵，所以只需要一块一块考虑即可：<span class="math inline">\(AP_i=P_iJ_i(\lambda_i)\)</span>，其中 <span class="math inline">\(P_i\)</span> 是 <span class="math inline">\(P\)</span> 的对应列。显式地写出来： <span class="math display">\[A(p_1,p_2,\cdots,p_m)=(p_1,p_2,\cdots,p_m)\begin{bmatrix}\lambda_i&amp;1&amp;\cdots&amp;0\\0&amp;\lambda_i&amp;\cdots&amp;0\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0&amp;0&amp;\cdots&amp;\lambda_i\end{bmatrix}\]</span> 于是： <span class="math display">\[\begin{cases}Ap_1=\lambda_ip_1\\Ap_2=p_1+\lambda_ip_2\\Ap_3=p_2+\lambda_ip_3\\\quad\vdots\\Ap_m=p_{m-1}+\lambda_ip_m\end{cases}\implies\begin{cases}(\lambda_iI-A)p_1=0\\(\lambda_iI-A)p_2=-p_1\\(\lambda_iI-A)p_3=-p_2\\\quad\vdots\\(\lambda_iI-A)p_m=-p_{m-1}\end{cases}\]</span> 事实上这里 <span class="math inline">\(p_1\)</span> 是 <span class="math inline">\(A\)</span> 的特征向量，<span class="math inline">\(p_2,\ldots,p_m\)</span> 是 <span class="math inline">\(A\)</span> 的广义特征向量。也就是说，由于 <span class="math inline">\(\lambda_i\)</span> 的几何重数小于代数重数，所以找不到 <span class="math inline">\(m\)</span> 个特征向量，只能用广义特征向量填补。</p><h3 id="jordan-标准形与最小多项式">Jordan 标准形与最小多项式</h3><p>对于特征值 <span class="math inline">\(\lambda_i\)</span>，其在最小多项式中的次数等于属于 <span class="math inline">\(\lambda_i\)</span> 的 Jordan 块的最高阶数。例如： <span class="math display">\[A=\begin{bmatrix}1&amp;1&amp;&amp;&amp;&amp;&amp;&amp;\\&amp;1&amp;1&amp;&amp;&amp;&amp;&amp;\\&amp;&amp;1&amp;&amp;&amp;&amp;&amp;\\&amp;&amp;&amp;1&amp;1&amp;&amp;&amp;\\&amp;&amp;&amp;&amp;1&amp;&amp;&amp;\\&amp;&amp;&amp;&amp;&amp;-1&amp;1&amp;\\&amp;&amp;&amp;&amp;&amp;&amp;-1&amp;\\&amp;&amp;&amp;&amp;&amp;&amp;&amp;-1\end{bmatrix}\]</span> 可以看到 <span class="math inline">\(\lambda=1\)</span> 有一个 3 阶和一个 2 阶的 Jordan 块，所以最小多项式中 <span class="math inline">\((\lambda-1)\)</span> 的次数为 3；同理，<span class="math inline">\((\lambda+1)\)</span> 的次数为 2. 于是： <span class="math display">\[m(\lambda)=(\lambda-1)^3(\lambda+1)^2\]</span></p><h3 id="jordan-标准形的多项式">Jordan 标准形的多项式</h3><p>我们知道相似对角化的一个重要作用就是简化 <span class="math inline">\(A^k\)</span> 的计算：<span class="math inline">\(A=P\Lambda P^{-1}\implies A^k=P\Lambda^kP^{-1}\)</span>. 而对于无法相似对角化的矩阵而言，Jordan 标准形也起到了类似的作用：<span class="math inline">\(A=PJP^{-1}\implies A^k=PJ^kP^{-1}\)</span>. 因此我们现在需要关注 <span class="math inline">\(J^k\)</span> 的计算。</p><p>由于 <span class="math inline">\(J\)</span> 是分块对角矩阵，所以我们只需要逐个考虑每一块即可。将 <span class="math inline">\(J(\lambda)\)</span> 写作： <span class="math display">\[J(\lambda)=\lambda E_{r\times r}+L,\quad L=\begin{bmatrix}0&amp;1&amp;&amp;&amp;\\&amp;0&amp;1&amp;&amp;\\&amp;&amp;\ddots&amp;\ddots&amp;\\&amp;&amp;&amp;0&amp;1\\&amp;&amp;&amp;&amp;0\end{bmatrix}_{r\times r}\]</span> 其中 <span class="math inline">\(L\)</span> 是一个幂零矩阵，满足 <span class="math inline">\(L^r=0\)</span> 而 <span class="math inline">\(L^{r-1}\neq 0\)</span>. 于是： <span class="math display">\[\begin{align}J(\lambda)^k&amp;=\sum_{i=0}^k\binom{k}{i}\lambda^{k-i}L^i=\sum_{i=0}^k\frac{k(k-1)\cdots(k-i+1)}{i!}\lambda^{k-i}L^i\\&amp;=\sum_{i=0}^k\frac{1}{i!}(\lambda^k)^{(i)}L^i=\sum_{i=0}^{\min(k,r-1)}\frac{1}{i!}(\lambda^k)^{(i)}L^i\\\end{align}\]</span> 那么，对于多项式 <span class="math inline">\(f(x)=\sum_{k=0}^sa_kx^k\)</span>，有： <span class="math display">\[\begin{align}f(J(\lambda))&amp;=\sum_{k=0}^sa_kJ(\lambda)^k=\sum_{k=0}^sa_k\sum_{i=0}^{\min(k,r-1)}\frac{1}{i!}(\lambda^k)^{(i)}L^i\\&amp;=\sum_{i=0}^{s}\frac{1}{i!}\left(\sum_{k=i}^sa_k\lambda^k\right)^{(i)}L^i=\sum_{i=0}^{s}\frac{1}{i!}\left(\sum_{k=0}^sa_k\lambda^k\right)^{(i)}L^i\\&amp;=\sum_{i=0}^{s}\frac{1}{i!}\left(f(\lambda)\right)^{(i)}L^i\\\\&amp;=\begin{bmatrix}f(\lambda)&amp;f&#39;(\lambda)&amp;\frac{f&#39;&#39;(\lambda)}{2!}&amp;\frac{f&#39;&#39;&#39;(\lambda)}{3!}&amp;\cdots&amp;\frac{f^{(r-1)}(\lambda)}{(r-1)!}\\&amp;f(\lambda)&amp;f&#39;(\lambda)&amp;\frac{f&#39;&#39;(\lambda)}{2!}&amp;\cdots&amp;\frac{f^{(r-2)}(\lambda)}{(r-2)!}\\&amp;&amp;f(\lambda)&amp;f&#39;(\lambda)&amp;\cdots&amp;\frac{f^{(r-3)}(\lambda)}{(r-3)!}\\&amp;&amp;&amp;\ddots&amp;\ddots&amp;\vdots\\&amp;&amp;&amp;&amp;f(\lambda)&amp;f&#39;(\lambda)\\&amp;&amp;&amp;&amp;&amp;f(\lambda)\end{bmatrix}_{r\times r}\end{align}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[矩阵论]1.1线性空间</title>
    <link href="/blog-main/2023/10/10/%E7%9F%A9%E9%98%B5%E8%AE%BA-1-1%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4/"/>
    <url>/blog-main/2023/10/10/%E7%9F%A9%E9%98%B5%E8%AE%BA-1-1%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<h3 id="集值映射">集值映射</h3><p>由单点映射 <span class="math inline">\(\sigma:S\to S&#39;\)</span> 导出集值映射 <span class="math inline">\(\sigma:2^S\to 2^{S&#39;}\)</span>： <span class="math display">\[\begin{align}&amp;\sigma(\Omega)=\{y:y=\sigma(x),\exists x\in\Omega\}\\&amp;\sigma^{-1}(\Omega&#39;)=\{x:y=\sigma(x),\exists y\in\Omega&#39;\}\end{align}\]</span> <img src="set-proj.png" width=30% /></p><h3 id="线性空间-vkcdot">线性空间 <span class="math inline">\((V,K,+,\cdot)\)</span></h3><p>满足 8 条性质（加法和数乘各 4 条）：</p><ul><li>结合律：<span class="math inline">\(x+(y+z)=(x+y)+z\)</span></li><li>交换律：<span class="math inline">\(x+y=y+x\)</span></li><li>存在零元素：<span class="math inline">\(x+0=x\)</span></li><li>存在负元素：<span class="math inline">\(x+(-x)=0\)</span></li><li>数因子分配律：<span class="math inline">\(k(x+y)=kx+ky\)</span></li><li>分配律：<span class="math inline">\((k+l)x=kx+lx\)</span></li><li>结合律：<span class="math inline">\(k(lx)=(kl)x\)</span></li><li>存在单位元：<span class="math inline">\(1x=x\)</span></li></ul><div class="note note-success">            <p>集合中的元素无需一定是列向量，可以是矩阵、多项式等；加法和数乘也不一定是我们熟悉的加法和数乘，只要满足上述 8 条性质即可。</p><p>因此线性空间是多种多样的，这么多的线性空间无法一一研究，我们后面会说明：<strong>所有线性空间都与 <span class="math inline">\(\mathbb R^n\)</span> 或 <span class="math inline">\(\mathbb C^n\)</span> 代数同构，因此只需研究 <span class="math inline">\(\mathbb R^n\)</span> 和 <span class="math inline">\(\mathbb C^n\)</span> 即可</strong>。</p>          </div><h3 id="线性相关线性无关极大线性无关组线性空间的维度"><strong>线性相关、线性无关、极大线性无关组、线性空间的维度</strong></h3><p>定义略。本课程只讨论有限维空间，无穷维空间属于泛函分析的范畴。</p><h3 id="基-x_1ldotsx_n">基 <span class="math inline">\((x_1,\ldots,x_n)\)</span></h3><p>满足 2 条性质：</p><ol type="1"><li><span class="math inline">\(x_1,\ldots,x_n\)</span> 线性无关；</li><li><span class="math inline">\(V\)</span> 中任意向量都是 <span class="math inline">\(x_1,\ldots,x_n\)</span> 的线性组合。</li></ol><div class="note note-success">            <p>组成基的向量排列是<strong>有顺序</strong>的！因为向量在这个基下的坐标表示是有顺序的，例如 <span class="math inline">\((1,2)\neq (2,1)\)</span>.</p>          </div><h3 id="坐标表示">坐标表示</h3><p>设 <span class="math inline">\(X=(x_1,\ldots,x_n)\)</span> 是一个基，向量 <span class="math inline">\(x\)</span> 在这个基下的坐标表示为 <span class="math inline">\(\xi=(\xi_1,\ldots,\xi_n)^{T}\)</span>，则可写作： <span class="math display">\[x=\xi_1 x_1+\cdots+\xi_nx_n=X\xi\]</span></p><div class="note note-success">            <p>式 <span class="math inline">\(x=X\xi\)</span> 非常重要，日后将经常使用。</p><p>其中 <span class="math inline">\(X=(x_1,\ldots,x_n)\)</span> 表示向量组而非矩阵，<span class="math inline">\(X\xi\)</span> 也并非矩阵乘法，只是可以按照矩阵乘法来理解。</p>          </div><h3 id="任何-n-维线性空间-v-都与-mathbb-rn-或-mathbb-cn-同构">任何 <span class="math inline">\(n\)</span> 维线性空间 <span class="math inline">\(V\)</span> 都与 <span class="math inline">\(\mathbb R^n\)</span> 或 <span class="math inline">\(\mathbb C^n\)</span> 同构</h3><p>即存在一个一一映射 <span class="math inline">\(\sigma:V\to\mathbb R^n(\mathbb C^n)\)</span>，使得： <span class="math display">\[\begin{align}&amp;\sigma(x+y)=\sigma(x)+\sigma(y)&amp;&amp;\quad x,y\in V\\&amp;\sigma(kx)=k\sigma(x)&amp;&amp;\quad x\in V,\,k\in K\end{align}\]</span> <div class="note note-secondary">            <p>证明：</p><p>充分性：验证 8 条性质即可。</p><p>必要性：<strong>任给 <span class="math inline">\(V\)</span> 中的一个基，那么该基下的坐标表示就是一个符合条件的同构映射</strong>！</p>          </div></p><h3 id="基变换与过渡矩阵">基变换与过渡矩阵</h3><p>设有两个基：<span class="math inline">\(X=(x_1,\ldots,x_n)\)</span>，<span class="math inline">\(Y=(y_1,\ldots,y_n)\)</span>，<span class="math inline">\(Y\)</span> 中每一个基向量由 <span class="math inline">\(X\)</span> 的基向量线性表示为： <span class="math display">\[\begin{cases}y_1=c_{11}x_1+\cdots+c_{n1}x_n\\\quad\vdots\\y_n=c_{1n}x_1+\cdots+c_{nn}x_n\\\end{cases}\implies Y=XC\]</span> 称 <span class="math inline">\(C\)</span> 为过渡矩阵。过渡矩阵一定非奇异。</p><p>设有一向量 <span class="math inline">\(x\)</span>，在两个基下的坐标表示分别为 <span class="math inline">\(\xi=(\xi_1,\ldots,\xi_n)^T\)</span> 和 <span class="math inline">\(\eta=(\eta_1,\ldots,\eta_n)^T\)</span>，则： <span class="math display">\[x=X\xi=Y\eta\implies\xi=C\eta\;(\eta=C^{-1}\xi)\]</span> 自然基下，向量 <span class="math inline">\(x\)</span> 和坐标表示是一致的，常常不加区别地用同一符号表示。</p><div class="note note-success">            <p>式 <span class="math inline">\(Y=XC\)</span> 和 <span class="math inline">\(\xi=C\eta\)</span> 日后也将经常使用。</p>          </div><h3 id="线性子空间">线性子空间</h3><p>只需验证 2 条性质：</p><ol type="1"><li>对加法封闭：若 <span class="math inline">\(x,y\in V_1\)</span>，则 <span class="math inline">\(x+y\in V_1\)</span>；</li><li>对数乘封闭：若 <span class="math inline">\(x\in V_1,\,k\in K\)</span>，则 <span class="math inline">\(kx\in V_1\)</span>.</li></ol><p>零子空间：仅由 0 构成的子空间，其维度为 0.</p><h3 id="交和直和">交、和、直和</h3><p>子空间的交仍然是子空间，子空间的和仍然是子空间。当交为空时称和为直和。</p><p><strong>子空间和的维数公式</strong>： <span class="math display">\[\dim V_1+\dim V_2=\dim (V_1+V_2)+\dim (V_1\cap V_2)\]</span> <div class="note note-success">            <p>非常重要的证明思路——<strong>基扩充</strong>：从最小的子空间 <span class="math inline">\(V_1\cap V_2\)</span> 出发，构造它的一个基 <span class="math inline">\(X=(x_1,\ldots,x_r)\)</span>，然后分别扩充成 <span class="math inline">\(V_1\)</span> 的基 <span class="math inline">\((X,Y)=(x_1,\ldots,x_r,y_1,\ldots,y_s)\)</span> 和 <span class="math inline">\(V_2\)</span> 的基 <span class="math inline">\((X,Z)=(x_1,\ldots,x_r,z_1,\ldots,z_t)\)</span>，最后证明 <span class="math inline">\((X,Y,Z)=(x_1,\ldots,x_r,y_1,\ldots,y_s,z_1,\ldots,z_t)\)</span> 为 <span class="math inline">\(V_1+V_2\)</span> 的基。</p>          </div></p><div class="note note-secondary">            <p>证明：要证明 <span class="math inline">\((X,Y,Z)=(x_1,\ldots,x_r,y_1,\ldots,y_s,z_1,\ldots,z_t)\)</span> 为 <span class="math inline">\(V_1+V_2\)</span> 的基，只需证明 1) 线性无关；2) 可表示任一 <span class="math inline">\(v\in V_1+V_2\)</span>.</p><p>首先证明线性无关。设： <span class="math display">\[\sum_{i=1}^r a_ix_i+\sum_{j=1}^s b_jy_j+\sum_{k=1}^t c_kz_k=0\]</span> 则： <span class="math display">\[\sum_{i=1}^r a_ix_i+\sum_{j=1}^s b_jy_j=-\sum_{k=1}^t c_kz_k\in V_1\]</span> 由于 <span class="math inline">\(z_k\notin V_1\)</span>，故 <span class="math inline">\(c_k=0\)</span>，进而 <span class="math inline">\(a_i=b_j=0\)</span>. 故线性无关。</p><p>其次，任取 <span class="math inline">\(v\in V_1+V_2\)</span>，则 <span class="math inline">\(\exists v_1\in V_1,v_2\in V_2\)</span> 使得 <span class="math inline">\(v_1+v_2=v\)</span>. 设： <span class="math display">\[v_1=\sum_{i=1}^r a_ix_i+\sum_{j=1}^s c_jy_j,\quad v_2=\sum_{i=1}^r b_ix_i+\sum_{k=1}^t d_kz_k\]</span> 则： <span class="math display">\[v=\sum_{i=1}^r(a_i+b_i)x_i+\sum_{j=1}^s c_jy_j+\sum_{k=1}^t d_kz_k\]</span> 综上，<span class="math inline">\((X,Y,Z)\)</span> 是 <span class="math inline">\(V_1+V_2\)</span> 的一个基。</p>          </div><h3 id="向量组扩张为子空间">向量组扩张为子空间</h3><p>由单个向量 <span class="math inline">\(x\)</span> 对数乘运算封闭构成一维子空间：<span class="math inline">\(L(x)=\{z\vert z=kx,k\in K\}\)</span>；</p><p>由向量组 <span class="math inline">\(x_1,\ldots,x_m\)</span> 扩张成的子空间：<span class="math inline">\(L(x_1,\ldots,x_m)=L(x_1)+\cdots+L(x_m)\)</span>.</p><p>显然 <span class="math inline">\(\dim L(x_1,\ldots,x_m)\leq m\)</span>.</p><h3 id="值域列空间核空间零空间与秩">值域（列空间）、核空间（零空间）与秩</h3><p>设 <span class="math inline">\(A\in\mathbb R^{m\times n}\)</span>：</p><ul><li><span class="math inline">\(R(A)=L(a_1,\ldots,a_n)\subset \mathbb R^m\)</span></li><li><span class="math inline">\(N(A)=\{x\mid Ax=0\}\subset\mathbb R^n\)</span></li><li><span class="math inline">\(\text{rank}(A)=\text{构成极大线性无关组的列向量个数}=\dim(R(A))\)</span></li></ul><p><strong>定理</strong>： <span class="math display">\[\dim(R(A))+\dim(N(A))=n\]</span> <div class="note note-secondary">            <p>证明思路依旧是<strong>基扩充</strong>：设 <span class="math inline">\((x_1,\ldots,x_s)\)</span> 为 <span class="math inline">\(N(A)\)</span> 的一个基，将其扩充为 <span class="math inline">\(\mathbb R^n\)</span> 的基 <span class="math inline">\((x_1,\ldots,x_s,y_1,\ldots,y_{n-s})\)</span>. 只需证明 <span class="math inline">\((Ay_1,\ldots, Ay_{n-s})\)</span> 是 <span class="math inline">\(R(A)\)</span> 的基。</p><p>首先证明线性无关。假设： <span class="math display">\[\sum_{j=1}^{n-s}b_j(Ay_j)=0\]</span> 由于 <span class="math inline">\(Ax_i=0\ (i=1,\ldots,s)\)</span>，所以： <span class="math display">\[\sum_{j=1}^{n-s}b_jAy_j=\sum_{i=1}^sa_i Ax_i+\sum_{j=1}^{n-s}b_jAy_j=A\left(\sum_{i=1}^sa_i x_i+\sum_{j=1}^{n-s}b_jy_j\right)=0\]</span> 也就是说： <span class="math display">\[\sum_{i=1}^sa_i x_i+\sum_{j=1}^{n-s}b_jy_j\in N(A)\]</span> 但是 <span class="math inline">\((x_1,\ldots,x_s)\)</span> 与 <span class="math inline">\((y_1,\ldots,y_{n-s})\)</span> 是线性无关的，所以只能是 <span class="math inline">\(b_j=0, (j=1,\ldots,n-s)\)</span>. 因此线性无关。</p><p>其次，任取 <span class="math inline">\(z\in R(A)\)</span>，则存在 <span class="math inline">\(w\in\mathbb R^n\)</span> 使得 <span class="math inline">\(z=Aw\)</span>. 设 <span class="math inline">\(w\)</span> 在 <span class="math inline">\((x_1,\ldots,x_s,y_1,\ldots,y_{n-s})\)</span> 这个基下可以线性表示为： <span class="math display">\[w=\sum_{i=1}^s c_ix_i+\sum_{j=1}^{n-s}d_j y_j\]</span> 那么 <span class="math inline">\(z\)</span> 可以由 <span class="math inline">\((Ay_1,\ldots,Ay_{n-s})\)</span> 线性表示为： <span class="math display">\[z=Aw=\sum_{i=1}^s c_i (Ax_i)+\sum_{j=1}^{n-s}d_j(Ay_j)=\sum_{j=1}^{n-s}d_j(Ay_j)\]</span> 证毕。</p>          </div></p><div class="note note-success">            <p>Gilbert Strang 的著名的四个基本子空间：</p><p><img src="space.png" width=60% /></p>          </div>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>矩阵论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linear algebra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[模式分类]贝叶斯决策论</title>
    <link href="/blog-main/2023/10/02/%E6%A8%A1%E5%BC%8F%E5%88%86%E7%B1%BB-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E8%AE%BA/"/>
    <url>/blog-main/2023/10/02/%E6%A8%A1%E5%BC%8F%E5%88%86%E7%B1%BB-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文对应《模式分类》的第 2 章。</p></blockquote><h2 id="最小错误率贝叶斯决策">最小错误率贝叶斯决策</h2><p>设特征向量 <span class="math inline">\(\mathbf x\in\mathbb R^d\)</span>，属于 <span class="math inline">\(c\)</span> 个类别 <span class="math inline">\(\{\omega_1,\ldots,\omega_c\}\)</span> 之一。假设下述概率分布都是已知的：</p><ul><li><strong>各类别先验概率</strong>：<span class="math inline">\(P(\omega_i),\,i=1,\ldots,c\)</span></li><li><strong>类条件概率</strong>：<span class="math inline">\(p(\mathbf x\vert\omega_i),\,i=1,\ldots,c\)</span></li></ul><p>那么，如果观测到样本 <span class="math inline">\(\mathbf x\)</span>，应该将其分到哪一类，使得错误率最小呢？</p><p>首先，根据<strong>贝叶斯公式</strong>，我们可以计算<strong>后验概率</strong>： <span class="math display">\[P(\omega_i\vert\mathbf x)=\frac{p(\mathbf x\vert\omega_i)P(\omega_i)}{p(\mathbf x)},\quad i=1,\ldots,c\]</span> 一个自然的决策是选取后验概率最大的那一类： <span class="math display">\[\mathbf x\in\omega=\mathop{\text{argmax }}_{i=1}^c P(\omega_i\vert\mathbf x)\tag{1}\label{1}\]</span> 事实上，这也确实是使得错误率最小的决策。因为，所谓错误率（总错误率 / 平均错误率），就是： <span class="math display">\[P(\text{error})=\mathbb E_{\mathbf x}[P(\text{error}\vert\mathbf x)]=\int P(\text{error}\vert\mathbf x)p(\mathbf x)\mathrm d\mathbf x\]</span> 那么要使得 <span class="math inline">\(P(\text{error})\)</span> 最小，只需要对每一个 <span class="math inline">\(\mathbf x\)</span> 让 <span class="math inline">\(P(\text{error}\vert\mathbf x)\)</span> 最小： <span class="math display">\[P(\text{error}\vert\mathbf x)=1-P(\omega_i\vert\mathbf x)\quad\text{if we decide }\mathbf x\in\omega_i\]</span> 显然，我们的决策应该是找最小的 <span class="math inline">\(1-P(\omega_i\vert\mathbf x)\)</span>，整理一下也就是 <span class="math inline">\(\eqref{1}\)</span> 式。</p><p><br/></p><p>我们可以将这个过程形式化为以下模式：对每个 <span class="math inline">\(\mathbf x\)</span>，计算<strong>判别函数</strong> <span class="math inline">\(g_i(\mathbf x),\,i=1,\ldots,c\)</span>，如果： <span class="math display">\[g_k(\mathbf x)=\max_{i=1}^c g_i(\mathbf x)\]</span> 我们就将 <span class="math inline">\(\mathbf x\)</span> 分类为 <span class="math inline">\(\omega_k\)</span>. 这就是一个分类器。</p><p>对于 <span class="math inline">\(\eqref{1}\)</span> 式而言，其判别函数就是后验概率 <span class="math inline">\(g_i(\mathbf x)=P(\omega_i\vert\mathbf x)\)</span>. 然而，后验概率的分母部分（即归一化因子）<span class="math inline">\(p(\mathbf x)\)</span> 是个积分/求和，一般难以计算。不过，注意到 <span class="math inline">\(p(\mathbf x)\)</span> 其实与决策无关——不管哪个类都有这一项，所以完全可以丢掉。另外，判别函数的单增函数并不会改变判别结果，所以事实上，以下选择都是可行的判别函数： <span class="math display">\[\begin{align}&amp;g_i(\mathbf x)=P(\omega_i\vert\mathbf x)\tag{2}\label{2}\\&amp;g_i(\mathbf x)=p(\mathbf x\vert\omega_i)P(\omega_i)\tag{3}\label{3}\\&amp;g_i(\mathbf x)=\ln p(\mathbf x\vert\omega_i)+\ln P(\omega_i)\tag{4}\label{4}\end{align}\]</span> 实际应用中，哪种形式方便就用哪一种。</p><p>有了判别函数，任取两类 <span class="math inline">\(i,j\)</span>，那么 <span class="math inline">\(g_i(\mathbf x)=g_j(\mathbf x)\)</span> 就是<strong>决策面方程</strong>——决策面的两边分属 <span class="math inline">\(\omega_i\)</span> 和 <span class="math inline">\(\omega_j\)</span>. 当判别函数是关于 <span class="math inline">\(\mathbf x\)</span> 的线性函数时，决策面为一个超平面（反过来不成立，若决策面是超平面，判别函数并不一定是线性的，非线性的判别函数的交面也可以是超平面）。</p><p><br/></p><p>特别地，在<strong>两类情形</strong>下，我们的判决规则就是【若 <span class="math inline">\(P(\omega_1\vert\mathbf x)&gt;P(\omega_2\vert\mathbf x)\)</span>，则 <span class="math inline">\(\mathbf x\in\omega_1\)</span>；否则 <span class="math inline">\(\mathbf x\in\omega_2\)</span>】。去掉分母并稍作移项，得：当 <span class="math display">\[l(\mathbf x)=\frac{p(\mathbf x\vert\omega_1)}{p(\mathbf x\vert\omega_2)}&gt;\frac{P(\omega_2)}{P(\omega_1)}\]</span> 时，判决 <span class="math inline">\(\mathbf x\in\omega_1\)</span>；否则 <span class="math inline">\(\mathbf x\in\omega_2\)</span>. 这里 <span class="math inline">\(l(\mathbf x)\)</span> 称作<strong>似然比</strong>。</p><h2 id="最小风险贝叶斯决策">最小风险贝叶斯决策</h2><p>在一些实际问题中，分类错误导致的风险是不同的。自动驾驶汽车没能检测出障碍物的风险很大（车毁人亡），但误检出障碍物的风险就小很多（无非就是莫名其妙地刹车）。设 <span class="math inline">\(\{\alpha_1,\ldots,\alpha_a\}\)</span> 表示 <span class="math inline">\(a\)</span> 种行动，<strong>风险函数</strong> <span class="math inline">\(\lambda(\alpha_i\vert\omega_j)\)</span> 表示在类别为 <span class="math inline">\(\omega_j\)</span> 时采取行动 <span class="math inline">\(\alpha_i\)</span> 的风险，简记作 <span class="math inline">\(\lambda_{ij}\)</span>. 我们的目标从最小化错误率变成了最小化风险。事实上，如果定义行动 <span class="math inline">\(\alpha_i\)</span> 表示判决类别为 <span class="math inline">\(\omega_i\)</span>，那么最小化错误率可以看作是在最小化 0-1 风险： <span class="math display">\[\lambda(\alpha_i\vert\omega_j)=\begin{cases}0&amp;i=j\\1&amp;i\neq j\end{cases}\quad\quad i,j=1,\ldots,c\]</span> 一般地，给定 <span class="math inline">\(\mathbf x\)</span>，采取各个动作的<strong>条件风险</strong>为： <span class="math display">\[R(\alpha_i\vert\mathbf x)=\sum_{j=1}^c\lambda(\alpha_i\vert\omega_j)P(\omega_j\vert\mathbf x),\quad i=1,\ldots,a\tag{5}\label{5}\]</span> 设对每个 <span class="math inline">\(\mathbf x\)</span>，<span class="math inline">\(\alpha(\mathbf x)\)</span> 为采取的行动，那么总风险 / 平均风险就是： <span class="math display">\[R=\mathbb E_\mathbf x[R(\alpha(\mathbf x)\vert\mathbf x)]=\int R(\alpha(\mathbf x)\vert\mathbf x)p(\mathbf x)\mathrm d\mathbf x\]</span> 为了让 <span class="math inline">\(R\)</span> 最小，只需要对每个 <span class="math inline">\(\mathbf x\)</span> 让 <span class="math inline">\(R(\alpha(\mathbf x)\vert\mathbf x)\)</span> 最小。显然，我们的决策是选取使得条件风险最小的那个动作： <span class="math display">\[\alpha(\mathbf x)=\mathop{\text{argmin }}_{i=1}^aR(\alpha_i\vert\mathbf x)\tag{6}\label{6}\]</span> <br/></p><p>特别地，考察<strong>两类情形</strong>，我们的判决规则就是【若 <span class="math inline">\(R(\alpha_1\vert\mathbf x)&lt;R(\alpha_2\vert\mathbf x)\)</span>，则 <span class="math inline">\(\alpha(\mathbf x)=\alpha_1\)</span>；否则 <span class="math inline">\(\alpha(\mathbf x)=\alpha_2\)</span>】。代入 <span class="math inline">\(\eqref{5}\)</span> 式展开得： <span class="math display">\[R(\alpha_1\vert\mathbf x)&lt;R(\alpha_2\vert\mathbf x)\implies\lambda_{11}P(\omega_1\vert\mathbf x)+\lambda_{12}P(\omega_2\vert\mathbf x)&lt;\lambda_{21}P(\omega_1\vert\mathbf x)+\lambda_{22}P(\omega_2\vert\mathbf x)\]</span> 由于一般而言分类错误的代价比正确的代价高，所以我们可以合理地假设 <span class="math inline">\(\lambda_{11}&lt;\lambda_{21},\,\lambda_{22}&lt;\lambda_{12}\)</span>，那么上式最终可以化简为：当 <span class="math display">\[l(\mathbf x)=\frac{p(\mathbf x\vert\omega_1)}{p(\mathbf x\vert\omega_2)}&gt;\frac{P(\omega_2)}{P(\omega_1)}\cdot\frac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}}\]</span> 时，取 <span class="math inline">\(\alpha(\mathbf x)=\alpha_1\)</span>；否则 <span class="math inline">\(\alpha(\mathbf x)=\alpha_2\)</span>.</p><h2 id="例子正态分布">例子：正态分布</h2><p>上面的讨论只假定 <span class="math inline">\(P(\omega_i)\)</span> 和 <span class="math inline">\(p(\mathbf x\vert\omega_i)\)</span> 是已知的，但没有给出具体的形式，所以这一节我们计算一下各类条件概率密度函数为多元正态分布的情形，即： <span class="math display">\[p(\mathbf x\vert\omega_i)=\mathcal N(\boldsymbol\mu_i,\Sigma_i)\]</span> 那么在最小错误率决策框架下，根据 <span class="math inline">\(\eqref{4}\)</span> 式，各<strong>判别函数</strong>为： <span class="math display">\[g_i(\mathbf x)=-\frac{1}{2}(\mathbf x-\boldsymbol\mu_i)^T\Sigma_i^{-1}(\mathbf x-\boldsymbol\mu_i)-\frac{d}{2}\ln(2\pi)-\frac{1}{2}\ln |\Sigma_i|+\ln P(\omega_i)\tag{7}\label{7}\]</span> 我们下面继续分三种情形讨论。</p><h3 id="情形一sigma_isigma2mathbf-i">情形一：<span class="math inline">\(\Sigma_i=\sigma^2\mathbf I\)</span></h3><p>所有类都服从各向同性、方差相同的正态分布，判别函数 <span class="math inline">\(\eqref{7}\)</span> 式可以简化为： <span class="math display">\[g_i(\mathbf x)=-\frac{\Vert\mathbf x-\boldsymbol\mu_i\Vert_2^2}{2\sigma^2}+\ln P(\omega_i)\tag{8}\label{8}\]</span> 这看起来是 <span class="math inline">\(\mathbf x\)</span> 的二次型，但是二次项 <span class="math inline">\(\mathbf x^T\mathbf x\)</span> 对各类其实是相同的，对决策并没有作用，因此可以丢掉，于是 <span class="math inline">\(\eqref{8}\)</span> 式简化为了一个<strong>线性判别函数</strong>： <span class="math display">\[\begin{align}&amp;g_i(\mathbf x)=\mathbf w_i^T\mathbf x+w_{i0}\\\text{where}\quad&amp;\mathbf w_i=\frac{1}{\sigma^2}\boldsymbol\mu_i\\&amp;w_{i0}=-\frac{1}{2\sigma^2}\boldsymbol\mu_i^T\boldsymbol\mu_i+\ln P(\omega_i)\end{align}\tag{9}\label{9}\]</span> 那么<strong>决策面方程</strong> <span class="math inline">\(g_i(\mathbf x)=g_j(\mathbf x)\)</span> 为： <span class="math display">\[\begin{align}&amp;\mathbf w^T(\mathbf x-\mathbf x_0)=0\\\text{where}\quad&amp;\mathbf w=\boldsymbol\mu_i-\boldsymbol\mu_j\\&amp;\mathbf x_0=\frac{1}{2}(\boldsymbol\mu_i+\boldsymbol\mu_j)-\frac{\sigma^2}{\Vert\boldsymbol\mu_i-\boldsymbol\mu_j\Vert^2_2}\ln\frac{P(\omega_i)}{P(\omega_j)}(\boldsymbol\mu_i-\boldsymbol\mu_j)\end{align}\]</span> 这是一个超平面，法向量为 <span class="math inline">\(\mathbf w\)</span>，即中心点的连线。若先验概率是相等的，<span class="math inline">\(P(\omega_i)=P(\omega_j)\)</span>，超平面过连线中点；否则，超平面会朝一侧偏移。</p><h3 id="情形二-sigma_isigma">情形二： <span class="math inline">\(\Sigma_i=\Sigma\)</span></h3><p>所有类的协方差矩阵相同，判别函数 <span class="math inline">\(\eqref{7}\)</span> 式可以简化为： <span class="math display">\[g_i(\mathbf x)=-\frac{1}{2}(\mathbf x-\boldsymbol\mu_i)^T\Sigma^{-1}(\mathbf x-\boldsymbol\mu_i)+\ln P(\omega_i)\tag{10}\label{10}\]</span> 同理二次项 <span class="math inline">\(\mathbf x^T\Sigma^{-1}\mathbf x\)</span> 可以丢掉，因此 <span class="math inline">\(\eqref{10}\)</span> 式依旧简化为了一个<strong>线性判别函数</strong>： <span class="math display">\[\begin{align}&amp;g_i(\mathbf x)=\mathbf w_i^T\mathbf x+w_{i0}\\\text{where}\quad&amp;\mathbf w_i=\Sigma^{-1}\boldsymbol\mu_i\\&amp;w_{i0}=-\frac{1}{2}\boldsymbol\mu_i^T\Sigma^{-1}\boldsymbol\mu_i+\ln P(\omega_i)\end{align}\tag{11}\label{11}\]</span> 那么<strong>决策面方程</strong> <span class="math inline">\(g_i(\mathbf x)=g_j(\mathbf x)\)</span> 为： <span class="math display">\[\begin{align}&amp;\mathbf w^T(\mathbf x-\mathbf x_0)=0\\\text{where}\quad&amp;\mathbf w=\Sigma^{-1}(\boldsymbol\mu_i-\boldsymbol\mu_j)\\&amp;\mathbf x_0=\frac{1}{2}(\boldsymbol\mu_i+\boldsymbol\mu_j)-\frac{1}{(\boldsymbol\mu_i-\boldsymbol\mu_j)^T\Sigma^{-1}(\boldsymbol\mu_i-\boldsymbol\mu_j)}\ln\frac{P(\omega_i)}{P(\omega_j)}(\boldsymbol\mu_i-\boldsymbol\mu_j)\end{align}\]</span> 这也是一个超平面，但与上一种情形不同的是，其法向量 <span class="math inline">\(\mathbf w\)</span> 不再朝着 <span class="math inline">\(\boldsymbol\mu_i-\boldsymbol\mu_j\)</span> 方向，而是有一定的旋转（<span class="math inline">\(\Sigma^{-1}\)</span> 的作用）。不过，当先验概率相等时，超平面依旧过中心点连线的中点；否则，超平面朝一侧偏移。</p><h3 id="情形三sigma_i-任意">情形三：<span class="math inline">\(\Sigma_i\)</span> 任意</h3><p>判别函数 <span class="math inline">\(\eqref{7}\)</span> 式只能丢掉常数项，依旧是一个<strong>二次型</strong>： <span class="math display">\[\begin{align}&amp;g_i(\mathbf x)=\mathbf x^T W_i\mathbf x+\mathbf w_i^T\mathbf x+w_{i0}\\\text{where}\quad&amp;W_i=-\frac{1}{2}\Sigma_i^{-1}\\&amp;\mathbf w_i=\Sigma_i^{-1}\boldsymbol\mu_i\\&amp;w_{i0}=-\frac{1}{2}\boldsymbol\mu_i^T\Sigma_i^{-1}\boldsymbol\mu_i-\frac{1}{2}\ln|\Sigma_i|+\ln P(\omega_i)\end{align}\tag{12}\label{12}\]</span> 此时的<strong>决策面方程</strong>将是超二次曲面——超平面、超平面对、超球体、超椭球体、超抛物面、超双曲面等。</p><h2 id="丢失特征和噪声特征">丢失特征和噪声特征</h2><p>设 <span class="math inline">\(\mathbf x=[\mathbf x_g,\mathbf x_b]\)</span>，其中 <span class="math inline">\(\mathbf x_g\)</span> 表示已知或完好的特征，<span class="math inline">\(\mathbf x_b\)</span> 表示未知或丢失的特征，那么根据已知的特征表示后验概率为： <span class="math display">\[\begin{align}P(\omega_i\vert\mathbf x_g)&amp;=\frac{p(\omega_i,\mathbf x_g)}{p(\mathbf x_g)}=\frac{\int p(\omega_i,\mathbf x_g,\mathbf x_b)\mathrm d\mathbf x_b}{\int p(\mathbf x_g,\mathbf x_b)\mathrm d\mathbf x_b}\\&amp;=\frac{\int P(\omega_i\vert\mathbf x_g,\mathbf x_b)p(\mathbf x_g,\mathbf x_b)\mathrm d\mathbf x_b}{\int p(\mathbf x_g,\mathbf x_b)\mathrm d\mathbf x_b}\\&amp;=\frac{\int g_i(\mathbf x)p(\mathbf x)\mathrm d\mathbf x_b}{\int p(\mathbf x)\mathrm d\mathbf x_b}\end{align}\tag{13}\label{13}\]</span> 分子分母都相当于在变量 <span class="math inline">\(\mathbf x_b\)</span> 上进行“边缘化”。</p><p>进一步地，如果 <span class="math inline">\(\mathbf x_b\)</span> 表示的是受到噪声干扰的特征，其真实值为 <span class="math inline">\(\mathbf x_t\)</span>，噪声模型记作 <span class="math inline">\(p(\mathbf x_b\vert\mathbf x_t)\)</span>. 注意当真实特征值 <span class="math inline">\(\mathbf x_t\)</span> 已知时 <span class="math inline">\(\mathbf x_b\)</span> 与 <span class="math inline">\(\omega_i\)</span> 和 <span class="math inline">\(\mathbf x_g\)</span> 无关。那么，后验分布为： <span class="math display">\[\begin{align}P(\omega_i\vert\mathbf x_g,\mathbf x_b)&amp;=\frac{p(\omega_i,\mathbf x_g,\mathbf x_b)}{p(\mathbf x_g,\mathbf x_b)}=\frac{\int p(\omega_i,\mathbf x_g,\mathbf x_b,\mathbf x_t)\mathrm d\mathbf x_t}{\int p(\mathbf x_g,\mathbf x_b,\mathbf x_t)\mathrm d\mathbf x_t}\\&amp;=\frac{\int P(\omega_i\vert\mathbf x_g,\mathbf x_b,\mathbf x_t)p(\mathbf x_b\vert\mathbf x_g,\mathbf x_t)p(\mathbf x_g,\mathbf x_t)\mathrm d\mathbf x_t}{\int p(\mathbf x_b\vert\mathbf x_g,\mathbf x_t)p(\mathbf x_g,\mathbf x_t)\mathrm d\mathbf x_t}\\&amp;=\frac{\int P(\mathbf \omega_i\vert\mathbf x_g,\mathbf x_t)p(\mathbf x_b\vert\mathbf x_t)p(\mathbf x_g,\mathbf x_t)\mathrm d\mathbf x_t}{\int p(\mathbf x_b\vert\mathbf x_t)p(\mathbf x_g,\mathbf x_t)\mathrm d\mathbf x_t}\\&amp;=\frac{\int g_i(\mathbf x)p(\mathbf x)p(\mathbf x_b\vert\mathbf x_t)\mathrm d\mathbf x_t}{\int p(\mathbf x)p(\mathbf x_b\vert\mathbf x_t)\mathrm d\mathbf x_t}\end{align}\tag{14}\label{14}\]</span> <strong>对比 <span class="math inline">\(\eqref{13}\)</span> 式，<span class="math inline">\(\eqref{14}\)</span> 式对被积函数按噪声模型进行了加权</strong>。在极端情况下，<span class="math inline">\(p(\mathbf x_b\vert\mathbf x_t)\)</span> 在整个空间为 <span class="math inline">\(1\)</span>，即 <span class="math inline">\(\mathbf x_b\)</span> 不包含任何关于 <span class="math inline">\(\mathbf x_t\)</span> 的信息，那么这个特征相当于丢失了，<span class="math inline">\(\eqref{14}\)</span> 式也就退化到了 <span class="math inline">\(\eqref{13}\)</span> 式。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>模式分类</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>生成模型中的互信息</title>
    <link href="/blog-main/2023/09/15/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E4%BA%92%E4%BF%A1%E6%81%AF/"/>
    <url>/blog-main/2023/09/15/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E4%BA%92%E4%BF%A1%E6%81%AF/</url>
    
    <content type="html"><![CDATA[<h2 id="基础知识">基础知识</h2><p>在信息论中，随机变量 <span class="math inline">\(X\)</span> 的<strong>（微分）熵</strong>定义为 <span class="math inline">\(-\log p(x)\)</span> 的期望： <span class="math display">\[H(X)=-\int_xp(x)\log p(x)\mathrm dx=-\mathbb E_X[\log p(X)]\]</span> 当涉及两个随机变量 <span class="math inline">\(X,Y\)</span> 时，对它们的联合分布求熵也就得到了<strong>联合熵</strong>： <span class="math display">\[H(X,Y)=-\int_x\int_yp(x,y)\log p(x,y)\mathrm dx\mathrm dy=-\mathbb E_{X,Y}[\log p(X,Y)]\]</span> 当其中一个随机变量给定时，例如 <span class="math inline">\(X=x\)</span>，我们可以对条件概率分布 <span class="math inline">\(p(Y\vert X=x)\)</span> 求它的熵： <span class="math display">\[H(Y\vert X=x)=-\int_y p(y\vert x)\log p(y\vert x)\mathrm dy=-\mathbb E_{Y\vert X=x}[\log p(Y\vert X=x)]\]</span> 值得注意的是，<span class="math inline">\(H(Y\vert X=x)\)</span> 建立在已知 <span class="math inline">\(X\)</span> 取值为 <span class="math inline">\(x\)</span> 的情况下。那么在平均意义下，继续对 <span class="math inline">\(X\)</span> 取期望，就得到了<strong>条件熵</strong>： <span class="math display">\[\begin{align}H(Y\vert X)&amp;=-\int_xp(x)\int_y p(y\vert x)\log p(y\vert x)\mathrm dx\mathrm dy\\&amp;=-\int_x\int_y p(x,y)\log p(y\vert x)\mathrm dx\mathrm dy\\&amp;=-\mathbb E_{X,Y}[\log p(Y\vert X)]\end{align}\]</span> 不难证明，条件熵加上作为条件的那个随机变量的熵（也许可以称作边缘熵？），正好就是联合熵： <span class="math display">\[\begin{align}H(Y\vert X)+H(X)&amp;=-\int_x\int_y p(x,y)\log p(y\vert x)\mathrm dx\mathrm dy-\int_xp(x)\log p(x)\mathrm dx\\&amp;=-\int_x\int_y p(x,y)\log p(y\vert x)\mathrm dx\mathrm dy-\int_x\int_yp(x,y)\log p(x)\mathrm dx\mathrm dy\\&amp;=-\int_x\int_yp(x,y)\log\big(p(y\vert x)p(x)\big)\mathrm dx\mathrm dy\\&amp;=-\int_x\int_yp(x,y)\log p(x,y)\mathrm dx\mathrm dy\\&amp;=H(X,Y)\end{align}\]</span> 这个关系式可以类比 <span class="math inline">\(p(y\vert x)p(x)=p(x,y)\)</span> 来记忆。</p><p>条件熵 <span class="math inline">\(H(Y\vert X)\)</span> 可以理解为在给定 <span class="math inline">\(X\)</span> 的条件下，<span class="math inline">\(Y\)</span> 还剩下的不确定性。例如，当 <span class="math inline">\(X\)</span> 与 <span class="math inline">\(Y\)</span> 独立时，<span class="math inline">\(X\)</span> 不能给 <span class="math inline">\(Y\)</span> 带来任何新的信息，即 <span class="math inline">\(H(Y\vert X)=H(Y)\)</span>，<span class="math inline">\(Y\)</span> 的不确定性不变；当 <span class="math inline">\(X\)</span> 完全决定了 <span class="math inline">\(Y\)</span> 时，给定 <span class="math inline">\(X\)</span> 的条件下 <span class="math inline">\(Y\)</span> 没有任何的不确定性，即 <span class="math inline">\(H(Y\vert X)=0\)</span>. 因此，我们用熵减去条件熵来表示 <span class="math inline">\(X\)</span> 带给 <span class="math inline">\(Y\)</span> 的不确定性，即<strong>互信息</strong>： <span class="math display">\[I(X;Y)=H(Y)-H(Y\vert X)\]</span> 如果把互信息的表达式展开： <span class="math display">\[\begin{align}I(X;Y)&amp;=H(Y)-H(Y\vert X)\\&amp;=-\int_yp(y)\log p(y)\mathrm dy+\int_x\int_yp(x,y)\log p(y\vert x)\mathrm dx\mathrm dy\\&amp;=-\int_x\int_yp(x,y)\log p(y)\mathrm dx\mathrm dy+\int_x\int_yp(x,y)\log p(y\vert x)\mathrm dx\mathrm dy\\&amp;=\int_x\int_y p(x,y)\log\frac{p(y\vert x)}{p(y)}\mathrm dx\mathrm dy\\&amp;=\int_x\int_y p(x,y)\log\frac{p(x,y)}{p(x)p(y)}\mathrm dx\mathrm dy\\&amp;=\text{KL}\big(p(x,y)\Vert p(x)p(y)\big)\end{align}\]</span> 我们发现互信息其实就是 <span class="math inline">\(p(x,y)\)</span> 与 <span class="math inline">\(p(x)p(y)\)</span> 之间的 KL 散度。由于 KL 散度衡量了两个分布之间的差异，所以从这个角度看，互信息在衡量 <span class="math inline">\(p(x,y)\)</span> 与 <span class="math inline">\(p(x)p(y)\)</span> 之间的差异（注意 <span class="math inline">\(p(x)p(y)\)</span> 确实是一个合法的概率分布）。当 <span class="math inline">\(X\)</span> 与 <span class="math inline">\(Y\)</span> 独立时，<span class="math inline">\(p(x,y)=p(x)p(y)\)</span>，KL 散度为 0，也即互信息为 0；否则，<span class="math inline">\(p(x,y)\)</span> 不能拆成 <span class="math inline">\(p(x)p(y)\)</span>，这两个分布存在差异，KL 散度非零，即互信息非零。</p><p>从上式也能看出互信息其实是对称的，<span class="math inline">\(X\)</span> 带给 <span class="math inline">\(Y\)</span> 的不确定性等于 <span class="math inline">\(Y\)</span> 带给 <span class="math inline">\(X\)</span> 的不确定性： <span class="math display">\[I(X;Y)=H(Y)-H(Y\vert X)=H(X)-H(X\vert Y)\]</span> 以上涉及到的关系式可以用如下韦恩图直观地可视化出来：</p><p><img src="relation-diagram.png" width=40% /></p><h2 id="infogan-infovae">InfoGAN &amp; InfoVAE</h2><p>在生成模型的研究中，我们常常认为观测到的数据 <span class="math inline">\(\mathbf x\)</span> 背后是由维度更低的隐变量 <span class="math inline">\(\mathbf z\)</span> 控制的。记数据的真实分布为 <span class="math inline">\(p_\text{data}(\mathbf x)\)</span>，我们无法直接写出它的形式，只能从中采样若干样本构成训练集。因此，为了对未知的 <span class="math inline">\(p_\text{data}(\mathbf x)\)</span> 进行建模，我们可以构建一个解码器（生成器） <span class="math inline">\(p_\theta(\mathbf x\vert\mathbf z)\)</span>，并预定义一个隐变量的先验分布 <span class="math inline">\(p(\mathbf z)\)</span>（例如标准正态分布），那么就能够生成如下的数据分布： <span class="math display">\[p_\theta(\mathbf x)=\int_{\mathbf z}p_\theta(\mathbf x\vert\mathbf z)p(\mathbf z)\mathrm d\mathbf z\]</span> 训练生成模型的目标就是学习参数 <span class="math inline">\(\theta\)</span> 使得 <span class="math inline">\(p_\theta(\mathbf x)\)</span> 近似于 <span class="math inline">\(p_\text{data}(\mathbf x)\)</span>. 然而，直接计算或遍历隐空间近似 <span class="math inline">\(p_\theta(\mathbf x)\)</span> 是不可行的，因此不同生成模型采用了不同的方法来解决这个问题。</p><p>对于 GANs 一类生成模型，我们使用一个判别器与生成器做对抗，促使生成器的数据分布尽可能接近真实的数据分布。可以证明，GANs 在隐式地最小化 <span class="math inline">\(p_\theta(\mathbf x)\)</span> 与 <span class="math inline">\(p_\text{data}(\mathbf x)\)</span> 之间的 JS 散度（或 F 散度、Wasserstein 距离等）。在这个情形下，为了避免<strong>生成器</strong>忽略掉隐变量 <span class="math inline">\(\mathbf z\)</span>，我们可以最大化互信息 <span class="math inline">\(I_{p_\theta}(\mathbf x;\mathbf z)\)</span>，这就是 InfoGAN 的核心思想。</p><p>对于 VAEs 一类生成模型，我们引入了变分后验 <span class="math inline">\(q_\phi(\mathbf z\vert\mathbf x)\)</span> 推导出对数似然 <span class="math inline">\(\log p_\theta(\mathbf x)\)</span> 的变分下界 ELBO. 其中，<span class="math inline">\(q_\phi(\mathbf z\vert\mathbf x)\)</span> 实现为一个神经网络编码器。在这个情形下，为了避免<strong>编码器</strong>将不同的 <span class="math inline">\(\mathbf x\)</span> 都映射到没有差别的 <span class="math inline">\(\mathbf z\)</span> 中，即隐变量不包含输入的任何信息，我们可以最大化互信息 <span class="math inline">\(I_{q_\phi}(\mathbf x;\mathbf z)\)</span>，这就是 InfoVAE 的核心思想。</p><p>可以看见，InfoGAN 和 InfoVAE 优化的互信息其实是不同的——前者是为了保留生成器（也就是解码器）的输入和输出之间的信息传递，而后者是为了保留编码器的输入和输出之间的信息传递。</p><h3 id="infogan">InfoGAN</h3><p>前文提到，InfoGAN 希望最大化互信息 <span class="math inline">\(I_{p_\theta}(\mathbf x;\mathbf z)\)</span>. 然而，<span class="math inline">\(I_{p_\theta}(\mathbf x;\mathbf z)\)</span> 本身是 intractable 的，这是因为： <span class="math display">\[I_{p_\theta}(\mathbf x;\mathbf z)=H(\mathbf z)-H(\mathbf z\vert\mathbf x)=H(\mathbf z)+\mathbb E_{p_\theta(\mathbf x,\mathbf z)}\left[\log p_\theta(\mathbf z\vert\mathbf x)\right]\]</span> 其中涉及到了后验分布的计算： <span class="math display">\[\begin{align}p_\theta(\mathbf z\vert\mathbf x)=\frac{p(\mathbf z)p_\theta(\mathbf x\vert\mathbf z)}{p_\theta(\mathbf x)}=\frac{p(\mathbf z)p_\theta(\mathbf x\vert\mathbf z)}{\int_{\mathbf z&#39;}p(\mathbf z&#39;)p_\theta(\mathbf x\vert\mathbf z&#39;)}\end{align}\]</span> 分母部分需要遍历隐空间——这在绝大多数情况下是不可行的。InfoGAN 的解决方案类似于 VAE——引入变分后验 <span class="math inline">\(q_\phi(\mathbf z\vert\mathbf x)\)</span> 去近似不可解的真实后验 <span class="math inline">\(p_\theta(\mathbf z\vert\mathbf x)\)</span>： <span class="math display">\[\begin{align}I_{p_\theta}(\mathbf x;\mathbf z)&amp;=H(\mathbf z)+\mathbb E_{p_\theta(\mathbf x,\mathbf z)}\left[\log p_\theta(\mathbf z\vert\mathbf x)\right]\\&amp;=H(\mathbf z)+\mathbb E_{p_\theta(\mathbf x,\mathbf z)}\left[\log\frac{p_\theta(\mathbf z\vert\mathbf x)}{q_\phi(\mathbf z\vert\mathbf x)}+\log q_\phi(\mathbf z\vert\mathbf x)\right]\\&amp;=H(\mathbf z)+\mathbb E_{p_\theta(\mathbf x)}\Big[\underbrace{\text{KL}(p_\theta(\mathbf z\vert\mathbf x)\Vert q_\phi(\mathbf z\vert\mathbf x))}_{\geq 0}+\mathbb E_{p_\theta(\mathbf z\vert\mathbf x)}[\log q_\phi(\mathbf z\vert\mathbf x)]\Big]\\&amp;\geq H(\mathbf z)+\mathbb E_{p_\theta(\mathbf x,\mathbf z)}\left[\log q_\phi(\mathbf z\vert\mathbf x)\right]\\&amp;=H(\mathbf z)+\mathbb E_{p(\mathbf z)}\left[\mathbb E_{p_\theta(\mathbf x\vert\mathbf z)}\left[\log q_\phi(\mathbf z\vert\mathbf x)\right]\right]\end{align}\]</span> 于是我们可以通过最大化这个变分下界来最大化互信息。特别地，如果我们固定取 <span class="math inline">\(p(\mathbf z)\)</span> 为标准正态分布，那么 <span class="math inline">\(H(\mathbf z)\)</span> 为常数，我们只需要优化 <span class="math inline">\(\mathbb E_{p(\mathbf z)}\left[\mathbb E_{p_\theta(\mathbf x\vert\mathbf z)}\left[\log q_\phi(\mathbf z\vert\mathbf x)\right]\right]\)</span> 即可。</p><p>具体而言，InfoGAN 引入互信息的动机是希望在 GAN 的学习过程中鼓励隐变量的解耦。作者其实并没有直接最大化 <span class="math inline">\(I(\mathbf x;\mathbf z)\)</span>，而是将隐变量分为两部分：<span class="math inline">\(\mathbf z\)</span> 是无法解耦的部分，<span class="math inline">\(\mathbf c\)</span> 是可解耦的部分，并只最大化 <span class="math inline">\(I(\mathbf x;\mathbf c)\)</span>. 根据上面的讨论，引入变分后验 <span class="math inline">\(q_\phi(\mathbf c\vert\mathbf x)\)</span>： <span class="math display">\[I(\mathbf x;\mathbf c)\geq \mathcal L_I=H(\mathbf c)+\mathbb E_{p(\mathbf c)}\left[\mathbb E_{p_\theta(\mathbf x\vert\mathbf z,\mathbf c)}\left[\log q_\phi(\mathbf c\vert\mathbf x)\right]\right],\quad\mathbf z\sim p(\mathbf z)\]</span> 在实现上，<span class="math inline">\(\mathbf c\)</span> 可以是离散的，也可以是连续的。为了简便起见，我们将离散 <span class="math inline">\(\mathbf c\)</span> 的先验分布设为均匀类别分布，连续 <span class="math inline">\(\mathbf c\)</span> 的先验分布设为标准正态分布，那么上式中的 <span class="math inline">\(H(\mathbf c)\)</span> 也变成了常数。</p><p><span class="math inline">\(q_\phi(\mathbf c\vert\mathbf x)\)</span> 使用一个神经网络解码器实现——特别地，这个网络可以与已有的判别器共用浅层部分，只需要最后拉出一个新的 head，因而 InfoGAN 新引入的计算量非常小。对于离散的 <span class="math inline">\(\mathbf c\)</span>，取 <span class="math inline">\(q_\phi(\mathbf c\vert\mathbf x)\)</span> 为 softmax 分布，那么容易推出 <span class="math inline">\(\mathcal L_I\)</span> 就是一个交叉熵损失；对于连续的 <span class="math inline">\(\mathbf c\)</span>，取 <span class="math inline">\(q_\phi(\mathbf c\vert\mathbf x)\)</span> 为高斯分布，那么 <span class="math inline">\(\mathcal L_I\)</span> 就是一个 MSE 损失函数。</p><h3 id="infovae">InfoVAE</h3><p>如前文所述，InfoVAE 希望最大化互信息 <span class="math inline">\(I_{q_\phi}(\mathbf x;\mathbf z)\)</span>. 在已有的编码器 <span class="math inline">\(q_\phi(\mathbf z\vert\mathbf x)\)</span> 下，可以推出 <span class="math inline">\(I_{q_\phi}(\mathbf x;\mathbf z)\)</span> 由两部分组成： <span class="math display">\[\begin{align}I_{q_\phi}(\mathbf x;\mathbf z)&amp;=\mathbb E_{p_\text{data}(\mathbf x)}\left[\mathbb E_{q_\phi(\mathbf z\vert\mathbf x)}\left[\log\frac{q_\phi(\mathbf z\vert\mathbf x)}{q_\phi(\mathbf z)}\right]\right]\\&amp;=\mathbb E_{p_\text{data}(\mathbf x)}\left[\mathbb E_{q_\phi(\mathbf z\vert\mathbf x)}\left[\log\frac{q_\phi(\mathbf z\vert\mathbf x)}{p(\mathbf z)}+\log\frac{p(\mathbf z)}{q_\phi(\mathbf z)}\right]\right]\\&amp;=\mathbb E_{p_\text{data}(\mathbf x)}\big[\underbrace{\text{KL}\big(q_\phi(\mathbf z\vert\mathbf x)\Vert p(\mathbf z)\big)}_\text{regularization}\big]-\underbrace{\text{KL}\big(q_\phi(\mathbf z)\Vert p(\mathbf z)\big)}_\text{prior matching}\end{align}\]</span></p><p>我们发现第一项正好是原 VAE 的 ELBO 中的正则项： <span class="math display">\[\mathcal L_\text{ELBO}=\mathbb E_{p_\text{data}(\mathbf x)}\big[\underbrace{\mathbb E_{q_\phi(\mathbf z\vert\mathbf x)}\left[\log p_\theta(\mathbf x\vert\mathbf z)\right]}_\text{reconstruction}-\underbrace{\text{KL}\big(q_\phi(\mathbf z\vert\mathbf x)\Vert p(\mathbf z)\big)}_\text{regularization}\big]\]</span> 如果将 ELBO 与 <span class="math inline">\(I_{q_\phi}(\mathbf x;\mathbf z)\)</span> 相加（相当于为原 VAE 新添加一个最大化互信息的目标），这一正则项就被抵消了： <span class="math display">\[\mathcal L_\text{ELBO}+I_{q_\phi}(\mathbf x;\mathbf z)=\mathbb E_{p_\text{data}(\mathbf x)}\left[\mathbb E_{q_\phi(\mathbf z\vert\mathbf x)}\left[\log p_\theta(\mathbf x\vert\mathbf z)\right]\right]-\text{KL}\big(q_\phi(\mathbf z)\Vert p(\mathbf z)\big) \tag{1}\label{1}\]</span> 或者我们也可以理解为，保留重构项不变，将原来的正则项替换成了现在的先验匹配项。直观上，原正则项让不同 <span class="math inline">\(\mathbf x\)</span> 编码出来的 <span class="math inline">\(\mathbf z\)</span> 都趋向于同一个先验分布 <span class="math inline">\(p(\mathbf x)\)</span>，这显然与最大化互信息 <span class="math inline">\(I_{q_\phi}(\mathbf x,\mathbf z)\)</span> 是矛盾的。而新的先验匹配项只要求<strong>整体意义下</strong>编码出的 <span class="math inline">\(\mathbf z\)</span> 与先验分布 <span class="math inline">\(p(\mathbf x)\)</span> 相近即可，不同的 <span class="math inline">\(\mathbf x\)</span> 编码出的 <span class="math inline">\(\mathbf z\)</span> 依旧可以不同，因此有助于保留 <span class="math inline">\(\mathbf x\)</span> 与 <span class="math inline">\(\mathbf z\)</span> 之间的互信息。</p><p>然而，<span class="math inline">\(q_\phi(\mathbf z)\)</span> 是 intractable 的，为此，我们可以把 KL 散度换成其他衡量两个分布差异的指标 <span class="math inline">\(D\)</span>，使得 <span class="math inline">\(D(q_\phi(\mathbf z)\Vert p(\mathbf z))\)</span>​ 可解。例如，取 <span class="math inline">\(D\)</span> 为 JS 散度，那么我们可以通过对抗训练的方式（GANs）来隐式地实现 JS 散度，这就是 Adversarial Autoencoders<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="Makhzani, Alireza, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. *arXiv preprint arXiv:1511.05644* (2015).">[7]</span></a></sup>；而取 <span class="math inline">\(D\)</span> 为 MMD，则可以按如下方式计算： <span class="math display">\[D_\text{MMD}\big(q_\phi(\mathbf z)\Vert p(\mathbf z)\big)=\mathbb E_{p(\mathbf z),p(\mathbf z&#39;)}[k(\mathbf z,\mathbf z&#39;)]-2\mathbb E_{q_\phi(\mathbf z),p(\mathbf z&#39;)}[k(\mathbf z,\mathbf z&#39;)]+\mathbb E_{q_\phi(\mathbf z),q_\phi(\mathbf z&#39;)}[k(\mathbf z,\mathbf z&#39;)]\]</span> 其中 <span class="math inline">\(k(\cdot,\cdot)\)</span> 是任一正定核，例如高斯核。作者称这样的模型为 MMD-VAE.</p><p>进一步地，为了更好的通用性，作者在 <span class="math inline">\(\eqref{1}\)</span> 式的基础上引入了两个超参数 <span class="math inline">\(\alpha,\lambda\)</span> 来调节原 VAE 正则项与新先验匹配项的权重系数。因此，InfoVAE 最终的目标函数为： <span class="math display">\[\begin{align}\mathcal L_\text{InfoVAE}=&amp;\ \mathbb E_{p_\text{data}(\mathbf x)}\left[\mathbb E_{q_\phi(\mathbf z\vert\mathbf x)}\left[\log p_\theta(\mathbf x\vert\mathbf z)\right]\right]-\\&amp;\ (1-\alpha)\mathbb E_{p_\text{data}(\mathbf x)}\big[\text{KL}\big(q_\phi(\mathbf z\vert\mathbf x)\Vert p(\mathbf z)\big)\big]-\\&amp;\ (\alpha+\lambda-1) D(q_\phi(\mathbf z)\Vert p(\mathbf z))\end{align}\]</span> 作者在 MNIST 实验中取 <span class="math inline">\(\lambda=1000,\alpha=0\)</span>.</p><h2 id="参考资料">参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>互信息(Mutual Information)浅尝辄止（一）：基础概念 - idejie的文章 - 知乎 https://zhuanlan.zhihu.com/p/240676850 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Mutual information. https://en.wikipedia.org/wiki/Mutual_information <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>深度学习中常见的互信息的变分上下界(详细推导) - sonta的文章 - 知乎 https://zhuanlan.zhihu.com/p/91900950 <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Barber, David, and Felix Agakov. The im algorithm: a variational approach to information maximization. <em>Advances in neural information processing systems</em> 16, no. 320 (2004): 201. <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>A Tutorial on Information Maximizing Variational Autoencoders (InfoVAE). https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/ <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Zhao, Shengjia, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational autoencoders. <em>arXiv preprint arXiv:1706.02262</em> (2017). <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Makhzani, Alireza, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoencoders. <em>arXiv preprint arXiv:1511.05644</em> (2015). <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
      <tag>information theory</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Sublime Text 配置</title>
    <link href="/blog-main/2023/09/09/Sublime-Text-%E9%85%8D%E7%BD%AE/"/>
    <url>/blog-main/2023/09/09/Sublime-Text-%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="官网下载-sublime-text-4">官网下载 Sublime Text 4</h2><p>官网链接：<a href="https://www.sublimetext.com/" class="uri">https://www.sublimetext.com/</a></p><h2 id="安装-package-control">安装 Package Control</h2><p><strong>简介</strong>：Package Control 是管理 Sublime Text 插件的插件，以后下载其他插件可以通过它来下载。</p><p><strong>下载</strong>：<code>cmd+shift+p</code> 打开 command palette，输入 <code>Install Package Control</code>，回车。</p><p><strong>使用</strong>：<code>cmd+shift+p</code> 打开 command palette，输入 <code>Package Control: Install Package</code>，回车，输入要下载的插件，回车。</p><p><strong>常见问题</strong>：MacOS 下安装完成后无法使用 Package Control，表现为 <code>Settings... -&gt; Package Settings -&gt; Package Control -&gt; Settings</code> 为灰色。</p><p><strong>解决方案</strong>：根据参考资料<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="[ST3/4] Package Control not working on macOS #1612. https://github.com/wbond/package_control/issues/1612">[1]</span></a></sup>，可按如下方式解决：</p><ol type="1"><li><p>下载 <code>openssl@1.1</code>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">brew install openssl@1.1<br></code></pre></td></tr></table></figure><p>注：在下载 <code>openssl@1.1</code> 之前使用 <code>brew list</code> 发现有 <code>openssl@3</code>，不清楚 <code>openssl@3</code> 是否也可以。</p></li><li><p>在 <code>/usr/local/lib</code> 下创建软链接：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ln -sf /usr/local/Cellar/openssl@1.1/1.1.1v/lib/libcrypto.dylib /usr/local/lib/<br></code></pre></td></tr></table></figure><p>注：参考资料不是 <code>1.1.1v</code>，而是 <code>1.1.1n</code> 或者 <code>1.1.1o</code>，总之按照实际情况输入即可。</p></li></ol><h2 id="安装-sidebarenhancements">安装 SidebarEnhancements</h2><p><strong>简介</strong>：SidebarEnhancements 为 Sublime Text 的侧栏文件和文件夹操作提供增强功能。</p><p><strong>下载</strong>：使用 Package Control 下载即可。</p><h2 id="安装-terminus">安装 Terminus</h2><p><strong>简介</strong>：Terminus 能在 Sublime Text 的 Panel 或者 Tab 中打开一个终端，可正常输入输出交互，弥补 Sublime Text 本身的 Panel 无法接收输入的不足。</p><p><strong>下载</strong>：使用 Package Control 下载即可。</p><h2 id="自定义-c-编译系统">自定义 C++ 编译系统</h2><p><strong>简介</strong>：Sublime Text 默认的编译系统中有个 C++ Single File，包含两种命令：只编译和编译+运行。但是，只编译显然并不方便，编译+运行打开的 Panel 无法进行输入交互，也不方便，因此我们有必要自定义命令。</p><p><strong>步骤</strong>：根据参考资料<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="【全网最新、最详细】如何使用 Sublime Text 4 优雅地写C++？ - 南瓜瓜的文章 - 知乎 https://zhuanlan.zhihu.com/p/586687010">[2]</span></a></sup>，</p><ol type="1"><li><p>点击 <code>Tools -&gt; Build System -&gt; New Build System...</code>；</p></li><li><p>粘贴以下内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<br>    // https://zhuanlan.zhihu.com/p/586687010<br><br>    <span class="hljs-string">&quot;cmd&quot;</span>: [<br>        <span class="hljs-string">&quot;bash&quot;</span>,<br>        <span class="hljs-string">&quot;-c&quot;</span>,<br>        <span class="hljs-string">&quot;clang++ &#x27;<span class="hljs-variable">$&#123;file&#125;</span>&#x27; -std=c++11 -stdlib=libc++ -o &#x27;<span class="hljs-variable">$&#123;file_path&#125;</span>/<span class="hljs-variable">$&#123;file_base_name&#125;</span>&#x27; &amp;&amp; <span class="hljs-variable">$&#123;file_path&#125;</span>/<span class="hljs-variable">$&#123;file_base_name&#125;</span>&quot;</span><br>    ],<br>    <span class="hljs-string">&quot;file_regex&quot;</span>: <span class="hljs-string">&quot;^(..&#123;FNXX==XXFN&#125;*):([0-9]+):?([0-9]+)?:? (.*)$&quot;</span>,<br>    <span class="hljs-string">&quot;working_dir&quot;</span>: <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;file_path&#125;</span>&quot;</span>,<br>    <span class="hljs-string">&quot;encoding&quot;</span>: <span class="hljs-string">&quot;utf-8&quot;</span>,<br>    <span class="hljs-string">&quot;selector&quot;</span>: <span class="hljs-string">&quot;source.c, source.c++&quot;</span>,<br>    <span class="hljs-string">&quot;variants&quot;</span>: [<br>        &#123;<br>            <span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;Run In Terminus&quot;</span>,<br>            <span class="hljs-string">&quot;target&quot;</span>: <span class="hljs-string">&quot;terminus_exec&quot;</span>,<br>            <span class="hljs-string">&quot;cancel&quot;</span>: <span class="hljs-string">&quot;terminus_cancel_build&quot;</span>,<br>            <span class="hljs-string">&quot;cmd&quot;</span>: [<br>                <span class="hljs-string">&quot;bash&quot;</span>,<br>                <span class="hljs-string">&quot;-c&quot;</span>,<br>                <span class="hljs-string">&quot;clang++ &#x27;<span class="hljs-variable">$&#123;file&#125;</span>&#x27; -std=c++11 -stdlib=libc++ -o &#x27;<span class="hljs-variable">$&#123;file_path&#125;</span>/<span class="hljs-variable">$&#123;file_base_name&#125;</span>&#x27; &amp;&amp; <span class="hljs-variable">$&#123;file_path&#125;</span>/<span class="hljs-variable">$&#123;file_base_name&#125;</span>&quot;</span><br>            ]<br>        &#125;,<br>        &#123;<br>            <span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;Create Input File&quot;</span>,<br>            <span class="hljs-string">&quot;cmd&quot;</span>: [<br>                <span class="hljs-string">&quot;bash&quot;</span>,<br>                <span class="hljs-string">&quot;-c&quot;</span>,<br>                <span class="hljs-string">&quot;touch <span class="hljs-variable">$&#123;file_path&#125;</span>/<span class="hljs-variable">$&#123;file_base_name&#125;</span>.in &amp;&amp; open -a Sublime\\ Text <span class="hljs-variable">$&#123;file_path&#125;</span>/<span class="hljs-variable">$&#123;file_base_name&#125;</span>.in&quot;</span><br>            ]<br>        &#125;,<br>    ]<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>保存为 <code>C++-mine.sublime-build</code>；</p></li><li><p><code>cmd+shift+b</code> 选择编译系统时，选择 <code>C++-mine Run In Terminus</code>，即可编译并打开 <code>Terminus</code> 交互；选择 <code>C++-mine Create Input File</code>，即可在代码文件同级目录处创建并打开一个名字与代码文件相同、后缀为 <code>.in</code> 的文件，供写入输入数据。</p></li></ol><h2 id="参考资料">参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>[ST3/4] Package Control not working on macOS #1612. https://github.com/wbond/package_control/issues/1612 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>【全网最新、最详细】如何使用 Sublime Text 4 优雅地写C++？ - 南瓜瓜的文章 - 知乎 https://zhuanlan.zhihu.com/p/586687010 <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>技术栈</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>扩散模型中的信噪比</title>
    <link href="/blog-main/2023/08/08/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E4%BF%A1%E5%99%AA%E6%AF%94/"/>
    <url>/blog-main/2023/08/08/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E4%BF%A1%E5%99%AA%E6%AF%94/</url>
    
    <content type="html"><![CDATA[<p>由于扩散模型存在多种解释角度，并且有很多人在研究它，因此大家用的推导体系和书写符号或多或少有一些差异。在 Google 的这两篇论文中——Variational Diffusion Models<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Kingma, Diederik, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. *Advances in neural information processing systems* 34 (2021): 21696-21707.">[1]</span></a></sup>、Progressive Distillation for Fast Sampling of Diffusion Models<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Salimans, Tim, and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. *arXiv preprint arXiv:2202.00512* (2022).">[2]</span></a></sup>，作者将<strong>信噪比</strong>显式地写入了扩散模型的公式之中，并由此引出了对<strong>可学习噪声序列</strong>、<strong>模型预测目标</strong>和<strong>损失函数权重</strong>的讨论。</p><h2 id="引入信噪比">引入信噪比</h2><p>本节探讨扩散模型的形式化，主要内容其实与 DDPM<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ho, Jonathan, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. *Advances in neural information processing systems* 33 (2020): 6840-6851.">[3]</span></a></sup> 基本一样，不同之处在于：</p><ol type="1"><li>采用<strong>连续时间</strong>步 <span class="math inline">\(t\in[0,1]\)</span> 而非离散时间步 <span class="math inline">\(t\in\{1,\ldots,1000\}\)</span>，这有助于我们在具体实现时进行任意形式的离散化；</li><li>引入了<strong>信噪比</strong>的概念，方便后续对噪声序列、损失函数权重等方面进行讨论，有助于我们更好地理解扩散模型的加噪和去噪过程。</li></ol><p>记 <span class="math inline">\(\mathbf x\sim p(\mathbf x)\)</span> 为真实数据，<span class="math inline">\(\mathbf z_t\)</span> 为 <span class="math inline">\(t\)</span> 时刻的隐变量，<span class="math inline">\(t\in[0,1]\)</span>. 前向加噪过程定义为： <span class="math display">\[q(\mathbf z_t\vert\mathbf x)=\mathcal N\left(\mathbf z_t;\alpha_t\mathbf x,\sigma_t^2\mathbf I\right)\label{1}\tag{1}\]</span> 其中 <span class="math inline">\(\alpha_t,\sigma_t^2\)</span> 是关于 <span class="math inline">\(t\)</span> 的光滑可导函数。事实上，DDPM 对应 <span class="math inline">\(\alpha_t=\sqrt{1-\sigma_t^2}\)</span> 的情形，而 SMLD<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, Yang, and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. *Advances in neural information processing systems* 32 (2019).">[4]</span></a></sup> 对应 <span class="math inline">\(\alpha_t=1\)</span> 的情形。</p><p>在 <span class="math inline">\(\eqref{1}\)</span> 式的定义下，我们定义<strong>信噪比</strong>为： <span class="math display">\[\text{SNR}(t)=\alpha_t^2/\sigma_t^2\label{2}\tag{2}\]</span> 并假设信噪比随时间严格单调递减，即随着 <span class="math inline">\(t\)</span> 增大，噪声占比越多。记 <span class="math inline">\(\lambda_t\)</span> 表示对数信噪比，即 <span class="math inline">\(\lambda_t=\log\text{SNR}(t)=\log(\alpha_t^2/\sigma_t^2)\)</span>.</p><p>根据 <span class="math inline">\(\eqref{1}\)</span> 式，我们可以仿照 DDPM 的方法推导出 <span class="math inline">\(q(\mathbf z_t\vert\mathbf z_s)\)</span>，其中 <span class="math inline">\(0\leq s&lt;t\leq 1\)</span>： <span class="math display">\[\begin{align}&amp;q(\mathbf z_t\vert\mathbf z_s)=\mathcal N\left(\mathbf z_t;\alpha_{t|s} \mathbf z_s,\sigma_{t|s}^2\mathbf I\right)\\\text{where}\quad&amp;\alpha_{t|s}=\alpha_t/\alpha_s\\&amp;\sigma_{t|s}^2=\sigma_t^2-\alpha_{t|s}^2\sigma_s^2=\left(1-e^{\lambda_t-\lambda_s}\right)\sigma_t^2\end{align}\tag{3}\label{3}\]</span> 同样与 DDPM 类似，利用贝叶斯定理可以推导出 <span class="math inline">\(q(\mathbf z_s\vert\mathbf z_t,\mathbf x)\)</span>： <span class="math display">\[\begin{align}&amp;q(\mathbf z_s\vert\mathbf z_t,\mathbf x)=\mathcal N\left(\mathbf z_s;\tilde{\boldsymbol\mu}_{s|t}(\mathbf z_t,\mathbf x),\tilde\sigma^2_{s|t}\mathbf I\right)\\\text{where}\quad&amp;\tilde{\boldsymbol\mu}_{s|t}(\mathbf z_t,\mathbf x)=\frac{\alpha_{t|s}\sigma_s^2}{\sigma_t^2}\mathbf z_s+\frac{\alpha_s\sigma_{t|s}^2}{\sigma_t^2}\mathbf x=e^{\lambda_t-\lambda_s}(\alpha_s/\alpha_t)\mathbf z_s+(1-e^{\lambda_t-\lambda_s})\alpha_s\mathbf x\\&amp;\tilde\sigma^2_{s|t}=\frac{\sigma_{t|s}^2\sigma_s^2}{\sigma_t^2}=(1-e^{\lambda_t-\lambda_s})\sigma_s^2\end{align}\tag{4}\label{4}\]</span> <span class="math inline">\(\eqref{3},\eqref{4}\)</span> 式的具体推导过程见本文最后的附录。</p><p><br/></p><p>设生成模型为 <span class="math inline">\(p_\theta(\mathbf z_s\vert\mathbf z_t)\)</span>，为方便计算，取： <span class="math display">\[p_\theta(\mathbf z_s\vert\mathbf z_t)=q(\mathbf z_s\vert\mathbf z_t,\mathbf x=\hat{\mathbf x}_\theta(\mathbf z_t))\]</span> 即将 <span class="math inline">\(q(\mathbf z_s\vert\mathbf z_t,\mathbf x)\)</span> 中的真实数据 <span class="math inline">\(\mathbf x\)</span> 换成模型 <span class="math inline">\(\hat{\mathbf x}_\theta(\mathbf z_t)\)</span>，换句话说，<span class="math inline">\(\hat{\mathbf x}_\theta(\mathbf z_t)\)</span> 的作用就是去预测真实无噪图像 <span class="math inline">\(\mathbf x\)</span>. 注意这里为了书写简便省略了与时间步 <span class="math inline">\(t\)</span> 有关的输入。与 DDPM 一样，我们也可以将模型 <span class="math inline">\(\hat{\mathbf x}_\theta(\mathbf z_t)\)</span> 重参数化为噪声预测模型 <span class="math inline">\(\hat{\boldsymbol\epsilon}_\theta(\mathbf z_t)\)</span>： <span class="math display">\[\hat{\mathbf x}_\theta(\mathbf z_t)=\frac{1}{\alpha_t}\left(\mathbf z_t-\sigma_t\hat{\boldsymbol\epsilon}_\theta(\mathbf z_t)\right)\tag{5}\label{5}\]</span> <strong>尽管重参数化在数学上是等价的，但是对网络训练的影响是不同的</strong>。最直观的现象就是——当模型预测噪声时，由于 <span class="math inline">\(t\)</span> 越大，<span class="math inline">\(\mathbf z_t\)</span> 中原图分量的系数越接近 0，噪声分量的系数越接近 1，所以当 <span class="math inline">\(t\to1\)</span> 时（信噪比很小时）模型只需要把输入复制到输出就能达到很低的 loss；换句话说，<span class="math inline">\(t\)</span> 越大模型的学习越容易。相反，当模型预测原图时，<span class="math inline">\(t\)</span> 越小其学习越容易。<strong>这也是为什么损失函数中对不同 <span class="math inline">\(t\)</span> 的加权系数很重要</strong>。我们将在后文继续讨论这个问题。</p><p><br/></p><p>扩散模型的训练是通过最大化 ELBO 完成的。本文只考虑离散时间情形——事实上，论文<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Kingma, Diederik, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. *Advances in neural information processing systems* 34 (2021): 21696-21707.">[1]</span></a></sup>还讨论了连续时间情形，这里暂且不谈。设一共有 <span class="math inline">\(T\)</span> 步，为了与前文 <span class="math inline">\(t\in[0,1]\)</span> 的设定相符，可以将 <span class="math inline">\([0,1]\)</span> 分成 <span class="math inline">\(T\)</span> 份，并记 <span class="math inline">\(t(i)=i/T,s(i)=(i-1)/T\)</span>. 为了书写方便，后文直接用 <span class="math inline">\(t,s\)</span> 表示 <span class="math inline">\(t(i),s(i)\)</span>. ELBO 的推导与 DDPM 完全相同，这里就直接放结果： <span class="math display">\[\begin{align}\text{ELBO}=\underbrace{\mathbb E_{\mathbf z_0\sim q(\mathbf z_0\vert\mathbf x)}[\log p_\theta(\mathbf x\vert\mathbf z_0)]}_{\text{reconstruction}}-\underbrace{\text{KL}(q(\mathbf z_1\vert \mathbf x)\|p(\mathbf z_1))}_{\text{regularization}}-\sum_{i=1}^T\underbrace{\mathbb E_{\mathbf z_{t}\sim q(\mathbf z_{t}\vert\mathbf x)}\left[\text{KL}(q(\mathbf z_{s}\vert\mathbf z_{t},\mathbf x)\|p_\theta(\mathbf z_{s}\vert\mathbf z_{t}))\right]}_{\text{matching}}\end{align}\]</span> 在 DDPM 中，ELBO 最后简化为了一个均方差损失，且把系数给直接丢掉了。但是正如前文所说，加权系数是很重要的，在本文的语境下，<strong>这个系数其实能用信噪比表示出来</strong>，具体而言： <span class="math display">\[\begin{align}\text{KL}\left(q(\mathbf z_{s}\vert\mathbf z_{t},\mathbf x)\|p_\theta(\mathbf z_{s}\vert\mathbf z_{t})\right)&amp;=\frac{1}{2\tilde\sigma^2_{s|t}}\Big\Vert\tilde{\boldsymbol\mu}_{s|t}(\mathbf z_{t},\mathbf x)-\hat{\boldsymbol\mu}_\theta(\mathbf z_{t})\Big\Vert^2\\&amp;=\frac{\sigma_{t}^2}{2\sigma_{s}^2\sigma_{t|s}^2}\cdot\left(\frac{\alpha_{s}\sigma_{t|s}^2}{\sigma_{t}^2}\right)^2\cdot\Big\Vert\mathbf x-\hat{\mathbf x}_\theta(\mathbf z_{t})\Big\Vert^2\\&amp;=\frac{\alpha_{s}^2\sigma_{t|s}^2}{2\sigma_{s}^2\sigma_{t}^2}\Big\Vert\mathbf x-\hat{\mathbf x}_\theta(\mathbf z_{t})\Big\Vert^2\\&amp;=\frac{\alpha_{s}^2\left(\sigma_{t}^2-\alpha_{t|s}^2\sigma_{s}^2\right)}{2\sigma_{s}^2\sigma_{t}^2}\Big\Vert\mathbf x-\hat{\mathbf x}_\theta(\mathbf z_{t})\Big\Vert^2\\&amp;=\left(\frac{\alpha_{s}^2}{2\sigma_{s}^2}-\frac{\alpha_{t}^2}{2\sigma_{t}^2}\right)\Big\Vert\mathbf x-\hat{\mathbf x}_\theta(\mathbf z_{t})\Big\Vert^2\\&amp;=\frac{1}{2}\Big(\text{SNR}(s)-\text{SNR}(t)\Big)\Big\Vert\mathbf x-\hat{\mathbf x}_\theta(\mathbf z_{t})\Big\Vert^2\tag{6}\label{6}\end{align}\]</span> 如果模型预测噪声，那么根据 <span class="math inline">\(\eqref{5}\)</span> 式的重参数化，有： <span class="math display">\[\Big\Vert\mathbf x-\hat{\mathbf x}_\theta(\mathbf z_{t})\Big\Vert^2=\frac{\sigma_{t}^2}{\alpha_{t}^2}\Big\Vert\boldsymbol\epsilon-\hat{\boldsymbol\epsilon}_\theta(\mathbf z_{t})\Big\Vert^2=\frac{1}{\text{SNR}(t)}\Big\Vert\boldsymbol\epsilon-\hat{\boldsymbol\epsilon}_\theta(\mathbf z_{t})\Big\Vert^2\tag{7}\label{7}\]</span> 将 <span class="math inline">\(\eqref{7}\)</span> 式代入 <span class="math inline">\(\eqref{6}\)</span> 式得： <span class="math display">\[\text{KL}\left(q(\mathbf z_{s}\vert\mathbf z_{t},\mathbf x)\|p_\theta(\mathbf z_{s}\vert\mathbf z_{t})\right)=\frac{1}{2}\left(\frac{\text{SNR}(s)}{\text{SNR}(t)}-1\right)\Big\Vert\boldsymbol\epsilon-\hat{\boldsymbol\epsilon}_\theta(\mathbf z_{t})\Big\Vert^2\tag{8}\label{8}\]</span> 可见 DDPM 丢掉的系数就是前面那一坨。在 DDPM 的噪声序列设置下，这个系数与时间步的关系长这样：</p><p><img src="coef.png" width=40% /></p><p>因此，丢掉系数的作用就是减小了 <span class="math inline">\(t\)</span> 较小时的权重，让模型更加注重学习 <span class="math inline">\(t\)</span> 较大的情形，这一点也在 DDPM 原论文中提到了。</p><h2 id="可学习噪声序列">可学习噪声序列</h2><p>Variational Diffusion Models 一文将可学习参数引入了加噪过程，使得噪声序列是学习出来的而非人为设置的。具体而言，作者将噪声序列参数化为： <span class="math display">\[\sigma_t^2=\text{sigmoid}(\gamma_{\boldsymbol\eta}(t))\]</span> 如果遵循 DDPM 的设定 <span class="math inline">\(\alpha_t=\sqrt{1-\sigma_t^2}\)</span>，那么容易得到： <span class="math display">\[\begin{align}&amp;\alpha_t^2=\text{sigmoid}(-\gamma_{\boldsymbol\eta}(t))\\&amp;\text{SNR}(t)=\exp(-\gamma_{\boldsymbol\eta}(t))\end{align}\]</span> 于是 <span class="math inline">\(\eqref{8}\)</span> 式写作： <span class="math display">\[\text{KL}\left(q(\mathbf z_{s}\vert\mathbf z_{t},\mathbf x)\|p_\theta(\mathbf z_{s}\vert\mathbf z_{t})\right)=\frac{1}{2}\left(\exp(\gamma_{\boldsymbol\eta}(t)-\gamma_{\boldsymbol\eta}(s))-1\right)\Big\Vert\boldsymbol\epsilon-\hat{\boldsymbol\epsilon}_\theta(\mathbf z_{t})\Big\Vert^2\]</span> 就可以依此训练噪声序列了。值得注意的是，<span class="math inline">\(\exp(\bullet)-1\)</span> 这个操作应使用各大深度学习库的 <code>expm1()</code> 函数以避免数值计算问题。</p><p>值得注意的是，由于 <span class="math inline">\(\text{SNR}(t)\)</span> 应该是单调递减的，这要求 <span class="math inline">\(\gamma_{\boldsymbol\eta}(t)\)</span> 应该是单调递增的，所以作者将其建模为一个单调神经网络。具体而言，这个神经网络有三层 linear，权重均限制为正数，中间有一个 sigmoid 激活，具体写作： <span class="math display">\[\gamma_{\boldsymbol\eta}(t)=l_1(t)+l_2(\phi(l_2(l_1(t))))\]</span> 其中 <span class="math inline">\(l_2\)</span> 有 1024 个输出，其余仅有 1 个。</p><p>作者在论文中展示了学习出来的噪声与其他人为设置的噪声的对比图，如下所示：</p><p><img src="learned.png" width=40% /></p><p>可见，学习出来的噪声信噪比在初期下降远远慢于人为设置的噪声。</p><h2 id="模型预测目标">模型预测目标</h2><p>众所周知，原始 DDPM 预测的是噪声 <span class="math inline">\(\boldsymbol\epsilon\)</span> 或者原数据 <span class="math inline">\(\mathbf x\)</span>，那么有没有其他的预测目标呢？Progressive Distillation<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Salimans, Tim, and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. *arXiv preprint arXiv:2202.00512* (2022).">[2]</span></a></sup> 这篇论文对此做了讨论。特别说明，本节只考虑 DDPM 情形，即满足 <span class="math inline">\(\sigma_t^2+\alpha_t^2=1\)</span>.</p><p>首先，为什么要寻求新的预测目标？前文提到，<span class="math inline">\(t\)</span> 越大，预测噪声越简单，而预测原数据越困难；反过来，<span class="math inline">\(t\)</span> 越小则前者越困难、后者越简单。所谓简单，就是网络只需要 copy 输入即可，并不需要真的去学习数据分布，这无疑是不好的。因此，一个很直接的想法是让模型同时预测 <span class="math inline">\(\boldsymbol\epsilon\)</span> 和 <span class="math inline">\(\mathbf x\)</span>，推断时可以按照如下方式合并： <span class="math display">\[\hat{\mathbf x}=\sigma_t^2\hat{\mathbf x}_\theta(\mathbf z_t)+\alpha_t(\mathbf z_t-\sigma_t\hat{\boldsymbol\epsilon}_\theta(\mathbf z_t))\]</span> 当 <span class="math inline">\(t\to1\)</span> 时，<span class="math inline">\(\sigma_t^2\to1,\,\alpha_t\to0\)</span>，所以 <span class="math inline">\(\hat{\mathbf x}\approx\hat{\mathbf x}_\theta(\mathbf z_t)\)</span>；反过来，当 <span class="math inline">\(t\to0\)</span> 时，<span class="math inline">\(\hat{\mathbf x}\approx\mathbf z_t-\sigma_t\hat{\boldsymbol\epsilon}_\theta(\mathbf z_t)\)</span>. 因此，上述方法能够自动地根据 <span class="math inline">\(t\)</span> 的大小在两种方案之间做插值。</p><p>不过，这样做看起来还是不够精巧，有没有更好的方法呢？论文提出了一个新的预测目标，我们称之为 v-prediction： <span class="math display">\[\mathbf v_t\equiv\alpha_t\boldsymbol\epsilon-\sigma_t\mathbf x\tag{9}\label{9}\]</span> 在上述定义下，训练模型 <span class="math inline">\(\hat{\mathbf v}_\theta(\mathbf z_t)\)</span> 来近似 <span class="math inline">\(\mathbf v_t\)</span>，可以推出： <span class="math display">\[\hat{\mathbf x}=\alpha_t\mathbf z_t-\sigma_t\hat{\mathbf v}_\theta(\mathbf z_t)\]</span> 可以看见，随着 <span class="math inline">\(t\)</span> 从 0 变到 1，<span class="math inline">\(\mathbf v_t\)</span> 会从 <span class="math inline">\(\boldsymbol\epsilon\)</span> 逐渐变到 <span class="math inline">\(-\mathbf x\)</span>，也就是说这个目标本身就是在做插值，能够调和预测 <span class="math inline">\(\boldsymbol\epsilon\)</span> 和预测 <span class="math inline">\(\mathbf x\)</span> 总在某一头变得太过简单的问题，始终保持模型在努力地学习。因此，v-prediction 在后续的许多工作中都受到了人们的青睐。</p><h2 id="损失函数权重">损失函数权重</h2><p>目前为止，我们已经看到了好几种参数化方式，即不同的预测目标。它们之间相互关联，<strong>如果全部重参数化到 <span class="math inline">\(\mathbf x\)</span> 空间</strong>，就体现为不同的权重系数：</p><ul><li><p><strong>SNR weighing</strong>：预测 <span class="math inline">\(\boldsymbol\epsilon\)</span> 等价于以 <span class="math inline">\(\text{SNR}(t)\)</span> 加权： <span class="math display">\[\Big\Vert\boldsymbol\epsilon-\hat{\boldsymbol\epsilon}_\theta(\mathbf z_t)\Big\Vert^2=\text{SNR}(t)\Big\Vert\mathbf x-\hat{\mathbf x}_\theta(\mathbf z_{t})\Big\Vert^2\]</span></p></li><li><p><strong>SNR+1 weighting</strong>：预测 <span class="math inline">\(\mathbf v_t\)</span> 等价于以 <span class="math inline">\(\text{SNR}(t)+1\)</span> 加权： <span class="math display">\[\Big\Vert\mathbf v_t-\hat{\mathbf v}_\theta(\mathbf z_t)\Big\Vert^2=(\text{SNR}(t)+1)\Big\Vert\mathbf x-\hat{\mathbf x}_\theta(\mathbf z_{t})\Big\Vert^2\]</span></p></li><li><p><strong>Truncated SNR weighting</strong>：预测 <span class="math inline">\(\mathbf x\)</span> 与预测 <span class="math inline">\(\boldsymbol\epsilon\)</span> 取 <span class="math inline">\(\max\)</span>： <span class="math display">\[\max\left(\Vert\mathbf x-\hat{\mathbf x}_\theta(\mathbf z_{t})\Vert^2,\Vert\boldsymbol\epsilon-\hat{\boldsymbol\epsilon}_\theta(\mathbf z_t)\Vert^2\right)=\max\big(\text{SNR}(t),1\big)\Big\Vert\mathbf x-\hat{\mathbf x}_\theta(\mathbf z_{t})\Big\Vert^2\]</span></p></li></ul><p>当然，直接预测 <span class="math inline">\(\mathbf x\)</span> 本身就是不加权了。</p><p><img src="weighting.png" width=80% /></p><p>事实上，我们可以将预测目标与权重设置解耦开——虽然预测 <span class="math inline">\(\boldsymbol\epsilon\)</span> 等价于以 <span class="math inline">\(\text{SNR}(t)\)</span> 加权预测 <span class="math inline">\(\mathbf x\)</span>，但是如果我们想用 SNR+1 weighting，只需要乘一个系数 <span class="math inline">\((\text{SNR}(t)+1)/\text{SNR}(t)\)</span> 即可。因此，三种加权方式和四种预测目标（<span class="math inline">\(\boldsymbol\epsilon\)</span>、<span class="math inline">\(\mathbf x\)</span>、<span class="math inline">\(\mathbf v\)</span> 以及同时预测 <span class="math inline">\(\boldsymbol\epsilon,\mathbf x\)</span>）一共形成 12 种组合，作者分别进行了测试：</p><p><img src="compare.png" width=80% /></p><p>由表可见，除了预测 <span class="math inline">\(\boldsymbol\epsilon\)</span> + Truncated SNR weighting 的组合无法收敛以外，其他组合表现都差不多。再次强调，这里的加权都是指重参数化到 <span class="math inline">\(\mathbf x\)</span> 空间后的权重。</p><h2 id="附录">附录</h2><h3 id="qmathbf-z_tvertmathbf-z_s-的推导"><span class="math inline">\(q(\mathbf z_t\vert\mathbf z_s)\)</span> 的推导</h3><p>设 <span class="math inline">\(q(\mathbf z_t\vert\mathbf z_s)=\mathcal N\left(\mathbf z_t;\alpha_{t|s} \mathbf z_s,\sigma_{t|s}^2\mathbf I\right)\)</span>，那么由 <span class="math inline">\(\mathbf x\)</span> 到 <span class="math inline">\(\mathbf z_s\)</span> 的一次采样可以写作： <span class="math display">\[\mathbf z_s=\alpha_s\mathbf x+\sigma_s\boldsymbol\epsilon_1,\quad\boldsymbol\epsilon_1\sim\mathcal N(\mathbf 0,\mathbf I)\]</span> 由 <span class="math inline">\(\mathbf z_s\)</span> 到 <span class="math inline">\(\mathbf z_t\)</span> 的一次采样可以写作： <span class="math display">\[\mathbf z_t=\alpha_{t|s}\mathbf z_s+\sigma_{t|s}\boldsymbol\epsilon_2,\quad\boldsymbol\epsilon_2\sim\mathcal N(\mathbf 0,\mathbf I)\]</span> 联立二式得： <span class="math display">\[\mathbf z_t=\alpha_{t|s}\alpha_s\mathbf x+\alpha_{t|s}\sigma_s\boldsymbol\epsilon_1+\sigma_{t|s}\boldsymbol\epsilon_2\]</span> 等价于进行如下一次采样： <span class="math display">\[\mathbf z_t=\underbrace{\alpha_{t|s}\alpha_s}_{\alpha_t}\mathbf x+\underbrace{\sqrt{\alpha_{t|s}^2\sigma_s^2+\sigma_{t|s}^2}}_{\sigma_t}\boldsymbol\epsilon,\quad\boldsymbol\epsilon\sim\mathcal N(\mathbf0,\mathbf I)\]</span> 因此： <span class="math display">\[\begin{align}&amp;\alpha_{t|s}=\alpha_t/\alpha_s\\&amp;\sigma_{t|s}^2=\sigma_t^2-\alpha_{t|s}^2\sigma_s^2\end{align}\]</span></p><h3 id="qmathbf-z_svertmathbf-z_tmathbf-x-的推导"><span class="math inline">\(q(\mathbf z_s\vert\mathbf z_t,\mathbf x)\)</span> 的推导</h3><p>求解 <span class="math inline">\(q(\mathbf z_s\vert\mathbf z_t,\mathbf x)\)</span> 可以使用贝叶斯定理： <span class="math display">\[\begin{align}q(\mathbf z_s\vert\mathbf z_t,\mathbf x)&amp;=\frac{q(\mathbf z_t\vert\mathbf z_s,\mathbf x)q(\mathbf z_s\vert\mathbf x)}{q(\mathbf z_t\vert\mathbf x)}=\frac{q(\mathbf z_t\vert\mathbf z_s)q(\mathbf z_s\vert\mathbf x)}{q(\mathbf z_t\vert\mathbf x)}\\&amp;\propto\exp\left(-\frac{1}{2}\left(\frac{\Vert\mathbf z_t-\alpha_{t|s}\mathbf z_s\Vert^2}{\sigma_{t|s}^2}+\frac{\Vert\mathbf z_s-\alpha_s\mathbf x\Vert^2}{\sigma_s^2}-\frac{\Vert\mathbf z_t-\alpha_t\mathbf x\Vert^2}{\sigma_t^2}\right)\right)\\&amp;=\exp\left(-\frac{1}{2}\left(\underbrace{\left(\frac{\alpha_{t|s}^2}{\sigma_{t|s}^2}+\frac{1}{\sigma_s^2}\right)}_{A}\Vert\mathbf z_s\Vert^2+\underbrace{\left(\frac{-2\alpha_{t|s}\mathbf z_t}{\sigma_{t|s}^2}+\frac{-2\alpha_s\mathbf x}{\sigma_s^2}\right)}_{B}\cdot\mathbf z_s+C(\mathbf z_t,\mathbf x)\right)\right)\end{align}\]</span> 因此 <span class="math inline">\(q(\mathbf z_s\vert\mathbf z_t,\mathbf x)\)</span> 服从正态分布 <span class="math inline">\(\mathcal N\left(\tilde{\boldsymbol\mu}_{s|t}(\mathbf z_t,\mathbf x),\tilde\sigma^2_{s|t}\mathbf I\right)\)</span>，且均值和方差分别为： <span class="math display">\[\begin{align}&amp;\tilde{\boldsymbol\mu}_{s|t}(\mathbf z_t,\mathbf x)=\frac{-B}{2A}=\frac{\alpha_{t|s}\sigma_s^2}{\sigma_t^2}\mathbf z_s+\frac{\alpha_s\sigma_{t|s}^2}{\sigma_t^2}\mathbf x\\&amp;\tilde\sigma^2_{s|t}=\frac{1}{A}=\frac{\sigma_{t|s}^2\sigma_s^2}{\sigma_t^2}\end{align}\]</span></p><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Kingma, Diederik, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. <em>Advances in neural information processing systems</em> 34 (2021): 21696-21707. <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Salimans, Tim, and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. <em>arXiv preprint arXiv:2202.00512</em> (2022). <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Ho, Jonathan, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. <em>Advances in neural information processing systems</em> 33 (2020): 6840-6851. <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Song, Yang, and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. <em>Advances in neural information processing systems</em> 32 (2019). <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>Diffusion Models</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Diffusion+VAE</title>
    <link href="/blog-main/2023/07/29/Diffusion+VAE/"/>
    <url>/blog-main/2023/07/29/Diffusion+VAE/</url>
    
    <content type="html"><![CDATA[<p>近期有几篇工作不约而同地都尝试了结合 Diffusion Models 与 VAE，尽管它们的动机并不相同。本文首先以一个结合 Diffusion 与 AE 的工作为引入，然后推导 Diffusion + VAE 的基本框架，再在这个基本框架下分别介绍相关的工作。</p><h2 id="diffusion-ae">Diffusion + AE</h2><p>在与 VAE 结合之前，论文<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Preechakul, Konpat, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward a meaningful and decodable representation. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 10619-10629. 2022.">[1]</span></a></sup>提出了 Diffusion Autoencoders，结合了 Diffusion 与 AE. 作者的动机是<strong>为扩散模型寻找一个有语义的隐空间</strong>，因为即便是像 DDIM 那样的确定性采样过程，其“隐空间”，即 <span class="math inline">\(\mathbf x_T\)</span> 所在空间也并不理想，典型表现就是 DDIM 的插值结果并不是平滑变化的。Diffusion Autoencoders 希望构造一个像 GANs 和 VAEs 一样方便操纵的隐空间，能够平滑地插值、控制语义和编辑图像属性。为此，作者设计了如下架构：</p><p><img src="diffusion-autoencoders.png" width=60% /></p><p>其中，Semantic encoder 是一个卷积编码器，目的是提取输入图像的语义特征 <span class="math inline">\(z_\text{sem}\)</span>；Conditional DDIM 是以 <span class="math inline">\(z_\text{sem}\)</span> 为条件（通过 AdaGN 融入）的扩散模型，可以把 <span class="math inline">\(\mathbf x_0\to\mathbf x_T\)</span> 过程看作“编码器”，<span class="math inline">\(\mathbf x_T\to\mathbf x_0\)</span> 过程看作“解码器”；Latent DDIM 稍后再做解释。在这个架构下，<strong><span class="math inline">\(z_\text{sem}\)</span> 和 <span class="math inline">\(\mathbf x_T\)</span> 共同形成了输入图像的隐空间</strong>。前者编码语义信息，让我们能够操纵生成图像的各种属性（如人的性别、年龄、微笑）；后者编码了 <span class="math inline">\(z_\text{sem}\)</span> 遗留的其他信息，往往是一些随机细节。二者共同作用，既有扩散模型能够几乎完美地重建输入图像的优势，又得到了 high-level 的语义表征 <span class="math inline">\(z_\text{sem}\)</span> 供下游任务的使用。当然，我们并没有显式地要求模型把语义编码在 <span class="math inline">\(z_\text{sem}\)</span> 而不是 <span class="math inline">\(\mathbf x_T\)</span> 之中，但实验发现——如果固定 <span class="math inline">\(z_\text{sem}\)</span> 不变，随机采样 <span class="math inline">\(\mathbf x_T\)</span>，那么生成的结果也大体不变，只有细节改变——这证明了确实是 <span class="math inline">\(z_\text{sem}\)</span> 编码图像语义、<span class="math inline">\(\mathbf x_T\)</span> 编码随机细节，符合我们的预期。</p><p>为了支持无条件生成，我们需要对 <span class="math inline">\(z_\text{sem}\)</span> 的分布进行建模，这就是 Latent DDIM 的用途。当然，任何生成模型都可以用来建模 <span class="math inline">\(z_\text{sem}\)</span>，只是作者觉得扩散模型更好罢了。</p><h2 id="diffusion-vae">Diffusion + VAE</h2><h3 id="ddpm-回顾">DDPM 回顾</h3><p><img src="diffusion.png" width=100% /></p><p>如上图所示，DDPM 将前向过程和逆向过程都设计为了马尔可夫链的形式： <span class="math display">\[q(\mathbf x_{1:T}\vert\mathbf x_0)=\prod_{t=1}^{T}q(\mathbf x_t\vert\mathbf x_{t-1})\quad\quad p(\mathbf x_{0:T})=p(\mathbf x_T)\prod_{t=1}^{T}p(\mathbf x_{t-1}\vert\mathbf x_t)\]</span> 于是 ELBO 可按如下方式推导： <span class="math display">\[\begin{align}\text{ELBO}(\mathbf x_0)&amp;=\mathbb E_{q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{p(\mathbf x_{0:T})}{q(\mathbf x_{1:T}\vert\mathbf x_0)}\right]\\&amp;=\mathbb E_{q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{p(\mathbf x_T)\prod_{t=1}^{T}p(\mathbf x_{t-1}\vert\mathbf x_t)}{\prod_{t=1}^{T}q(\mathbf x_t\vert\mathbf x_{t-1})}\right]\\&amp;=\mathbb E_{q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{p(\mathbf x_T)\prod_{t=1}^{T}p(\mathbf x_{t-1}\vert\mathbf x_t)}{q(\mathbf x_1\vert\mathbf x_0)\prod_{t=2}^{T}q(\mathbf x_t\vert\mathbf x_{t-1},\mathbf x_0)}\right]\\&amp;=\mathbb E_{q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{p(\mathbf x_T)\prod_{t=1}^{T}p(\mathbf x_{t-1}\vert\mathbf x_t)}{q(\mathbf x_1\vert\mathbf x_0)\prod_{t=2}^{T}\frac{q(\mathbf x_t\vert\mathbf x_0)q(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf x_0)}{q(\mathbf x_{t-1}\vert\mathbf x_0)}}\right]\\&amp;=\mathbb E_{q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{p(\mathbf x_T)\prod_{t=1}^{T}p(\mathbf x_{t-1}\vert\mathbf x_t)}{q(\mathbf x_T\vert\mathbf x_0)\prod_{t=2}^{T}q(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf x_0)}\right]\\&amp;=\mathbb E_{q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log p(\mathbf x_0\vert\mathbf x_1)\right]+\mathbb E_{q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{p(\mathbf x_T)}{q(\mathbf x_T\vert\mathbf x_0)}\right]+\sum_{t=2}^T\mathbb E_{q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{p(\mathbf x_{t-1}\vert\mathbf x_t)}{q(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf x_0)}\right]\\&amp;=\mathbb E_{q(\mathbf x_{1}\vert\mathbf x_0)}\left[\log p(\mathbf x_0\vert\mathbf x_1)\right]+\mathbb E_{q(\mathbf x_{T}\vert\mathbf x_0)}\left[\log\frac{p(\mathbf x_T)}{q(\mathbf x_T\vert\mathbf x_0)}\right]+\sum_{t=2}^T\mathbb E_{q(\mathbf x_t\vert\mathbf x_0)}\mathbb E_{q(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf x_0)}\left[\log\frac{p(\mathbf x_{t-1}\vert\mathbf x_t)}{q(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf x_0)}\right]\\&amp;=\underbrace{\mathbb E_{q(\mathbf x_{1}\vert\mathbf x_0)}\left[\log p(\mathbf x_0\vert\mathbf x_1)\right]}_\text{reconstruction term}-\underbrace{\text{KL}(q(\mathbf x_T\vert\mathbf x_0)\Vert p(\mathbf x_T))}_\text{regularization term}-\sum_{t=2}^T\mathbb E_{q(\mathbf x_t\vert\mathbf x_0)}\underbrace{\left[\text{KL}(q(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf x_0)\Vert p(\mathbf x_{t-1}\vert\mathbf x_t))\right]}_\text{denoising matching terms}\\\end{align}\]</span> 可以看见，ELBO 由三项构成：重构项、正则项和去噪匹配项。DDPM 通过巧妙地设计 <span class="math inline">\(q\)</span>，使得 <span class="math inline">\(q(\mathbf x_t\vert\mathbf x_0)\)</span> 和 <span class="math inline">\(q(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf x_0)\)</span> 都可以解析地写作正态分布的形式，并取 <span class="math inline">\(p\)</span> 为类似形式，那么上述重构项和去噪匹配项都能写作 MSE Loss，而正则项在 <span class="math inline">\(T\)</span> 充分大时趋近于 <span class="math inline">\(0\)</span>. 在进一步简化和重参数化后，就可以得到最终的损失函数（具体细节此处略过）： <span class="math display">\[L_\text{simple}(\theta)=\mathbb E_{t,\mathbf x_0,\epsilon}\left[\left\Vert\epsilon-\epsilon_\theta\left(\sqrt{\bar\alpha_t}\mathbf x_0+\sqrt{1-\bar\alpha_t}\epsilon,t\right)\right\Vert^2\right]\]</span></p><h3 id="基本框架">基本框架</h3><p>受到 Diffusion AE 的启发，我们在 DDPM 的概率图中新增一个隐变量 <span class="math inline">\(\mathbf z\)</span>，并且让逆向过程的每一步都以 <span class="math inline">\(\mathbf z\)</span> 为条件输入，如下图所示：</p><p><img src="diffusion+vae.png" width=100% /> <span class="math display">\[\begin{align}&amp;q(\mathbf x_{1:T},\mathbf z\vert\mathbf x_0)=q(\mathbf z\vert\mathbf x_0)q(\mathbf x_{1:T}\vert\mathbf x_0)=q(\mathbf z\vert\mathbf x_0)\prod_{t=1}^{T}q(\mathbf x_t\vert\mathbf x_{t-1})\label{inference}\tag{1}\\&amp;p(\mathbf x_{0:T},\mathbf z)=p(\mathbf z)p(\mathbf x_{0:T}\vert\mathbf z)=p(\mathbf z)p(\mathbf x_T\vert\mathbf z)\prod_{t=1}^T p(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf z)\label{generative}\tag{2}\end{align}\]</span> 那么 ELBO 可以写作： <span class="math display">\[\begin{align}\text{ELBO}(\mathbf x_0)&amp;=\mathbb E_{q(\mathbf x_{1:T},\mathbf z\vert\mathbf x_0)}\left[\log\frac{p(\mathbf x_{0:T},\mathbf z)}{q(\mathbf x_{1:T},\mathbf z\vert\mathbf x_0)}\right]\\&amp;=\mathbb E_{q(\mathbf x_{1:T},\mathbf z\vert\mathbf x_0)}\left[\log\frac{p(\mathbf z)p(\mathbf x_{0:T}\vert\mathbf z)}{q(\mathbf z\vert\mathbf x_0)q(\mathbf x_{1:T}\vert\mathbf x_0)}\right]\\&amp;=\mathbb E_{q(\mathbf z\vert\mathbf x_0)}\left[\log\frac{p(\mathbf z)}{q(\mathbf z\vert\mathbf x_0)}\right]+\mathbb E_{q(\mathbf x_{1:T},\mathbf z\vert\mathbf x_0)}\left[\log\frac{p(\mathbf x_{0:T}\vert\mathbf z)}{q(\mathbf x_{1:T}\vert\mathbf x_0)}\right]\\&amp;=-\underbrace{\text{KL}(q(\mathbf z\vert\mathbf x_0)\Vert p(\mathbf z))}_\text{VAE regularization term}+\mathbb E_{q(\mathbf z\vert\mathbf x_0)}\underbrace{\left[\mathbb E_{q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{p(\mathbf x_{0:T}\vert\mathbf z)}{q(\mathbf x_{1:T}\vert\mathbf x_0)}\right]\right]}_\text{almost the same as DDPM}\end{align}\]</span> 其中第二项的推导过程与 DDPM 几乎没有区别，只需要把所有的 <span class="math inline">\(p\)</span> 都加上 <span class="math inline">\(\mathbf z\)</span> 作为条件即可，而多出来的第一项正好是 VAE 的正则项。因此总的来说，现在的优化目标由四项构成：</p><ul><li><p><strong>VAE 正则项</strong>： <span class="math display">\[-\text{KL}(q(\mathbf z\vert\mathbf x_0)\Vert p(\mathbf z)))\label{vae-reg}\tag{3}\]</span></p></li><li><p><strong>VAE 重构项 / Diffusion 正则项</strong>： <span class="math display">\[-\mathbb E_{q(\mathbf z\vert\mathbf x_0)}\left[\text{KL}(q(\mathbf x_T\vert\mathbf x_0)\Vert p(\mathbf x_T\vert\mathbf z))\right]\label{diff-reg}\tag{4}\]</span></p><blockquote><p>注：从 VAE 的角度看，<span class="math inline">\(q(\mathbf x_T\vert\mathbf x_0)\)</span> 是 <span class="math inline">\(\mathbf x_T\)</span> 的真实分布，<span class="math inline">\(p(\mathbf x_T\vert\mathbf z)\)</span> 是预测的分布，因此相当于重构项；而从 Diffusion 的角度看，<span class="math inline">\(p(\mathbf x_T\vert\mathbf z)\)</span> 是先验分布，<span class="math inline">\(q(\mathbf x_T\vert\mathbf x_0)\)</span> 是后验分布，因此是正则项。</p></blockquote></li><li><p><strong>Diffusion 重构项</strong>： <span class="math display">\[\mathbb E_{q(\mathbf z\vert\mathbf x_0)}\mathbb E_{q(\mathbf x_{1}\vert\mathbf x_0)}\left[\log p(\mathbf x_0\vert\mathbf x_1,\mathbf z)\right]\label{diff-rec}\tag{5}\]</span></p></li><li><p><strong>Diffusion 去噪匹配项</strong>： <span class="math display">\[-\sum_{t=2}^T\mathbb E_{q(\mathbf z\vert\mathbf x_0)}\mathbb E_{q(\mathbf x_t\vert\mathbf x_0)}\left[\text{KL}(q(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf x_0)\Vert p(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf z))\right]\label{diff-match}\tag{6}\]</span></p></li></ul><p>下面我们介绍几个相关的工作。由于它们有着不同的动机，因此对上述基本框架或多或少进行了一些修改，我们可以着重关注修改的地方。</p><h3 id="es-ddpm">ES-DDPM</h3><p>ES-DDPM<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Lyu, Zhaoyang, Xudong Xu, Ceyuan Yang, Dahua Lin, and Bo Dai. Accelerating diffusion models via early stop of the diffusion process. *arXiv preprint arXiv:2205.12524* (2022).">[2]</span></a></sup> 的研究动机是希望加速扩散模型的训练和采样。不同于 improved DDPM 和 DDIM 在原马尔可夫链上跳跃式前进的加速采样方法，ES-DDPM 直接对加噪过程做一个截断（Early-Stop），不把原图像加噪到标准正态，如此不仅能加速采样，也能加速训练。这样做带来的一个问题是如何对非正态的 <span class="math inline">\(\mathbf x_{T&#39;}\)</span> 的分布进行建模。一个自然的想法就是用另一个生成模型，比如 VAE，来建模 <span class="math inline">\(\mathbf x_{T&#39;}\)</span>. 因此，ES-DDPM 的整体框架如下图所示：</p><p><img src="es-ddpm.png" width=100% /></p><p>可以看见，这与我们画的基本框架非常相像。一个略有不同的点在于，作者为了能够分别独自训练 VAE 和 DDPM，规定 <span class="math inline">\(\mathbf z\)</span> 与 <span class="math inline">\(p(\mathbf x_{t-1}\vert\mathbf x_t)\)</span> 二者独立： <span class="math display">\[p(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf z)=p(\mathbf x_{t-1}\vert\mathbf x_t),\quad t=1,2,\ldots,T\]</span> 如此一来，<span class="math inline">\(\eqref{diff-rec}\)</span> 式和 <span class="math inline">\(\eqref{diff-match}\)</span> 式就与原本的 DDPM 彻底一样了，只是训练的时间步少一些。</p><p>对于 <span class="math inline">\(\eqref{diff-reg}\)</span> 式，注意现在 <span class="math inline">\(\mathbf x_{T&#39;}\)</span> 不是标准正态随机变量，所以并不为 <span class="math inline">\(0\)</span>，因此我们需要为 <span class="math inline">\(p(\mathbf x_T\vert\mathbf z)\)</span> 找一个便于计算的参数化形式。考虑到 DDPM 将 <span class="math inline">\(q(\mathbf x_{T&#39;}\vert\mathbf x_0)\)</span> 设计为了如下形式： <span class="math display">\[q(\mathbf x_{T&#39;}\vert\mathbf x_0)=\mathcal N\left(\mathbf x_{T&#39;};\sqrt{\bar\alpha_t}\mathbf x_0,(1-\bar\alpha_t)\mathbf I\right)\]</span> 自然想到将 <span class="math inline">\(p(\mathbf x_{T&#39;}\vert\mathbf z)\)</span> 设计为类似的形式： <span class="math display">\[p(\mathbf x_{T&#39;}\vert\mathbf z)=\mathcal N\left(\mathbf x_{T&#39;};\sqrt{\bar\alpha_t}f_\phi(\mathbf z),(1-\bar\alpha_t)\mathbf I\right)\]</span> 这个 <span class="math inline">\(f_\phi\)</span> 就相当于是 VAE 的解码器。那么很容易推出，<span class="math inline">\(\eqref{diff-reg}\)</span> 式的 KL 散度可以写作： <span class="math display">\[\text{KL}(q(\mathbf x_{T&#39;}\vert\mathbf x_0)\Vert p(\mathbf x_{T&#39;}\vert\mathbf z))=C_1\left\Vert\mathbf x_0-f_\phi(\mathbf z)\right\Vert^2+C_2\]</span> 直观来看就是用 <span class="math inline">\(\mathbf z\)</span> 去预测 <span class="math inline">\(\mathbf x_0\)</span>. 而 <span class="math inline">\(\eqref{vae-reg}\)</span> 式就是普通的 VAE 正则项，取先验 <span class="math inline">\(p(\mathbf z)\)</span> 为标准正态即可。那么现在 <span class="math inline">\(\eqref{vae-reg},\eqref{diff-reg},\eqref{diff-rec},\eqref{diff-match}\)</span> 式都被我们解析地写了出来，就可以开始<s>愉快地</s>训练了。</p><p>训练结束后，我们依照 <span class="math inline">\(\eqref{generative}\)</span> 式采样即可：首先从标准正态中采样 <span class="math inline">\(\mathbf z\)</span>，然后计算 <span class="math inline">\(f_\phi(\mathbf z)\)</span>，再从 <span class="math inline">\(p(\mathbf x_{T&#39;}\vert\mathbf z)\)</span> 中采样 <span class="math inline">\(\mathbf x_{T&#39;}\)</span>，最后根据 <span class="math inline">\(p(\mathbf x_{t-1}\vert\mathbf x_t)\)</span> 去噪。注意到采样过程并不需要 VAE 的编码器 <span class="math inline">\(q(\mathbf z\vert\mathbf x_0)\)</span> 的参与，只需要解码器 <span class="math inline">\(f_\phi(\mathbf z)\)</span>，因此作者还提出，可以把 VAE 换做 GAN，并且实验发现性能更好。事实上，无论是 VAE 还是 GAN，ES-DDPM 干的事情可以简单地总结为：用一个生成模型（VAE / GAN）生成一张图像，然后进行一定程度地加噪，再用扩散模型去噪——本质上就是用 SDEdit 的方式来改良 VAE/GAN 生成的图像。</p><h3 id="lrdm">LRDM</h3><p>LRDM<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Traub, Jeremias. Representation Learning with Diffusion Models. *arXiv preprint arXiv:2210.11058* (2022).">[3]</span></a></sup> 其实是海德堡大学的一篇硕士论文，其动机与本文开头的 Diffusion Autoencoders 一致——为扩散模型寻找一个有语义的隐空间。考虑到 VAE 拥有这样的隐空间，因此作者考虑将 VAE 与 Diffusion 相结合，整体框架如下左图所示：</p><p><img src="lrdm.png" width=100% /></p><p>可以看见，除了变量名与本文略有出入以外，这与我们的基本框架几乎完全一致。唯一的一个小区别在于没有从 <span class="math inline">\(\mathbf z\)</span> 到 <span class="math inline">\(\mathbf x_T\)</span> 的线，但这一点无关紧要，因为在 LRDM 中 <span class="math inline">\(T\)</span> 是充分大的，<span class="math inline">\(q(\mathbf x_T\vert\mathbf x_0)\)</span> 趋近于标准正态，所以可以直接取 <span class="math inline">\(p(\mathbf x_T\vert\mathbf z)\)</span> 为标准正态，自然有没有 <span class="math inline">\(\mathbf z\)</span> 的条件都无所谓了。换句话说，<span class="math inline">\(\eqref{diff-reg}\)</span> 式就像原始的 DDPM 一样变成了 <span class="math inline">\(0\)</span>，这一点与 ES-DDPM 形成了鲜明对比（事实上本节涉及的方法中除了 ES-DDPM，<span class="math inline">\(\eqref{diff-reg}\)</span> 式都退化为 <span class="math inline">\(0\)</span>）。</p><p>作者还做了进一步的扩展，考虑了给不同时间步以不同的隐变量的情形，如上右图所示。具体实现上可以通过给编码器以时间步 <span class="math inline">\(t\)</span> 作为条件来完成。相关推导应该也大差不差，这里就不赘述了。</p><h3 id="diffusevae">DiffuseVAE</h3><p>DiffuseVAE<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Pandey, Kushagra, Avideep Mukherjee, Piyush Rai, and Abhishek Kumar. Diffusevae: Efficient, controllable and high-fidelity generation from low-dimensional latents. *arXiv preprint arXiv:2201.00308* (2022).">[4]</span></a></sup> 的想法很直接：首先用 VAE 生成图像 <span class="math inline">\(\hat{\mathbf x}_0\)</span>，然后将其作为条件给到扩散模型来精细化，整体框架如下所示：</p><p><img src="diffusevae.png" width=100% /></p><p>乍一看这好像与我们的基本框架不太相像，但仔细一瞧其实是类似的，只不过由于 DiffuseVAE 是分阶段训练而非联合训练的，在训练扩散模型之前已经训练好 VAE 了，所以可以把条件从隐变量 <span class="math inline">\(\mathbf z\)</span> 换成 VAE 重构的图像 <span class="math inline">\(\hat{\mathbf x}_0\)</span>，即： <span class="math display">\[p(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf z)=p(\mathbf x_{t-1}\vert\mathbf x_t,\hat{\mathbf x}_0),\quad t=1,2,\ldots,T\]</span> 相当于训练一个条件扩散模型做超分（类似于 SR3）。也就是说，由于 DiffuseVAE 选择了分阶段训练，我们原本统一的框架被拆分成了两个已有工作的拼接，多少有点遗憾。甚至作者也在论文的 limitation 一节说端到端的联合训练是值得探索的。不过有趣的是，我们框架中的前向过程是与条件独立的（即 <span class="math inline">\(\eqref{inference}\)</span> 式），而本文的作者还提出了一种带条件的前向过程形式化： <span class="math display">\[\begin{align}&amp;q(\mathbf x_1\vert\mathbf x_0,\hat{\mathbf x}_0)=\mathcal N\left(\mathbf x_1;\sqrt{1-\beta_1}\mathbf x_0+\hat{\mathbf x}_0,\beta_1\mathbf I\right)\\&amp;q(\mathbf x_t\vert\mathbf x_{t-1},\hat{\mathbf x}_0)=\mathcal N\left(\mathbf x_t;\sqrt{1-\beta_t}\mathbf x_{t-1}+(1-\sqrt{1-\beta_t})\hat{\mathbf x}_0,\beta_t\mathbf I\right)\end{align}\]</span> 在这样的形式化下，可以推出： <span class="math display">\[q(\mathbf x_t\vert\mathbf x_0,\hat{\mathbf x}_0)=\mathcal N\left(\mathbf x_t;\sqrt{\bar\alpha_t}\mathbf x_0+\hat{\mathbf x}_0,(1-\bar\alpha_t)\mathbf I\right)\]</span> 因此，当 <span class="math inline">\(T\to\infty\)</span> 时，<span class="math inline">\(q(\mathbf x_T\vert\mathbf x_0,\hat{\mathbf x}_0)\to\mathcal N(\mathbf x_T;\hat{\mathbf x}_0,\mathbf I)\)</span>，也就是说，模型不再是把任何图像都加噪到标准正态分布，而是加噪到以原图为中心的正态分布。当然，在这样的形式化下，原始 DDPM 中的相关公式都要重新推导，鉴于这已经偏离了本文的主题，就不再赘述了。</p><h3 id="infodiffusion">InfoDiffusion</h3><p>InfoDiffusion<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Wang, Yingheng, Yair Schiff, Aaron Gokaslan, Weishen Pan, Fei Wang, Christopher De Sa, and Volodymyr Kuleshov. InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models. *arXiv preprint arXiv:2306.08757* (2023).">[5]</span></a></sup> 的目的依然是寻找一个有语义的隐空间，整体框架如下图所示：</p><p><img src="infodiffusion.png" width=60% /></p><p>可见这就是在我们的基础框架上增加了两个正则项（红色框框），分别是：</p><ul><li><p><strong>Mutual Information Maximization</strong>：最大化隐变量 <span class="math inline">\(\mathbf z\)</span> 与图像 <span class="math inline">\(\mathbf x_0\)</span> 之间的互信息，用于避免模型忽略隐变量 <span class="math inline">\(\mathbf z\)</span>. <span class="math display">\[\text{MI}_{\mathbf x_0,\mathbf z}=\mathbb E_{q(\mathbf x_0,\mathbf z)}\left[\log\frac{q(\mathbf x_0,\mathbf z)}{q(\mathbf x_0)q(\mathbf z)}\right]\]</span> 这个思想最早来自于著名的 InfoGAN，而后被用在了 InfoVAE 之中，所以现在用在 Diffusion 中也并不意外。</p></li><li><p><strong>Prior Regularization</strong>：让 <span class="math inline">\(\mathbf z\)</span> 的后验逼近一个可灵活选取的先验分布，防止后验坍塌。 <span class="math display">\[\mathcal R=\mathbf D(q(\mathbf z)\Vert p(\mathbf z))\]</span> 其中 <span class="math inline">\(\mathbf D\)</span> 可以是任意一种散度。</p></li></ul><p>那么总的优化目标就是： <span class="math display">\[\mathbb E_{q(\mathbf x_0)}[\text{ELBO}(\mathbf x_0)]+\zeta\cdot\text{MI}_{\mathbf x_0,\mathbf z}-\beta\cdot\mathcal R\]</span> 其中 <span class="math inline">\(\zeta,\beta&gt;0\)</span> 是控制正则项大小的系数。</p><p>然而，这两个正则项都不能直接写出解析形式，需要进一步的推导。为此，我们首先重写 VAE 正则项（<span class="math inline">\(\eqref{vae-reg}\)</span> 式）： <span class="math display">\[\begin{align}-\mathbb E_{q(\mathbf x_0)}\left[\text{KL}(q(\mathbf z\vert\mathbf x_0)\Vert p(\mathbf z)))\right]&amp;=\mathbb E_{q(\mathbf x_0)}\mathbb E_{q(\mathbf z\vert\mathbf x_0)}\left[\log\frac{p(\mathbf z)}{q(\mathbf z\vert\mathbf x_0)}\right]\\&amp;=\mathbb E_{q(\mathbf x_0,\mathbf z)}\left[\log\left(\frac{p(\mathbf z)}{q(\mathbf z\vert\mathbf x_0)}\cdot\frac{q(\mathbf z)}{q(\mathbf z)}\right)\right]\\&amp;=\mathbb E_{q(\mathbf x_0,\mathbf z)}\left[\log\frac{p(\mathbf z)}{q(\mathbf z)}+\log\frac{q(\mathbf z)}{q(\mathbf z\vert\mathbf x_0)}\right]\\&amp;=-\text{KL}(q(\mathbf z)\Vert p(\mathbf z))-\text{MI}_{\mathbf x_0,\mathbf z}\end{align}\]</span> 发现 VAE 正则项本身就由两部分组成：互信息 <span class="math inline">\(\text{MI}_{\mathbf x_0,\mathbf z}\)</span> 与 <span class="math inline">\(\mathcal R\)</span>（在取 <span class="math inline">\(\mathbf D\)</span> 为 KL 散度的情况下），所以 InfoDiffusion 添加的两个正则项本质上是在对原本的 VAE 正则项的两个组成部分进行重新加权。我们首先解决互信息无法解析表达的问题，只需要反过来用 VAE 正则项和 <span class="math inline">\(\mathcal R\)</span> 来替换掉互信息即可： <span class="math display">\[\begin{align}&amp;-\mathbb E_{q(\mathbf x_0)}\left[\text{KL}(q(\mathbf z\vert\mathbf x_0)\Vert p(\mathbf z)))\right]+\zeta\cdot\text{MI}_{\mathbf x_0,\mathbf z}-\beta\cdot\mathcal R\\=&amp;-\mathbb E_{q(\mathbf x_0)}[\text{KL}(q(\mathbf z\vert\mathbf x_0)\Vert p(\mathbf z))]+\zeta\cdot\left(-\mathcal R+\mathbb E_{q(\mathbf x_0)}[\text{KL}(q(\mathbf z\vert\mathbf x_0)\Vert p(\mathbf z))]\right)-\beta\cdot\mathcal R\\=&amp;\ (\zeta-1)\mathbb E_{q(\mathbf x_0)}[\text{KL}(q(\mathbf z\vert\mathbf x_0)\Vert p(\mathbf z))]-(\zeta+\beta)\cdot\mathcal R\end{align}\]</span> 最后把剩下的其他项（<span class="math inline">\(\eqref{diff-reg},\eqref{diff-rec},\eqref{diff-match}\)</span> 式套上对 <span class="math inline">\(\mathbf x_0\)</span> 求期望）加进来，就得到了最终的优化目标： <span class="math display">\[\begin{align}\mathcal L_I=\ &amp;\mathbb E_{q(\mathbf z\vert\mathbf x_0)}\mathbb E_{q(\mathbf x_0,\mathbf x_1)}\left[\log p(\mathbf x_0\vert\mathbf x_1,\mathbf z)\right]-\mathbb E_{q(\mathbf x_0)}\left[\text{KL}(q(\mathbf x_T\vert\mathbf x_0)\Vert p(\mathbf x_T))\right]\\&amp;-\sum_{t=2}^T\mathbb E_{q(\mathbf x_0,\mathbf x_t)}\mathbb E_{q(\mathbf z\vert\mathbf x_0)}\left[\text{KL}(q(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf x_0)\Vert p(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf z))\right]\\&amp;-(\zeta+\beta)\cdot\text{KL}(q(\mathbf z)\Vert p(\mathbf z))-(1-\zeta)\mathbb E_{q(\mathbf x_0)}[\text{KL}(q(\mathbf z\vert\mathbf x_0)\Vert p(\mathbf z))]\end{align}\]</span> 现在还有一个遗留问题就是 <span class="math inline">\(\mathcal R=\text{KL}(q(\mathbf z)\Vert p(\mathbf z))\)</span> 依旧无法写出解析形式。为此，作者证明了，虽然上述推导建立在取 <span class="math inline">\(\mathcal R\)</span> 中的散度 <span class="math inline">\(\mathbf D\)</span> 为 KL 散度的情形下，但是直接把 KL 散度换成其他散度也是没问题的。最终作者选用 MMD (maximum mean discrepancy)： <span class="math display">\[\begin{align}\text{MMD}(q(\mathbf z)\Vert p(\mathbf z))&amp;=\mathbb E_{\mathbf z,\mathbf z&#39;\sim q(\mathbf z)}[k(\mathbf z,\mathbf z&#39;)]\\&amp;+\mathbb E_{\mathbf z,\mathbf z&#39;\sim p(\mathbf z)}[k(\mathbf z,\mathbf z&#39;)]\\&amp;-2\mathbb E_{\mathbf z\sim q(\mathbf z),\mathbf z&#39;\sim p(\mathbf z)}[k(\mathbf z,\mathbf z&#39;)]\end{align}\]</span> 其中 <span class="math inline">\(k\)</span> 是一个正定核函数，对 <span class="math inline">\(q(\mathbf z)\)</span> 的期望通过采样 <span class="math inline">\(\{\mathbf x_0^{(i)}\}_{i=1}^N\sim q(\mathbf x_0)\)</span> 的方式来近似。</p><p>事实上，InfoDiffusion 新加入的这两个正则项都是 InfoVAE<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Zhao, Shengjia, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational autoencoders. *arXiv preprint arXiv:1706.02262* (2017).">[6]</span></a></sup> 提出的，感兴趣的读者可以参考 <a href="/blog-main/2023/09/15/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E4%BA%92%E4%BF%A1%E6%81%AF/" title="生成模型中的互信息">生成模型中的互信息</a>一文。</p><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Preechakul, Konpat, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward a meaningful and decodable representation. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 10619-10629. 2022. <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Lyu, Zhaoyang, Xudong Xu, Ceyuan Yang, Dahua Lin, and Bo Dai. Accelerating diffusion models via early stop of the diffusion process. <em>arXiv preprint arXiv:2205.12524</em> (2022). <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Traub, Jeremias. Representation Learning with Diffusion Models. <em>arXiv preprint arXiv:2210.11058</em> (2022). <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Pandey, Kushagra, Avideep Mukherjee, Piyush Rai, and Abhishek Kumar. Diffusevae: Efficient, controllable and high-fidelity generation from low-dimensional latents. <em>arXiv preprint arXiv:2201.00308</em> (2022). <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Wang, Yingheng, Yair Schiff, Aaron Gokaslan, Weishen Pan, Fei Wang, Christopher De Sa, and Volodymyr Kuleshov. InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models. <em>arXiv preprint arXiv:2306.08757</em> (2023). <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Zhao, Shengjia, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational autoencoders. <em>arXiv preprint arXiv:1706.02262</em> (2017). <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>Diffusion Models</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Stable Diffusion]训练你的LoRA(Linux)</title>
    <link href="/blog-main/2023/06/21/Stable-Diffusion-%E8%AE%AD%E7%BB%83%E4%BD%A0%E7%9A%84LoRA-Linux/"/>
    <url>/blog-main/2023/06/21/Stable-Diffusion-%E8%AE%AD%E7%BB%83%E4%BD%A0%E7%9A%84LoRA-Linux/</url>
    
    <content type="html"><![CDATA[<h2 id="简介">简介</h2><p>LoRA 是一种参数高效微调方法（PEFT），最早由 <a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a> 提出并应用于微调语言大模型之中，后来由 <a href="https://github.com/cloneofsimo/lora">Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning</a> 引入到对 Stable Diffusion 模型的微调之中。LoRA 并不改变原模型的权重，而是在线性层旁边新增一个下采样-上采样的支路，通过训练这个支路来完成微调。因此，同一个基底 Stable Diffusion 模型可以搭载不同的 LoRA 使用，具有很高的灵活性。由于 LoRA 支路网络的参数量小，相比微调整个模型，对算力的需求更加友好，并且也能达到不错的效果，因此很快受到大家的热烈欢迎，成为了目前最流行的微调 Stable Diffusion 的方法之一。</p><p>网络上有非常多训练 LoRA 的脚本和 GUI 界面，B 站上也有很多视频教程，但它们大多是面向 Windows 用户和 GUI 用户。由于我本人使用的是 Linux，因此选择 <a href="https://github.com/kohya-ss">kohya-ss</a>/<a href="https://github.com/kohya-ss/sd-scripts">sd-scripts</a> 在终端中使用命令进行训练。事实上，该代码库不仅支持训练 LoRA，还支持训练 DreamBooth、直接微调、训练 Textual Inversion 以及转换模型格式（ckpt、safetensors、Diffusers 格式互相转换）等，且训练出来的模型可以直接加载到 webui 中使用，是最广为使用的代码库之一（目前 2.2k stars）。由于该代码库的文档是用日文书写的，所以有人 fork 了一份并用 ChatGPT-4 将其翻译为了英文：<a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs">英文文档链接</a>. 本文有大量内容参考自英文文档。</p><p>特别说明：本文所用 sd-scripts 版本的 commit hash 为 c7fd336，后续代码更新后可能会出现与本文讲解不一致的地方。</p><h2 id="环境配置">环境配置</h2><ol type="1"><li><p><strong>下载代码</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">git clone https://github.com/kohya-ss/sd-scripts.git<br>cd sd-scripts<br></code></pre></td></tr></table></figure></li><li><p><strong>新建并激活 conda 环境</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda create -n sd-scripts python=3.10<br>conda activate sd-scripts<br></code></pre></td></tr></table></figure></li><li><p><strong>安装依赖包</strong>：</p><p>先在 <code>requirements.txt</code> 中把 <code>bitsandbytes==0.35.0</code> 改成 <code>bitsandbytes-cuda111==0.26.0.post2</code>，然后：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple<br>pip install xformers -i https://pypi.tuna.tsinghua.edu.cn/simple<br></code></pre></td></tr></table></figure></li><li><p>代码库会使用 huggingface transformers 库集成的 CLIP 模型，但由于众所周知的网络原因，我们很可能在自动下载权重的时候卡住。为了解决这个问题，可以先手动把 CLIP 模型下载下来：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">git lfs install<br>git clone https://huggingface.co/openai/clip-vit-large-patch14<br></code></pre></td></tr></table></figure><p>然后更改 <code>library/model_util.py</code> 文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">text_model = CLIPTextModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-large-patch14&quot;</span>).to(device)  <span class="hljs-comment"># 删掉或者注释掉</span><br>text_model = CLIPTextModel.from_pretrained(<span class="hljs-string">&#x27;./clip-vit-large-patch14&#x27;</span>).to(device)       <span class="hljs-comment"># 改成这个</span><br></code></pre></td></tr></table></figure><p>再更改 <code>library/train_util.py</code> 文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">TOKENIZER_PATH = <span class="hljs-string">&quot;openai/clip-vit-large-patch14&quot;</span>  <span class="hljs-comment"># 删掉或者注释掉</span><br>TOKENIZER_PATH = <span class="hljs-string">&#x27;./clip-vit-large-patch14&#x27;</span>       <span class="hljs-comment"># 改成这个</span><br></code></pre></td></tr></table></figure><p>改动这两个文件后代码就会直接加载本地下载好的模型而非去联网下载。</p></li></ol><h2 id="数据准备">数据准备</h2><p>准备数据是训练 LoRA 过程中非常重要的一环，甚至可能是用时最长的环节。数据质量非常重要，如果是人物角色，应尽可能多地包含各种角度、姿态和背景，否则模型的输出将过于单一。譬如，如果训练集里没有角色的背面，那模型自然也无法学会生成其背面。好消息是，15~20 张图片足以训练出一个还不错的 LoRA，所以人工检查训练集的质量并不是件难事。当然，在保证质量的前提下，数据肯定是越多越好。</p><p>为了训练 LoRA，sd-scripts 支持三种不同的数据准备方式：</p><ol type="1"><li><p><strong>DreamBooth, class+identifier 方法</strong></p><p>第一种方法启发自 DreamBooth，不过只微调 LoRA 网络而非微调整个模型。这种方法会把一个特殊的词汇（标识符）与训练目标（人物/动物/物体）绑定起来，于是在使用这个标识符时就能够生成对应目标的图片。该方法<strong>不需要为每张图片准备文字描述</strong>，因此较为<strong>简单直接</strong>，但缺点是会将训练集中出现的各种特征硬编码进模型，<strong>缺乏灵活性</strong>。例如，假设要训练一个格温蜘蛛侠，由于素材图片中其头发都是（或绝大部分是）金色，所以模型会认为金发是格温的固有属性（换句话说，不是金发就不是格温），因此训练好之后想改变发色会比较困难。另外，该方法支持正则化图片来防止模型过拟合，例如，素材是我自己养的猫，那么正则化图片就是其他各种猫，防止训练之后模型在生成猫的时候只会生成自己的猫，不会生成其他猫了。</p></li><li><p><strong>DreamBooth, caption 方法</strong></p><p>该方法<strong>要求对每张训练图片提供对应的文本描述</strong>，因此准备过程比第一种方法麻烦一些，但<strong>可以将特征与目标解耦</strong>。例如，对金发格温的图片，我们在描述词中加入「金发」，那么模型就会知道金发是由文本描述决定的、而不是格温的固有属性，于是当描述词改成红发时，模型就会根据描述词去改变发色；反过来，如果希望生成的角色一定包含某种属性，比如蓝色眼睛，那么描述词中就不能有「蓝眼」，这样蓝眼才会与模型绑定起来。另外，该方法也支持正则化图片。</p></li><li><p><strong>Fine-tuning 方法</strong></p><p>文本描述需要提前收集到一个元数据文件之中，不支持正则化图片。这种方法对于训练 LoRA 而言似乎不是很重要。</p></li></ol><p>sd-scripts 使用 TOML 配置文件来指示数据的配置，官方示例如下：</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs toml"><span class="hljs-section">[general]</span><br><span class="hljs-attr">shuffle_caption</span> = <span class="hljs-literal">true</span><br><span class="hljs-attr">caption_extension</span> = <span class="hljs-string">&#x27;.txt&#x27;</span><br><span class="hljs-attr">keep_tokens</span> = <span class="hljs-number">1</span><br><br><span class="hljs-comment"># This is a DreamBooth-style dataset</span><br><span class="hljs-section">[[datasets]]</span><br><span class="hljs-attr">resolution</span> = <span class="hljs-number">512</span><br><span class="hljs-attr">batch_size</span> = <span class="hljs-number">4</span><br><span class="hljs-attr">keep_tokens</span> = <span class="hljs-number">2</span><br><br>  <span class="hljs-section">[[datasets.subsets]]</span><br>  <span class="hljs-attr">image_dir</span> = <span class="hljs-string">&#x27;C:\hoge&#x27;</span><br>  <span class="hljs-attr">class_tokens</span> = <span class="hljs-string">&#x27;hoge girl&#x27;</span><br>  <span class="hljs-comment"># This subset has keep_tokens = 2 (using the value of the parent datasets)</span><br><br>  <span class="hljs-section">[[datasets.subsets]]</span><br>  <span class="hljs-attr">image_dir</span> = <span class="hljs-string">&#x27;C:\fuga&#x27;</span><br>  <span class="hljs-attr">class_tokens</span> = <span class="hljs-string">&#x27;fuga boy&#x27;</span><br>  <span class="hljs-attr">keep_tokens</span> = <span class="hljs-number">3</span><br><br>  <span class="hljs-section">[[datasets.subsets]]</span><br>  <span class="hljs-attr">is_reg</span> = <span class="hljs-literal">true</span><br>  <span class="hljs-attr">image_dir</span> = <span class="hljs-string">&#x27;C:\reg&#x27;</span><br>  <span class="hljs-attr">class_tokens</span> = <span class="hljs-string">&#x27;human&#x27;</span><br>  <span class="hljs-attr">keep_tokens</span> = <span class="hljs-number">1</span><br><br><span class="hljs-comment"># This is a fine-tuning-style dataset</span><br><span class="hljs-section">[[datasets]]</span><br><span class="hljs-attr">resolution</span> = [<span class="hljs-number">768</span>, <span class="hljs-number">768</span>]<br><span class="hljs-attr">batch_size</span> = <span class="hljs-number">2</span><br><br>  <span class="hljs-section">[[datasets.subsets]]</span><br>  <span class="hljs-attr">image_dir</span> = <span class="hljs-string">&#x27;C:\piyo&#x27;</span><br>  <span class="hljs-attr">metadata_file</span> = <span class="hljs-string">&#x27;C:\piyo\piyo_md.json&#x27;</span><br>  <span class="hljs-comment"># This subset has keep_tokens = 1 (using the general value)</span><br></code></pre></td></tr></table></figure><p>可以看见，<code>[general]</code> 下是通用的配置项，每个配置文件可以包含多个 <code>[[datasets]]</code>，每个 dataset 可以包含多个 <code>[[datasets.subsets]]</code>. 这些不同的数据集会一起被训练。代码会根据是否有 <code>metadata_file</code> 这一项来判断对应数据集配置是 DreamBooth-style 还是 fine-tuning-style. 所有参数列表可以参见<a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/config_README-en.md">文档</a>，一些常用的参数如下所示。</p><p><strong>所有方法都可使用的参数</strong>：</p><table><thead><tr class="header"><th>Option Name</th><th>Example Setting</th><th><code>[general]</code></th><th><code>[[datasets]]</code></th><th><code>[[dataset.subsets]]</code></th><th>说明</th></tr></thead><tbody><tr class="odd"><td><code>batch_size</code></td><td><code>1</code></td><td>o</td><td>o</td><td></td><td></td></tr><tr class="even"><td><code>enable_bucket</code></td><td><code>true</code></td><td>o</td><td>o</td><td></td><td>开启 bucket 来支持不同长宽比的训练图片</td></tr><tr class="odd"><td><code>resolution</code></td><td><code>256</code>, <code>[512, 512]</code></td><td>o</td><td>o</td><td></td><td>训练时的图片分辨率</td></tr><tr class="even"><td><code>flip_aug</code></td><td><code>true</code></td><td>o</td><td>o</td><td>o</td><td>水平翻转数据增强，要求训练目标对左右方向不敏感</td></tr><tr class="odd"><td><code>random_crop</code></td><td><code>false</code></td><td>o</td><td>o</td><td>o</td><td>随机裁剪数据增强</td></tr><tr class="even"><td><code>color_aug</code></td><td><code>false</code></td><td>o</td><td>o</td><td>o</td><td>颜色数据增强，要求训练目标对颜色不敏感</td></tr><tr class="odd"><td><code>shuffle_caption</code></td><td><code>true</code></td><td>o</td><td>o</td><td>o</td><td>打乱文本描述</td></tr><tr class="even"><td><code>keep_tokens</code></td><td><code>2</code></td><td>o</td><td>o</td><td>o</td><td>保持前多少个 token 顺序不被打乱</td></tr><tr class="odd"><td><code>num_repeats</code></td><td><code>10</code></td><td>o</td><td>o</td><td>o</td><td>每张图片在一个 epoch 内重复多少次</td></tr></tbody></table><p><strong>DreamBooth-style 特有的参数</strong>：</p><table><thead><tr class="header"><th>Option name</th><th>Example</th><th><code>[general]</code></th><th><code>[[datasets]]</code></th><th><code>[[dataset.subsets]]</code></th><th>说明</th></tr></thead><tbody><tr class="odd"><td><code>image_dir</code></td><td><code>‘C:\hoge’</code></td><td>-</td><td>-</td><td>o (required)</td><td>图片目录路径，必需项，图片应直接存在该目录下</td></tr><tr class="even"><td><code>caption_extension</code></td><td><code>".txt"</code></td><td>o</td><td>o</td><td>o</td><td>文本描述文件的扩展名</td></tr><tr class="odd"><td><code>class_tokens</code></td><td><code>“sks girl”</code></td><td>-</td><td>-</td><td>o</td><td>标识符+类别</td></tr><tr class="even"><td><code>is_reg</code></td><td><code>false</code></td><td>-</td><td>-</td><td>o</td><td>是否是正则化图片</td></tr></tbody></table><p><strong>Fine-tuning-style 特有的参数</strong>：</p><table><thead><tr class="header"><th>Option name</th><th>Example</th><th><code>[general]</code></th><th><code>[[datasets]]</code></th><th><code>[[dataset.subsets]]</code></th><th>说明</th></tr></thead><tbody><tr class="odd"><td><code>image_dir</code></td><td><code>‘C:\hoge’</code></td><td>-</td><td>-</td><td>o</td><td>图片目录路径，图片应直接存在该目录下</td></tr><tr class="even"><td><code>metadata_file</code></td><td><code>'C:\piyo\piyo_md.json'</code></td><td>-</td><td>-</td><td>o (required)</td><td>元数据文件路径，必需项</td></tr></tbody></table><p>值得一提的是，网上有些教程让把图片目录按照 <code>number_identifier class</code> 的格式命名，这是旧版本的做法，最新版本改用上述 TOML 配置文件来配置数据，不必再依此命名。</p><h2 id="训练脚本">训练脚本</h2><p>训练 LoRA 的脚本是 <code>train_network.py</code>. sd-scripts 使用 huggingface 的 accelerate 库来启动脚本，可以先执行 <code>accelerate config</code> 进行基本配置。训练脚本包含很多参数，可以参见<a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_README-en.md#commonly-used-options-across-scripts">文档</a>和<a href="https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning">文档</a>，一些常用的参数如下所示：</p><ul><li><code>--pretrained_model_name_or_path</code>：指向基底模型的路径，支持 <code>.ckpt</code>、<code>.safetensors</code> 和 Diffusers 格式。可以考虑使用与数据集画风接近的基底模型。</li><li><code>--output_dir</code>：指定模型保存的路径。</li><li><code>--output_name</code>：指定模型保存的文件名（不含扩展名）。</li><li><code>--save_model_as</code>：模型保存格式，<code>ckpt, safetensors, diffusers, diffusers_safetensors</code>.</li><li><code>--dataset_config</code>：指向 TOML 配置文件的路径。</li><li><code>--max_train_steps</code> / <code>--max_train_epochs</code>：指定训练的 steps 数或者 epochs 数。</li><li><code>--save_every_n_steps</code> / <code>--save_every_n_epochs</code>：每隔多少 steps 或者 epochs 保存模型。</li><li><code>--mixed_precision</code>：使用混合精度来节省显存。</li><li><code>--gradient_checkpointing</code>：用于节省显存，但是会增加训练时间。</li><li><code>--xformers</code> / <code>--mem_eff_attn</code>：用于节省显存。</li><li><code>--clip_skip</code>：使用 CLIP 的倒数第几层特征，最好与基底模型保持一致。</li><li><code>--network_dim</code>: 指定 LoRA 的秩（即网络维度），默认为 4. 值越大网络越大、参数越多、能力越强，但是不应盲目增大。训练人物可以考虑设为 16/32/64.</li><li><code>--network_alpha</code>: 用于保证训练过程的数值稳定性，防止下溢，默认为 1.</li><li><code>--network_weights</code>: 加载预训练的 LoRA 模型并继续训练。</li><li><code>--network_train_unet_only</code>: 只训练 U-Net 的 LoRA. 也许对 fine-tuning-style 有用。</li><li><code>--network_train_text_encoder_only</code>: 只训练 Text Encoder 的 LoRA. 类似于 Textual Inversion 的效果。</li><li><code>--optimizer_type</code>：选择优化器。</li><li><code>--learning_rate</code>：设置学习率。</li><li><code>--unet_lr</code>: 对 U-Net 的 LoRA 单独设置学习率，一般可以设为 1e-4，覆盖 <code>--learning rate</code> 的设置。</li><li><code>--text_encoder_lr</code>: 为 Text Encoder 的 LoRA 单独设置学习率，一般可以设为 5e-5，覆盖 <code>--learning rate</code> 的设置。</li><li><code>--lr_scheduler</code> / <code>--lr_warmup_steps</code> / <code>--lr_scheduler_num_cycles</code> / <code>--lr_scheduler_power</code>：设置学习率 scheduler、warmup.</li></ul><h2 id="示例">示例</h2><p>凭空讲解还是太过抽象，这一节我们用两个示例来详细阐述训练过程。两个示例分别使用前文介绍的两种数据准备方式（class+identifier 与 caption），供读者参考。</p><h3 id="角色-lorajudy-zootopia">角色 LoRA：Judy-Zootopia</h3><p>我们用疯狂动物城的 Judy 来演示<strong>第一种方法（DreamBooth, class+identifier）</strong>。这种方法不需要准备文本描述，只需要在配置文件中设置 <code>class_tokens</code> 为标识符+类别即可。首先，我在网上找了 27 张 Judy 的图片放在 <code>./data/Judy-Zootopia</code> 下：</p><p><img src="judy.png" /></p><p>这些图片的分辨率最好在 512x512 及以上，因为大部分基底模型都是在这个分辨率上训练的。图片的长宽比并不一定要保持一致，因为 sd-scripts 支持 bucketing，即会自动按照长宽比分组训练。</p><p>接下来创建一个数据配置文件 <code>./configs/Judy-Zootopia.toml</code>，仿照前文的官方示例，填写如下内容：</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs toml"><span class="hljs-section">[general]</span><br><span class="hljs-attr">enable_bucket</span> = <span class="hljs-literal">true</span><br><br><span class="hljs-section">[[datasets]]</span><br><span class="hljs-attr">resolution</span> = <span class="hljs-number">512</span><br><span class="hljs-attr">batch_size</span> = <span class="hljs-number">4</span><br><br>  <span class="hljs-section">[[datasets.subsets]]</span><br>  <span class="hljs-attr">image_dir</span> = <span class="hljs-string">&#x27;./data/Judy-Zootopia&#x27;</span><br>  <span class="hljs-attr">class_tokens</span> = <span class="hljs-string">&#x27;Judy bunny&#x27;</span><br>  <span class="hljs-attr">num_repeats</span> = <span class="hljs-number">20</span><br></code></pre></td></tr></table></figure><p>其中，<code>enable_bucket=true</code> 让我们能够用不同长宽比的训练图片；<code>class_tokens</code> 填写「标识符+类别」Judy bunny，如果省略类别效果可能会差一点；<code>num_repeats</code> 表示在一个 epoch 里每张训练图片重复出现的次数，例如，我有 27 张图片，<code>num_repeats=20</code>，所以一个 epoch 会过 540 张图片，又因为 <code>batch_size=4</code>，所以（在不进行梯度累积的情况下）一个 epoch 包含 135 个 iterations (steps)；另外，我也没有使用正则化图片。</p><p>最后选择训练参数，运行训练脚本即可：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs shell">accelerate launch --num_cpu_threads_per_process 8 train_network.py \<br>    --pretrained_model_name_or_path=../stable-diffusion-webui/models/Stable-diffusion/revAnimated_v122.safetensors \<br>    --dataset_config=./configs/Judy-Zootopia.toml \<br>    --output_dir=./output/Judy-Zootopia/ \<br>    --output_name=Judy-Zootopia \<br>    --save_model_as=safetensors  \<br>    --max_train_steps=2000  \<br>    --learning_rate=1e-4  \<br>    --unet_lr=1e-4 \<br>    --text_encoder_lr=5e-5 \<br>    --optimizer_type=&quot;AdamW8bit&quot;  \<br>    --lr_scheduler=&quot;constant_with_warmup&quot; \<br>    --lr_warmup_steps=135 \<br>    --xformers  \<br>    --mixed_precision=&quot;fp16&quot;  \<br>    --cache_latents  \<br>    --gradient_checkpointing \<br>    --save_every_n_steps=500  \<br>    --network_module=networks.lora \<br>    --network_dim 8<br></code></pre></td></tr></table></figure><p>单卡 3080Ti 用时约 15min 完成训练，显存占用近 5GB.</p><p>训练结束后将 LoRA 放入 webui 测试。使用 LoRA 时可以指定权重，权重越高 LoRA 效果越明显，但高到一定程度后会导致图片失真。不同 LoRA 模型适宜的权重范围并不一样，需要人工测试。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="judy-lora.png" /></div></div></div><p>以下展示一些挑选后的结果：</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="judy-1.png" /></div><div class="group-image-wrap"><img src="judy-2.png" /></div><div class="group-image-wrap"><img src="judy-3.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="judy-4.png" /></div><div class="group-image-wrap"><img src="judy-5.png" /></div><div class="group-image-wrap"><img src="judy-6.png" /></div></div></div><p><br/></p><h3 id="姿态-lorasuperhero-landing">姿态 LoRA：Superhero-Landing</h3><blockquote><p>死侍表示很赞😎</p></blockquote><p>除了训练人物角色/物体，我们还可以训练一些抽象的概念，比如人物的姿态。这里我们尝试训练一个超级英雄落地式姿态。要想 LoRA 只学会姿态而不是训练集中的人物特征，我们必须把姿态与其他特征解耦开，因此<strong>选择第二种方法（Dreambooth, caption）</strong>。</p><p>首先还是收集数据。我在网上找了 25 张超级英雄落地图放在 <code>./data/Superhero-Landing</code> 下：</p><p><img src="superhero-landing.png" /></p><p>接下来为每张图片写文本描述，可以分为三个小步：</p><ol type="1"><li><p><strong>利用工具自动生成</strong>：webui 和 sd-scripts 都提供了相应的功能，可以任选一种。</p><ul><li><p>使用 webui：点击 Train 选项卡，点击 Process images 子选项卡，输入 source directory 和 destination directory，勾选 Keep original size，勾选 Use deepbooru for caption 或 Use BLIP for caption，点击 Preprocess 按钮即可，处理结束的图片文本对将存储在目标文件夹下。BLIP 生成的是自然语言，而 deepbooru 生成的是许多词汇的集合，大家可以酌情选择。</p><p><img src="captioning.png" /></p></li><li><p>使用 sd-scripts（仅支持 BLIP)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">python finetune/make_captions.py &lt;data_folder&gt; --batch_size &lt;batch_size&gt;<br></code></pre></td></tr></table></figure><p>更多参数可以通过 <code>--help</code> 查看。</p></li></ul></li><li><p><strong>逐个检查、修改这些文本描述</strong>：特别要保证人物姿态没有被写入描述，这样才能让姿态绑定到模型之中；而其他针对人物的描述应该尽可能详细，这样才能让这些特征与模型解耦开。</p></li><li><p><strong>添加触发词</strong>：在所有图片的文本描述前添加一个或多个词汇，那么这些词汇就起到了类似于触发 LoRA 的作用。比如我设置的触发词为 "superhero landing". 这一步并不是必需的。</p></li></ol><p>数据准备完成后，创建配置文件 <code>./configs/Superhero-Landing.toml</code>，填写如下配置：</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs toml"><span class="hljs-section">[general]</span><br><span class="hljs-attr">enable_bucket</span> = <span class="hljs-literal">true</span><br><span class="hljs-attr">shuffle_caption</span> = <span class="hljs-literal">true</span><br><span class="hljs-attr">caption_extension</span> = <span class="hljs-string">&#x27;.txt&#x27;</span><br><span class="hljs-attr">keep_tokens</span> = <span class="hljs-number">0</span><br><br><span class="hljs-section">[[datasets]]</span><br><span class="hljs-attr">resolution</span> = <span class="hljs-number">512</span><br><span class="hljs-attr">batch_size</span> = <span class="hljs-number">4</span><br><br>  <span class="hljs-section">[[datasets.subsets]]</span><br>  <span class="hljs-attr">flip_aug</span> = <span class="hljs-literal">true</span><br>  <span class="hljs-attr">image_dir</span> = <span class="hljs-string">&#x27;./data/Superhero-Landing-Processed&#x27;</span><br>  <span class="hljs-attr">num_repeats</span> = <span class="hljs-number">20</span><br></code></pre></td></tr></table></figure><p>由于超级英雄落地姿势是水平对称的，所以这里开启了 <code>flip_aug</code> 水平翻转增强。</p><p>最后选择训练参数，运行训练脚本即可：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs shell">accelerate launch --num_cpu_threads_per_process 8 train_network.py \<br>    --pretrained_model_name_or_path=../stable-diffusion-webui/models/Stable-diffusion/v1-5-pruned.safetensors \<br>    --dataset_config=./configs/Superhero-Landing.toml \<br>    --output_dir=./output/Superhero-Landing/ \<br>    --output_name=Superhero-Landing \<br>    --save_model_as=safetensors  \<br>    --max_train_steps=5000  \<br>    --learning_rate=1e-4  \<br>    --unet_lr=1e-4 \<br>    --text_encoder_lr=5e-5 \<br>    --optimizer_type=&quot;AdamW8bit&quot;  \<br>    --lr_scheduler=&quot;cosine&quot; \<br>    --lr_warmup_steps=200 \<br>    --xformers  \<br>    --mixed_precision=&quot;fp16&quot;  \<br>    --cache_latents  \<br>    --gradient_checkpointing \<br>    --save_every_n_steps=500  \<br>    --network_module=networks.lora \<br>    --network_dim 4<br></code></pre></td></tr></table></figure><p>单卡 3080Ti 用时约 30min 完成训练，显存占用近 5GB.</p><p>训练结束后将 LoRA 放入 webui 测试。姿态 LoRA 可以与人物 LoRA（比如刚才训练的 Judy）一起使用，但可能更容易带来肢体的扭曲。以下展示一些挑选后的结果：</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="superhero-1.png" /></div><div class="group-image-wrap"><img src="superhero-2.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="superhero-3.png" /></div><div class="group-image-wrap"><img src="superhero-4.png" /></div><div class="group-image-wrap"><img src="superhero-5.png" /></div></div></div><h2 id="参考资料">参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>https://github.com/kohya-ss/sd-scripts <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>https://github.com/darkstorm2150/sd-scripts/blob/main/docs <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>[Guide] Make your own Loras, easy and free. https://civitai.com/models/22530 <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Stable Diffusion Lora locon loha训练参数设置 - 凌璃的文章 - 知乎 https://zhuanlan.zhihu.com/p/618758020 <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>AIGC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
      <tag>AIGC</tag>
      
      <tag>stable diffusion</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Stable Diffusion]模型概览</title>
    <link href="/blog-main/2023/06/16/Stable-Diffusion-%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%A7%88/"/>
    <url>/blog-main/2023/06/16/Stable-Diffusion-%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%A7%88/</url>
    
    <content type="html"><![CDATA[<h2 id="前置">前置</h2><h3 id="代码库">代码库</h3><ul><li><strong>官方 GitHub 仓库</strong>：<ul><li>基于 Latent Diffusion 仓库搭建，常用于科研人员做基于 Stable Diffusion 的实验和开发。</li><li>Stable Diffusion v2.0 ~ v2.1: <a href="https://github.com/Stability-AI/StableDiffusion" class="uri">https://github.com/Stability-AI/StableDiffusion</a></li><li>Stable Diffusion v1.1 ~ v1.5: <a href="https://github.com/runwayml/stable-diffusion" class="uri">https://github.com/runwayml/stable-diffusion</a></li><li>Stable Diffusion v1.1 ~ v1.4: <a href="https://github.com/compvis/stable-diffusion" class="uri">https://github.com/compvis/stable-diffusion</a></li></ul></li><li><strong>HuggingFace 🧨Diffusers</strong>:<ul><li>HuggingFace 的扩散模型库，有着自己的一套 API，支持加载/微调 Stable Diffusion 模型。</li><li>GitHub：<a href="https://github.com/huggingface/diffusers" class="uri">https://github.com/huggingface/diffusers</a></li><li>文档：<a href="https://huggingface.co/docs/diffusers/index" class="uri">https://huggingface.co/docs/diffusers/index</a></li></ul></li><li><strong>webui</strong><ul><li>在 stablediffusion 和许多其他仓库（如 ESRGAN, CodeFormer 等）的基础上使用 gradio 搭建的 web 图形化界面，方便易用。</li><li>GitHub：<a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" class="uri">https://github.com/AUTOMATIC1111/stable-diffusion-webui</a></li></ul></li></ul><h3 id="文件格式">文件格式</h3><p>训练好的模型 checkpoint 有如下三种保存格式：</p><ul><li><strong><code>.ckpt</code></strong>：适用于官方 GitHub 仓库和 webui.</li><li><strong><code>.safetensors</code></strong>：HuggingFace 推出的文件格式，旨在更安全、更快速地加载权重，可用于 webui.</li><li><strong>Diffusers 格式</strong>：将 Stable Diffusion 的各个组件分别存储在子目录中，每个子目录包含 <code>.json</code> 配置文件和权重文件，适用于 HuggingFace Diffusers 库。</li></ul><p>举个例子，打开 <a href="https://huggingface.co/stabilityai/stable-diffusion-2-1/tree/main">Stable Diffusion v2.1 官方仓库</a>可以看到，里面既有 <code>.ckpt</code> 文件、又有 <code>.safetensors</code> 文件、还有按组件分目录存储的配置文件和权重文件。<strong>事实上，上传者是把上述三种格式一股脑都传到了仓库中，我们按需下载即可</strong>。</p><h2 id="官方模型">官方模型</h2><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="versions.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="v1-1-to-v1-5.png" /></div><div class="group-image-wrap"><img src="model-variants.jpg" /></div></div></div><blockquote><p>注：v2.0 指代 v2-base，v2.0-v 指代 v2.</p></blockquote><p>FID Score 指示图像质量，CLIP Score 指示图像与文本的匹配程度。对于每个版本的模型，遍历 8 种 cfg-scale (classifier-free guidance scale)，连接成一条曲线。理论上，cfg-scale 越大，图像与文本越匹配，但图像质量可能会被损坏（例如颜色过饱和），这解释了曲线后半段的上升。但当 cfg-scale 比较小时，增大 cfg-scale 反而能让图像质量更好，推测这与训练时条件生成相对无条件生成占比更大有关。</p><ul><li><strong>Stable Diffusion v2.1</strong><ul><li><strong>说明</strong>：以 v2 权重初始化，在同一数据集上继续训练 55k steps，768x768 分辨率；然后在减轻了 NSFW 过滤限制的数据集上继续训练另外 155k steps.</li><li><strong>下载</strong>：<a href="https://huggingface.co/stabilityai/stable-diffusion-2-1/tree/main">链接</a>，包含全部三种文件格式，可按需下载。</li><li><strong>在官方 GitHub 仓库中使用</strong>：见 <a href="https://github.com/Stability-AI/stablediffusion#reference-sampling-script">README</a>.</li><li><strong>在 Diffusers 库中使用</strong>：见 <a href="https://huggingface.co/stabilityai/stable-diffusion-2-1#examples">Examples</a>.</li></ul></li><li><strong>Stable Diffusion v2.1-base</strong><ul><li><strong>说明</strong>：以 v2-base 权重初始化，继续训练 220k steps，512x512 分辨率，减轻了数据集的 NSFW 过滤限制。</li><li><strong>下载</strong>：<a href="https://huggingface.co/stabilityai/stable-diffusion-2-1-base/tree/main">链接</a>，包含全部三种文件格式，可按需下载。</li><li><strong>在官方 GitHub 仓库中使用</strong>：见 <a href="https://github.com/Stability-AI/stablediffusion#reference-sampling-script">README</a>.</li><li><strong>在 Diffusers 库中使用</strong>：见 <a href="https://huggingface.co/stabilityai/stable-diffusion-2-1-base#examples">Examples</a>.</li></ul></li><li><strong>Stable Diffusion x4-upscaler</strong><ul><li><strong>说明</strong>：4 倍上采样器。在 LAION 数据集中大于 2048x2048 分辨率的 10M 张图片上训练 1.25M steps.</li><li><strong>下载</strong>：<a href="https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler/tree/main">链接</a>，包含全部三种文件格式，可按需下载。</li><li><strong>在官方 GitHub 仓库中使用</strong>：见 <a href="https://github.com/Stability-AI/stablediffusion#image-upscaling-with-stable-diffusion">README</a>.</li><li><strong>在 Diffusers 库中使用</strong>：见 <a href="https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler#examples">Examples</a>.</li></ul></li><li><strong>Stable Diffusion v2</strong><ul><li><strong>说明</strong>：以 v2-base 权重初始化，在同一数据集上训练 150k steps，但训练目标换成了 v-prediction. 随后以 768x768 分辨率又训练了 140k steps.</li><li><strong>下载</strong>：<a href="https://huggingface.co/stabilityai/stable-diffusion-2/tree/main">链接</a>，包含全部三种文件格式，可按需下载。</li><li><strong>在官方 GitHub 仓库中使用</strong>：见 <a href="https://github.com/Stability-AI/stablediffusion#reference-sampling-script">README</a>.</li><li><strong>在 Diffusers 库中使用</strong>：见 <a href="https://huggingface.co/stabilityai/stable-diffusion-2#examples">Examples</a>.</li></ul></li><li><strong>Stable Diffusion v2-depth</strong><ul><li><strong>说明</strong>：以深度图为条件的图像生成。以 v2-base 权重初始化，训练 200k steps. 添加了一个额外的输入通道用于输入 <a href="https://github.com/isl-org/MiDaS">MiDaS</a> 模型预测的深度图。</li><li><strong>下载</strong>：<a href="https://huggingface.co/stabilityai/stable-diffusion-2-depth/tree/main">链接</a>，包含全部三种文件格式，可按需下载。</li><li><strong>在官方 GitHub 仓库中使用</strong>：见 <a href="https://github.com/Stability-AI/stablediffusion#depth-conditional-stable-diffusion">README</a>.</li><li><strong>在 Diffusers 库中使用</strong>：见 <a href="https://huggingface.co/stabilityai/stable-diffusion-2-depth#examples">Examples</a>.</li></ul></li><li><strong>Stable Diffusion v2-inpainting</strong><ul><li><strong>说明</strong>：图像填充模型。以 v2-base 权重初始化，训练 200k steps. 参考 <a href="https://github.com/saic-mdal/lama">LaMa</a> 的掩码生成策略，加上输入图像的 VAE 隐空间表示一同作为 UNet 的条件。</li><li><strong>下载</strong>：<a href="https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/tree/main">链接</a>，包含全部三种文件格式，可按需下载。</li><li><strong>在官方 GitHub 仓库中使用</strong>：见 <a href="https://github.com/Stability-AI/stablediffusion#image-inpainting-with-stable-diffusion">README</a>.</li><li><strong>在 Diffusers 库中使用</strong>：见 <a href="https://huggingface.co/stabilityai/stable-diffusion-2-inpainting#examples">Examples</a>.</li></ul></li><li><strong>Stable Diffusion v2-base</strong><ul><li><strong>说明</strong>：从头训练，在过滤后的 LAION-5B 数据集上以 256x256 分辨率训练 550k steps. 接着在同样的数据集上以 512x512 分辨率训练 850k steps.</li><li><strong>与 v1 对比</strong>：<ul><li>使用更大的文本编码器：从 v1 的 CLIP ViT-L/14 换成了 <a href="https://github.com/mlfoundations/open_clip">OpenCLIP-ViT/H</a>.</li><li>使用 CLIP 倒数第二层而非倒数第一层，与 Imagen 和 novelai 一样。</li><li>UNet 的结构略有调整：attention head 固定 dim=64 而不是固定 head 数量，不影响参数量；由于换了文本编码器，text embedding dim 从 768 变成了 1024，使得 cross-attention 的参数量有轻微变化。</li></ul></li><li><strong>下载</strong>：<a href="https://huggingface.co/stabilityai/stable-diffusion-2-base/tree/main">链接</a>，包含全部三种文件格式，可按需下载。</li><li><strong>在官方 GitHub 仓库中使用</strong>：见 <a href="https://github.com/Stability-AI/stablediffusion#reference-sampling-script">README</a>.</li><li><strong>在 Diffusers 库中使用</strong>：见 <a href="https://huggingface.co/stabilityai/stable-diffusion-2-base#examples">Examples</a>.</li></ul></li><li><strong>Stable Diffusion v1.5</strong><ul><li><strong>说明</strong>：以 v1.2 权重初始化，在 laion-aesthetics v2 5+ 数据集上以 512x512 分辨率训练 595k steps，同时以 10% 的概率丢弃文本条件来提升 classifier-free guidance 性能。</li><li><strong>下载</strong>：<a href="https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main">链接</a>，包含全部三种文件格式，可按需下载。其中，以 <code>v1-5-pruned-emaonly</code> 命名的文件只包含 EMA 权重，适用于推理；以 <code>v1-5-pruned</code> 命名的文件包含 EMA 权重和 non-EMA 权重，适用于接着微调。</li><li><strong>在官方 GitHub 仓库中使用</strong>：见 <a href="https://github.com/runwayml/stable-diffusion">README</a>.</li><li><strong>在 Diffusers 库中使用</strong>：见 <a href="https://huggingface.co/runwayml/stable-diffusion-v1-5#diffusers">Examples</a>.</li></ul></li><li><strong>Stable Diffusion v1.5 inpainting</strong><ul><li><strong>说明</strong>：图像填充模型。以 v1.5 权重初始化，在 laion-aesthetics v2 5+ 数据集上以 512x512 分辨率训练 440k steps，同时以 10% 的概率丢弃文本条件。为了支持 inpainting，给 UNet 增加了 5 个额外的输入通道（4 个用于编码输入图片，1 个用于掩码本身），它们以零值初始化。训练时生成随机掩码，并且以 25% 的概率遮掉整张图。</li><li><strong>下载</strong>：<a href="https://huggingface.co/runwayml/stable-diffusion-inpainting">链接</a>，包含 <code>.ckpt</code> 和 Diffusers 两种格式，可按需下载。</li><li><strong>在官方 GitHub 仓库中使用</strong>：见 <a href="https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion">README</a>.</li><li><strong>在 Diffusers 库中使用</strong>：见 <a href="https://huggingface.co/runwayml/stable-diffusion-inpainting#diffusers">Examples</a>.</li></ul></li><li><strong>Stable Diffusion v1.4</strong><ul><li><strong>说明</strong>：以 v1.2 权重初始化，在 laion-aesthetics v2 5+ 数据集上以 512x512 分辨率训练 225k steps，同时以 10% 的概率丢弃文本条件来提升 classifier-free guidance 性能。</li><li><strong>下载</strong>：<a href="https://huggingface.co/CompVis/stable-diffusion-v1-4">Diffusers 格式</a>｜<a href="https://huggingface.co/CompVis/stable-diffusion-v-1-4-original"><code>.ckpt</code> 格式</a>，其中 <code>sd-v1-4.ckpt</code> 只包含 EMA 权重；<code>sd-v1-4-full-ema.ckpt</code> 包含 EMA 权重和 non-EMA 权重。</li></ul></li><li><strong>Stable diffusion v1.3</strong><ul><li><strong>说明</strong>：以 v1.2 权重初始化，在 laion-improved-aesthetics 数据集上以 512x512 分辨率训练 195k steps，同时以 10% 的概率丢弃文本条件来提升 classifier-free guidance 性能。</li><li><strong>下载</strong>：<a href="https://huggingface.co/CompVis/stable-diffusion-v1-3">Diffusers 格式</a>｜<a href="https://huggingface.co/CompVis/stable-diffusion-v-1-3-original"><code>.ckpt</code> 格式</a></li></ul></li><li><strong>Stable Diffusion v1.2</strong><ul><li><strong>说明</strong>：以 v1.1 权重初始化，在 laion-improved-aesthetics 数据集上以 512x512 分辨率训练 515k steps.</li><li><strong>下载</strong>：<a href="https://huggingface.co/CompVis/stable-diffusion-v1-2">Diffusers 格式</a>｜<a href="https://huggingface.co/CompVis/stable-diffusion-v-1-2-original"><code>.ckpt</code> 格式</a></li></ul></li><li><strong>Stable Diffusion v1.1</strong><ul><li><strong>说明</strong>：在 laion2B-en 数据集上以 256x256 分辨率训练 237k steps；在 laion-high-resolution 数据集上以 512x512 分辨率训练194k steps.</li><li><strong>下载</strong>：<a href="https://huggingface.co/CompVis/stable-diffusion-v1-1">Diffusers 格式</a>｜<a href="https://huggingface.co/CompVis/stable-diffusion-v-1-1-original"><code>.ckpt</code> 格式</a></li></ul></li></ul><p><strong>在 webui 中使用</strong>：下载 <code>.ckpt</code> 或 <code>.safetensors</code> 格式的权重，放在 <code>./models/Stable-diffusion</code> 目录下，刷新 webui 顶部的模型选择框即可看到目录下所有的模型。</p><h2 id="vae">VAE</h2><p>VAE 是 Stable Diffusion (Latent Diffusion) 的一个组件，其编码器将输入图片从图像空间 <span class="math inline">\(\mathbb R^{3\times H\times W}\)</span> 映射到隐空间 <span class="math inline">\(\mathbb R^{4\times H/f\times W/f}\)</span>（<span class="math inline">\(f\)</span> 为下采样系数），在隐空间中通过扩散模型学习数据分布后，再由解码器映射回图像空间。VAE 编码图像的高频细节信息，使得隐空间中的扩散模型能把精力着重花在学习图像的 high-level 语义上，从而摆脱繁琐的细节。</p><p>理论上，扩散模型的学习依赖于预训练好的 VAE，但我们也可以在训练好扩散模型后，固定住扩散模型而微调 VAE，使得图片的细节和色调更加完善。因此，我们有时能见到人们单独发布 VAE 权重。</p><p><strong>在 webui 中使用</strong></p><p>常见的模型权重一般会内置 VAE，无需再额外挂载；但有些模型没有内置 VAE，或者额外训练了自己的 VAE，这时就需要选择正确的 VAE 挂载上去。VAE 的选择框被淹没在了设置标签页里的众多设置项之中，我们可以在 Settings → User interface → Quicksettings list 中添加 sd_vae 一项，这样选择框就会被固定在整个页面上方，方便随时切换。</p><p>在 webui 中，有两种使用 VAE 的方式：</p><ol type="1"><li>将 VAE 权重命名为对应模型相同的名字（例如 <code>&lt;model name&gt;.vae.pt</code>），放在 <code>./models/Stable-diffusion</code> 目录下，那么 webui 会自动加载与模型名匹配的 VAE.</li><li>将 VAE 权重放在 <code>./models/VAE</code> 下，在设置项中选择使用哪个 VAE.</li></ol><p><strong>社区模型举例</strong></p><ul><li><a href="https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.safetensors">sd-vae-ft-mse-original</a>：StabilityAI 在原始 kl-f8 autoencoder（使用 KL 作为正则项，下采样到原图大小的 1/8）上微调而来。数据集为 LAION-Aesthetics 和 LAION-Humans 按 1:1 比例混合。首先加载原 EMA 权重并以 L1 + LPIPS 为目标训练 313198 steps，然后以 MSE + 0.1 * LPIPS 为目标训练另外 280k steps. 只微调了 decoder 部分。</li><li><a href="https://huggingface.co/hakurei/waifu-diffusion-v1-4/tree/main/vae">kl-f8-anime2.ckpt</a>：Waifu Diffusion 的 VAE，顾名思义，专门对日漫风格微调。</li><li><a href="https://huggingface.co/NoCrypt/blessed_vae/blob/main/blessed2.vae.pt">blessed_vae</a>：据称能提高对比度。</li><li><a href="https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/VAEs/orangemix.vae.pt">orangemix</a>：OrangeMixs 系列模型的 VAE.</li></ul><p><strong>测试</strong>：基底模型为 AnythingV5Ink_v32Ink.</p><p><img src="vae.png" /></p><h2 id="微调模型-dreambooth-合并模型">微调模型 / Dreambooth / 合并模型</h2><p>在 CivitAI 社区中以 "Checkpoint" 标签标识的模型，即通常说的基底模型（底模）。这些模型或是通过微调、或是通过合并权重得到，往往有着显著的风格，在特定领域上的出图效果往往远好于 Stable Diffusion 官方权重。</p><p>其中，Dreambooth 指的是一种微调整个模型使之生成指定对象（即个性化生成）的方法，由 Google 在<a href="https://arxiv.org/abs/2208.12242">这篇论文</a>中提出。在 LoRA（详见后文）被引入 Stable Diffusion 之后，人们也会依照 DreamBooth 的方式微调 LoRA.</p><p><strong>社区模型举例</strong></p><ul><li><a href="https://civitai.com/models/6424">ChilloutMix</a>：写实人物</li><li><a href="https://civitai.com/models/43331">majicMIX realistic</a>：写实人物，偏好东亚脸</li><li><a href="https://civitai.com/models/4384">DreamShaper</a>：2.5D / 写实</li><li><a href="https://civitai.com/models/4201">Realistic Vision</a>：写实</li><li><a href="https://civitai.com/models/7371">ReV Animated</a>：2.5D</li><li><a href="https://civitai.com/models/4468">Counterfeit-V3.0</a>：2D，日漫</li><li><a href="https://civitai.com/models/15003">CyberRealistic</a>：写实人物，偏好欧美脸</li><li><a href="https://civitai.com/models/9409">Anything</a>：2D，日漫</li><li><a href="https://civitai.com/models/81458">AbsoluteReality</a>：写实</li><li><a href="https://civitai.com/models/25694">epiCRealism</a>：写实</li><li><a href="https://civitai.com/models/36520">GhostMix</a>：2.5D，CG</li></ul><p>另外，该<a href="https://rentry.org/sdmodels">网站</a>收集了一些模型。</p><h2 id="textual-inversion-embeddings">Textual Inversion Embeddings</h2><p>由<a href="https://arxiv.org/abs/2208.01618">这篇论文</a>提出的个性化生成方法，通过对一个特殊 text token 做 inversion，使之绑定到某特定对象上。该方法不改动基底模型的权重，仅需保存得到的 text embedding，所以文件非常小（几十到几百 KB）。但毕竟没有微调模型权重，其效果往往不如 LoRA.</p><p>特殊用法：对低质量图片做 textual inversion，那么结果可作为 negative embedding 放入 negative prompts 中。</p><p><strong>在 webui 中使用</strong></p><p>将训练/下载的 textual inversion 文件放在 <code>./embeddings</code> 目录下，使用时找到 Generate 按钮下方第三个按钮 Show/hide extra networks，在弹出的页面中点击想使用的 embedding，则相关词语会出现在 prompt 框中。注意不要把 negative embedding 写在正向 prompt 里面了。</p><p><strong>社区模型举例</strong>（Negative embeddings）</p><ul><li><a href="https://civitai.com/models/7808/easynegative">EasyNegative</a></li><li><a href="https://civitai.com/models/71961/fast-negative-embedding-fastnegativev2">Fast Negative Embedding</a></li><li><a href="https://civitai.com/models/5224/bad-artist-negative-embedding">Bad artist Negative embedding</a></li><li><a href="https://civitai.com/models/4629/deep-negative-v1x">Deep Negative V1.x</a></li></ul><h2 id="lora-模型">LoRA 模型</h2><p>LoRA 是一种参数高效微调方法（PEFT），最早由<a href="https://arxiv.org/abs/2106.09685">这篇论文</a>提出并应用于微调语言大模型之中，后来由<a href="https://github.com/cloneofsimo/lora">这个代码库</a>引入到对 Stable Diffusion 模型的微调之中。LoRA 并不改变原模型的权重，而是在线性层旁边新增一个下采样-上采样的支路，通过训练这个支路来完成微调。因此，同一个基底 Stable Diffusion 模型可以搭载不同的 LoRA 使用，具有很高的灵活性。由于 LoRA 支路网络的参数量小（文件大小在几十到几百 MB 左右），相比微调整个模型，对算力的需求更加友好，并且也能达到不错的效果，因此很快受到大家的热烈欢迎，成为了目前最流行的微调 Stable Diffusion 的方法之一。</p><p><strong>在 webui 中使用</strong></p><p>将训练/下载的 LoRA 模型放在 <code>./models/Lora</code> 目录下，使用时找到 Generate 按钮下方第三个按钮 Show/hide extra networks，在弹出的页面中点击想使用的 LoRA，则 <code>&lt;lora:xxx:1&gt;</code> 会出现在 prompt 框中，其中 <code>1</code> 代表 LoRA 权重，可以按需修改。</p><p><strong>使用疑问</strong></p><ol type="1"><li>按理说，使用 LoRA 时需要配合训练用到的基底模型才能 work，但是实测发现同一个 LoRA 有时能够在不同基底模型上产生效果，这是为什么？猜测是因为这些基底模型都从同一个权重（如官方权重）微调而来，参数上或多或少差异不大。</li><li>如何融合多个 LoRA？直接将参数加权求和即可。</li></ol><p><strong>社区模型举例</strong></p><ul><li><a href="https://civitai.com/models/58390/detail-tweaker-lora-lora">Detail Tweaker LoRA</a>：细节调整</li><li><a href="https://civitai.com/models/64471/real-mechanical-parts">Real Mechanical Parts</a>：人体机械</li><li><a href="https://civitai.com/models/26124/koreandolllikeness-v20">KoreanDollLikeness (v2.0)</a></li><li><a href="https://civitai.com/models/48363/taiwandolllikeness-v20">TaiwanDollLikeness (v2.0)</a></li><li><a href="https://civitai.com/models/25995/blindbox">blindbox/大概是盲盒</a>：盲盒</li><li><a href="https://civitai.com/models/83816/spider-gwen-commission-or-goofy-ai">Spider Gwen (commission) | Goofy Ai</a>：格温蜘蛛侠</li><li><a href="https://civitai.com/models/36805/fonglets-hermione-granger-philosphers-stone">Fonglets Hermione Granger (Philosphers Stone)</a>：魔法石赫敏</li></ul><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>https://github.com/compvis/stable-diffusion <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>https://github.com/runwayml/stable-diffusion <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>https://github.com/Stability-AI/StableDiffusion <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>https://cyberes.github.io/stable-diffusion-models/ <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>文生图模型之Stable Diffusion - 小小将的文章 - 知乎 https://zhuanlan.zhihu.com/p/617134893 <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>StableDiffusion模型资源探索食用指南 - SeASnAkE的文章 - 知乎 https://zhuanlan.zhihu.com/p/597504900 <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>在StableDiffusion中说起VAE时,我们在谈论什么? - SeASnAkE的文章 - 知乎 https://zhuanlan.zhihu.com/p/599129815 <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>AIGC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
      <tag>AIGC</tag>
      
      <tag>stable diffusion</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Stable Diffusion]webui部署(Linux)</title>
    <link href="/blog-main/2023/06/14/Stable-Diffusion-webui%E9%83%A8%E7%BD%B2(Linux)/"/>
    <url>/blog-main/2023/06/14/Stable-Diffusion-webui%E9%83%A8%E7%BD%B2(Linux)/</url>
    
    <content type="html"><![CDATA[<p>Github: <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" class="uri">https://github.com/AUTOMATIC1111/stable-diffusion-webui</a></p><h2 id="部署环境">部署环境</h2><ul><li>操作系统：Ubuntu Server 22.04（无图形界面）</li><li>GPU：一张 NVIDIA RTX 3080Ti 显卡（12 GB 显存）</li></ul><p>准备部署完成后通过其他电脑的浏览器访问服务器的 webui 服务。</p><h2 id="全自动安装">全自动安装</h2><p>官方给出的 Linux 安装指导非常简单：<strong>下载 <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/blob/master/webui.sh"><code>webui.sh</code></a>，运行它，然后一切都会被全自动地下载、安装完成</strong>。</p><p>但是我是不情愿让别人对我的电脑做手脚而自己却不知情的，所以决定研究一下 <code>webui.sh</code>. 在查看其内容之后，我发现它无非是干这么几件事：</p><ol type="1"><li>下载 webui 仓库；</li><li>在 <code>./venv</code> 下创建并激活 python 虚拟环境；</li><li>调用 <code>launch.py</code> 安装需要的包及下载需要的仓库；</li><li>启动 webui.</li></ol><p>虽然 <code>webui.sh</code> 帮我们全自动地完成了这些工作，但这里面有几个问题：</p><ul><li>它使用 venv 管理环境，而我更倾向于使用 conda 管理环境；</li><li>由于网络原因，希望把 <code>pip install</code> 换到国内镜像源；</li><li>由于网络原因，<code>git clone</code> 有时无法访问导致失败。</li></ul><p>对于第二个问题，可以全局设置镜像，或者在运行 <code>webui.sh</code> 时设置环境变量 <code>INDEX_URL</code>；对于第三个问题，可以直接把代码里出现的 <code>github.com</code> 改成镜像站。但第一个问题无法简单地解决，因此我还是决定手动分解步骤安装，见下一节。</p><h2 id="分解步骤安装">分解步骤安装</h2><ol type="1"><li><p><strong>下载 stable-diffusion-webui 仓库</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git<br>cd stable-diffusion-webui<br></code></pre></td></tr></table></figure></li><li><p><strong>下载其他仓库</strong></p><p>webui 仅仅是一个外层界面包装，核心功能是通过其他仓库（或以仓库的形式、或以安装包的形式）支撑的。其他仓库需放在 <code>./repositories</code> 文件夹下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir repositories<br></code></pre></td></tr></table></figure><p>然后依次下载以下仓库，并切换到合适的版本以避免可能的版本适配问题：</p><ul><li><p><strong>stablediffusion</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">git clone https://github.com/Stability-AI/stablediffusion.git ./repositories/stable-diffusion-stability-ai<br>git -C ./repositories/stable-diffusion-stability-ai checkout cf1d67a6fd5ea1aa600c4df58e5b47da45f6bdbf<br></code></pre></td></tr></table></figure></li><li><p><strong>taming-transformers</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">git clone https://github.com/CompVis/taming-transformers.git ./repositories/taming-transformers<br>git -C ./repositories/taming-transformers checkout 24268930bf1dce879235a7fddd0b2355b84d7ea6<br></code></pre></td></tr></table></figure></li><li><p><strong>k-diffusion</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">git clone https://github.com/crowsonkb/k-diffusion.git ./repositories/k-diffusion<br>git -C ./repositories/k-diffusion checkout c9fe758757e022f05ca5a53fa8fac28889e4f1cf<br></code></pre></td></tr></table></figure></li><li><p><strong>CodeFormer</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">git clone https://github.com/sczhou/CodeFormer.git ./repositories/CodeFormer<br>git -C ./repositories/CodeFormer checkout c5b4593074ba6214284d6acd5f1719b6c5d739af<br></code></pre></td></tr></table></figure></li><li><p><strong>BLIP</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">git clone https://github.com/salesforce/BLIP.git ./repositories/BLIP<br>git -C ./repositories/BLIP checkout 48211a1594f1321b00f14c9f7a5b4813144b2fb9<br></code></pre></td></tr></table></figure></li></ul></li><li><p><strong>建立并激活新 conda 环境</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">conda create -n sd-webui python=3.10<br>conda activate sd-webui<br></code></pre></td></tr></table></figure></li><li><p><strong>安装 requirements.txt 中的依赖包</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install torch torchvision -i https://pypi.tuna.tsinghua.edu.cn/simple<br>pip install tb-nightly<br>pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple<br></code></pre></td></tr></table></figure><div class="note note-info">            <p>说明：</p><ul><li>必须先安装 <code>torch</code>：因为 <code>basicsr</code> 的安装程序会 <code>import torch</code>.</li><li>安装 <code>tb-nightly</code>：清华源缺少这个包，但安装 <code>basicsr</code> 时需要它，所以先不换源把它安装上。</li></ul>          </div></li><li><p><strong>安装其他依赖包</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install ftfy regex tqdm -i https://pypi.tuna.tsinghua.edu.cn/simple<br>pip install git+https://github.com/openai/CLIP.git<br>pip install open_clip_torch xformers -i https://pypi.tuna.tsinghua.edu.cn/simple<br>pip install -r repositories/CodeFormer/requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple<br></code></pre></td></tr></table></figure></li><li><p><strong>下载基础模型</strong></p><p>从<a href="https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors">链接</a>下载基础模型（Stable Diffusion v2.1，safetensors 格式，5.21 GB），放在 <code>./models/Stable-diffusion/</code> 下。</p></li><li><p><strong>启动 webui</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python webui.py<br></code></pre></td></tr></table></figure><p>服务默认运行在 <code>127.0.0.1:7860</code> 上。</p><div class="note note-info">            <p>如果部署在远程服务器上，想通过其他电脑浏览器访问服务，则需要添加 <code>--listen</code> 参数，这样服务运行在服务器的 <code>0.0.0.0:7860</code> 上，我们在浏览器输入 <code>&lt;ip&gt;:7860</code> 才能访问。</p>          </div><div class="note note-info">            <p>推荐添加 <code>--xformers</code> 参数，使用 xformers 优化时间和空间。</p>          </div><div class="note note-info">            <p>添加 <code>--port xxxx</code> 参数更改端口。</p>          </div><div class="note note-info">            <p>事实上，上述命令等价于：<code>python launch.py --skip-prepare-environment</code>，其中 <code>--skip-prepare-environment</code> 表示不需要自动激活 venv 并安装依赖包了，因为我们已经准备好了 conda 环境。</p>          </div></li></ol>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>AIGC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
      <tag>AIGC</tag>
      
      <tag>stable diffusion</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>StyleGAN 系列</title>
    <link href="/blog-main/2023/06/08/StyleGAN-%E7%B3%BB%E5%88%97/"/>
    <url>/blog-main/2023/06/08/StyleGAN-%E7%B3%BB%E5%88%97/</url>
    
    <content type="html"><![CDATA[<h2 id="stylegan">StyleGAN</h2><p>如果要说 StyleGAN 的最大的贡献，无疑是改变了传统的生成器架构，通过把隐变量分层引入到 backbone 网络，揭示了网络的各层能够控制生成图像的不同抽象程度的语义，从而在一定程度上实现了无监督特征解耦。另外，作者提出的 FFHQ 数据集也是一个很大的贡献，在之后的生成模型研究乃至 low-level vision 领域中都经常用到。</p><h3 id="网络架构设计">网络架构设计</h3><p><img src="stylegan-arch.png" width=60% /></p><p>在 StyleGAN 以前，传统生成器的网络架构通常如上左图所示——隐变量 <span class="math inline">\(\mathbf z\)</span> 输入给网络的第一层，经过分辨率逐渐增大网络模块后输出最终结果；而 StyleGAN 将隐变量 <span class="math inline">\(\mathbf w\)</span> 给到了生成器的各层，网络第一层的输入仅仅是一个（可学习的）常数向量，如上右图所示。这里，有三个问题需要详细说明：</p><ol type="1"><li><p><strong>为什么要用一个 mapping network（8 层 MLP）把隐变量 <span class="math inline">\(z\)</span> 映射成 <span class="math inline">\(w\)</span>，再给到生成器网络？</strong></p><p>一般而言，隐变量 <span class="math inline">\(\mathbf z\in\mathcal Z\)</span> 采样自标准高斯分布或者均匀分布，这些分布都是各向同性的。然而，图像的属性之间可能并非如此。例如，在头发长度与性别这两个属性的组合之中，长发男子的出现频率较低。极端情况下，也许数据集中并没有长发男子，那么模型为了在各向同性的隐空间中编码这两个属性，势必会引起较大的扭曲，增加学习难度。为此，作者引入 mapping network 对 <span class="math inline">\(\mathbf z\)</span> 做非线性变换，让模型自己学习一个隐空间 <span class="math inline">\(\mathcal W\)</span>，那么 <span class="math inline">\(\mathcal W\)</span> 的扭曲程度就可以得到缓解，如下图所示：</p><p><img src="stylegan-w.png" width=50% /></p><p>实验证明，mapping network 确实能提高生成图像的质量，也能帮助更好的解耦。</p><p>值得注意的是，论文中作者对 <span class="math inline">\(\mathbf w\)</span> 采用了 truncation trick 来提高可视化的图像质量。而且，受益于 StyleGAN 的设计，我们可以只对低分辨率阶段（<span class="math inline">\(4^2\sim32^2\)</span>）使用 truncation trick，从而保留高频细节的丰富性。具体而言，由于 <span class="math inline">\(\mathcal W\)</span> 是学习出来的，我们无法直接对其截断，因此作者首先计算 <span class="math inline">\(\mathbf w\)</span> 的平均值：<span class="math inline">\(\bar{\mathbf w}=\mathbb E_{\mathbf z\sim \mathcal P(\mathbf z)}[f(\mathbf z)]\)</span>，然后对于采样出的 <span class="math inline">\(\mathbf w\)</span>，将其与 <span class="math inline">\(\bar{\mathbf w}\)</span> 做插值来实现类似于截断的效果：<span class="math inline">\(\mathbf w&#39;=\bar{\mathbf w}+\psi(\mathbf w-\bar{\mathbf w})\)</span>.</p></li><li><p><strong><span class="math inline">\(w\)</span> 是如何融入生成器网络的（即图中的 AdaIN 具体是怎么操作的）？</strong></p><p>AdaIN，即 Adaptive InstanceNorm，源自于风格迁移工作，这也是 StyleGAN 名字中 "Style" 的来源。我们首先需要明确 InstanceNorm (IN) 是在哪些维度上做归一化的，这里借用 GroupNorm 论文的图：</p><p><img src="norm.png" width=70% /></p><p>可以看到，InstanceNorm 是在每张图片的<strong>每个单通道特征图内</strong>做归一化的，注意与 LayerNorm 区分开。</p><p>回到 StyleGAN，从上文的架构图中可以看到，<span class="math inline">\(\mathbf w\)</span> 会被<strong>复制多份</strong>送到生成器网络的各个层次之中。在每个层次里，<span class="math inline">\(\mathbf w\)</span> 会先经过一个可学习的仿射变换（图中的 A 框，其实就是一个全连接层），得到所谓的“风格” <span class="math inline">\(\mathbf y\)</span>： <span class="math display">\[\mathbf y=A(\mathbf w)=(\mathbf y_s,\mathbf y_b)\]</span> 其中 <span class="math inline">\(\mathbf y_s,\mathbf y_b\in\mathbb R^C\)</span>，<span class="math inline">\(C\)</span> 是当前特征图的通道数。然后以 <span class="math inline">\(\mathbf y_s\)</span> 为 scale、<span class="math inline">\(\mathbf y_b\)</span> 为 bias 逐通道<strong>调制（modulate）</strong>归一化后的结果： <span class="math display">\[\text{AdaIN}(\mathbf x_i,\mathbf y)=\mathbf y_{s,i}\frac{\mathbf x_i−\mu(\mathbf x_i)}{\sigma(\mathbf x_i)}+\mathbf y_{b,i}\quad(\text{for the }i\text{&#39;th channel})\]</span> 换句话说，就是把原本第 <span class="math inline">\(i\)</span> 个通道的特征图 <span class="math inline">\(\mathbf x_i\)</span> 的均值和标准差强行赋值为 <span class="math inline">\(\mathbf y_{b,i}\)</span> 和 <span class="math inline">\(\mathbf y_{s,i}\)</span>.</p></li><li><p><strong>最右侧的 noise 是什么？有什么作用？</strong></p><p>Noise 的添加是为了给生成的结果引入一些随机微小扰动，这些扰动会改变发丝、皱纹、胡子、毛孔等细节。与 <span class="math inline">\(\mathbf w\)</span> 不同的是，给到不同层次的 noise 是单独采样、互相独立的。这些 noise 是单通道的高斯噪声，通过可学习的逐通道缩放系数广播（broadcast）为对应特征图的 shape（图中的 B 框），加到卷积后的特征图上。</p><p>一个自然的问题是，为什么 noise 不会上升到与 <span class="math inline">\(\mathbf w\)</span> 一样的地位，大幅度地影响生成结果呢？作者认为，这与风格迁移文献观察到的现象有关——具有空间不变性的统计量，如 Gram 矩阵、逐通道均值、逐通道方差等，能很好地编码一张图片的风格；相反，对空间位置敏感的特征编码具体的实体。在 StyleGAN 中，风格 <span class="math inline">\(\mathbf y\)</span> 通过 AdaIN 操作调制了特征图的均值和方差，因而能控制生成图像的姿态、光照等全局信息；而 noise 是各像素独立的，因此自然而然地被用来控制局部的随机变化。</p></li></ol><h3 id="style-mixing">Style Mixing</h3><p>虽然我们现在清楚了 StyleGAN 的设计细节，但更重要的是要明白这样设计的动机，换句话说，StyleGAN 的设计有什么优势？其实在文章开头已经提到了，这样的设计使得不同抽象程度的语义与网络不同层次的隐变量形成了对应关系，在一定程度上实现了特征解耦，我们能够通过控制网络内部的各个层级来控制生成结果的各种语义。</p><p>文章用 <strong>style mixing</strong> 展示了这一点。所谓 style mixing，即随机选择网络的一个层次，在这之前使用 <span class="math inline">\(\mathbf w_1\)</span>、之后使用 <span class="math inline">\(\mathbf w_2\)</span>，那么生成的图像就会服从对应于 <span class="math inline">\(\mathbf w_1\)</span> 的粗糙语义（如人物姿态、发型、是否戴眼镜等）和对应于 <span class="math inline">\(\mathbf w_2\)</span> 的精细语义（如颜色、一些脸部特征等）。</p><p><img src="style-mixing.png" width=80% /></p><p>如上图所示，source A 和 source B 都是 StyleGAN 生成的图片。前三行是在 source A 的基础上，把粗糙层次（<span class="math inline">\(4^2\sim8^2\)</span>）的 <span class="math inline">\(\mathbf w\)</span> 换成 source B，于是交叉得到的图片在姿态、发型、脸型、眼镜上与 source B 保持一致，而色调和细节脸部特征像 source A；中间两行是把中间分辨率（<span class="math inline">\(16^2\sim32^2\)</span>）的 <span class="math inline">\(\mathbf w\)</span> 换成 source B；最后一行是把精细层次（<span class="math inline">\(64^2\sim1024^2\)</span>）的 <span class="math inline">\(\mathbf w\)</span> 换成 source B.</p><p>事实上，style mixing 并不只是一个可视化技巧，作者也将其作为一种正则化技巧用在了训练之中并取名为 mixing regularization，以避免网络认为相邻的两个 style 是相关的。</p><h3 id="解耦性能指标">解耦性能指标</h3><p>我们知道，评价生成模型的常用指标有 FID、Inception Score 等，但对一个生成模型的完整评价不应仅仅关注于结果，还应该关注网络内部的特征表示。一方面，我们希望网络从隐变量到图像的映射是足够“平滑”的，在隐空间中插值能够导致生成图像的平滑变化；另一方面 ，如果能找到隐空间的一些线性子空间，分别独立地控制图像的某种属性的变化，那便是更好的。因此，论文提出了两个指标——perceptual path length (PPL) 用于衡量生成器的映射是否平滑、linear separability 用于衡量隐空间的解耦程度。</p><p><strong>Perceptual path length</strong>：PPL 的思想非常简单，当我们在隐空间中插值时，相邻两个隐变量生成的图像应该比较相似，这种相似性可以使用感知距离 lpips 衡量。因此，当我们沿着隐空间 <span class="math inline">\(\mathcal Z\)</span> 的一条路径走时，这条路径总的长度可以定义为每一小段的 lpips 之和，再取段长趋近于 0. 实际操作中，我们取步长 <span class="math inline">\(\epsilon=10^{-4}\)</span> 将积分离散化为求和进行计算。平均 PPL 就是对所有路径端点对求平均，即： <span class="math display">\[l_{\mathcal Z}=\mathbb E_{\mathbf z_1,\mathbf z_2\sim P(\mathbf z),t\sim U(0,1)}\left[\frac{1}{\epsilon^2}d(G(\text{slerp}(\mathbf z_1,\mathbf z_2;t)),G(\text{slerp}(\mathbf z_1,\mathbf z_2;t+\epsilon)))\right]\]</span> 类似地，也可以在 <span class="math inline">\(\mathcal W\)</span> 空间做计算： <span class="math display">\[l_{\mathcal Z}=\mathbb E_{\mathbf z_1,\mathbf z_2\sim P(\mathbf z),t\sim U(0,1)}\left[\frac{1}{\epsilon^2}d(G(\text{lerp}(f(\mathbf z_1),f(\mathbf z_2);t)),G(\text{lerp}(f(\mathbf z_1),f(\mathbf z_2);t+\epsilon)))\right]\]</span> 一个细节是 <span class="math inline">\(\mathcal Z\)</span> 空间的插值用的是球面线性插值（slerp），而 <span class="math inline">\(\mathcal W\)</span> 空间是线性插值（lerp）。</p><p><strong>Linear separability</strong>：如果一个隐空间不是纠缠起来的，那么我们应该能够在其中找到一个方向，这个方向对应着某个属性的变化。换句话说，我们可以找到一个线性超平面，能根据对该属性做二分类（例如是否微笑、性别等）。</p><p>因此，作者首先使用 CelebA 数据集提供的 40 个属性标签在 CelebA-HQ 数据集上训练了分类网络，然后随机采样了 200000 张图像并用分类器对它们分类，保留置信度最高的一半，得到 100000 个有标签的隐向量。</p><p>对每个属性，作者拟合一个线性 SVM 来预测隐向量的标签。那么，条件熵 <span class="math inline">\(H(Y|X)\)</span> 就可以用来反映隐空间与生成图像对于这个属性表示的一致程度，其中 <span class="math inline">\(X\)</span> 是 SVM 预测的类别、<span class="math inline">\(Y\)</span> 是分类器打的标签类别。最终的得分为： <span class="math display">\[\exp\left(\sum_i H(Y_i|X_i)\right)\]</span> 其中 <span class="math inline">\(\exp\)</span> 将条件熵从对数域变换到线性域以方便比较。</p><h2 id="stylegan2">StyleGAN2</h2><h3 id="从-adain-到调制卷积">从 AdaIN 到调制卷积</h3><p>StyleGAN2 的主要动机是作者发现 StyleGAN 生成的图像（以及中间的特征图）中常常伴有“水滴”状的 artifact：</p><p><img src="stylegan2-artifact.png" width=80% /></p><p>经过排查，作者发现这是 AdaIN 的锅——AdaIN 的逐通道归一化和调制破坏了它们之间的相对大小信息，于是网络试图用一个局部的尖峰来主导统计量，进而帮助判断各通道信号的相对大小。</p><p>然而，AdaIN 是 StyleGAN 将输入 <span class="math inline">\(\mathbf w\)</span> 融入网络 backbone 的手段，是非常重要的组件。为此，我们首先回顾一下 StyleGAN 的生成器架构：</p><p><img src="stylegan2.png" width=100% /></p><p>图 (a) 即我们熟悉的 StyleGAN，由于问题在 AdaIN 上，为了更好地说明，我们把 AdaIN 拆成两部分——normalization 归一化和 modulation 调制，同时把卷积层的 weight 和 bias 显式地画出来，得到图 (b). 特别地，在拆分 AdaIN 后，我们可以把 「Mod-Conv-Add-Norm」视作一个 style block，如图 (b) 的灰色框框所示。</p><p>观察图 (b)，作者发现一个不协调之处——StyleGAN 的 bias 和 noise 都加在归一化层的前面，导致它们的影响受制于当前 style 的大小。因此，作者把 bias 和 noise 移动到了 style block 后面。进一步地，作者发现只需要对标准差做归一化和调制就足够了，因此去除了对均值的归一化和调制。最后，作者觉得没有必要对输入到第一层的那个常量做归一化和加 bias 与 noise，于是把它们去掉，最终得到了图 (c).</p><p>现在，为了解决 AdaIN 带来的问题，最直接的方案就是把归一化层（Norm）去掉，但是作者提出了一个更好的方案，其主要思想是让归一化操作基于特征图的期望的统计量、而非真实的统计量。首先，「Mod std」块可以融入卷积核之中——因为对特征图乘上 <span class="math inline">\(s_i\)</span>（即上一节的 <span class="math inline">\(\mathbf y_{s,i}\)</span>）再做卷积等价于直接对卷积核乘上 <span class="math inline">\(s_i\)</span>： <span class="math display">\[w&#39;_{ijk}=s_i\cdot w_{ijk}\]</span> 其中 <span class="math inline">\(i\)</span> 表示某输入通道、<span class="math inline">\(j\)</span> 表示某输出通道、<span class="math inline">\(k\)</span> 表示卷积核的某空间位置。接下来，不考虑当前特征图真实的统计量，而是假设特征图的各像素是独立同分布的单位方差随机变量，那么在卷积操作之后，第 <span class="math inline">\(j\)</span> 个通道的标准差为： <span class="math display">\[\sigma_j=\sqrt{\sum_{i,k}{w&#39;_{ijk}}^2}\]</span> 因此，「Norm std」块变成了基于这个假设的标准差的归一化。同理，其等价于直接将卷积核除以 <span class="math inline">\(\sigma_j\)</span>： <span class="math display">\[w&#39;&#39;_{ijk}=\frac{w&#39;_{ijk}}{\sqrt{\sum_{i,k}{w&#39;_{ijk}}^2+\epsilon}}\]</span> 作者称之为<strong>解调（demodulation）</strong>操作。</p><p>综上所述，图 (c) 变成了图 (d) 的形式，即是 StyleGAN2 网络的基本模块。实验证明，将 AdaIN 的归一化换成解调后，生成的图像以及特征图都没有了“水滴” artifact.</p><h3 id="正则化">正则化</h3><p>作者探索了两个正则化技巧：</p><p><strong>Lazy regularization</strong>：StyleGAN 采用的对抗损失是原始 GAN 的损失配合 <span class="math inline">\(R_1\)</span> 正则化。作者发现，并不需要每个 iteration 都计算正则项并与对抗损失一同优化，每 <span class="math inline">\(k=16\)</span> 个 minibatches 计算一次正则项即可，性能不会损失但减少了计算和内存开销。值得注意的是，由于我们实际上是用了 <span class="math inline">\(k+1\)</span> 个 iteration 完成原本的 <span class="math inline">\(k\)</span> 个 iteration，所以 Adam 优化器的参数要做相应调整： <span class="math display">\[\lambda&#39;=c\cdot\lambda,\quad\beta&#39;_1=(\beta_1)^c,\quad\beta&#39;_2=(\beta_2)^c\]</span> 其中 <span class="math inline">\(c=k/(k+1)\)</span>. 另外，正则项也乘上 <span class="math inline">\(k\)</span> 来保持总的梯度数量级不变。</p><p><strong>Path length regularization</strong>：在 StyleGAN 中作者提出了 PPL 来衡量生成器映射的平滑程度，然而这个指标并不能直接用作优化目标帮助训练，否则网络显然会坍缩到一个点上。因此，作者提出了一个新的正则化方法来鼓励生成器的平滑性。</p><p>作者希望在 <span class="math inline">\(\mathcal W\)</span> 中朝不同方向走相同距离的步长时，生成图像也会产生固定量级的变化，而这样的变化可以反映在回传到 <span class="math inline">\(\mathbf w\)</span> 的梯度上。因此，记生成器为 <span class="math inline">\(g(\mathbf w):\mathcal W\mapsto\mathcal Y\)</span>，这是一个向量到向量的映射，所以其 Jacobian 矩阵 <span class="math inline">\(\mathbf J_\mathbf w=\partial g(\mathbf w)/\partial\mathbf w\)</span> 就包含了 <span class="math inline">\(\mathbf w\)</span> 处的所有梯度。作者将正则化项定义为： <span class="math display">\[\mathbb E_{\mathbf w,\mathbf y\sim\mathcal N(\mathbf 0,\mathbf I)}\left(\Vert\mathbf J_{\mathbf w}^T\mathbf y\Vert_2-a\right)^2\]</span> 其中 <span class="math inline">\(\mathbf y\)</span> 是一张随机高斯噪声图像。为了避免显式的计算 Jacobian 矩阵，利用等式 <span class="math inline">\(\mathbf J_{\mathbf w}^T\mathbf y=\nabla_\mathbf w(g(\mathbf w)\cdot\mathbf y)\)</span>，我们可以通过反向传播方便地计算。<span class="math inline">\(a\)</span> 被设置为 <span class="math inline">\(\Vert\mathbf J_{\mathbf w}^T\mathbf y\Vert_2\)</span> 的指数移动平均，使得网络能够自适应地调整其大小。</p><h3 id="网络整体架构">网络整体架构</h3><p>第一节讲的是网络的基本模块，而这一节讲 StyleGAN2 的整体架构。StyleGAN 使用的是简单的 feedforward 设计，没有跳跃连接、残差块等。因此，作者决定探索更复杂的网络架构。</p><p><img src="stylegan2-arch.png" width=50% /></p><p>如图 (a) 所示，MSG-GAN 在生成器与判别器相同分辨率的层之间加入了跳跃连接，其中 tRGB 表示把特征图变为 RGB 图像，fRGB 反之。StyleGAN2 作者受其启发，将其简化为了图 (b)，最终输出图像是各个分辨率层转换的 RGB 图之和。另外，作者也设计了带残差连接的网络 (c)，类似于 LAPGAN. 生成器和判别器分别都有这三种设计，一共 9 种组合，作者在两个数据集上一一测试了它们的效果（有钱就是任性啊），最后发现 output skips 式生成器（图 (b)）与残差连接式判别器（图 (c)）的组合最好。</p><h2 id="stylegan2-ada">StyleGAN2-ADA</h2><h2 id="stylegan3">StyleGAN3</h2><h2 id="eigengan">EigenGAN</h2><p>前文介绍 StyleGAN 时说到，StyleGAN 揭示了生成器网络各层具有不同抽象程度的语义，并通过 style mixing 的方式进行了可视化。但是它毕竟没有<strong>显式</strong>地解耦出来——即没有显式地给出对应某个语义的隐空间方向。当然，我们能事后通过有监督的分类或其他手段来分析隐空间，从而找到一些语义对应的方向。但我们能不能在训练网络的同时就无监督地完成解耦呢？</p><p><img src="eigengan.png" width=100% /></p><p>EigenGAN 的网络架构如上图所示，可以看见它与 StyleGAN 一样采用的是分层加入隐变量的设计，不过每一层的隐空间都显示地表达了出来——<span class="math inline">\(\mathbf U_i\)</span> 由一组标准正交基向量构成，每个基向量方向被用来指示一种语义；对角矩阵 <span class="math inline">\(\mathbf L_i\)</span> 拉伸基向量的长度，即决定各个基向量的权重，也可视为一种维度选择；<span class="math inline">\(\mu_i\)</span> 是这个隐空间的原点。<span class="math inline">\(\mathbf U_i\)</span>、<span class="math inline">\(\mathbf L_i\)</span> 和 <span class="math inline">\(\mu_i\)</span> 以及卷积核都是可学习的，其中 <span class="math inline">\(\mathbf U_i\)</span> 的正交性由正则化项 <span class="math inline">\(\Vert\mathbf U_i^{\mathrm T}\mathbf U_i-\mathbf I\Vert_F^2\)</span> 来做约束。</p><p>在这样的设计下，随机采样的 <span class="math inline">\(\mathbf z_i\sim\mathcal N(\mathbf 0,\mathbf I)\)</span> 其实就是这个隐空间的相对坐标，即基向量的线性组合。因此，每个样本都对应着各层级隐空间的基向量的一种组合方式，因此我们有理由相信，在训练完成后不同基向量就会编码着各种不同的语义。事实上，一个更有力的证据是，只有一层的线性 EigenGAN 的解与 PCA 完全相同，相关推导可查看论文 Appendix A.</p><p>虽然都是分层设计，但与 StyleGAN 采用 AdaIN 或调制卷积相比，EigenGAN 采用更直接的加法来融入隐变量，因此也有着更清晰的 manifold：</p><p><img src="eigengan-manifold.png" width=100% /></p><p>通过改变某层某方向的隐变量大小并可视化出来，我们就能够知道这个方向代表的语义。实验显示不同层的不同方向确实学习到了不同的语义：</p><p><img src="eigengan-result.png" width=100% /></p><p>更多的结果请参阅原论文。</p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>GANs</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Midjourney 调研</title>
    <link href="/blog-main/2023/06/03/Midjourney-%E8%B0%83%E7%A0%94/"/>
    <url>/blog-main/2023/06/03/Midjourney-%E8%B0%83%E7%A0%94/</url>
    
    <content type="html"><![CDATA[<p><a href="https://www.midjourney.com/">官网</a> | <a href="https://docs.midjourney.com/">文档</a></p><div class="note note-warning">            <p><strong>特别说明：本文展示的所有图片都经过了大幅度的缩小和压缩处理，并非原图。</strong></p>          </div><h2 id="模型版本">模型版本</h2><p>使用 <code>--version</code> 或 <code>--v</code> 参数来指定版本。</p><ul><li><strong>v5.2</strong>：该版本有更好的颜色、对比度和结构，有略微更好的文本理解能力。对 <code>--stylize</code> 参数响应度更高。特别地，如果不想要 Midjourney 的默认审美风格，可以加参数 <code>--style raw</code>.</li><li><strong>v5.1</strong>：该版本有更强的<strong>默认审美</strong>，因此使用比较简单的 prompt 就能达到很好的效果。它能与 prompt 保持很高的一致性，生成的图像更清晰（sharp），还支持一些新的特性，例如 <code>--tile</code> 参数（详见下文）。特别地，如果不想要 Midjourney 的默认审美风格，可以加参数 <code>--style raw</code>.</li><li><strong>v5</strong>：相比 v5.1，v5 版本生成的图像会<strong>更像摄影</strong>，但需要更长的 prompts 来描述。</li><li><strong>Niji Model 5</strong>：Midjourney 和 Spellbrush 合作微调的版本，专注于日本漫画风格（anime style），使用时只需在 prompt 后添加 <code>--niji 5</code> 即可，还可以使用 <code>--style</code> 参数得到更细分的风格。</li></ul><p>官网的例子我就不放了。这里展示我自己尝试的一个例子：</p><table><tr><th style="text-align: center; width: 25%">prompt</th><th style="text-align: center; width: 25%">v5.2</th><th style="text-align: center; width: 25%">v5.1</th><th style="text-align: center; width: 25%">v5</th></tr><tr><td style="text-align: center">a cup of latte</td><td><img src="latte_v5_2.jpg"/></td><td><img src="latte_v5_1.jpg"/></td><td><img src="latte_v5.jpg"/></td></tr><tr><th style="text-align: center">v5.2 raw</th><th style="text-align: center">v5.1 raw</th><th style="text-align: center">niji 5</th><th style="text-align: center">v4</th></tr><tr><td><img src="latte_v5_2_raw.jpg"/></td><td><img src="latte_v5_1_raw.jpg"/></td><td><img src="latte_niji5.jpg"/></td><td><img src="latte_v4.jpg"/></td></tr></table><p>可以看出：</p><ul><li>v5.2/v5.1/v5 相比 v4 确实有质的飞越，前者的光影、反射比后者精致不少。面对粗糙的 prompt，v5.2/v5.1/v5 能自动添加勺子、桌子、背景绿植、咖啡豆等元素，使得画面更为丰富；相比之下，v4 基本真的就只是一杯拿铁。</li><li>v5.2/v5.1 更偏艺术作品（所谓的默认审美）；而 raw 和 v5 都更加真实、更偏摄影照片。</li></ul><h2 id="基础功能">基础功能</h2><h3 id="imagine-text-prompt"><code>/imagine &lt;text prompt&gt;</code></h3><p>返回 4 张图片拼成的 grid，可以点击 U1/U2/U3/U4 来增大某张图片的分辨率（但是对于 v5 以上版本，默认返回的就是 1024x1024 的图，U 按钮不起超分作用，只是把这张图片单独分离出来）；可以点击 V1/V2/V3/V4 来基于某张图片做变化（variation）；可以点击 🔄 按钮重新生成。</p><p><strong>生成速度</strong>：</p><ul><li>Fast 模式：40s 左右</li><li>Relax 模式（排队）：3min 左右，具体时间取决于队列长度</li></ul><p><strong>来自官网的 Prompting Tip</strong>：使用简单、短小的句子，不要提很多很长的要求。例如，</p><ul><li>❌ Show me a picture of lots of blooming California poppies, make them bright, vibrant orange, and draw them in an illustrated style with colored pencils</li><li>✔️ Bright orange California poppies drawn with colored pencils</li></ul><table><tr><th style="text-align: center; width: 50%">冗长的 prompt</th><th style="text-align: center; width: 50%">精简的 prompt</th></tr><tr><td><img src="prompt-long.jpg"/></td><td><img src="prompt-short.jpg"/></td></tr></table><p>实测感觉差别不是很大～</p><h3 id="blend"><code>/blend</code></h3><p>混合 2-5 张上传的图像。所谓混合，即提取每张图像的概念和风格，生成一张新的图像。</p><table><tr><th style="text-align: center; width: 33%">input image 1</th><th style="text-align: center; width: 33%">input image 2</th><th style="text-align: center; width: 33%">output images</th></tr><tr><td><img src="blend-1-input-1.jpg"/></td><td><img src="blend-1-input-2.jpg"/></td><td><img src="blend-1-output.jpg"/></td></tr></table><p>其中第一张输入图像是英国画家 Turner 的著名画作「Snow Storm: Steamboat off a Harbour's Mouth」。可以看见，Midjourney 保留了第二张输入图像的主体（黑色小狗），并试图加上第一张图像的风格以及背景。经测试，交换图片顺序并不会影响生成结果。</p><table><tr><th style="text-align: center; width: 33%">input image 1</th><th style="text-align: center; width: 33%">input image 2</th><th style="text-align: center; width: 33%">output images</th></tr><tr><td><img src="blend-2-input-1.jpg"/></td><td><img src="blend-2-input-2.jpg"/></td><td><img src="blend-2-output.jpg"/></td></tr></table><p>第一个例子中一张图片风格比较明显，另一张主体比较明显，那如果两张图片都是主体呢？这个例子中，可以看见输出图片把二者放在了一起，但其中一个主体更为突出；更有意思的是，它把花儿上的水滴画在了哈士奇的脸上，某种程度上算是一种风格融合。</p><table><tr><th style="text-align: center; width: 33%">input image 1</th><th style="text-align: center; width: 33%">input image 2</th><th style="text-align: center; width: 33%">output images</th></tr><tr><td><img src="blend-3-input-1.jpg"/></td><td><img src="blend-3-input-2.jpg"/></td><td><img src="blend-3-output.jpg"/></td></tr></table><p>无论顺序如何，Midjourney 总是以《星月夜》为整体构图，可如果我想要星月夜式的神奈川怎么办呢？</p><table><tr><th colspan="2" style="text-align: center; width: 50%">input images</th><th style="text-align: center; width: 50%">output images</th></tr><tr><td style="width: 25%"><img src="blend-4-input-1.jpg"/></td><td style="width: 25%"><img src="blend-4-input-2.jpg"/></td><td rowspan="2"><img src="blend-4-output.jpg"/></td></tr><tr><td><img src="blend-4-input-3.jpg"/></td><td><img src="blend-4-input-4.jpg"/></td></tr></table><p>来个艺术作品大杂烩……呃……好吧😑</p><p>总而言之，不太清楚 <code>/blend</code> 命令会提取哪张输入图片的什么特征进行融合，因此可能不能生成用户期望的结果。</p><h3 id="describe"><code>/describe</code></h3><p>返回 4 个描述上传的图像的 prompts，可以用这个方法来学习新的词汇和写 prompt 的方式。点击 1️⃣ 2️⃣ 3️⃣ 4️⃣ 按钮即可使用对应的 prompt 去生成图像，注意这并不意味着用返回的 prompt 就能够重新生成这张上传的图像。</p><p><strong>图-文-图测试</strong>：上传一张图像，用 <code>/describe</code> 功能得到文字描述，再用这一段描述去生成图像，对比输入输出图像。</p><table><tr><th style="text-align: center; width: 25%">input image &amp;<br/>re-generated images</th><th style="text-align: center; width: 75%">output prompts</th></tr><tr><td rowspan="4" style="text-align: center"><img src="describe-1-input.jpeg"/><br/>⇩<br/><img src="describe-1-re.jpg"/></td><td>a brown terrier in the field, in the style of lively action poses, flickr, hasselblad 1600f, soft lighting, striped, joyful --ar 62:57</td></tr><tr><td><b>a puppy running through grass outdoors, in the style of hasselblad hc 100mm f/2.2, suffolk coast views, flickr, playful caricature, light brown and light amber, spiky mounds, bold lines, vibrant color --ar 62:57</b></td></tr><tr><td>a small dark brown dog running through the grass, in the style of danish golden age, flickr, hasselblad 1600f, suffolk coast views, joyful and optimistic, soft lighting --ar 62:57</td></tr><tr><td>a beautiful field, in the style of petcore, dynamic pose, flickr, amber, toyism, walter langley, alpo jaakola --ar 62:57</td></tr><tr><td rowspan="4" style="text-align: center"><img src="describe-2-input.jpg"/><br/>⇩<br/><img src="describe-2-re.jpg"/></td><td>deer in the night, blue moon and stars on an illustration, in the style of dark yellow and light blue, made of insects, nature-inspired installations, sparklecore, lith printing, light installations, joyful celebration of nature</td></tr><tr><td>hudson rebecca deer at the moon with night sky, in the style of dark azure and yellow, roger dean, glittery, luminescent installations, joyful celebration of nature, jonathan wolstenholme, the stars art group (xing xing)</td></tr><tr><td><b>a deer standing in the forest at night in the moonlight, in the style of sky-blue and yellow, dan matutina, damien hirst, glittery, joyful celebration of nature, richard doyle, wallpaper</b></td></tr><tr><td>deer of the night, teddy bear, forest in the light, in the style of dark sky-blue and yellow, silhouettes in space, splattered paint, i can't believe how beautiful this is, tangled nests, made of insects, characterful animal portraits</td></tr></table><p>（加粗的 prompt 是我选择用来重新生成图片的 prompt）</p><p>可以看见，Midjourney 给出的 prompts 一般遵循以下模式：首先一句话描述画面内容（a puppy running through grass outdoors），然后描述风格（in the style of lively action poses），对摄影作品加上相机品牌和参数（hasselblad 1600f）、图库（flickr），然后描述颜色和光影（soft lighting、light brown and light amber），描述整体情绪（joyful and optimistic），还可以加上艺术家名字（walter langley、alpo jaakola）或时代（danish golden age）。</p><p>另外，使用这些 prompt 重新生成的图片，风格和主题与输入图像一致，但是内容差异较大。因此，这个功能的价值更多地在于获取高端上档次的 prompt.</p><h2 id="进阶功能">进阶功能</h2><h3 id="zoom-out">Zoom out</h3><p>v5.2 版本新增功能，用于扩大图片。在使用 <code>/imagine</code> 生成 4 张图片后，使用 U 按钮分离出其中一张，就可以看到 zoom out 按钮。<strong>值得注意的是，该功能并不会对图片分辨率有大的改动，仅是画幅增大</strong>。</p><p>该功能又分为几个子功能：</p><ul><li>Zoom Out 2X：扩大 2 倍，上下左右都会扩展</li><li>Zoom Out 1.5X：扩大 1.5 倍，上下左右都会扩展</li><li>Make Square：将非正方形图片扩展为正方形</li><li>Custom Zoom：点击后弹出一个输入框，可以通过 <code>--ar</code> 参数自定义长宽比、通过 <code>--zoom</code> 参数自定义扩大倍数（取值范围 1~2）。特别地，当取 <code>--zoom</code> 为 1 时，相当于在使用 <code>--ar</code> 来调整长宽比。另外，还可以在此处更改 prompts.</li></ul><p>所有功能都会返回 4 张图片。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="zoom.png" width=100% /></div></div></div><h3 id="image-prompts">Image prompts</h3><p><img src="https://cdn.document360.io/3040c2b6-fead-4744-a3a9-d56d621c6c7e/Images/Documentation/MJ%20Prompt.png" /></p><p>如图所示，<code>/imagine</code> 命令其实不仅仅接受文本，还接受图像的 URL，从而生成类似于给定图像的新图像。事实上，上文介绍的 <code>/blend</code> 命令就是其简化版本，相当于只允许输入 image prompts 的 <code>/imagine</code> 命令。特别地，对于本地图像，将其粘贴到对话框中发送出去，就可以在 discord 里面复制其 URL 了。</p><p>参数 <code>--iw</code> 可以调节图像的参考权重（image weight），对于 v5 版本，权重范围 0~2，默认为 1.</p><p>以下是我尝试的一个例子，参考图像为一只黑白相间的鸟，文本 prompt 为 "mechanical cyberpunk bird"，在不同 <code>--iw</code> 参数下结果如下（w/o 代表没有 image prompt，作为对照组）：</p><table><tr><th style="text-align: center; width: 25%">image prompt</th><th style="text-align: center; width: 25%">w/o</th><th style="text-align: center; width: 25%">iw=0.0</th><th style="text-align: center; width: 25%">iw=0.2</th></tr><tr><td><img src="imagep-input.jpg"/></td><td><img src="imagep-wo.jpg"/></td><td><img src="imagep-iw0_0.jpg"/></td><td><img src="imagep-iw0_2.jpg"/></td></tr><tr><th style="text-align: center; width: 25%">iw=0.5</th><th style="text-align: center; width: 25%">iw=0.8</th><th style="text-align: center; width: 25%">iw=1.0</th><th style="text-align: center; width: 25%">iw=2.0</th></tr><tr><td><img src="imagep-iw0_5.jpg"/></td><td><img src="imagep-iw0_8.jpg"/></td><td><img src="imagep-iw1_0.jpg"/></td><td><img src="imagep-iw2_0.jpg"/></td></tr></table><p>可以观察到：</p><ul><li>一般而言，<code>iw</code> 越大，生成图像与参考图像越接近；<code>iw</code> 越小，生成图像与文本描述越接近。但是 <code>iw=0</code> 是一个例外，原以为 0 意味着完全不参考图像，等价于 w/o，结果它生成的图像反而接近 <code>iw=1</code> 的效果，也许是一个 bug；</li><li>鸟的朝向并不会保持与输入图像一致，这一点很奇怪；由于 Midjourney 并没有公布任何技术细节，我只能猜想他们在训练时给图像条件加了水平翻转的增强。</li></ul><h3 id="multi-prompts">Multi prompts</h3><p>基础用法是用 <code>::</code> 分隔 prompt，强制让 Midjourney 分别独立地考虑 <code>::</code> 前后的两个词。</p><p>官网举了一个形象的例子：<code>hot dog</code> v.s. <code>hot:: dog</code>，我这里尝试了另一个例子：</p><table><tr><th style="text-align: center; width: 50%">dragon fly</th><th style="text-align: center; width: 50%">dragon:: fly</th></tr><tr><td><img src="multi-no.jpg"/></td><td><img src="multi-1-1.jpg"/></td></tr></table><p>果然，前者被理解为了蜻蜓，而后者被理解为了飞龙。</p><p>进一步地，可以在 <code>::</code> 后面加上数字来调整词语的权重，比如 <code>hot::2 dog</code> 就是“非常”热的狗。承接上文，我试了试不同权重下的 <code>dragon:: fly</code>：</p><table><tr><th style="text-align: center; width: 25%">dragon::5 fly::</th><th style="text-align: center; width: 25%">dragon::2 fly::</th><th style="text-align: center; width: 25%">dragon:: fly::2</th><th style="text-align: center; width: 25%">dragon:: fly::5</th></tr><tr><td><img src="multi-5-1.jpg"/></td><td><img src="multi-2-1.jpg"/></td><td><img src="multi-1-2.jpg"/></td><td><img src="multi-1-5.jpg"/></td></tr></table><p>可以看见，5:1 的权重下，所有龙都不飞了；2:1 的权重下，有些龙不飞；1:2 的权重下，龙飞得更张扬了，而且还有一张图把 fly 理解成了苍蝇；而 1:5 的权重下，fly 全被理解成了苍蝇，所有图都跟龙没啥关系了。</p><p>以上权重都是正数，也可以使用负数权重来去除不想要的元素。更方便地，使用 <code>--no</code> 参数相当于把对应词汇的权重设置为 -0.5.</p><table><tr><th style="text-align: center; width: 50%">dragon:: fly --no sky</th><th style="text-align: center; width: 50%">dragon:: fly:: sky::-1.5</th></tr><tr><td><img src="multi-no-sky.jpg"/></td><td><img src="multi-no-sky2.jpg"/></td></tr></table><p>使用 <code>--no sky</code> 之后，前两只龙都变成了落在岩石上，但第三只龙还明显在天上；加强负数权重后，不管是龙也好苍蝇也好，都落在了地上，fly 一词主要体现在张开的翅膀上，还是蛮有意思的吼。</p><h3 id="参数">参数</h3><p>上文中我们已经用到了一些参数，比如 <code>--version</code> (<code>--v</code>) 指定模型版本、<code>--style</code> 指定某版本下的细分风格、<code>--iw</code> 控制参考图像的权重、<code>--no</code> 去除不想要的元素等。本小节将介绍更多的参数。由于参数很多，就不一一尝试了，相关示例请查看官网。</p><p>下表摘取自 Midjourney 官网：</p><table><thead><tr class="header"><th style="text-align: left;"></th><th style="text-align: center;">Affects initial generation</th><th style="text-align: center;">Affects variations + remix</th><th style="text-align: center;">Ver. 5</th><th style="text-align: center;">Ver. 4</th><th style="text-align: center;">Niji 5</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">Max Aspect Ratio</td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td><td style="text-align: center;">any</td><td style="text-align: center;">1:2 or 2:1</td><td style="text-align: center;">any</td></tr><tr class="even"><td style="text-align: left;">Chaos</td><td style="text-align: center;">✓</td><td style="text-align: center;"></td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td></tr><tr class="odd"><td style="text-align: left;">Image Weight</td><td style="text-align: center;">✓</td><td style="text-align: center;"></td><td style="text-align: center;">.5–2 default=1</td><td style="text-align: center;"></td><td style="text-align: center;">.5–2 default=1</td></tr><tr class="even"><td style="text-align: left;">No</td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td></tr><tr class="odd"><td style="text-align: left;">Quality</td><td style="text-align: center;">✓</td><td style="text-align: center;"></td><td style="text-align: center;">.5–2</td><td style="text-align: center;">.5–2</td><td style="text-align: center;">.5–2</td></tr><tr class="even"><td style="text-align: left;">Repeat</td><td style="text-align: center;">✓</td><td style="text-align: center;"></td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td></tr><tr class="odd"><td style="text-align: left;">Seed</td><td style="text-align: center;">✓</td><td style="text-align: center;"></td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td></tr><tr class="even"><td style="text-align: left;">Stop</td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td></tr><tr class="odd"><td style="text-align: left;">Style</td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;">raw</td><td style="text-align: center;">4a, and 4b</td><td style="text-align: center;">cute, expressive, original and scenic</td></tr><tr class="even"><td style="text-align: left;">Stylize</td><td style="text-align: center;">✓</td><td style="text-align: center;"></td><td style="text-align: center;">0–1000 default=100</td><td style="text-align: center;">0–1000 default=100</td><td style="text-align: center;">0–1000 default=100)</td></tr><tr class="odd"><td style="text-align: left;">Tile</td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td><td style="text-align: center;">✓</td><td style="text-align: center;"></td><td style="text-align: center;">✓</td></tr></tbody></table><ul><li><code>--aspect</code> 或 <code>--ar</code>：指定长宽比。v5 版本已经能够支持任意长宽比，但是官网有一句提示：「大于 2:1 的长宽比是实验功能，可能生成不可预测的结果」。结合非正常比例的图像耗时更长的现象，我猜想大于 2:1 的图像是依靠 outpainting + 滑动窗口实现的。</li><li><code>--chaos</code> 或 <code>--c</code>：有没有发现上面展示的图片中，每个 grid，也即每跑一次得到的 4 张图都比较相像？这是因为没有指定 <code>chaos</code> 参数，自动取默认值为 0 了。<code>chaos</code> 参数范围在 0~100 之间，其值越大，grid 中的 4 张图片就越不一样。结合扩散模型的原理——采样过程中前期时间步控制构图、后期时间步控制高频细节，不难猜想其实现原理。</li><li><code>--quality</code> 或 <code>--q</code>：控制图像质量，质量越高耗时越长。猜想应该是依靠改变采样步数实现的，甚至可能会用不同的采样器。</li><li><code>--repeat</code> 或 <code>--r</code>：反复多次跑一个任务。结合 <code>--chaos</code> 可以用来快速探索。</li><li><code>--seed</code>：固定随机种子。</li><li><code>--stop</code>：在采样过程中途停下来，返回模糊的图片（不出意外的话就是扩散模型每步预测的 <span class="math inline">\(\mathbf x_0\)</span>）。</li><li><code>--stylize</code> 或 <code>--s</code>：Midjourney 的模型被训练得偏好于艺术性的色彩、组成和形式。<code>stylize</code> 参数决定这种“艺术性”的强弱。</li><li><code>--tile</code>：生成的图像能够像瓷砖那样无缝拼接。</li></ul><h2 id="常见问题测试">常见问题测试</h2><p>以下测试均在 v5.1 版本上进行。</p><h3 id="属性对应绑定">属性对应/绑定</h3><table><tr><th style="text-align: center; width: 50%">A yellow book and a red vase</th><th style="text-align: center; width: 50%">A smooth ball and a fluffy cube</th></tr><tr><td><img src="attribute-1.jpg"/></td><td><img src="attribute-2.jpg"/></td></tr></table><p>Midjourney 并不总能正确绑定物体的属性，例如颜色错乱（黄色的花瓶）和材质错乱（毛茸茸的球）。</p><h3 id="位置关系">位置关系</h3><table><tr><th style="text-align: center; width: 50%">A glass of milk on the left of a giant cookie</th><th style="text-align: center; width: 50%">A wine glass on top of a dog</th></tr><tr><td><img src="position-1.jpg"/></td><td><img src="position-2.jpg"/></td></tr></table><p>Midjourney 并不能正确处理物体之间的位置关系。</p><h3 id="对象缺失">对象缺失</h3><table><tr><th style="text-align: center; width: 50%">A cat and a dog finding shells on the beach</th><th style="text-align: center; width: 50%">A Tyrannosaurus rex chasing a Triceratops in the forest</th></tr><tr><td><img src="object-1.jpg"/></td><td><img src="object-2.jpeg"/></td></tr></table><p>Midjourney 有时会缺少指定的对象，例如第一个例子的第一张图片缺少了狗。而第二个例子不止是对象缺失那么简单，霸王龙和三角龙莫名其妙地被融合在了一起，虽然挺有意思，但毕竟不符文意。</p><h3 id="数量测试">数量测试</h3><table><tr><th style="text-align: center; width: 50%">One cat and two dogs sitting on the grass</th><th style="text-align: center; width: 50%">Four horses galloping across the grassland at sunset</th></tr><tr><td><img src="number-1.jpg"/></td><td><img src="number-2.jpg"/></td></tr></table><p>Midjourney 并不能正确处理物体的数量。第一个例子明明是要一只猫和两只狗，但给的是一只狗和两只猫（当然这也有可能是属性绑定的问题）；第二个例子有时候只画了三匹马。</p><h3 id="书写文字">书写文字</h3><table><tr><th style="text-align: center; width: 50%">A thick hardcover book titled "Midjourney"</th><th style="text-align: center; width: 50%">An ice sculpture of letter "M"</th></tr><tr><td><img src="letter-test-1.jpg"/></td><td><img src="letter-test-2.jpg"/></td></tr></table><p>Midjourney 并不能正确书写文字。在第一个例子中，第一幅图意识到了 "Midjourney" 是书的标题，但无法正确拼写出来；其他图片都把 Midjourney 当成了描述性词汇，把书的封面绘制成了旅程地图（莫名想到魔戒）。在第二个例子中，前两幅图把字母 M 弄成了 N，后两幅图也怪怪的。</p><h3 id="人物群像">人物群像</h3><table><tr><th style="text-align: center; width: 50%">Messi, Suarez and Neymar celebrating a wonderful goal, photorealistic, canon 5d,1fujifilm xt100,Sony alpha, face shot --v 5</th><th style="text-align: center; width: 50%">a study group in university discussing around a round table, laptops, caffe --v 5</th></tr><tr><td><img src="portrait-1.jpg"/></td><td><img src="portrait-2.jpg"/></td></tr></table><p>手指和胳膊惨不忍睹；当人物距离较远时，脸部会扭曲变形。</p><h3 id="小结">小结</h3><p>其实前五个问题都与文本编码器有很大关系。根据 <a href="https://arxiv.org/abs/2112.10741">GLIDE</a>、<a href="https://arxiv.org/abs/2204.06125">DALL·E 2</a> 和 <a href="https://arxiv.org/abs/2205.11487">Imagen</a> 的论文，使用 CLIP 作为文本编码器时会有属性无法对应、无法书写文字等问题，而 T5 的表现会好很多，甚至像 GLIDE 那样从头训练一个 Transformer 都会在属性绑定问题上表现得更好。所以可以不负责任地猜想一下，Midjourney 团队有可能使用的是 <a href="https://github.com/mlfoundations/open_clip">OpenCLIP</a> 的预训练文本编码器。</p><h2 id="changelog">ChangeLog</h2><p>2023.06.27：增加对 v5.2 版本的介绍，包括 zoom out 功能。</p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>AIGC</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
      <tag>AIGC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[PRML]2.Probability Distributions</title>
    <link href="/blog-main/2023/05/04/PRML-2-Probability-Distributions/"/>
    <url>/blog-main/2023/05/04/PRML-2-Probability-Distributions/</url>
    
    <content type="html"><![CDATA[<p>本章介绍一些常见的概率分布，同时也会穿插一些在贝叶斯推断中非常重要的统计学概念。</p><p>一个常见的问题是密度估计（density estimation）：给定一个观察到的数据集 <span class="math inline">\(\mathbf x_1,\ldots,\mathbf x_N\)</span>（假设独立同分布），为随机变量 <span class="math inline">\(\mathbf x\)</span> 的概率分布 <span class="math inline">\(p(\mathbf x)\)</span> 建模。值得注意的是，这个问题是 ill-posed 的——存在无数种概率分布都能得到这一数据集。事实上，只要在 <span class="math inline">\(\mathbf x_1,\ldots,\mathbf x_N\)</span> 处概率不为零即可。</p><p>密度估计方法可分为参数估计和非参估计。参数估计指用于建模的概率分布具有特定的形式并由一套参数决定，比如二项分布、多项分布和高斯分布等。用它们来做密度估计，其实就是找到对应参数的合适的值。例如，频率学派常常采用极大似然法，贝叶斯学派则先为这些参数指定一个先验分布，然后用贝叶斯定理计算后验分布。</p><p>这里引入一个重要的概念——<strong>共轭先验（conjugate priors），即先验分布与后验分布具有相同的形式</strong>，这会对大大简化贝叶斯的分析过程。例如，多项分布的共轭先验是 Dirichlet 分布，一个高斯分布的均值的共轭先验是另一个高斯分布。这些分布都属于指数族，具有一些重要的性质。</p><p>与参数估计相反，非参估计并不假设分布的某种具体形式，而是与数据集大小有关。这些模型仍然具有参数，但参数是为了控制模型的复杂度而非分布的形式。典型的方法有最近邻、核密度估计等。</p><h2 id="binary-variables">Binary Variables</h2><p>考虑一个二值随机变量 <span class="math inline">\(x\in\{0,1\}\)</span>，例如抛硬币是否为正面朝上，其分布由一个参数 <span class="math inline">\(\mu\)</span> 决定：<span class="math inline">\(x=1\)</span> 的概率为 <span class="math inline">\(\mu\)</span>，<span class="math inline">\(x=0\)</span> 的概率为 <span class="math inline">\(1-\mu\)</span>，即： <span class="math display">\[\text{Bern}(x\vert \mu)=\mu^x(1-\mu)^{1-x}\]</span> 这被称作 Bernoulli 分布。容易得到其均值和方差分别为： <span class="math display">\[\begin{align}\mathbb E[x]&amp;=\mu\\\text{var}[x]&amp;=\mu(1-\mu)\end{align}\]</span> 设有一个独立同分布的数据集 <span class="math inline">\(\mathcal D=\{x_1,\ldots,x_N\}\)</span>，那么似然为： <span class="math display">\[p(\mathcal D\vert\mu)=\prod_{n=1}^N\mu^{x_n}(1-\mu)^{1-x_n}\]</span> 对数似然为： <span class="math display">\[\ln p(\mathcal D\vert\mu)=\sum_{n=1}^N\left(x_n\ln\mu+(1-x_n)\ln(1-\mu)\right)\]</span> 值得注意的是，计算这个对数似然其实只需要知道 <span class="math inline">\(\sum_nx_n\)</span>，并不需要知道每一个 <span class="math inline">\(x_n\)</span>，这就涉及到了充分统计量（sufficient statistic）的概念，我们将在稍后叙述。若采用频率学派的极大似然法，对上式求导取零，解得： <span class="math display">\[\mu_\text{ML}=\frac{1}{N}\sum_{n=1}^N x_n\]</span> 即样本均值。</p><p>现在考虑一种情况：抛了 3 次硬币而 3 次都是正面朝上，那么极大似然法会给出 <span class="math inline">\(\mu_\text{ML}=\frac{1}{3}(1+1+1)=1\)</span> 的解。也就是说，模型会预测未来始终都是正面朝上！常识告诉我们这并不合理，<strong>事实上这是一个极大似然法导致过拟合问题的极端例子</strong>，稍后我们将看到如何采用贝叶斯方法避免过拟合问题。</p><p>假设 <span class="math inline">\(x=1\)</span>（正面朝上）发生的次数是 <span class="math inline">\(m\)</span>，则 <span class="math inline">\(m\)</span> 服从二项分布： <span class="math display">\[\text{Bin}(m\vert N,\mu)=\binom{N}{m}\mu^m(1-\mu)^{N-m}\]</span> 二项分布的均值和方差分别为： <span class="math display">\[\begin{align}\mathbb E[m]&amp;=N\mu\\\text{var}[m]&amp;=N\mu(1-\mu)\end{align}\]</span></p><h3 id="the-beta-distribution">The beta distribution</h3><p>上文我们看到，极大似然法容易导致过拟合。那么为了使用贝叶斯方法，我们需要为参数 <span class="math inline">\(\mu\)</span> 设置一个先验分布。为了方便，我们还希望先验分布与后验分布具有相同的形式。考虑到后验分布正比于似然乘以先验，而似然是 <span class="math inline">\(\mu^x(1-\mu)^{1-x}\)</span> 的形式，我们自然想到取先验也为 <span class="math inline">\(\mu\)</span> 和 <span class="math inline">\((1-\mu)\)</span> 的指数形式。因此，我们选择 beta 分布为先验分布： <span class="math display">\[\text{Beta}(\mu\vert a,b)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\]</span> 其中 <span class="math inline">\(\Gamma(x)\)</span> 是 gamma 函数，上式前面的系数保证了 beta 分布的归一性。beta 分布的均值和方差分别是： <span class="math display">\[\begin{align}\mathbb E[\mu]&amp;=\frac{a}{a+b}\\\text{var}[\mu]&amp;=\frac{ab}{(a+b)^2(a+b+1)}\end{align}\]</span> 参数 <span class="math inline">\(a,b\)</span> 通常被称作超参数，下图展示了不同 <span class="math inline">\(a,b\)</span> 下 beta 分布的形状：</p><p><img src="fig2.2.png" width=70% /></p><p>为了验证后验分布确实也是 beta 分布的形式，我们先暂时抛开归一化系数： <span class="math display">\[p(\mu\vert m,l,a,b)\propto p(m\vert l,\mu)\cdot p(\mu\vert a,b)\propto \mu^{m+a-1}(1-\mu)^{l+b-1}\]</span> 其中 <span class="math inline">\(l=N-m\)</span>，即反面朝上的次数。上式已经足以证明后验分布就是 beta 分布，现在只需要参照 beta 分布的形式把归一化系数加上即可： <span class="math display">\[p(\mu\vert m,l,a,b)=\frac{\Gamma(m+a+l+b)}{\Gamma(m+a)\Gamma(l+b)}\mu^{m+a-1}(1-\mu)^{l+b-1}\tag{1}\label{1}\]</span> 对比先验与后验分布，可见当我们观察到数据集中有 <span class="math inline">\(m\)</span> 次 <span class="math inline">\(x=1\)</span> 和 <span class="math inline">\(l\)</span> 次 <span class="math inline">\(x=0\)</span> 时，对参数分布的影响是给 <span class="math inline">\(a\)</span> 加上 <span class="math inline">\(m\)</span>，给 <span class="math inline">\(b\)</span> 加上 <span class="math inline">\(l\)</span>. 因此，我们可以把超参数 <span class="math inline">\(a,b\)</span> 理解为 <span class="math inline">\(x=1\)</span> 和 <span class="math inline">\(x=0\)</span> 的“有效观察次数”（不一定是整数）。进一步，如果我们观察到了新的数据，那么我们可以把当前的后验分布当作先验分布，继续计算新的后验分布。特别地，如果我们每次只观察到一个数据，那么如果是 <span class="math inline">\(x=1\)</span>，就给 <span class="math inline">\(a\)</span> 加 1，否则给 <span class="math inline">\(b\)</span> 加 1，下图展示了这样的一步更新：</p><p><img src="fig2.3.png" width=70% /></p><p>可见，在贝叶斯视角下，这种序列式（sequential）的学习方法是非常自然的。这可以被应用在 real-time learning 之中。</p><p>如果要预测下一次试验的结果，那么其实就是要求解 <span class="math inline">\(p(x\vert\mathcal D)\)</span>，根据贝叶斯方法，我们使用 sum rule 和 predict rule： <span class="math display">\[p(x=1\vert \mathcal D)=\int p(x=1\vert\mu)p(\mu\vert \mathcal D)\mathrm d\mu=\int_0^1\mu p(\mu\vert\mathcal D)\mathrm d\mu=\mathbb E[\mu\vert\mathcal D]\]</span> 即后验分布的均值，其中后验分布由 <span class="math inline">\(\eqref{1}\)</span> 式给出。根据 beta 分布的均值结论，得到： <span class="math display">\[p(x=1\vert\mathcal D)=\frac{m+a}{m+a+l+b}\]</span> 直观而言，这就是所有观察（包括真观察到的数据集和假设的先验观察）中 <span class="math inline">\(x=1\)</span> 所占比例。当数据集非常大时，<span class="math inline">\(m,l\to\infty\)</span>，上式收敛到与极大似然相同的解。事实上这是一个普遍规律：<strong>当数据集大小趋向无穷时，贝叶斯方法和极大似然法会趋向一致</strong>。（毕竟解决过拟合问题最本质的方案就是加大数据量嘛～）</p><p>另外，从图 2.2 中我们看到，<strong>随着观察的数据量增加，后验分布变得越来越集中。事实上，这是贝叶斯学习方法的普遍性质</strong>。为了说明这一点，考虑一个参数 <span class="math inline">\(\theta\)</span> 和一个数据集 <span class="math inline">\(\mathcal D\)</span>，它们构成联合分布 <span class="math inline">\(p(\theta,\mathcal D)\)</span>. 根据重期望公式（law of total expectation）： <span class="math display">\[\mathbb E_\theta[\theta]=\mathbb E_\mathcal D[\mathbb E_\theta[\theta\vert\mathcal D]]\]</span> 这说明，<span class="math inline">\(\theta\)</span> 的后验均值，在所有数据的平均意义下，等于先验均值。又根据全方差公式（law of total variance）： <span class="math display">\[\text{var}_\theta[\theta]=\mathbb E_\mathcal D[\text{var}_\theta[\theta\vert\mathcal D]]+\text{var}_\mathcal D[\mathbb E_\theta[\theta\vert\mathcal D]]&gt;\mathbb E_\mathcal D[\text{var}_\theta[\theta\vert\mathcal D]]\]</span> 这说明 <span class="math inline">\(\theta\)</span> 的后验方差，在所有数据的平均意义下，小于先验方差。也就是说，在观察到新的数据后，<span class="math inline">\(\theta\)</span> 的不确定性（在平均意义下）会变小，分布得更集中。</p><h2 id="multinomial-variables">Multinomial Variables</h2><p>前一节我们讨论了二值变量，引出 Bernoulli 分布和二项分布，以及二项分布的共轭分布——beta 分布。如果变量不止取两个值，而是 <span class="math inline">\(K\)</span> 个，那么我们也可以进行类似的讨论。我们可以用一个 <span class="math inline">\(K\)</span> 维向量 <span class="math inline">\(\mathbf x\)</span> 来表示随机变量的取值，<span class="math inline">\(\mathbf x\)</span> 只有一个分量为 1，其余为 0（one-hot），若 <span class="math inline">\(x_k=1\)</span>，就表示随机变量取值为 <span class="math inline">\(k\)</span>. 设参数 <span class="math inline">\(\mu_k\)</span> 表示 <span class="math inline">\(x_k=1\)</span> 的概率，那么： <span class="math display">\[p(\mathbf x\vert\boldsymbol\mu)=\prod_{k=1}^K \mu_k^{x_k}\]</span> 其中 <span class="math inline">\(\boldsymbol\mu=(\mu_1,\ldots,\mu_k)^\mathrm T\)</span>，且 <span class="math inline">\(\mu_k\geq 0,\,\sum_k\mu_k=1\)</span>.</p><p>考虑一个独立同分布的数据集 <span class="math inline">\(\mathbf x_1,\ldots,\mathbf x_N\)</span>，那么对应的似然函数为： <span class="math display">\[p(\mathcal D\vert\boldsymbol\mu)=\prod_{n=1}^N\prod_{k=1}^K\mu_k^{x_{nk}}=\prod_{k=1}^K\mu_k^{\sum_n x_{nk}}=\prod_{k=1}^K\mu_k^{m_k}\]</span> 其中 <span class="math inline">\(m_k=\sum_n x_{nk}\)</span> 为所有数据中 <span class="math inline">\(x_k=1\)</span> 的数量，是该分布的充分统计量。</p><p>依旧先考虑极大似然估计，注意这个问题有约束条件，所以拉格朗日函数为： <span class="math display">\[\sum_{k=1}^K m_k\ln \mu_k+\lambda\left(\sum_{k=1}^K\mu_k-1\right)\]</span> 求导取零，解得： <span class="math display">\[\mu_k^{\text{ML}}=\frac{m_k}{N}\]</span> 即所有样本中 <span class="math inline">\(x_k=1\)</span> 所占比例。</p><p>考虑 <span class="math inline">\(m_1,\ldots,m_K\)</span> 的联合分布，它是二项分布的推广，称作多项分布： <span class="math display">\[\text{Mult}(m_1,m_2,\ldots,m_K\vert \boldsymbol\mu,N)=\binom{N}{m_1m_2\ldots m_K}\prod_{k=1}^K\mu_k^{m_k}\]</span> 注意 <span class="math inline">\(m_k\)</span> 满足约束 <span class="math inline">\(\sum_{k} m_k=N\)</span>.</p><h3 id="the-dirichlet-distribution">The Dirichlet distribution</h3><p>同第一节一样，我们希望用贝叶斯方法推断参数 <span class="math inline">\(\boldsymbol\mu\)</span>，且希望先验分布与后验分布具有相同的形式。观察似然函数，它是 <span class="math inline">\(\mu_k\)</span> 的指数的乘积，所以一个自然的想法就是取先验分布也是 <span class="math inline">\(\mu_k\)</span> 的指数的乘积形式。或者我们也可以依葫芦画瓢，推广一下 beta 分布。无论如何，先验分布应该是如下形式： <span class="math display">\[p(\boldsymbol\mu\vert\boldsymbol\alpha)\propto \prod_{k=1}^K\mu_k^{\alpha_k-1}\]</span> 加上归一化系数，即得到 Dirichlet 分布： <span class="math display">\[\text{Dir}(\boldsymbol\mu\vert\boldsymbol\alpha)=\frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}\prod_{k=1}^K\mu_k^{\alpha_k-1}\]</span> 其中 <span class="math display">\[\alpha_0=\sum_{k=1}^K \alpha_k\]</span> 值得注意的是，由于约束条件 <span class="math inline">\(\sum_k\mu_k=1\)</span> 的存在，所以 Dirichlet 分布的支撑集是 <span class="math inline">\(K-1\)</span> 阶的单纯形（simplex），即一个有界线性流形，如下图所示：</p><p><img src="fig2.4.png" width=70% /></p><p>不同 <span class="math inline">\(\boldsymbol\alpha\)</span> 下的 Dirichlet 分布示意图如下图所示：</p><p><img src="fig2.5.png" width=70% /></p><p>容易验证后验分布确实也是一个 Dirichlet 分布： <span class="math display">\[p(\boldsymbol\mu\vert \mathcal D,\boldsymbol\alpha)\propto p(\mathcal D\vert\boldsymbol\mu)\cdot p(\mu\vert\boldsymbol\alpha)\propto \prod_{k=1}^K\mu_k^{m_k+\alpha_k-1}\]</span> 加上归一化系数： <span class="math display">\[p(\boldsymbol\mu\vert\mathcal D,\boldsymbol\alpha)=\text{Dir}(\boldsymbol\mu\vert\boldsymbol\alpha+\mathbf m)=\frac{\Gamma(\alpha_0+N)}{\Gamma(\alpha_1+m_1)\cdots\Gamma(\alpha_K+m_K)}\prod_{k=1}^K\mu_k^{m_k+\alpha_k-1}\]</span></p><h2 id="the-gaussian-distribution">The Gaussian Distribution</h2><p>前两节我们分别讨论了二值变量和多值变量，但它们都是离散变量。这一节我们讨论连续变量，毫无疑问从高斯分布（正态分布）开始。</p><p>单变量的高斯分布为： <span class="math display">\[\mathcal N(x\vert\mu,\sigma^2)=\frac{1}{(2\pi\sigma^2)^{1/2}}\exp\left(-\frac{1}{2\sigma^2}(x-\mu)^2\right)\]</span> <span class="math inline">\(D\)</span> 维随机向量 <span class="math inline">\(\mathbf x\)</span> 的高斯分布为： <span class="math display">\[\mathcal N(\mathbf x\vert\boldsymbol\mu,\boldsymbol\Sigma)=\frac{1}{(2\pi)^{D/2}}\frac{1}{(\boldsymbol\Sigma)^{1/2}}\exp\left(-\frac{1}{2}(\mathbf x-\boldsymbol\mu)^\mathrm T{\boldsymbol\Sigma}^{-1}(\mathbf x-\boldsymbol\mu)\right)\]</span> 高斯分布可以从多种角度推导出来。例如我们在第一章已经看到了，对于一个随机变量，高斯分布是让它的熵取到最大的分布。对于多元高斯分布也是如此。</p><p>另一种推导高斯分布的角度是考虑多个随机变量之和。中心极限定理告诉我们，在一些条件下，随着随机变量的数量增加，它们的和（或均值）趋向于高斯分布。例如，二项随机变量可以视作 <span class="math inline">\(N\)</span> 个 Bernoulli 随机变量的和，所以随着 <span class="math inline">\(N\)</span> 增加，二项分布就会趋向于高斯分布。</p><p>高斯分布具有很多重要的分析性质，这些性质是以后章节的更复杂的模型的基础，所以务必应掌握。</p><p>首先考虑高斯分布的几何形式。从其概率密度函数可以看出，高斯分布以平方的形式依赖于 <span class="math inline">\(\mathbf x\)</span>： <span class="math display">\[\Delta^2=(\mathbf x-\boldsymbol\mu)^\mathrm T{\boldsymbol\Sigma}^{-1}(\mathbf x-\boldsymbol\mu)\]</span> 这里 <span class="math inline">\(\Delta\)</span> 被称作 <span class="math inline">\(\mathbf x\)</span> 与 <span class="math inline">\(\boldsymbol\mu\)</span> 之间的马氏距离（Mahalanobis distance），当 <span class="math inline">\(\boldsymbol\Sigma\)</span> 是单位矩阵时，马氏距离退化为欧氏距离。</p><p>未完待续。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>PRML</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[PRML]Appendix E.Lagrange Multipliers</title>
    <link href="/blog-main/2023/04/28/PRML-Appendix-E-Lagrange-Multipliers/"/>
    <url>/blog-main/2023/04/28/PRML-Appendix-E-Lagrange-Multipliers/</url>
    
    <content type="html"><![CDATA[<p>PRML 在 Appendix E 中用直观的几何视角介绍了用拉格朗日乘数法求解带约束条件的极值问题。</p><h2 id="等式约束">等式约束</h2><p>考虑在约束条件 <span class="math inline">\(g(x_1,x_2)=0\)</span> 下最大化函数 <span class="math inline">\(f(x_1,x_2)\)</span>，最直接的方法是从 <span class="math inline">\(g(x_1,x_2)=0\)</span> 中解出 <span class="math inline">\(x_2=h(x_1)\)</span>，然后带回优化目标得到 <span class="math inline">\(f(x_1,h(x_1))\)</span>，于是问题转化为无条件的优化问题。然而，我们一般很难把 <span class="math inline">\(x_2\)</span> 显式地写作 <span class="math inline">\(x_1\)</span> 的解析形式；另外，这个方法也会破坏 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> 的天然对称性。</p><p>一个更优雅和简单的方法是拉格朗日乘数法。我们可以从几何视角看拉格朗日乘数法的动机。考虑一个 <span class="math inline">\(D\)</span> 维变量 <span class="math inline">\(\mathbf x=(x_1,\ldots,x_D)^\mathrm T\)</span>，则约束条件 <span class="math inline">\(g(\mathbf x)=0\)</span> 表示 <span class="math inline">\(\mathbf x\)</span> 所在空间中的一个 <span class="math inline">\(D-1\)</span> 维曲面，如下图所示：</p><p><img src="fige.1.png" width=70% /></p><p>首先，我们注意到<strong>约束曲面上任一点关于 <span class="math inline">\(g\)</span> 的梯度 <span class="math inline">\(\nabla g(\mathbf x)\)</span> 与约束曲面是正交的</strong>。为了说明这一点，考虑约束曲面上的一点 <span class="math inline">\(\mathbf x\)</span> 和也在约束曲面上的邻近的一点 <span class="math inline">\(\mathbf x+\epsilon\)</span>. 在 <span class="math inline">\(\mathbf x\)</span> 处做泰勒展开： <span class="math display">\[g(\mathbf x+\epsilon)\simeq g(\mathbf x)+\epsilon^\mathrm T\nabla g(\mathbf x)\]</span> 由于 <span class="math inline">\(\mathbf x,\mathbf x+\epsilon\)</span> 都在约束曲面上，所以 <span class="math inline">\(g(\mathbf x+\epsilon)=g(\mathbf x)=0\)</span>，于是 <span class="math inline">\(\epsilon^\mathrm T\nabla g(\mathbf x)\simeq 0\)</span>. 当 <span class="math inline">\(\Vert\epsilon\Vert\to0\)</span> 时，有 <span class="math inline">\(\epsilon^\mathrm T\nabla g(\mathbf x)\to 0\)</span>. 由于 <span class="math inline">\(\epsilon\)</span> 是平行于约束曲面的，所以 <span class="math inline">\(\nabla g(\mathbf x)\)</span> 就是约束曲面的法向量方向，即与之正交。</p><p>现在我们想找到一个 <span class="math inline">\(\mathbf x^\ast\)</span> 使得 <span class="math inline">\(f(\mathbf x)\)</span> 达到最大。<strong>这样的点一定满足 <span class="math inline">\(\nabla f(\mathbf x)\)</span> 也与约束曲面正交，否则我们可以将其继续在约束曲面上往梯度方向移动</strong>。因此，<span class="math inline">\(\nabla f(\mathbf x)\)</span> 与 <span class="math inline">\(\nabla g(\mathbf x)\)</span> 平行，即存在一个 <span class="math inline">\(\lambda\)</span>，使得： <span class="math display">\[\nabla f(\mathbf x)+\lambda\nabla g(\mathbf x)=0\tag{1}\label{1}\]</span> <span class="math inline">\(\lambda\neq0\)</span> 即拉格朗日乘子，注意可以取正或取负。</p><p>如果定义拉格朗日函数为： <span class="math display">\[L(\mathbf x,\lambda)\equiv f(\mathbf x)+\lambda g(\mathbf x)\tag{2}\label{2}\]</span> 那么只要令 <span class="math inline">\(\nabla_\mathbf xL=0\)</span>，就得到了 <span class="math inline">\(\eqref{1}\)</span> 式。进一步地，令 <span class="math inline">\(\partial L/\partial \lambda=0\)</span> 就得到了约束条件 <span class="math inline">\(g(\mathbf x)=0\)</span>. 这相当于在求解拉格朗日函数 <span class="math inline">\(L(\mathbf x,\lambda)\)</span> 的驻点。所以我们就把带约束条件的极值问题转化为了无约束条件的极值问题。</p><h2 id="不等约束与-kkt-条件">不等约束与 KKT 条件</h2><p>上一节我们考虑的是等式约束 <span class="math inline">\(g(\mathbf x)=0\)</span>，现在我们考虑不等约束 <span class="math inline">\(g(\mathbf x)\geq 0\)</span> 下的优化问题。</p><p>在不等约束下，解有两种可能：</p><ol type="1"><li><p>落在约束区域以内，即 <span class="math inline">\(g(\mathbf x)&gt;0\)</span>.</p><p>此时，约束条件并没有发挥什么作用，<span class="math inline">\(\mathbf x\)</span> 是驻点的条件就是 <span class="math inline">\(\nabla f(\mathbf x)=0\)</span>，相当于在拉格朗日函数中取 <span class="math inline">\(\lambda=0\)</span>.</p></li><li><p>落在约束区域的边界上，即 <span class="math inline">\(g(\mathbf x)=0\)</span>.</p><p>此时回到了等式约束下的优化问题，通过解拉格朗日函数的驻点求解。但是<strong>为了让 <span class="math inline">\(f(\mathbf x)\)</span> 最大，<span class="math inline">\(\nabla f(\mathbf x)\)</span> 必然是朝向区域 <span class="math inline">\(g(\mathbf x)&gt;0\)</span> 的外面</strong>（如下图所示），否则我们能在 <span class="math inline">\(g(\mathbf x)&gt;0\)</span> 区域内找到更大的 <span class="math inline">\(f(\mathbf x)\)</span>. 由于 <span class="math inline">\(\nabla f(\mathbf x)=-\lambda g(\mathbf x)\)</span>，所以此时 <span class="math inline">\(\lambda&gt;0\)</span>.</p><p><img src="fige.3.png" width=70% /></p></li></ol><p>两种情况可以通过 <span class="math inline">\(\lambda g(\mathbf x)=0\)</span> 统一起来。因此，在 <span class="math inline">\(g(\mathbf x)\geq0\)</span> 的条件下最大化 <span class="math inline">\(f(\mathbf x)\)</span> 的解法是，在以下条件下优化拉格朗日函数 <span class="math inline">\(\eqref{2}\)</span> 式： <span class="math display">\[\begin{align}g(\mathbf x)&amp;\geq 0\\\lambda&amp;\geq0\\\lambda g(\mathbf x)&amp;=0\end{align}\]</span> 这些条件被称作 KKT（Karush-Kuhn-Tucker）条件。</p><p><strong>注意上面是最大化 <span class="math inline">\(f(\mathbf x)\)</span>，如果是最小化 <span class="math inline">\(f(\mathbf x)\)</span>，那么 <span class="math inline">\(\nabla f(\mathbf x)\)</span> 的方向与 <span class="math inline">\(\nabla g(\mathbf x)\)</span> 应该是相同的。若保持 <span class="math inline">\(\lambda\geq0\)</span> 不变，那么应该把拉格朗日函数改写作：<span class="math inline">\(L(\mathbf x,\lambda)=f(\mathbf x)-\lambda g(\mathbf x)\)</span>.</strong></p><p><br/></p><p>若有多个约束条件，对每个条件都引入一个拉格朗日乘子即可。例如，我们希望求解： <span class="math display">\[\begin{align}\max\quad&amp;f(\mathbf x)\\\text{s.t.}\quad&amp;\begin{cases}g_j(\mathbf x)=0,&amp;j=1,\ldots,J\\h_k(\mathbf x)\geq0,&amp;k=1,\ldots,K\\\end{cases}\end{align}\]</span> 那么定义拉格朗日函数为： <span class="math display">\[\begin{align}&amp;L(\mathbf x,\{\lambda_j\},\{\mu_k\})=f(\mathbf x)+\sum_{j=1}^J\lambda_j g_j(\mathbf x)+\sum_{k=1}^K\mu_k h_k(\mathbf x)\\\text{s.t.}\quad&amp;\begin{cases}\mu_k\geq 0,&amp;k=1,\ldots,K\\\mu_kh_k(\mathbf x)=0,&amp;k=1,\ldots,K\end{cases}\end{align}\]</span> 联立 <span class="math inline">\(\nabla_\mathbf xL=0\)</span>、两个约束条件和上述 KKT 条件，解该不等式方程组即可。</p><p>类似的方法也可以扩展到约束条件下的泛函优化问题。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>PRML</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[PRML]Appendix D.Calculus of Variations</title>
    <link href="/blog-main/2023/04/25/PRML-Appendix-D-Calculus-of-Variations/"/>
    <url>/blog-main/2023/04/25/PRML-Appendix-D-Calculus-of-Variations/</url>
    
    <content type="html"><![CDATA[<p>在 PRML 第一章中我们遇到了用变分法（calculus of variations）求解优化问题，那么变分法究竟是什么呢？PRML 在 Appendix D 做了介绍。</p><h2 id="泛函">泛函</h2><p>众所周知，函数是数到数的映射：输入为数值 <span class="math inline">\(x\)</span>，输出为数值 <span class="math inline">\(y(x)\)</span>.</p><p>将函数的概念进行扩展，我们定义泛函（functional）为函数到数的映射：输入为函数 <span class="math inline">\(y(x)\)</span>，输出为数值 <span class="math inline">\(F[y]\)</span>. 直观地讲，泛函就是“函数的函数”。</p><p><strong><em>Example 1</em></strong>：给定平面上的两点 <span class="math inline">\((x_1,y_1),(x_2,y_2)\)</span>，穿过它们的路径有无数条。对其中某条路径 <span class="math inline">\(y=y(x)\)</span>，以这两个点为端点的路径长度可以通过曲线积分得到<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="变分法简介Part 1.（Calculus of Variations） - Dr.Stein的文章 - 知乎 https://zhuanlan.zhihu.com/p/20718489">[1]</span></a></sup>： <span class="math display">\[L=\int_{x_1}^{x_2}\sqrt{1+(y&#39;)^2}\mathrm dx\]</span> <img src="fig.ref1.png" width=30% /></p><p>由于 <span class="math inline">\(L\)</span> 会随着 <span class="math inline">\(y\)</span> 的不同选择而改变，并且 <span class="math inline">\(y\)</span> 本身是一个函数，所以 <span class="math inline">\(L\)</span> 就是 <span class="math inline">\(y\)</span> 的泛函。</p><p><strong><em>Example 2</em></strong>：在著名的最速降线问题中，建立一个 <span class="math inline">\(y\)</span> 轴向下的坐标系，我们欲求出一条轨迹 <span class="math inline">\(y=y(x)\)</span> 使得小球从 <span class="math inline">\((0,0)\)</span> 沿轨迹下降到 <span class="math inline">\((a,b)\)</span> 的用时最短。基于一系列物理定律，我们能推导出所需时间为<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="浅谈变分原理 - 烤羚羊的文章 - 知乎 https://zhuanlan.zhihu.com/p/139018146">[2]</span></a></sup>： <span class="math display">\[T=\frac{1}{\sqrt{2g}}\int_0^a\sqrt{\frac{1+y&#39;^2}{y}}\mathrm dx\]</span> <img src="fig.ref2.jpg" width=30% /></p><p>由于 <span class="math inline">\(T\)</span> 会随着 <span class="math inline">\(y\)</span> 的不同选择而改变，并且 <span class="math inline">\(y\)</span> 本身是一个函数，所以 <span class="math inline">\(T\)</span> 就是 <span class="math inline">\(y\)</span> 的泛函。</p><p><strong><em>Example 3</em></strong>：在模式识别和机器学习中，一个常见的泛函就是熵 <span class="math inline">\(H[x]\)</span>，因为熵是概率密度函数 <span class="math inline">\(p(x)\)</span> 的函数： <span class="math display">\[H=-\int p(x)\ln p(x)\mathrm  dx\]</span> 我们也可以把熵记作 <span class="math inline">\(H[p]\)</span>，来明确表示 <span class="math inline">\(p(x)\)</span> 是自变量。</p><p>变分法是求解泛函极值的一种方法，即找到一个函数 <span class="math inline">\(y(x)\)</span>，使得 <span class="math inline">\(F[y]\)</span> 极大/极小。在上述三个例子中，运用变分法我们可以证明两点之间直线段最短、找到最速降线的表达式或者证明高斯分布使得熵达到最大。</p><h2 id="泰勒展开与泛函导数">泰勒展开与泛函导数</h2><p>回忆对函数 <span class="math inline">\(y(x)\)</span> 求极值的方法——计算导数（derivative）<span class="math inline">\(\mathrm dy/\mathrm dx\)</span> 并令其为零。</p><p>类似的，要求泛函 <span class="math inline">\(F[y]\)</span> 的极值，我们可以求泛函导数（functional derivative）<span class="math inline">\(\delta F/\delta y\)</span> 并令其为零。接下来我们通过模仿一元和多元函数的泰勒展开式来定义什么是泛函导数。</p><p>对一元函数 <span class="math inline">\(y(x)\)</span>，设对 <span class="math inline">\(x\)</span> 做微小扰动 <span class="math inline">\(\epsilon\)</span>，那么有泰勒展开： <span class="math display">\[y(x+\epsilon)=y(x)+{\color{dodgerblue}{\frac{\mathrm dy}{\mathrm dx}}}\epsilon+O(\epsilon^2)\tag{1}\label{1}\]</span> 对多元函数 <span class="math inline">\(y(x_1,\ldots,x_D)\)</span>，设对 <span class="math inline">\(x_i\)</span> 做微小扰动 <span class="math inline">\(\epsilon_i\)</span>，那么有泰勒展开： <span class="math display">\[y(x_1+\epsilon_1,\ldots,x_D+\epsilon_D)=y(x_1,\ldots,x_D)+\sum_{i=1}^D{\color{dodgerblue}{\frac{\partial y}{\partial x_i}}}\epsilon_i+O(\epsilon^2)\tag{2}\label{2}\]</span> 仿照这种做法，对于泛函 <span class="math inline">\(F[y]\)</span>，设对 <span class="math inline">\(y(x)\)</span> 做微小扰动 <span class="math inline">\(\epsilon\eta(x)\)</span>（称为“变分”），其中 <span class="math inline">\(\eta(x)\)</span> 是任意函数，如图所示：</p><p><img src="figd.1.png" width=70% /></p><p>那么可以类比出泰勒展开： <span class="math display">\[F[y(x)+\epsilon\eta(x)]=F[y(x)]+\epsilon\int{\color{dodgerblue}{\frac{\delta F}{\delta y(x)}}}\eta(x)\mathrm dx+O(\epsilon^2)\tag{3}\label{3}\]</span></p><blockquote><p>怎么类比的？</p><p>观察 <span class="math inline">\(\eqref{1},\eqref{2}\)</span> 式，前者自变量为标量，或者说是“1 维向量”，于是其一阶项系数就是一个导数；后者有 <span class="math inline">\(D\)</span> 个自变量，或者说是“<span class="math inline">\(D\)</span> 维向量”，于是其一阶项系数就是 <span class="math inline">\(D\)</span> 个偏导数之和。那么对于泛函 <span class="math inline">\(F[y]\)</span>，它的自变量 <span class="math inline">\(y(x)\)</span> 是函数，可以视为“无穷维向量”，于是其一阶项系数就类比成了积分。</p></blockquote><p>换句话说，对于一个泛函 <span class="math inline">\(F[y]\)</span>，如果其泰勒展开能够写作 <span class="math inline">\(\eqref{3}\)</span> 式的形式，那么就得知了其泛函导数 <span class="math inline">\(\delta F/\delta y\)</span>.</p><h2 id="最简泛函变分">最简泛函变分</h2><p>特别地，我们考虑如下形式的泛函（最简泛函）： <span class="math display">\[F[y]=\int G(y(x),y&#39;(x),x)\mathrm dx\]</span> 其中 <span class="math inline">\(G\)</span> 是 <span class="math inline">\(y(x),y&#39;(x),x\)</span> 的函数。那么： <span class="math display">\[\begin{align}F[y(x)+\epsilon\eta(x)]&amp;=\int G(y(x)+\epsilon\eta(x),y&#39;(x)+\epsilon\eta&#39;(x),x)\mathrm dx\\&amp;=F[y(x)]+\epsilon\int\left(\frac{\partial G}{\partial y(x)}\eta(x)+\frac{\partial G}{\partial y&#39;(x)}\eta&#39;(x)\right)\mathrm dx+O(\epsilon^2)\\\end{align}\tag{4}\label{4}\]</span> 对括号中的第二项运用分部积分： <span class="math display">\[\int\frac{\partial G}{\partial y&#39;(x)}\eta&#39;(x)\mathrm dx=\int\frac{\partial G}{\partial y&#39;(x)}\mathrm d\eta(x)=\frac{\partial G}{\partial y&#39;(x)}\eta(x)-\int\eta(x)\frac{\mathrm d}{\mathrm dx}\left(\frac{\partial G}{\partial y&#39;(x)}\right)\mathrm dx\]</span> 注意上面这些积分其实是定积分，只不过为书写方便省略了积分范围。假设 <span class="math inline">\(\eta(x)\)</span> 在积分边界处为 <span class="math inline">\(0\)</span>，那么上式第一项其实是 <span class="math inline">\(0\)</span>，代回 <span class="math inline">\(\eqref{4}\)</span> 式： <span class="math display">\[F[y(x)+\epsilon\eta(x)]=F[y(x)]+\epsilon\int\left(\frac{\partial G}{\partial y(x)}-\frac{\mathrm d}{\mathrm dx}\left(\frac{\partial G}{\partial y&#39;(x)}\right)\right)\eta(x)\mathrm dx\]</span> 对比 <span class="math inline">\(\eqref{3}\)</span> 式，得知泛函导数就是： <span class="math display">\[\frac{\delta F}{\delta y(x)}=\frac{\partial G}{\partial y(x)}-\frac{\mathrm d}{\mathrm dx}\left(\frac{\partial G}{\partial y&#39;(x)}\right)\]</span> 要求极值，只需令其为零： <span class="math display">\[\frac{\partial G}{\partial y}-\frac{\mathrm d}{\mathrm dx}\left(\frac{\partial G}{\partial y&#39;}\right)=0\]</span> 这被称作欧拉-拉格朗日方程（Euler-Lagrange Equation）。</p><h2 id="例子">例子</h2><p>在 PRML 第一章中有两个地方用到了变分法，当时直接给出了结果，现在我们回过头来看一下解的过程。</p><p>在信息论一节中，我们遇到了最大微分熵的问题： <span class="math display">\[\begin{align}\max_{p(x)}&amp;\quad H[x]=-\int_{-\infty}^{+\infty}p(x)\ln p(x)\mathrm dx\\\text{s.t.}&amp;\quad \begin{cases}&amp;\int_{-\infty}^{+\infty}p(x)\mathrm dx=1\\&amp;\int_{-\infty}^{+\infty}xp(x)\mathrm dx=\mu\\&amp;\int_{-\infty}^{+\infty}(x-\mu)^2p(x)\mathrm dx=\sigma^2\end{cases}\end{align}\]</span> 由于这是一个带有约束条件的优化问题，所以先使用拉格朗日乘数法： <span class="math display">\[\begin{align}\mathcal L[p]=&amp;-\int_{-\infty}^{+\infty}p(x)\ln p(x)\mathrm dx \\&amp;+\lambda_1\left(\int_{-\infty}^{+\infty}p(x)\mathrm dx-1\right) \\&amp;+\lambda_2\left(\int_{-\infty}^{+\infty}xp(x)\mathrm dx-\mu\right) \\&amp;+\lambda_3\left(\int_{-\infty}^{+\infty}(x-\mu)^2p(x)\mathrm dx-\sigma^2\right)\end{align}\]</span> 虽然看着繁冗，但这依然是最简泛函的形式： <span class="math display">\[G(p(x),p&#39;(x),x)=-p(x)\ln p(x)+\lambda_1 p(x)+\lambda_2 xp(x)+\lambda_3(x-\mu)^2p(x)\]</span> 甚至还不包含 <span class="math inline">\(p&#39;(x)\)</span>，所以直接： <span class="math display">\[\frac{\partial G}{\partial p(x)}=-\ln p(x)-1+\lambda_1+\lambda_2 x+\lambda_3(x-\mu)^2=0\]</span> 得： <span class="math display">\[p(x)=\exp\left(-1+\lambda_1+\lambda_2x+\lambda_3(x-\mu)^2\right)\]</span> 再结合约束条件，一通代入计算即可解出高斯分布。</p><p><br/></p><p>在决策论一节中，出现了这样的优化问题（注意自变量是 <span class="math inline">\(y(\mathbf x)\)</span>，<span class="math inline">\(p(\mathbf x,t)\)</span> 是已知量）： <span class="math display">\[\min_{y(\mathbf x)}\mathbb E[L]=\iint (y(\mathbf x)-t)^2p(\mathbf x,t)\mathrm d\mathbf x\mathrm d t\]</span> 好像看着有点复杂，但还是最简泛函的形式： <span class="math display">\[G(y(\mathbf x),y&#39;(\mathbf x),\mathbf x)=\int(y(\mathbf x)-t)^2p(\mathbf x,t)\mathrm dt\]</span> 所以直接： <span class="math display">\[\frac{\partial G}{\partial y(\mathbf x)}=2\int(y(\mathbf x)-t)p(\mathbf x,t)\mathrm dt=0\]</span> 解得： <span class="math display">\[y(\mathbf x)=\frac{\int tp(\mathbf x,t)\mathrm dt}{p(\mathbf x)}=\int t p(\mathrm t\vert \mathbf x)\mathrm dt=\mathbb E[t\vert\mathbf x]\]</span></p><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>变分法简介Part 1.（Calculus of Variations） - Dr.Stein的文章 - 知乎 https://zhuanlan.zhihu.com/p/20718489 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>浅谈变分原理 - 烤羚羊的文章 - 知乎 https://zhuanlan.zhihu.com/p/139018146 <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>PRML 学习笔记（附录）：变分法 (Calculus of Variations) - Lucius的文章 - 知乎 https://zhuanlan.zhihu.com/p/610516538 <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>PRML</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[PRML]1.Introduction</title>
    <link href="/blog-main/2023/04/23/PRML-1-Introduction/"/>
    <url>/blog-main/2023/04/23/PRML-1-Introduction/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\bsf}[1]{\boldsymbol{\mathsf{ #1}}}\]</span></p><h2 id="example-polynomial-curve-fitting">Example: Polynomial Curve Fitting</h2><p>PRML 的第一章是围绕着一个简单的回归问题——多项式拟合展开的。问题虽然简单，但其中蕴藏着许多奥妙。作者分别阐述了概率论、决策论和信息论三个贯穿全书的重要工具，展示了频率学派和贝叶斯学派面对问题的不同思考与处理手段，尤其侧重于贝叶斯方法相比频率方法体现出的优势。对于只看过吴恩达入门机器学习的我来说，本章直接为我踹开了贝叶斯的大门，刷新了我的认知。</p><p>废话不多说，书中举例的回归问题如下图所示：</p><p><img src="fig1.2.png" width=70% /></p><p>在这个例子中，训练数据（蓝色空心圆点）是基于 <span class="math inline">\(\sin(2\pi x)\)</span>（绿色曲线）添加随机高斯噪声生成的。记训练集为 <span class="math inline">\(\bsf{x}\equiv (x_1,\ldots,x_N)^\mathrm T\)</span>，对应的观测值为 <span class="math inline">\(\bsf{t}\equiv(t_1,\ldots,t_N)^\mathrm T\)</span>，图 1.2 展示了包含 10 个样本的训练集。我们希望从数据集中找到一些规律，使得询问一个新的 <span class="math inline">\(x\)</span> 时能预测其对应 <span class="math inline">\(t\)</span> 的值。</p><p>考虑用多项式做回归： <span class="math display">\[y(x,\mathbf w)=w_0+w_1x+w_2x^2+\cdots+w_Mx^M=\sum_{j=0}^M w_jx^j\]</span> 其中 <span class="math inline">\(\mathbf w=(w_0,\ldots,w_M)^\mathrm T\)</span> 是模型的参数，<span class="math inline">\(M\)</span> 为多项式的阶数。这是一个线性模型（linear model），因为 <span class="math inline">\(y(x,\mathbf w)\)</span> 是关于 <span class="math inline">\(\mathbf w\)</span> 的线性函数（尽管关于 <span class="math inline">\(x\)</span> 是非线性的）。</p><p>为了拟合训练数据，我们会定义一个误差函数并最小化之： <span class="math display">\[E(\mathbf w)=\frac{1}{2}\sum_{n=1}^N(y(x_n,\mathbf w)-t_n)^2\]</span> 通过求导取零一通计算，这个优化问题可以算出一个闭式解。不过先别急着算，这里还有一个问题尚待解决——如何选择 <span class="math inline">\(M\)</span>？不同的 <span class="math inline">\(M\)</span> 对应了不同的模型，会导致效果完全不同的解，如下图所示：</p><p><img src="fig1.4.png" width=70% /></p><p>有点机器学习基础的同学都知道，<span class="math inline">\(M=0,1\)</span> 时模型能力不够，发生了欠拟合；而 <span class="math inline">\(M=9\)</span> 时模型完美地穿过了所有训练数据，但会在测试数据上表现极差，发生了过拟合；<span class="math inline">\(M=3\)</span> 则刚刚好。过拟合现象其实挺讽刺的——<span class="math inline">\(M=9\)</span> 明明包含了 <span class="math inline">\(M=3\)</span>，它理应表现得至少不比后者差；另外，如果要用多项式无限逼近 <span class="math inline">\(\sin\)</span> 函数，甚至需要无限阶的多项式（泰勒级数），所以我们期待 <span class="math inline">\(M\)</span> 越大、模型效果越好才对。</p><p><span class="math inline">\(M=9\)</span> 的多项式有 10 个自由参数，所以刚好能拟合大小为 10 的训练集。那如果我们加大训练集的规模呢？下图展示了 15 和 100 个数据点下的结果：</p><p><img src="fig1.6.png" width=70% /></p><p>可以看见，随着训练集规模增大，过拟合现象得到了缓解。因此，一个启发式的经验是说，参数量应该比数据量少若干倍（如 5~10 倍）。但是，这样的解决方案其实不是很让人满意，我们更希望参数量与问题的复杂程度挂钩，而不是与数据量挂钩。<strong>在下文中，我们会看到上述最小二乘法的本质其实是极大似然估计，而过拟合是极大似然估计的一般属性。相反，贝叶斯方法可以避免过拟合问题。事实上，当参数量多于数据量时，贝叶斯模型能够自适应地调节有效参数量。</strong></p><p>另一个常见的解决过拟合的方案是<strong>正则化。</strong>如果我们考察不同 <span class="math inline">\(M\)</span> 下解出来的 <span class="math inline">\(\mathbf w\)</span> 数值，如下表所示：</p><p><img src="table1.1.png" width=70% /></p><p>可以看见，随着 <span class="math inline">\(M\)</span> 增大，高阶项的系数（绝对值）变得异常的大。为了惩罚过大的参数值，我们可以将参数的平方和加入误差函数，并用系数 <span class="math inline">\(\lambda\)</span> 调节其大小： <span class="math display">\[\tilde E(\mathbf w)=\frac{1}{2}\sum_{n=1}^N(y(x_n,\mathbf w)-t_n)^2+\frac{\lambda}{2}\Vert\mathbf w\Vert^2\]</span> 依旧取 <span class="math inline">\(M=9\)</span>，在合适的正则化参数下，拟合结果就平滑了许多。当然，如果正则化过分了，拟合得就不好了：</p><p><img src="fig1.7.png" width=70% /></p><p>在正则化的作用下，<span class="math inline">\(\mathbf w\)</span> 的数值得到了控制：</p><p><img src="table1.2.png" width=70% /></p><h2 id="probability-theory">Probability Theory</h2><p>基础的概率知识直接跳过，我们从贝叶斯说起。</p><h3 id="bayesian-probabilities">Bayesian probabilities</h3><p>经典频率学派将概率视作随机可重复事件发生的频率，而在贝叶斯视角下，概率是对不确定性（uncertainty）的度量。考虑一个不确定的事件，例如北极的冰川是否会在世纪末消失，这可不是一个可以重复试验的事情，但是我们依旧能对冰川的融化速度有大致的评估。如果我们现在有了一些新的观测，我们也许会更新之前的评估，进而调整我们的动作，例如减少温室气体的排放。这个过程可以用贝叶斯概率做定量地描述。</p><p>在上文多项式拟合的例子中，我们可以用概率来表示模型参数 <span class="math inline">\(\mathbf w\)</span> 的不确定性。我们首先为其指定一个先验概率（prior） <span class="math inline">\(p(\mathbf w)\)</span>，再设观察到的数据为 <span class="math inline">\(\mathcal D=\{t_1,\ldots,t_N\}\)</span>，那么数据在当前参数下的似然（likelihood）就是 <span class="math inline">\(p(\mathcal D\vert \mathbf w)\)</span>，于是，根据贝叶斯定理，我们可以计算后验概率（posterior）： <span class="math display">\[p(\mathbf w\vert \mathcal D)=\frac{p(\mathcal D\vert \mathbf w)p(\mathbf w)}{p(\mathcal D)}\]</span> 其中 <span class="math inline">\(p(\mathcal D)=\int p(\mathcal D\vert \mathbf w)p(\mathbf w)\mathrm d\mathbf w\)</span> 是归一化系数。后验概率说明了我们在观察到数据 <span class="math inline">\(\mathcal D\)</span> 之后对 <span class="math inline">\(\mathbf w\)</span> 的不确定性的更新。用先验、后验和似然的术语，贝叶斯定理可以表述为： <span class="math display">\[\text{posterior}\propto\text{likelihood}\times\text{prior}\]</span> 可见，在贝叶斯视角下，模型参数 <span class="math inline">\(\mathbf w\)</span> 具有不确定性、是个变量，服从一个概率分布。我们还能根据新的观察更新这个概率分布。相反，在频率学派视角下，<span class="math inline">\(\mathbf w\)</span> 是一个固定的、客观存在的参数，只不过需要我们去估计它。例如，让似然 <span class="math inline">\(p(\mathcal D\vert \mathbf w)\)</span> 最大的解就是一种估计，这种方法被称作极大似然估计（MLE）。</p><h3 id="the-gaussian-distribution">The Gaussian distribution</h3><p>上文说过，最小二乘法本质就是极大似然估计，为了建立二者的联系，我们先考虑另一个问题：用高斯分布为数据的分布建模。假设数据集为 <span class="math inline">\(N\)</span> 个（独立同分布）标量 <span class="math inline">\(\bsf x=(x_1,\ldots,x_N)^\mathrm T\)</span>，那么在高斯分布 <span class="math inline">\(\mathcal N(\mu,\sigma^2)\)</span> 下，数据的似然为： <span class="math display">\[p(\bsf x\vert\mu,\sigma^2)=\prod_{n=1}^N \mathcal N(x_n\vert\mu,\sigma^2)\]</span> 最大化似然等价于最小化负对数似然： <span class="math display">\[-\ln p(\bsf x\vert\mu,\sigma^2)=\frac{1}{2\sigma^2}\sum_{n=1}^N(x_n-\mu)^2+\frac{N}{2}\ln\sigma^2+\frac{N}{2}\ln(2\pi)\]</span> 求导取零，解得： <span class="math display">\[\begin{cases}\mu_{\text{ML}}=\frac{1}{N}\sum_{n=1}^N x_n\\\sigma^2_{\text{ML}}=\frac{1}{N}\sum_{n=1}^N(x_n-\mu_{\text{ML}})^2\end{cases}\]</span> 即极大似然估计的均值和方差分别是样本均值和样本方差。然而，这样计算得到的方差是有偏的——它比真正的方差偏小。为了验证这一点，我们可以假设数据采样自 <span class="math inline">\(\mathcal N(\mu,\sigma^2)\)</span>，那么经过计算有： <span class="math display">\[\mathbb E[\mu_\text{ML}]=\frac{1}{N}\sum_{n=1}^N\mathbb E[x_n]=\mu\]</span></p><p><span class="math display">\[\begin{align}\mathbb E[\sigma_\text{ML}^2]&amp;=\frac{1}{N}\sum_{n=1}^N\mathbb E\left[\left(x_n-\frac{1}{N}\sum_{i=1}^N x_i\right)^2\right]\\&amp;=\frac{1}{N}\sum_{n=1}^N\mathbb E\left[x_n^2-\frac{2}{N}x_n\left(\sum_{i=1}^Nx_i\right)+\frac{1}{N^2}\left(\sum_{i=1}^Nx_i\right)^2\right]\\&amp;=\frac{1}{N}\sum_{n=1}^N\mathbb E[x_n^2]-\frac{1}{N^2}\mathbb E\left[\left(\sum_{i=1}^Nx_i\right)^2\right]\\&amp;=\mu^2+\sigma^2-\frac{1}{N^2}\left(N(\mu^2+\sigma^2)+N(N-1)\mu^2\right)\\&amp;=\frac{N-1}{N}\sigma^2\end{align}\]</span></p><blockquote><p>推导过程中用了高斯分布的二阶矩为 <span class="math inline">\(\mu^2+\sigma^2\)</span> 的结论。</p></blockquote><p>下图直观地描绘了这个问题：</p><p><img src="fig1.15.png" width=70% /></p><p>当 <span class="math inline">\(N\to\infty\)</span> 时，<span class="math inline">\(\sigma^2_\text{ML}\to\sigma^2\)</span>，这个偏差（bias）不会引起太大的问题。但是本书将会考虑有很多参数的复杂模型，这时偏差问题就会很严重。<strong>事实上，我们将看到，极大似然估计带来的偏差问题正是过拟合的根源所在</strong>。</p><h3 id="curve-fitting-re-visited">Curve fitting re-visited</h3><p>现在让我们回过头来，从概率角度重新审视多项式曲线拟合问题。我们为模型的预测值赋以不确定性，并用高斯分布来建模： <span class="math display">\[p(t\vert x,\mathbf w,\beta)=\mathcal N(t\vert y(x,\mathbf w),\beta^{-1})\]</span> <img src="fig1.16.png" width=70% /></p><p>于是乎，使用极大似然估计，我们有似然： <span class="math display">\[p(\bsf t\vert \bsf x,\mathbf w,\beta)=\prod_{n=1}^N\mathcal N(t_n\vert x_n,\mathbf w,\beta^{-1})\]</span> 负对数似然： <span class="math display">\[-\ln p(\bsf t\vert\bsf x,\mathbf w,\beta)=\frac{\beta}{2}\sum_{n=1}^N(y(x_n,\mathbf w)-t_n)^2-\frac{N}{2}\ln\beta+\frac{N}{2}\ln(2\pi)\]</span> 为解 <span class="math inline">\(\mathbf w_\text{ML}\)</span>，由于后两项与 <span class="math inline">\(\mathbf w\)</span> 无关，所以可以直接丢掉；第一项的系数也与 <span class="math inline">\(\mathbf w\)</span> 无关，可以换成 <span class="math inline">\(\frac{1}{2}\)</span>——那么我们就得到了第一节中的平方和误差函数。<strong>因此，最小二乘法就是在高斯分布假设下的极大似然估计。</strong></p><p>当然，上式还有另一个参数 <span class="math inline">\(\beta\)</span>，根据上一小节的结论，其解为： <span class="math display">\[\frac{1}{\beta_\text{ML}}=\frac{1}{N}\sum_{n=1}^N(y(x_n,\mathbf w)-t_n)^2\]</span> 有了 <span class="math inline">\(\mathbf w_\text{ML}\)</span> 和 <span class="math inline">\(\beta_\text{ML}\)</span>，我们在预测时就不只是给出一个点，而是一个概率分布了： <span class="math display">\[p(t\vert x,\mathbf w_\text{ML},\beta_\text{ML})=\mathcal N(t\vert y(x,\mathbf w_\text{ML}),\beta_\text{ML}^{-1})\]</span> 虽然我们已经从点估计跃升为了预测概率分布，但这还不是贝叶斯，毕竟极大似然估计依旧在频率学派的范畴。前文提及，贝叶斯视角下的模型参数 <span class="math inline">\(\mathbf w\)</span> 也由概率分布描述。为简便起见，我们将先验分布设为以 <span class="math inline">\(\alpha\)</span> 为参数的多元高斯分布： <span class="math display">\[p(\mathbf w\vert\alpha)=\mathcal N(\mathbf w\vert \mathbf 0,\alpha^{-1}\mathbf I)=\left(\frac{\alpha}{2\pi}\right)^{(M+1)/2}\exp\left(-\frac{\alpha}{2}\mathbf w^\mathrm T\mathbf w\right)\]</span> 注意 <span class="math inline">\(\mathbf w\)</span> 有 <span class="math inline">\(M+1\)</span> 维而不是 <span class="math inline">\(M\)</span> 维。这里的 <span class="math inline">\(\alpha\)</span> 是人为设置的参数，即超参数。那么根据贝叶斯定理，我们有： <span class="math display">\[\underbrace{p(\mathbf w\vert\bsf x,\bsf t,\alpha,\beta)}_\text{posterior}\propto \underbrace{p(\bsf t\vert\bsf x,\mathbf w,\beta)}_\text{likelihood}\ \underbrace{p(\mathbf w\vert\alpha)}_\text{prior}\]</span> 现在，我们可以求出一个使后验分布最大的 <span class="math inline">\(\mathbf w^\ast\)</span>，这被称为最大后验估计（MAP）： <span class="math display">\[\begin{align}\mathbf w^\ast&amp;=\mathop{\text{argmax}}_{\mathbf w}\ p(\mathbf w\vert \bsf x,\bsf t,\alpha,\beta)\\&amp;=\mathop{\text{argmax}}_{\mathbf w}\ p(\bsf t\vert\bsf x,\mathbf w,\beta)p(\mathbf w\vert \alpha)\\&amp;=\mathop{\text{argmax}}_{\mathbf w}\ \ln p(\bsf t\vert\bsf x,\mathbf w,\beta)+\ln p(\mathbf w\vert\alpha)\\&amp;=\mathop{\text{argmin}}_{\mathbf w}\ \frac{\beta}{2}(y(x_n,\mathbf w)-t_n)^2+\frac{\alpha}{2}\mathbf w^\mathrm T\mathbf w\end{align}\]</span> 可以看到，第一项就是极大似然估计（最小二乘法）的优化目标，而第二项就是正则项。所以说，<strong>最大后验估计等价于极大似然估计加上一个与先验分布有关的正则项</strong>。</p><h3 id="bayesian-curve-fitting">Bayesian curve fitting</h3><p>虽然我们现在用先验、后验概率分布来描述参数 <span class="math inline">\(\mathbf w\)</span>，但是 MAP 给出的是点估计，所以依旧不能算做是完全的贝叶斯。<strong>完全的贝叶斯方法要求一贯使用概率的 sum rule 和 product rule 推导（而不是推一半突然取个 argmax），这往往需要我们对所有的 <span class="math inline">\(\mathbf w\)</span> 积分</strong>。不幸的是，这个积分不总是容易计算的，如何计算、估计或绕开这个积分成为了很多研究的关注点。</p><blockquote><p>本书中，sum rule 指的是 <span class="math inline">\(p(X)=\sum_Y p(X,Y)\)</span>；product rule 指的是 <span class="math inline">\(p(X,Y)=p(Y\vert X)p(X)\)</span>.</p></blockquote><p>在多项式曲线拟合问题中，我们已知的是 <span class="math inline">\(\bsf x,\bsf t\)</span>，目标是给定 <span class="math inline">\(x\)</span>，预测对应的 <span class="math inline">\(t\)</span>，因此我们希望求的是 <span class="math inline">\(p(t\vert x,\bsf x,\bsf t)\)</span>. 这里，我们假设 <span class="math inline">\(\alpha,\beta\)</span> 是固定已知的。</p><p>把 <span class="math inline">\(\bsf x,\bsf t\)</span> 和 <span class="math inline">\(x,t\)</span> 联系起来的是我们的回归模型，因此： <span class="math display">\[p(t\vert x,\bsf x,\bsf t)=\int p(t\vert x,\mathbf w)p(\mathbf w\vert \bsf x,\bsf t)\mathrm d\mathbf w\]</span> 右式中，<span class="math inline">\(p(t\vert x,\mathbf w)\)</span> 即模型预测的概率分布，定义在上一小节的最开始处；而 <span class="math inline">\(p(\mathbf w\vert\bsf x,\bsf t)\)</span> 是参数的后验分布，由贝叶斯公式计算得到： <span class="math display">\[p(\mathbf w\vert\bsf x,\bsf t)=\frac{p(\bsf t\vert\bsf x,\mathbf w,\beta)p(\mathbf w\vert\alpha)}{p(\bsf t\vert\bsf x)}=\frac{p(\bsf t\vert\bsf x,\mathbf w,\beta)p(\mathbf w\vert\alpha)}{\int p(\bsf t\vert\bsf x,\mathbf w,\beta)p(\mathbf w\vert\alpha)\mathrm d\mathbf w}\]</span> 分子中的似然和先验都在上一小节有所定义；而分母出现了棘手的对 <span class="math inline">\(\mathbf w\)</span> 的积分。幸运的是，在多项式曲线拟合问题中，所有的积分都可以计算出解析形式，因此最后我们能得到解析解。具体解的过程和结果此处略去，相关内容会在书的第三章详细阐述。结果可以绘制为下图：</p><p><img src="fig1.17.png" width=80% /></p><h2 id="the-curse-of-dimensionality">The Curse of Dimensionality</h2><p>多项式曲线拟合问题只有一个输入变量 <span class="math inline">\(x\)</span>，但是很多实际问题会涉及到更多的变量，这时我们会遇到维度灾难。书中举了这样的一个例子：每条数据有 12 个属性，即由一个 12 维向量表示，共分为 3 类。其中 <span class="math inline">\((x_6,x_7)\)</span> 这两维的特征可以可视化为下图：</p><p><img src="fig1.19.png" width=70% /></p><p>如果我们想对 × 做分类，一个简单的做法是把空间分成若干小格，每一个格子的类别定义为落在其中的点的大多数类别；那么询问一个新的数据点时，我们看它落在哪一个格子里即可，如下图所示：</p><p><img src="fig1.20.png" width=70% /></p><p>当然，这个方法比较 naive，存在很多问题，但是最重要的问题之一就是维度灾难。当维度从 2 维上升到更高维时，用来划分高维空间的格子数量将呈指数增长，那么，为了让每个格子里有足够多的数据点，所需要的数据数量也就随之呈指数增加。如下图所示：</p><p><img src="fig1.21.png" width=70% /></p><p>我们也可以从多项式拟合问题里看到维度灾难。假设我们有 <span class="math inline">\(D\)</span> 个输入变量，那么一个 3 阶多项式将长这样： <span class="math display">\[y(\mathbf x,\mathbf w)=w_0+\sum_{i=1}^D w_ix_i+\sum_{i=1}^D\sum_{j=1}^D w_{ij}x_ix_j+\sum_{i=1}^D\sum_{j=1}^D\sum_{k=1}^D w_{ijk}x_ix_jx_k\]</span> 也就是说，<span class="math inline">\(M\)</span> 阶多项式的参数数量将变成 <span class="math inline">\(O(D^M)\)</span>. 虽然这是幂增长而非指数增长，但依旧增长得很快，使得模型变得笨重而难以实用。</p><p>当维度变高后，很多低维空间下的直觉将变得不再正确。例如，设有 <span class="math inline">\(D\)</span> 维空间下的一个单位超球体，考虑位于半径 <span class="math inline">\(r=1-\epsilon\)</span> 到 <span class="math inline">\(r=1\)</span> 之间部分的（相对）体积： <span class="math display">\[\frac{V_D(1)-V_D(1-\epsilon)}{V_D(1)}=1-(1-\epsilon)^D\]</span> 当 <span class="math inline">\(D\)</span> 很大时，即便 <span class="math inline">\(\epsilon\)</span> 较小，这个比例依旧会接近 <span class="math inline">\(1\)</span>，可以做图以直观展示：</p><p><img src="fig1.22.png" width=70% /></p><p>这意味着，高维空间中的一个超球体，其大部分体积都集中在接近表面的薄薄的一层上！</p><p>高维高斯分布也有类似的情况。在极坐标下，做出 <span class="math inline">\(p(r)\)</span> 关于 <span class="math inline">\(r\)</span> 的图，可以看见大部分概率密度集中在某一个特定 <span class="math inline">\(r\)</span> 的附近：</p><p><img src="fig1.23.png" width=70% /></p><p>虽然如此，在实践中我们依旧能够有效地处理高维数据，原因有两点：</p><ol type="1"><li>真实数据往往处于低维的子空间/流形上；</li><li>真实数据往往有一定的光滑性，即输入变量的微小变化会引起目标变量的微小变化，因此我们可以用插值等方式对新的输入做预测。</li></ol><h2 id="decision-theory">Decision Theory</h2><p>本节我们围绕一个非常经典的例子——癌症诊断展开：输入一张 X 光片 <span class="math inline">\(\mathbf x\)</span>，决定病人是否患有癌症。这是一个分类问题，我们用类别 <span class="math inline">\(\mathcal C_1\)</span> 或 <span class="math inline">\(t=0\)</span> 表示患癌，类别 <span class="math inline">\(\mathcal C_2\)</span> 或 <span class="math inline">\(t=1\)</span> 表示健康。</p><p>使用第二节概率论的方法，我们能够推断出一些概率分布，其中核心是联合概率 <span class="math inline">\(p(\mathbf x,\mathcal C_k)\)</span>（条件概率或边缘概率都可以通过相关公式由联合概率推出来）。给定这些概率，我们如何决定病人是否真的患有癌症呢？这就是决策论要解决的问题。</p><p>直觉上，我们会选取 <span class="math inline">\(p(\mathcal C_k\vert \mathbf x)\)</span> 最大的那一类 <span class="math inline">\(k\)</span>. 的确，我们稍后会看到这确实是让错误率最小的方法。然而，如果我们的目标不是最小化错误率，那么这个决策可能会发生变化。</p><h3 id="minimizing-the-misclassification-rate">Minimizing the misclassification rate</h3><p>对于一种决策，设区域 <span class="math inline">\(\mathcal R_k\)</span> 内的数据点会被分类为 <span class="math inline">\(C_k\)</span>，这些决策区域的边界被称作“决策边界”。假设我们的目标是最小化错误分类率，那么在癌症诊断的例子中，错误率为： <span class="math display">\[\begin{align}p(\text{mistake})&amp;=p(\mathbf x\in \mathcal R_1,\mathcal C_2)+p(\mathbf x\in \mathcal R_2,\mathcal C_1)\\&amp;=\int_{\mathcal R_1} p(\mathbf x, \mathcal C_2)\mathrm d\mathbf x+\int_{\mathcal R_2} p(\mathbf x, \mathcal C_1)\mathrm d\mathbf x\end{align}\]</span> 因此，要让错误率最小，如果 <span class="math inline">\(p(\mathbf x,\mathcal C_1)&gt;p(\mathbf x,\mathcal C_2)\)</span>，我们就应该把 <span class="math inline">\(\mathbf x\)</span> 分类为 <span class="math inline">\(\mathcal C_1\)</span>.</p><p>拓展到 <span class="math inline">\(K\)</span> 分类： <span class="math display">\[p(\text{correct})=\sum_{k=1}^K p(\mathbf x\in\mathcal R_k,\mathcal C_k)=\sum_{k=1}^K\int_{\mathcal R_k}p(\mathbf x,\mathcal C_k)\mathrm d\mathbf x\]</span> 要让正确率最大，我们的决策就是把数据点 <span class="math inline">\(\mathbf x\)</span> 分类给 <span class="math inline">\(p(\mathbf x,\mathcal C_k)\)</span> 最大的那个 <span class="math inline">\(k\)</span>. 考虑到 <span class="math inline">\(p(\mathbf x,\mathcal C_k)=p(C_k\vert\mathbf x)p(\mathbf x)\)</span>，这个决策等价于把 <span class="math inline">\(\mathbf x\)</span> 分类给具有最大后验概率 <span class="math inline">\(p(\mathcal C_k\vert\mathbf x)\)</span> 的 <span class="math inline">\(k\)</span>，正如上文所言。</p><h3 id="minimizing-the-expected-loss">Minimizing the expected loss</h3><p>很多应用场景中，我们的目标比单单最小化错误率要复杂。例如癌症诊断，如果把一个健康的人诊断为患癌，那么大不了再多做点检测；但如果把一个患癌的人诊断为健康，那就要对他/她的生命负责了。虽然两种情况都是误诊，但后者的后果更为严重。为此，我们可以定义一个 loss matrix <span class="math inline">\(L\)</span>，<span class="math inline">\(L_{kj}\)</span> 表示真实类别为 <span class="math inline">\(k\)</span> 但是预测类别为 <span class="math inline">\(j\)</span> 的损失值，并试图最小化损失的期望： <span class="math display">\[\mathbb E[L]=\sum_k\sum_j\int_{\mathcal R_j} L_{kj}p(\mathbf x,\mathcal C_k)\mathrm d\mathbf x\]</span> 那么要使上式最小，我们的决策是把 <span class="math inline">\(\mathbf x\)</span> 分类给使得 <span class="math inline">\(\sum_k L_{kj}p(\mathbf x,\mathcal C_k)\)</span> 最小的 <span class="math inline">\(j\)</span>；或等价地，使得 <span class="math display">\[\sum_kL_{kj}p(\mathcal C_k\vert\mathbf x)\]</span> 最小的 <span class="math inline">\(j\)</span>.</p><p>例如，在癌症诊断的例子中，我们可以定义 <span class="math inline">\(L=\begin{bmatrix}0&amp;1000\\1&amp;0\end{bmatrix}\)</span>，让患癌但漏诊的损失非常大，从而减小这种情况的发生。</p><h3 id="the-reject-option">The reject option</h3><p>当最大的 <span class="math inline">\(p(\mathcal C_k\vert \mathbf x)\)</span> 都比 <span class="math inline">\(1\)</span> 小很多时，我们对 <span class="math inline">\(\mathbf x\)</span> 的类别预测有很大的不确定性，这个时候不如拒绝为其分类。我们可以设置一个阈值 <span class="math inline">\(\theta\)</span>，仅当 <span class="math inline">\(\max_k p(\mathcal C_k\vert \mathbf x)&gt;\theta\)</span> 时做出分类决策，反之拒绝分类（例如交给人工检测是否患癌）。</p><h3 id="inference-and-decision">Inference and decision</h3><p>通过上文的叙述，我们看到一个分类问题被划分为了两个阶段——先推断（inference），再决策（decision）。在推断阶段，我们用概率论方法获得 <span class="math inline">\(p(\mathbf x,\mathcal C_k)\)</span> 或 <span class="math inline">\(p(\mathcal C_k\vert \mathbf x)\)</span>，然后在决策阶段根据已得的概率值决定分类结果。另一种方法是一步到位——直接学习一个函数将输入 <span class="math inline">\(\mathbf x\)</span> 映射到决策，这样的函数被称作 discriminant function.</p><p>事实上，我们有三种解决决策问题的方案：</p><ol type="1"><li><p><strong>Generative models</strong>：首先对每个 <span class="math inline">\(\mathcal C_k\)</span> 推断 <span class="math inline">\(p(\mathbf x\vert\mathcal C_k)\)</span> 和 <span class="math inline">\(p(\mathcal C_k)\)</span>，然后运用贝叶斯定理： <span class="math display">\[p(\mathcal C_k\vert\mathbf x)=\frac{p(\mathbf x\vert\mathcal C_k)p(\mathcal C_k)}{p(\mathbf x)}=\frac{p(\mathbf x\vert\mathcal C_k)p(\mathcal C_k)}{\sum_kp(\mathbf x\vert\mathcal C_k)p(\mathcal C_k)}\]</span> 计算后验概率 <span class="math inline">\(p(\mathcal C_k\vert \mathbf x)\)</span>. 等价地，也可以先推断出联合分布 <span class="math inline">\(p(\mathbf x,\mathcal C_k)\)</span>，然后归一化得到后验概率。如此建模的模型被称作生成模型，因为我们可以从 <span class="math inline">\(p(\mathbf x\vert\mathcal C_k)\)</span> 中采样生成合成数据。</p></li><li><p><strong>Discriminative models</strong>：直接推断后验概率 <span class="math inline">\(p(\mathcal C_k\vert\mathbf x)\)</span>，然后依其做决策。如此建模的模型被称作判别模型。</p></li><li><p><strong>Discriminant function</strong>：将输入 <span class="math inline">\(\mathbf x\)</span> 直接映射到其类别标签 <span class="math inline">\(f(\mathbf x)\)</span>，跳过所有概率。</p></li></ol><p>对生成模型而言，在实际应用中 <span class="math inline">\(\mathbf x\)</span> 的维度常常很高，需要大量的数据来足够精确地估计 <span class="math inline">\(p(\mathbf x\vert\mathcal C_k)\)</span>. 而先验分布 <span class="math inline">\(p(\mathcal C_k)\)</span> 可以通过统计训练集获得。其优势在于能够计算边缘分布 <span class="math inline">\(p(\mathbf x)\)</span>，在离群点检测（outlier detection）等方面有所应用。然而，如果只为了解决分类问题，那么没有必要费劲建模 <span class="math inline">\(p(\mathbf x\vert\mathcal C_k)\)</span> 或联合分布，直接用判别模型建模 <span class="math inline">\(p(\mathcal C_k\vert\mathbf x)\)</span> 即可。</p><p>第三种方案抛弃了后验概率，但我们认为后验概率在很多时候还是很有用的：</p><ul><li><p><strong>Minimizing risk</strong>. 如果一个问题的 loss matrix 会随时间不断变化（在金融中很常见），那么有后验概率我们可以随时调整决策，但如果只有 discriminant function，每次变化就要重新训练一遍；</p></li><li><p><strong>Reject option</strong>. 根据后验概率的大小，我们可以拒绝分类，如上一小节所述；</p></li><li><p><strong>Compensating for class priors</strong>. 很多分类问题面临类别不平衡问题，比如癌症的 X 光片数量远少于健康的 X 光片数量。这时要训练一个好的分类器是很困难的，因为就算分类器无论输入是什么都输出健康，那它的正确率也非常高。可行的解决方案是人为构造一个类别平衡的数据集（如在多类中只采样和少类一样多的样本），在上面训练模型。但由于我们更改了数据类别的分布，所以回归实际应用时应该做相应的补偿。具体而言，假设原本数据的类别分布为 <span class="math inline">\(p(\mathcal C_k)\)</span>，更改后为 <span class="math inline">\(\tilde p(\mathcal C_k)\)</span>，在更改后的数据上训练的模型为 <span class="math inline">\(\tilde p(\mathcal C_k\vert\mathbf x)\)</span>，根据贝叶斯定理： <span class="math display">\[\tilde p(\mathcal C_k\vert\mathbf x)\propto \tilde p(\mathcal C_k)p(\mathbf x\vert\mathcal C_k)\]</span> 注意类别条件分布 <span class="math inline">\(p(\mathbf x\vert\mathcal C_k)\)</span> 是不会因为我们对数据集的更改而变化的，所以： <span class="math display">\[p(\mathcal C_k\vert\mathbf x)\propto \tilde p(\mathcal C_k\vert\mathbf x)\cdot\frac{p(\mathcal C_k)}{\tilde p(\mathcal C_k)}\]</span> 这样就解决了类别不平衡问题。如果我们没有后验概率，就无法完成这样的操作。</p></li><li><p><strong>Combining models</strong>. 对于复杂的应用，我们也许会在不同的特征上训练多个模型，这时我们能够依据它们的后验概率合并它们的输出。例如，在癌症诊断的例子中，假设除了 X 光片 <span class="math inline">\(\mathbf x_\text{I}\)</span>，我们还有血液样本 <span class="math inline">\(\mathbf x_\text{B}\)</span>，并且二者是条件独立的： <span class="math display">\[p(\mathbf x_\text{I},\mathbf x_\text{B}\vert\mathcal C_k)=p(\mathbf x_\text{I}\vert\mathcal C_k)p(\mathbf x_\text{B}\vert\mathcal C_k)\]</span> 那么： <span class="math display">\[\begin{align}p(\mathcal C_k\vert \mathbf x_\text{I},\mathbf x_\text{B})&amp;\propto p(\mathbf x_\text{I},\mathbf x_\text{B}\vert \mathcal C_k)p(\mathcal C_k)\\&amp;\propto p(\mathbf x_\text{I}\vert\mathcal C_k)p(\mathbf x_\text{B}\vert\mathcal C_k)p(\mathcal C_k)\\&amp;\propto \frac{p(\mathcal C_k\vert \mathbf x_\text{I})p(\mathcal C_k\vert \mathbf x_\text{B})}{p(\mathcal C_k)}\end{align}\]</span> 其中，条件独立假设的引入就是朴素贝叶斯模型的思想。</p></li></ul><h3 id="loss-functions-for-regression">Loss functions for regression</h3><p>这一节前面一直在讨论分类模型，其实回归模型也有类似的决策阶段。仍然以多项式拟合问题为例，在推断阶段我们已经计算了联合概率分布 <span class="math inline">\(p(\mathbf x,t)\)</span>，那么在决策阶段我们要为每个 <span class="math inline">\(\mathbf x\)</span> 确定其 <span class="math inline">\(y(\mathbf x)\)</span>，使得期望损失最小： <span class="math display">\[\mathbb E[L]=\iint L(t,y(\mathbf x))p(\mathbf x,t)\mathrm d\mathbf x\mathrm d t\]</span> 如果用平方误差作为损失函数，优化目标就是： <span class="math display">\[\mathbb E[L]=\iint (y(\mathbf x)-t)^2p(\mathbf x,t)\mathrm d\mathbf x\mathrm d t\]</span> 由于要优化的变量是函数 <span class="math inline">\(y(\mathbf x)\)</span>，所以可以把 <span class="math inline">\(\mathbb E[L]\)</span> 视为 <span class="math inline">\(y(\mathbf x)\)</span> 的泛函，运用变分法： <span class="math display">\[\frac{\delta \mathbb E[L]}{\delta y(\mathbf x)}=2\int (y(\mathbf x)-t)p(\mathbf x,t)\mathrm dt=0\]</span> 解得： <span class="math display">\[y(\mathbf x)=\frac{\int tp(\mathbf x,t)\mathrm dt}{p(\mathbf x)}=\int t p(\mathrm t\vert \mathbf x)\mathrm dt=\mathbb E[t\vert\mathbf x]\]</span> 即以 <span class="math inline">\(\mathbf x\)</span> 为条件下的均值。</p><p>当然，平方误差并不是唯一的损失函数的选择，如果用绝对值误差，那么解就是 <span class="math inline">\(\mathbf x\)</span> 条件下的中位数……</p><p>同分类问题一样，对于回归问题我们也有生成模型、判别模型和判别函数三种解决问题的方案，且有着同样的优缺点。</p><h2 id="information-theory">Information Theory</h2><p>信息论也是模式识别和机器学习中的重要工具。设有一个离散随机变量 <span class="math inline">\(x\)</span>，如果我们观测到了一个不太可能发生的事件，那么它带给我们的信息量是巨大的；相反，如果我们观测到一个一定会发生的事件，那我们也没有获取到什么信息。因此，信息量 <span class="math inline">\(h(x)\)</span> 应该与 <span class="math inline">\(p(x)\)</span> 有关。对于两个独立事件 <span class="math inline">\(x,y\)</span>，我们希望它们同时被观测到的信息量是二者信息量之和：<span class="math inline">\(h(x,y)=h(x)+h(y)\)</span>. 而考虑到此时 <span class="math inline">\(p(x,y)=p(x)p(y)\)</span>，所以一个自然的选择是取信息量为 <span class="math inline">\(p(x)\)</span> 的对数形式： <span class="math display">\[h(x)=-\log_2 p(x)\]</span> 负号是为了让 <span class="math inline">\(h(x)\geq 0\)</span>. 当对数底数为 <span class="math inline">\(2\)</span> 时，信息量的单位为比特（bits, binary digits）。</p><p>对于随机变量 <span class="math inline">\(x\)</span>，其平均信息量就是： <span class="math display">\[H[x]=-\sum_xp(x)\log_2 p(x)\]</span> 这被称作随机变量 <span class="math inline">\(x\)</span> 的熵（entropy）。</p><p>上述信息量和熵的定义方式显得非常“启发式”，给人不够严谨的感觉。事实上，熵最早来源于物理学中的热力学，并被用来表述一个系统的混乱程度。我们可以考虑将 <span class="math inline">\(N\)</span> 个相同物体分到若干个桶内，使得第 <span class="math inline">\(i\)</span> 个桶有 <span class="math inline">\(n_i\)</span> 个物体。这是个经典的计数问题，总方案数为： <span class="math display">\[W=\frac{N!}{\prod_i n_i!}\]</span> 那么熵 <span class="math inline">\(H\)</span> 被定义为其自然对数乘上一个缩放因子： <span class="math display">\[H=\frac{1}{N}\ln W=\frac{1}{N}\ln N!-\frac{1}{N}\sum_{i}\ln n_i!\]</span> 考虑取 <span class="math inline">\(N\to\infty\)</span>，且保证 <span class="math inline">\(p_i=n_i/N\)</span> 不变，根据 Stirling 近似，有： <span class="math display">\[\ln N!\simeq N\ln N-N\]</span> 于是： <span class="math display">\[\begin{align}H&amp;=\lim_{N\to\infty}\left(\ln N-1-\frac{1}{N}\sum_i\left(n_i\ln n_i-n_i\right)\right)\\&amp;=\lim_{N\to\infty}\sum_i\left(\frac{n_i}{N}\ln N-\frac{n_i}{N}-\frac{1}{N}(n_i\ln n_i-n_i)\right)\\&amp;=-\lim_{N\to\infty}\sum_i\left(\frac{n_i}{N}\ln\frac{N}{n_i}\right)\\&amp;=-\sum_i p_i\ln p_i\end{align}\]</span> 得到了和之前类似的定义。</p><p>显然，熵的最小值为 <span class="math inline">\(0\)</span>，当某个 <span class="math inline">\(p_i=1\)</span> 并且 <span class="math inline">\(p_{j\neq i}=0\)</span> 时取到。要求熵的最大值，我们可以利用拉格朗日乘数法，定义拉格朗日函数： <span class="math display">\[-\sum_i p_i\ln p_i+\lambda\left(\sum_i p_i-1\right)\]</span> 求导取零，解得<strong>熵的最大值为 <span class="math inline">\(\ln M\)</span>，当且仅当 <span class="math inline">\(p_i=\frac{1}{M},\,\forall i\)</span> 时取到</strong>。其中 <span class="math inline">\(M\)</span> 是 <span class="math inline">\(x\)</span> 可能的状态数（桶的数量）。</p><p>上面都是离散情形。对于连续分布 <span class="math inline">\(p(x)\)</span>，我们按如下方式推导。首先将 <span class="math inline">\(x\)</span> 划分为宽度为 <span class="math inline">\(\Delta\)</span> 的若干桶，那么中值定理告诉我们，对于每个桶，存在一个 <span class="math inline">\(x_i\)</span> 使得： <span class="math display">\[\int_{i\Delta}^{(i+1)\Delta} p(x)\mathrm dx=p(x_i)\Delta\]</span> 于是，我们可以把连续变量 <span class="math inline">\(x\)</span> 量化到各个桶的 <span class="math inline">\(x_i\)</span> 上，那么观察到 <span class="math inline">\(x_i\)</span> 概率就是 <span class="math inline">\(p(x_i)\Delta\)</span>. 因此，熵为： <span class="math display">\[\begin{align}H_\Delta&amp;=-\sum_ip(x_i)\Delta\ln (p(x_i)\Delta)\\&amp;=-\sum_ip(x_i)\Delta\ln p(x_i)-\sum_ip(x_i)\Delta\ln\Delta\\&amp;=-\sum_ip(x_i)\Delta\ln p(x_i)-\ln\Delta\end{align}\]</span> 当 <span class="math inline">\(\Delta\to0\)</span> 时，第一项： <span class="math display">\[\lim_{\Delta\to0}-\sum_i p(x_i)\Delta \ln p(x_i)=-\int p(x)\ln p(x)\mathrm dx\]</span> 称为微分熵（differential entropy）。<strong>注意离散情形下的熵和连续情形下的微分熵相差了一个 <span class="math inline">\(\ln \Delta\)</span>，而当 <span class="math inline">\(\Delta\to0\)</span> 时它是发散的，二者并不是等价的</strong>。</p><p>前文证明了，离散情形下的熵在均匀类别分布下取到最大，那么连续情形下也是如此吗？事实上这取决于约束条件<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="为什么熵值最大的分布状态是正态分布而不是均匀分布？ - 椎名的回答 - 知乎 https://www.zhihu.com/question/357032828/answer/907586249">[1]</span></a></sup>。如果我们约束分布的均值为 <span class="math inline">\(\mu\)</span>，方差为 <span class="math inline">\(\sigma^2\)</span>，结合归一化条件，那么我们有三个约束条件： <span class="math display">\[\begin{align}&amp;\int_{-\infty}^{+\infty}p(x)\mathrm dx=1\\&amp;\int_{-\infty}^{+\infty}xp(x)\mathrm dx=\mu\\&amp;\int_{-\infty}^{+\infty}(x-\mu)^2p(x)\mathrm dx=\sigma^2\end{align}\]</span> 运用拉格朗日乘数法和变分法，最终可解得最大值在正态分布时取到： <span class="math display">\[p(x)=\frac{1}{(2\pi\sigma^2)^{1/2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</span> 且最大值为： <span class="math display">\[\begin{align}H[x]&amp;=-\int p(x)\ln p(x)\mathrm dx\\&amp;=\int p(x)\left(\frac{1}{2}\ln(2\pi\sigma^2)+\frac{(x-\mu)^2}{2\sigma^2}\right)\mathrm dx\\&amp;=\frac{1}{2}\ln(2\pi\sigma^2)+\int\frac{(x-\mu)^2}{2\sigma^2}p(x)\mathrm dx\\&amp;=\frac{1}{2}\ln(2\pi\sigma^2)+\frac{1}{2\sigma^2}\sigma^2\\&amp;=\frac{1}{2}\left(1+\ln(2\pi\sigma^2)\right)\end{align}\]</span> <strong>可以看见，不同于离散情形的熵，微分熵可以是负的</strong>。</p><p>如果我们将约束条件更改为在支撑集 <span class="math inline">\([a,b]\)</span> 上，即： <span class="math display">\[\int_a^bp(x)\mathrm dx=1\]</span> 那么运用拉格朗日乘数法和变分法，解得最大值在均匀分布时取到： <span class="math display">\[p(x)=\dfrac{1}{b-a}\mathbf 1_{[a,b]}(x)\]</span> 如果我们有两个随机变量 <span class="math inline">\(\mathbf x,\mathbf y\)</span>，那么在 <span class="math inline">\(\mathbf x\)</span> 的条件下，<span class="math inline">\(\mathbf y\)</span> 的条件熵（conditional entropy）定义为： <span class="math display">\[H[\mathbf y\vert\mathbf x]=-\iint p(\mathbf y\vert\mathbf x)\ln p(\mathbf y\vert\mathbf x)\mathrm d\mathbf y\mathrm d\mathbf x\]</span> 容易证明： <span class="math display">\[H[\mathbf x,\mathbf y]=H[\mathbf y\vert\mathbf x]+H[\mathbf x]\]</span></p><h3 id="relative-entropy-and-mutual-information">Relative entropy and mutual information</h3><p>在机器学习中，一个常见的需求是用一个分布 <span class="math inline">\(q(\mathbf x)\)</span> 为另一个分布 <span class="math inline">\(p(\mathbf x)\)</span> 建模. 由于二者并不一定完全相同，所以用 <span class="math inline">\(q(\mathbf x)\)</span> 的平均信息量就比原本的熵多出了： <span class="math display">\[\begin{align}\text{KL}(p\Vert q)&amp;=-\int p(\mathbf x)\ln q(\mathbf x)\mathrm dx-\left(-\int p(\mathbf x)\ln p(\mathbf x)\mathrm d\mathbf x\right)\\&amp;=-\int p(\mathbf x)\ln\frac{q(\mathbf x)}{p(\mathbf x)}\mathrm d\mathbf x\end{align}\]</span> 这就是相对熵或 KL 散度。KL 散度并不对称，即 <span class="math inline">\(\text{KL}(p\Vert q)\not\equiv\text{KL}(q\Vert p)\)</span>.</p><p>我们可以用琴生不等式证明 KL 散度非负，并且当且仅当 <span class="math inline">\(p(\mathbf x)=q(\mathbf x)\)</span> 时取等： <span class="math display">\[\text{KL}(p\Vert q)=-\int p(\mathbf x)\ln\frac{q(\mathbf x)}{p(\mathbf x)}\mathrm d\mathbf x\geq-\ln\left(\int p(\mathbf x)\frac{q(\mathbf x)}{p(\mathbf x)}\mathrm d\mathbf x\right)=0\]</span> 因此 KL 散度被视作两个分布之间的距离。</p><p>在应用中，我们常常用一个参数化概率分布 <span class="math inline">\(q(\mathbf x\vert \theta)\)</span>（例如混合高斯）来近似一个未知的数据分布 <span class="math inline">\(p(\mathbf x)\)</span>. 一个自然的想法就是以最小化二者的 KL 散度为目标来优化 <span class="math inline">\(\theta\)</span>. 然而，由于我们不知道 <span class="math inline">\(p(\mathbf x)\)</span> 的形式，所以无法直接计算 KL 散度。一般而言，我们有的是从 <span class="math inline">\(p(\mathbf x)\)</span> 中采样的数据集 <span class="math inline">\(\{\mathbf x_1,\ldots,\mathbf x_N\}\)</span>. 因此，通过采样近似期望，有： <span class="math display">\[\text{KL}(p\Vert q)\simeq \sum_{n=1}^N\left[-\ln q(\mathbf x_n\vert\theta)+\ln p(\mathbf x_n)\right]\]</span></p><p>第二项与 <span class="math inline">\(\theta\)</span> 无关，第一项是 <span class="math inline">\(q(\mathbf x\vert\theta)\)</span> 下的负对数似然。因此，<strong>最小化 KL 散度等最大化似然函数</strong>。</p><p>现在考虑两个随机变量 <span class="math inline">\(\mathbf x,\mathbf y\)</span>，如果二者相互独立，那么 <span class="math inline">\(p(\mathbf x)p(\mathbf y)=p(\mathbf x,\mathbf y)\)</span>. 如果它们不是独立的，我们希望知道它们有多接近独立，可以用 <span class="math inline">\(p(\mathbf x)p(\mathbf y)\)</span> 和 <span class="math inline">\(p(\mathbf x,\mathbf y)\)</span> 之间的 KL 散度作为指标： <span class="math display">\[\begin{align}\text{I}[\mathbf x,\mathbf y]&amp;=\text{KL}(p(\mathbf x,\mathbf y)\Vert p(\mathbf x)p(\mathbf y))\\&amp;=-\iint p(\mathbf x,\mathbf y)\ln\frac{p(\mathbf x)p(\mathbf y)}{p(\mathbf x,\mathbf y)}\mathrm d\mathbf x\mathrm d\mathbf y\end{align}\]</span> 这个量被称为 <span class="math inline">\(\mathbf x\)</span> 与 <span class="math inline">\(\mathbf y\)</span> 之间的互信息。根据 KL 散度的性质，我们知道 <span class="math inline">\(\text{I}[\mathbf x,\mathbf y]\geq 0\)</span>，当且仅当二者独立时取等。另外，容易证明： <span class="math display">\[\text{I}[\mathbf x,\mathbf y]=H[\mathbf x]-H[\mathbf x\vert\mathbf y]=H[\mathbf y]-H[\mathbf y\vert\mathbf x]\]</span> 因此，互信息可以看作是观察到 <span class="math inline">\(\mathbf y\)</span> 给 <span class="math inline">\(\mathbf x\)</span> 的不确定性（熵）带来的减少量。</p><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>为什么熵值最大的分布状态是正态分布而不是均匀分布？ - 椎名的回答 - 知乎 https://www.zhihu.com/question/357032828/answer/907586249 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>PRML</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vector-Quantization</title>
    <link href="/blog-main/2023/03/29/Vector-Quantization/"/>
    <url>/blog-main/2023/03/29/Vector-Quantization/</url>
    
    <content type="html"><![CDATA[<h2 id="vq-vae">VQ-VAE</h2><p>VQ-VAE<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Van Den Oord, Aaron, and Oriol Vinyals. Neural discrete representation learning. *Advances in neural information processing systems* 30 (2017).">[1]</span></a></sup> 是 Google DeepMind 在 2017 年提出的一个类 VAE 生成模型，相比普通的 VAE，它有两点不同：</p><ol type="1"><li>隐空间是<strong>离散</strong>的，通过 VQ (Vector Quantization) 操作实现；</li><li>先验分布是学习出来的。</li></ol><p>为什么要用离散的隐空间呢？首先，离散的表征更符合一些模态的自然属性，比如语言、语音，而图像也能用语言描述；其次，离散表征更适合推理、规划等复杂任务。后来 OpenAI 的 DALL·E 正是使用了 VQ-VAE (discrete VAE)，证明了它强大的能力。</p><h3 id="vector-quantization">Vector Quantization</h3><p><img src="vqvae.png" width=80% /></p><p>神经网络的输出一般是连续值，如何得到离散的表征呢？这就是 VQ (Vector Quantization) 技术。如上图所示，我们设置一个 codebook (embedding space) <span class="math inline">\(e\in\mathbb R^{K\times D}\)</span>，包含 <span class="math inline">\(K\)</span> 个 <span class="math inline">\(D\)</span> 维向量 <span class="math inline">\(e_1,e_2,\ldots,e_K\)</span>. 设编码器的输出为 <span class="math inline">\(z_e(x)\)</span>，那么在 codebook 里寻找一个与 <span class="math inline">\(z_e(x)\)</span> <strong>距离最近</strong>的向量 <span class="math inline">\(e_k\)</span> 代替 <span class="math inline">\(z_e(x)\)</span> 给到解码器，记作 <span class="math inline">\(z_q(x)\)</span>： <span class="math display">\[z_q(x)=e_k,\quad \text{where } k=\arg\min_j\Vert z_e(x)-e_j\Vert_2\]</span> 换句话说，VQ-VAE 的后验分布 <span class="math inline">\(q(z\vert x)\)</span> 是离散的 onehot 类别分布： <span class="math display">\[q(z=k\vert x)=\begin{cases}1&amp;\text{for }k=\arg\min_j\Vert z_e(x)-e_j\Vert_2\\0&amp;\text{otherwise}\end{cases}\]</span> 这里，<strong>隐变量 <span class="math inline">\(z\)</span> 的取值范围为整数索引，相应隐空间为所有索引构成的空间</strong>，因此是离散的。</p><p>在 VQ-VAE 中，codebook 里的 codes 是可学习的，随着训练过程自适应地调整。所以可学习的参数一共包含编码器、解码器和 codebook 三个部分。如果我们简单地取先验分布 <span class="math inline">\(p(z)\)</span> 为离散的均匀类别分布，那么一件有趣的事情是 ELBO 中 KL 正则项变成了常数： <span class="math display">\[\begin{align}\text{KL}(q(z\vert x)\Vert p(z))&amp;=\sum_k q(z=k\vert x)\log \frac{q(z=k\vert x)}{p(z)}\\&amp;=\sum_k q(z=k\vert x)\log K+\sum_k q(z=k\vert x)\log q(z=k\vert x)\\&amp;=\log K+0\\&amp;=\log K\end{align}\]</span> 于是 ELBO 只剩下了重构项 <span class="math inline">\(\log p(x\vert z_q(x))\)</span>. 然而重构项并不能训练到 codebook，所以最终的损失函数其实是这样的： <span class="math display">\[\mathcal L=\underbrace{\log p(x\vert z_q(x))}_\text{reconstruction}+\underbrace{\Vert\text{sg}[z_e(x)]-e\Vert_2}_\text{vq}+\underbrace{\beta\Vert z_e(x)-\text{sg}[e]\Vert_2}_\text{commitment}\]</span> 其中 <span class="math inline">\(\text{sg}[\bullet]\)</span> 表示 stop gradient，即停止梯度的传播，在 pytorch 中可以用 <code>detach()</code> 操作实现。vq 项（或称作 embedding 项、codebook 项）让 codebook 里的 codes 接近编码器的输出，以此来训练 codebook；commitment 项反过来，让编码器的输出接近对应的 codes，避免输出波动太大在 codes 之间乱跳，影响训练。</p><p>至此事情还没完，由于 VQ 操作引入了不可导的 argmin 算子，梯度无法从解码器流向编码器，所以现在编码器根本得不到训练。为此，作者引入了 <strong>Straight-Through Estimator</strong>——直接把 <span class="math inline">\(z_q(x)\)</span> 的梯度复制给 <span class="math inline">\(z_e(x)\)</span>，如上图中红色箭头所示. 毕竟 VQ 是靠查询最近邻实现的，二者实际的梯度应该也相差不大。</p><div class="note note-info">            <p><strong>关于损失函数的一点理解</strong></p><p>初读 VQ-VAE 论文的时候难免会对损失函数的后两项产生疑惑——一个对 <span class="math inline">\(z_e(x)\)</span> stop gradient，另一个又对 <span class="math inline">\(e\)</span> stop gradient，这是想干嘛？不要 stop gradient 不就行了吗？我在此分享一下我的理解。</p><p>话还得从 VAE 中的 KL 正则项说起。回想一下 KL 正则项的作用是什么——把后验拉向一个预定义的分布，防止 VAE 退化成 AE. 但是现在，由于 <span class="math inline">\(q(z\vert x)\)</span> 一定是一个 onehot 类别分布，它无论如何也不可能接近一个均匀类别分布，所以 KL 项就失效了。但是 VQ-VAE 的 codebook 是可学习的，只要 codes 最后分布在合理的位置上，那没有这个 KL 项也罢。</p><p>我们现在来回答这个问题——为什么 vq 项中要对 <span class="math inline">\(z_e(x)\)</span> 停止梯度传播？站在「让 codes 分布合理」的角度，可以给出如下解释：随机初始化的 codes 并不一定是合理的，如果不停止梯度，那么它会对 <span class="math inline">\(z_e(x)\)</span> 产生负面的影响，让训练更加困难。相反，重构本身是一个还不错的寻找隐变量的手段（毕竟 autoencoder 就是这么干的），所以通过重构项来找到比较好的 <span class="math inline">\(z_e(x)\)</span>，然后只让 <span class="math inline">\(e\)</span> 去接近 <span class="math inline">\(z_e(x)\)</span>（而不是反过来），能让 codes 收敛到还不错的分布。</p><p>如此说来，commitment 项的存在就有点违背我们的解释了。的确，这一项更多地是经验性的举措，有人实验发现即使 <span class="math inline">\(\beta=0\)</span> 也没啥问题。</p><p>所以，就轻重缓急而言，重构项最重要，它直接决定了 <span class="math inline">\(z_e(x)\)</span> 和 <span class="math inline">\(e\)</span> 的分布大体应该如何；vq 项次之，但也不能省，毕竟解码器的输入终究是量化之后的向量，要和重构项打好配合才行；commitment 项最不重要，仅仅是一个稳定训练的经验性手段，不宜占据主导。因此我们应该把 vq 和 commitment 分开写，就变成损失函数里那让人困惑的样子了！</p><p>然而，论文称 <span class="math inline">\(\beta\in[0.1, 2.0]\)</span> 都有差不多的表现，这与我的理解中 commitment 项不宜占据主导又相矛盾了😓。唉，着实玄乎～</p>          </div><div class="note note-info">            <p><strong>VAE 名不副实？</strong></p><p>假设我们现在完成了训练，考察用 VQ-VAE 重构输入的过程——<span class="math inline">\(x\)</span> 经过编码器得到 <span class="math inline">\(z_e(x)\)</span>，在 codebook 里做最近邻查找得到 <span class="math inline">\(z_q(x)\)</span>，解码器输出 <span class="math inline">\(\hat x\)</span> 来重构 <span class="math inline">\(x\)</span>——整个过程没有一点随机性！<strong>VQ-VAE 更像是一个在 bottleneck 处做了量化的 AE，而不是 VAE</strong>. 作者将其冠以 VAE 的名号，可能是他们确实是从 VAE 开始思考的吧。</p>          </div><h3 id="ema-codebook">EMA Codebook</h3><p>上一节我们介绍了，VQ 操作是一个最近邻查找的过程，产生的效果是把编码器输出的特征向量替换为距离它最近的 code，这不就是在做<strong>聚类</strong>嘛，聚类中心就是 codebook 里面的 codes. 所以我们其实不必使用 vq 项来训练 codebook，而是直接像 <strong>K-Means</strong> 那样更新聚类中心： <span class="math display">\[e_i=\frac{1}{n_i}\sum_{j=1}^{n_i} z_{i,j}\]</span> 其中 <span class="math inline">\(z_{i,1},\ldots,z_{i,n_i}\)</span> 是编码器所有输出中被分配给 <span class="math inline">\(e_i\)</span> 的 <span class="math inline">\(n_i\)</span> 个特征向量。然而有一个问题，实操中是用 minibatch 训练的，直接用上式并不准确。所以，作者提出用指数平滑来更新 <span class="math inline">\(e_i\)</span>： <span class="math display">\[\begin{align}&amp;N_i^{(t)}=N_i^{(t-1)}\ast \gamma+n_i^{(t)}(1-\gamma)\\&amp;m_i^{(t)}=m_i^{(t-1)}\ast \gamma +\sum_{j=1}^{n_i}z_{i,j}^{(t)}(1-\gamma)\\&amp;e_i^{(t)}=\frac{m_i^{(t)}}{N_i^{(t)}}\end{align}\]</span></p><p>怎么理解？<span class="math inline">\(m_i^{(t)}\)</span> 可视为 <span class="math inline">\(t\)</span> 及其以前所有分配给 <span class="math inline">\(e_i\)</span> 的特征向量之和，<span class="math inline">\(N_i^{(t)}\)</span> 是特征向量个数和，所以 <span class="math inline">\(m_i^{(t)}\)</span> 除以 <span class="math inline">\(N_i^{(t)}\)</span> 就是新的聚类中心。不过这里的「和」都是 EMA 下的「加权和」——因为 codebook 随时间在不断更新，过往的分配关系已经不再准确，需要降低其权重。这与 MoCo 中的 Momentum Encoder 是类似的道理。</p><div class="note note-success">            <p>虽然论文的主要实验都是用 vq 项来更新 codebook 的，但我复现发现用 K-Means + EMA 的方式更新 codebook 效果更好。事实上，后续的 VQ-VAE-2 和很多其他工作都是用 EMA 方式更新 codebook 的。</p>          </div><h3 id="prior-learning">Prior Learning</h3><p>VQ-VAE 训练结束后，我们就可以用它重构输入图像了。但是怎么直接生成新图像呢？</p><p>在说明这一点之前，我们必须要对 VQ-VAE 的隐空间有充分的理解。设输入图像 <span class="math inline">\(x\in\mathbb R^{3\times H\times W}\)</span>，其编码器输出为 <span class="math inline">\(z_e(x)\in\mathbb R^{c\times h\times w}\)</span>，量化操作的索引矩阵记作 <span class="math inline">\(\text{index}(x)\in\mathbb N^{h\times w}\)</span>. 一个误解是 VQ-VAE 的隐空间是所有 <span class="math inline">\(\text{index}(x)\)</span> 构成的 <span class="math inline">\(\mathbb N^{h\times w}\)</span>，但这是错误的！在第一小节中我们说过，隐空间其实是一个个索引构成的 <span class="math inline">\(\mathbb N\)</span>. 这说明，VQ-VAE 并没有对索引矩阵中<strong>索引之间的关系</strong>进行建模，如果我们直接随机均匀采样 <span class="math inline">\(h\times w\)</span> 个索引凑在一起组成一张索引矩阵，它大概率并不对应一个自然图像的编码结果，所以也不能生成一个自然图像。</p><p>因此，为了生成新图像，我们必须学习一个新的生成模型，对索引之间的分布做建模。鉴于索引是离散的，一个自然的选择就是 PixelCNN. 如果是音频数据，那么就用 WaveNet. 当然，有兴趣也可以尝试其他生成模型。</p><h3 id="index-collapse-perplexity">Index Collapse &amp; Perplexity</h3><p>虽然作者声称 VQ-VAE 不会遭遇 VAE 的 posterior collapse 问题，但我在实践中发现，它很容易遭遇 index collapse——编码器输出的所有特征向量全部被量化到一个或少数几个 codes 上。从某种意义上说，这也算是一种 posterior collapse 吧。起初我以为自己的实现有误，但一番搜索发现连 <a href="https://github.com/karpathy/deep-vector-quantization">Andrej Karpathy</a> 也受其困扰。一个常见的解决方案是用 K-Means 初始化 codebook，但我实测发现用处不大😅，而<strong>调小学习率</strong>会有帮助。</p><p>在代码实现中有一个 trick 是输出 perplexity 来监视是否发生了 index collapse. 当发生 index collapse 时，所有特征向量被量化到一个或少数几个 codes 上，这意味着熵很低；而理想情况是各个索引被均匀地选到，意味着熵很高。因此，索引的熵是能够监视训练是否发生了 index collapse 的指标，而 perplexity 就定义为这个熵的指数。Perplexity 的值域是 <span class="math inline">\([1,K]\)</span>：假设所有特征向量都量化到唯一一个 code 上，那么熵为 0，perplexity 就是 1；反之，如果各个索引的选择概率是完全均匀的，那么熵达到最大 <span class="math inline">\(\log K\)</span>，perplexity 就是 <span class="math inline">\(K\)</span>. 因此，perplexity 可以视作平均有多少个索引会被选择。如果训练时发现 perplexity 太小，甚至是 1，那就要赶紧处理 index collapse 问题了。</p><h2 id="vq-vae-2">VQ-VAE-2</h2><p>VQ-VAE-2<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Razavi, Ali, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. *Advances in neural information processing systems* 32 (2019).">[2]</span></a></sup> 依旧是 DeepMind 提出的，作者团队只换了一个人。也许他们也意识到了 VQ-VAE 更像是一个 AE 而不是 VAE，论文全程没有用 VAE 的那套概率术语，而是从信息压缩的角度做了更自然的描述。我们知道自回归模型的一大缺点是生成速度慢，这是因为一张高清图像的像素成千上万，而自回归模型只能串行生成一个个像素。为了解决这个问题，我们可以把自回归模型放到隐空间去，这样不仅速度成倍加快，信息密度也更高——像素空间中冗余繁杂的细节信息被压缩掉了，模型只需要考虑真正重要的语义信息。在这个视角下，VQ-VAE 训练 Encoder + VQ + Decoder 其实就是在寻找隐空间，找到隐空间之后，在隐空间上训练 PixelCNN 自回归模型做生成。这样梳理 VQ-VAE 的思路就显得顺畅直观了很多。事实上，这个思路后来被用到了 Stable Diffusion (Latent Diffusion) 上——在隐空间学习扩散模型，就像 VQ-VAE 在隐空间学习自回归模型一样（注意：这两个隐空间其实并不相同，一个是量化后的，一个是量化前的，只是说它们的思路与动机一样）。</p><p>VQ-VAE-2 相比 VQ-VAE 没有太多理论上的创新，主要的改变是以下三点：</p><ol type="1"><li><p>层次化地堆叠多层 VQ-VAE，使得 top-level 关注更多的高层语义，而 bottom-level 关注低层细节，如下图所示：</p><p><img src="vqvae2.png" width=80% /></p></li><li><p>用了更强大的 prior——top-level 的 prior 用带有 self-attention layer 的 Gated PixelCNN 建模。</p></li><li><p>使用一个 classifier 对生成的图像做拒绝采样。</p></li></ol><p>VQ-VAE-2 能够达到和 BigGAN comparable 的水平，算是在 GANs 风头十足的时候给 VAEs 争了一口气吧。</p><h2 id="vqgan">VQGAN</h2><p>由于我是 2023 年才来读 VQGAN<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Esser, Patrick, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 12873-12883. 2021.">[3]</span></a></sup> 的，所以看到作者和单位的时候愣了一下——这不就是 Latent Diffusion 的作者吗！怪不得 Latent Diffusion 用的是 VQGAN 来得到隐空间呢！这说明一个优秀的工作真得能催生下一个优秀的工作，形成良性循环。</p><p><img src="vqgan.png" width=80% /></p><p>话说回来，从上面这个概览图可以看出，VQGAN 和 VQ-VAE 的流程完全一致——先学习 codebook、再学习 prior. 学习 codebook 的部分与 VQ-VAE 大同小异，不同之处在于：加了一个 Patch Discriminator 做对抗训练，以及把重构损失的 L2 loss 换成了 perceptual loss. 实验证明 VQ-VAE 的重构非常模糊，而 VQGAN 能保留很多细节。为了实现无条件生成， VQ-VAE 使用 PixelCNN 学习 latent prior，能力比较弱，而 VQGAN 采用了 Transformer (GPT-2 架构)，依旧用自回归的方式训练和推断。</p><p>除了无条件生成之外，VQGAN 也可以做有条件的生成。如果条件是类别标签，只需要把它融入 Transformer 的架构之中即可；如果条件是语义分割图、深度图、图像填充掩膜这种 2D 图像式的，那么作者对这些条件训练一个新的 VQGAN，把条件的隐变量表示同原图像的隐变量表示一并给到 Transformer 即可（因为 Transformer 是对 token 集合进行操作，所以直接给过去就行）。</p><p>使用 Transformer 的一个缺点是非常吃显存，单卡 12GB 最大只能支持 <span class="math inline">\(16\times16\)</span> 的序列长度——也就是说，如果编码器下采样了 <span class="math inline">\(m\)</span> 次，那么输入图像最大只能是 <span class="math inline">\((16\cdot 2^m)\times(16\cdot 2^m)\)</span>. 为了生成更大分辨率的图像，作者把 Transformer 用滑动窗口的形式使用，如下图所示：</p><p><img src="vqgan2.png" width=50% /></p><p>依靠这些改进，VQGAN 能够生成百万像素的高清图像。论文开头第一页就放了一个风景全景图，十分惊艳。</p><h2 id="vq-diffusion">VQ-Diffusion</h2><p>2022 是扩散模型井喷的一年，我盲猜有人会拿扩散模型做 prior learning，一搜果然有—— VQ-Diffusion<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Gu, Shuyang, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 10696-10706. 2022.">[4]</span></a></sup><sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Tang, Zhicong, Shuyang Gu, Jianmin Bao, Dong Chen, and Fang Wen. Improved vector quantized diffusion models. *arXiv preprint arXiv:2205.16007* (2022).">[5]</span></a></sup>.</p><p><img src="vqdiff.png" width=65% /></p><p>如上图所示，Step 1 毫不意外的就是训一个 VQ-VAE，所以本篇工作的重点在于 Step 2——怎么在<strong>离散</strong>的隐空间中训练扩散模型来学习 prior.</p><p><strong>由于本文的主题是 Vector Quantization，而 VQ-Diffusion 的主要贡献是在离散扩散模型方面，VQ 只是获取离散隐空间的手段，所以接下来的部分只稍微阐述一下离散扩散模型的设计思路，至于训练细节和模型改进（Improved VQ-Diffusion）就暂且略过，以免喧宾夺主。</strong></p><p>扩散模型的加噪、去噪过程都是针对连续情形而言的，所以我们必须为离散情形重新定义新的前向过程。之前的一些工作采用随机修改的方式作为前向过程： <span class="math display">\[q(x_t\vert x_{t-1})=v^{\mathsf T}(x_t)\mathbf Q_tv(x_{t-1}),\quad x_t,x_{t-1}\in\{1,2,\ldots, K\}\]</span> 其中 <span class="math inline">\(v(x)\)</span> 表示一个长度为 <span class="math inline">\(K\)</span>（codebook 大小）的 one-hot 向量，只在第 <span class="math inline">\(x\)</span> 个元素处为 <span class="math inline">\(1\)</span>，其余地方为 <span class="math inline">\(0\)</span>. <span class="math inline">\(\mathbf Q_t\in\mathbb R^{K\times K}\)</span> 为马尔可夫链的转移矩阵，<span class="math inline">\([\mathbf Q_t]_{mn}=q(x_t=m\vert x_{t-1}=n)\)</span> 表示从 token <span class="math inline">\(n\)</span> 转移到 token <span class="math inline">\(m\)</span> 的概率。与连续扩散模型类似，我们可以直接推出 <span class="math inline">\(x_0\)</span> 转移到 <span class="math inline">\(x_t\)</span> 的概率： <span class="math display">\[q(x_t\vert x_0)=v^{\mathsf T}(x_t){\mathbf{\bar Q}}_tv(x_0),\quad \text{with }\mathbf{\bar Q}_t=\mathbf Q_t\cdots \mathbf Q_1\]</span> 也能在 <span class="math inline">\(x_0\)</span> 的条件下推出逆向过程： <span class="math display">\[q(x_{t-1}\vert x_t,x_0)=\frac{q(x_t\vert x_{t-1},x_0)q(x_{t-1}\vert x_0)}{q(x_t\vert x_0)}=\frac{\left(v^{\mathsf T}(x_t)\mathbf Q_t v(x_{t-1})\right)\left(v^{\mathsf T}(x_{t-1})\mathbf{\bar Q}_{t-1} v(x_0)\right)}{v^{\mathsf T}(x_t)\mathbf{\bar Q}_t v(x_0)}\]</span> 具体而言，<span class="math inline">\(\mathbf Q_t\)</span> 设计如下： <span class="math display">\[\mathbf Q_t=\begin{bmatrix}\alpha_t+\beta_t&amp;\beta_t&amp;\cdots&amp;\beta_t\\\beta_t&amp;\alpha_t+\beta_t&amp;\cdots&amp;\beta_t\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\beta_t&amp;\beta_t&amp;\cdots&amp;\alpha_t+\beta_t\\\end{bmatrix}\]</span> 其中 <span class="math inline">\(\alpha_t\in[0,1],\,\beta_t=(1-\alpha_t)/K\)</span>. 直观地说，每个 token 都有 <span class="math inline">\(\alpha_t+\beta_t\)</span> 的概率保持自身不变，以及 <span class="math inline">\(K\beta_t\)</span> 的概率重新从 <span class="math inline">\(K\)</span> 个 tokens 里面均匀采样。</p><p>然而，这种随机修改的破坏程度太大了，模型的训练难度很大。为此，作者受到 MLM (masked language modeling) 的启发，采用<strong>掩码+随机修改</strong>的方式转移： <span class="math display">\[\mathbf Q_t=\begin{bmatrix}\alpha_t+\beta_t&amp;\beta_t&amp;\beta_t&amp;\cdots&amp;0\\\beta_t&amp;\alpha_t+\beta_t&amp;\beta_t&amp;\cdots&amp;0\\\beta_t&amp;\beta_t&amp;\alpha_t+\beta_t&amp;\cdots&amp;0\\\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\gamma_t&amp;\gamma_t&amp;\gamma_t&amp;\cdots&amp;1\\\end{bmatrix}\]</span> 每个 token 都有 <span class="math inline">\(\gamma_t\)</span> 的概率变成 <span class="math inline">\(\text{[MASK]}\)</span>，<span class="math inline">\(K\beta_t\)</span> 的概率重新采样，以及 <span class="math inline">\(\alpha_t=1-K\beta_t-\gamma_t\)</span> 的概率保持不变。由于模型能够在训练中认识到 <span class="math inline">\(\text{[MASK]}\)</span> 这个特殊 token，从而知道哪里被修改了，所以这种转移的破坏程度温和了许多，更容易训练了。</p><p>如本节开头所言，为避免喧宾夺主，本文对 VQ-Diffusion 的介绍到此做个截断。如果以后有机会，我再去专门调研一下离散扩散模型的方法（挖坑😂）。</p><h2 id="latent-diffusion-stable-diffusion">Latent Diffusion (Stable Diffusion)</h2><p>既然前面已经提到了 Latent Diffusion<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 10684-10695. 2022.">[6]</span></a></sup>，那就简单说一下，并与 VQ Diffusion 做个对比。</p><p><img src="latentdiff.png" width=70% /></p><p>顾名思义，Latent Diffusion 就是在隐空间上建立扩散模型，而这个隐空间正是通过训练 VAE 或 VQGAN 得到的——前者被作者称为 KL-reg，因为 VAE 可视为用 KL 作为正则项的 autoencoder；后者被称为 VQ-reg，即通过 VQ 操作做正则的 autoencoder.</p><p>值得注意的是，对于 VQ-reg，VQ 操作被算进了 Decoder 之中，即扩散模型<strong>之后</strong>，所以扩散模型仍然是在连续（而非量化后）的特征空间上执行的。这也是 Latent Diffusion 和 VQ-Diffusion 的不同之处。</p><table><thead><tr class="header"><th style="text-align: center;">Model</th><th style="text-align: center;">Stage-1<br/>(latent space learning)</th><th style="text-align: center;">Latent Space</th><th style="text-align: center;">Stage-2<br/>(prior learning)</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">VQ-VAE</td><td style="text-align: center;">VQ-VAE</td><td style="text-align: center;">Discrete<br/>(after quantization)</td><td style="text-align: center;">Autoregressive<br/>PixelCNN</td></tr><tr class="even"><td style="text-align: center;">VQGAN</td><td style="text-align: center;">VQGAN<br/>(VQ-VAE + GAN + Perceptual Loss)</td><td style="text-align: center;">Discrete<br/>(after quantization)</td><td style="text-align: center;">Autoregressive<br/>GPT-2 (Transformer)</td></tr><tr class="odd"><td style="text-align: center;">VQ-Diffusion</td><td style="text-align: center;">VQ-VAE</td><td style="text-align: center;">Discrete<br/>(after quantization)</td><td style="text-align: center;">Discrete Diffusion</td></tr><tr class="even"><td style="text-align: center;">Latent Diffusion<br/>(VQ-reg)</td><td style="text-align: center;">VAE or VQGAN</td><td style="text-align: center;">Continuous<br/>(before quantization)</td><td style="text-align: center;">Continuous Diffusion</td></tr></tbody></table><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Van Den Oord, Aaron, and Oriol Vinyals. Neural discrete representation learning. <em>Advances in neural information processing systems</em> 30 (2017). <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Razavi, Ali, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. <em>Advances in neural information processing systems</em> 32 (2019). <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Esser, Patrick, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp. 12873-12883. 2021. <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Gu, Shuyang, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 10696-10706. 2022. <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Tang, Zhicong, Shuyang Gu, Jianmin Bao, Dong Chen, and Fang Wen. Improved vector quantized diffusion models. <em>arXiv preprint arXiv:2205.16007</em> (2022). <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 10684-10695. 2022. <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>VAEs</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>扩散模型应用·可控生成</title>
    <link href="/blog-main/2023/02/11/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%C2%B7%E5%8F%AF%E6%8E%A7%E7%94%9F%E6%88%90/"/>
    <url>/blog-main/2023/02/11/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%C2%B7%E5%8F%AF%E6%8E%A7%E7%94%9F%E6%88%90/</url>
    
    <content type="html"><![CDATA[<blockquote><p>封面来自 <a href="https://civitai.com/images/1333866?modelVersionId=106565&amp;prioritizedUserIds=104089&amp;period=AllTime&amp;sort=Most+Reactions&amp;limit=20">CivitAI</a>.</p></blockquote><h2 id="gligen">GLIGEN</h2><p><span class="label label-primary">University of Wisconsin-Madison</span> <span class="label label-primary">Columbia University</span> <span class="label label-primary">Microsoft</span> <span class="label label-default">2023.01.17</span></p><h2 id="controlnet">ControlNet</h2><p><span class="label label-primary">Stanford</span> <span class="label label-success">ICCV 2023 best paper</span> <span class="label label-default">2023.02.10</span></p><p>尽管文生图大模型的出现让人们能够用自然语言方便地创作，但是文本的控制粒度终究还是比较粗糙，我们希望引入更多种类的条件进行细粒度的控制。显然，在每种条件上都训练一个大模型并不现实，于是人们尝试在基础的文生图模型上引入额外的网络来融入条件，其中，ControlNet<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Zhang, Lvmin, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. *arXiv preprint arXiv:2302.05543* (2023).">[1]</span></a></sup> 无疑是最为出名的工作。</p><p>其实 ControlNet 的思想很简单，就是把原有网络复制一份，通过 zero convolution，即权重初始化为 0 的卷积层连在一起。训练时原权重保持不变，因此 ControlNet 就像一个插件一样，随时可以插上或者去掉。zero convolution 的设置是为了避免训练初期破坏了原模型的生成能力。</p><p><img src="controlnet.png" width=80% /></p><p>具体到 Stable Diffusion 上，作者只复制了 encoder 部分，并将对应分辨率的特征加到了 decoder 部分，如下图所示：</p><p><img src="controlnet-stable.png" width=80% /></p><p>这样设计的好处在于训练时梯度无需回传到原来的 encoder 之中，节省计算量。</p><p>尽管是基于 Stable Diffusion 这样的大规模预训练模型，ControlNet 的训练时间还是相当可观的：</p><p><img src="controlnet-gpuhours.png" width=100% /></p><div class="note note-secondary">            <details><summary><b>点击查看 ControlNet 的生成样例（摘自论文）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="controlnet-ex1.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="controlnet-ex2.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="controlnet-ex3.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="controlnet-ex4.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="controlnet-ex5.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="controlnet-ex6.png" /></div></div></div></details>          </div><h2 id="mixture-of-diffusers">Mixture of Diffusers</h2><p><span class="label label-default">2023.02.05</span></p><ol type="1"><li>动机： 与 MultiDiffusion 类似，希望在同一张图片的不同区域使用预训练扩散模型，从而扩展图片尺寸或实现复杂构图的分区域控制。</li><li>方法： 也与 MultiDiffusion 类似，只不过是对预测的噪声做加权平均（而不是对去噪结果做）。实测效果比 MultiDiffusion 更好一些。</li></ol><h2 id="t2i-adapter">T2I-Adapter</h2><p><span class="label label-primary">PKU Shenzhen</span> <span class="label label-primary">Tencent</span> <span class="label label-default">2023.02.16</span></p><p>与 ControlNet 类似，T2I-Adapter<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Mou, Chong, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. *arXiv preprint arXiv:2302.08453* (2023).">[2]</span></a></sup> 也是使用额外网络为扩散模型引入条件，如图所示：</p><p><img src="t2iadapter.png" width=80% /></p><p>每种条件经由 Adapter 网络得到多尺度特征，加到对应尺度的原 Stable Diffusion 的 UNet 之中。不同网络的特征还可以加权和来达到多条件控制的目的（不过权重需要人为调整）。</p><p>与 ControlNet 相比，T2I-Adapter 更加轻量（参数量约 70MB，文件大小 308MB），在 4 卡 V100 上训练用时 3 天。</p><div class="note note-secondary">            <details><summary><b>点击查看 T2I-Adapter 的生成样例（摘自<a href="https://github.com/TencentARC/T2I-Adapter">官方 repo</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://camo.githubusercontent.com/f2c8aa54bf23b5e7f24e738dd3b4342ee11c9b8e6017d92dc30c46076866653a/68747470733a2f2f68756767696e67666163652e636f2f54656e63656e744152432f5432492d416461707465722f7265736f6c76652f6d61696e2f6173736574732f7465617365722e706e67" /></div></div></div></details>          </div><h2 id="multidiffusion">MultiDiffusion</h2><p><span class="label label-default">2023.02.16</span></p><ol type="1"><li>多个扩散模型生成一张图片的不同区域，融合它们的扩散过程使结果和谐、平滑；无需训练或微调，只需要一个预训练扩散模型</li><li>应用场景：<ol type="1"><li>极大扩展生成图像的尺寸和长宽比</li><li>分区域生成，不同区域可以基于不同条件</li></ol></li><li>方法（以扩展生成图像的尺寸为例）：首先将图片划分成有重叠的区域，分别用预训练模型进行一步去噪（t → t-1）；然后将去噪结果做加权平均（原文将其形式化为了一个优化问题，但其解析解就是加权平均）</li><li>简单粗暴但有效，在需要分区使用扩散模型时可以考虑</li></ol><h2 id="composer">Composer</h2><p><span class="label label-default">2023.02.22</span></p><h2 id="unicontrol">UniControl</h2><p><span class="label label-primary">Salesforce</span> <span class="label label-success">NeurIPS 2023</span> <span class="label label-default">2023.05.18</span></p><p>ControlNet 有一个显著的缺点——每一种条件都需要一个模型。因此，UniControl<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Qin, Can, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang et al. UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild. *arXiv preprint arXiv:2305.11147* (2023).">[6]</span></a></sup>的作者提出让一个（类似于 ControlNet 架构的）模型同时支持各种条件。</p><p><img src="unicontrol.png" width=80% /></p><p>从图中可以看见，UniControl 主要分为三个阶段：</p><ol type="1"><li><strong>MOE Adapter</strong> 对不同种类的条件分别用卷积网络提取特征（这也算 MOE?）；</li><li><strong>Task Aware HyperNet</strong> 接收关于任务的文本描述（如 "normal surface to image"），通过 CLIP Text Encoder 编码为 text embedding 后，经过可学习的 Hypernet 得到 task embedding.</li><li><strong>Modulated Zero Conv</strong> 使用步骤 2 得到的 task embedding 来调制卷积。</li></ol><p>为了训练 UniControl，作者收集并形成了有 20 million 个「图像-文本-条件」三元组的数据集 MultiGen-20M dataset. 训练耗时 5000 GPU hours (NVIDIA A100-40G)，与所有条件的 ControlNet 的训练总用时相当，还是非常可观的。</p><div class="note note-secondary">            <details><summary><b>点击查看 UniControl 的生成样例（摘自论文</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="unicontrol-ex.png" /></div></div></div></details>          </div><h2 id="uni-controlnet">Uni-ControlNet</h2><p><span class="label label-primary">HKU</span> <span class="label label-primary">Microsoft</span> <span class="label label-success">NeurIPS 2023</span> <span class="label label-default">2023.05.18</span></p><p>Uni-ControlNet<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="Zhao, Shihao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K. Wong. Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models. *arXiv preprint arXiv:2305.16322* (2023).">[7]</span></a></sup> 与 UniControl 解决的是同样的问题，而且名字很像，甚至挂在 arxiv 上的时间都是同一天……不过二者的方法还是挺不同的。</p><p>Uni-ControlNet 将控制条件分成了两组——<strong>local controls</strong> 和 <strong>global controls</strong>，前者包括边缘图、深度图、分割图等，后者只有一种——参考图像（用 CLIP image embedding 表示）。Local controls 和 global controls 分别用一个 adapter 来微调：</p><ul><li>Local controls 的 adapter 将控制信号转换后通过调制卷积的方式融入模型特征图</li><li>Global controls 的 adapter 将控制信号转换成 tokens 拼接在 text tokens 后</li></ul><p>如下图所示：</p><p><img src="uni-controlnet.png" width=80% /></p><p>为了训练模型，作者使用的数据集是从 LAION 中随机采样的 10 million 个文本图像对。</p><div class="note note-secondary">            <details><summary><b>点击查看 Uni-ControlNet 的生成样例（摘自<a href="https://github.com/ShihaoZhaoZSH/Uni-ControlNet">官方 repo</a></a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://raw.githubusercontent.com/ShihaoZhaoZSH/Uni-ControlNet/main/figs/results.png" /></div></div></div></details>          </div><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Zhang, Lvmin, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. <em>arXiv preprint arXiv:2302.05543</em> (2023). <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Mou, Chong, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. <em>arXiv preprint arXiv:2302.08453</em> (2023). <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Jiménez, Álvaro Barbero. Mixture of diffusers for scene composition and high resolution image generation. <em>arXiv preprint arXiv:2302.02412</em> (2023). <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Bar-Tal, Omer, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. (2023). <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Huang, Lianghua, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. <em>arXiv preprint arXiv:2302.09778</em> (2023). <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Qin, Can, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang et al. UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild. <em>arXiv preprint arXiv:2305.11147</em> (2023). <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Zhao, Shihao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K. Wong. Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models. <em>arXiv preprint arXiv:2305.16322</em> (2023). <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>Diffusion Models</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>扩散模型应用·个性化生成</title>
    <link href="/blog-main/2023/02/11/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%C2%B7%E4%B8%AA%E6%80%A7%E5%8C%96%E7%94%9F%E6%88%90/"/>
    <url>/blog-main/2023/02/11/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%C2%B7%E4%B8%AA%E6%80%A7%E5%8C%96%E7%94%9F%E6%88%90/</url>
    
    <content type="html"><![CDATA[<blockquote><p>封面来自 <a href="https://civitai.com/images/1058118?modelVersionId=90854&amp;prioritizedUserIds=262917&amp;period=AllTime&amp;sort=Most+Reactions&amp;limit=20">CivitAI</a>.</p></blockquote><p>个性化生成（personalized generation），也称作主体驱动生成（subject-driven generation），指的是用户提供若干张（甚至只有一张）某物体的照片，模型生成该物体其他图像。</p><h2 id="textual-inversion">Textual Inversion</h2><p><span class="label label-primary">Tel Aviv University</span> <span class="label label-primary">NVIDIA</span> <span class="label label-default">2022.08.02</span></p><p>顾名思义，Textual Inversion<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Gal, Rinon, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. *arXiv preprint arXiv:2208.01618* (2022).">[1]</span></a></sup> 采用类似于 GAN Inversion 的方式——在 text embedding 空间中寻找一个向量来重建原图。具体而言，给定 3-5 张描述某概念的图像，通过如下优化目标找到 word embedding <span class="math inline">\(v_\ast\)</span>，使得提示词 <span class="math inline">\(\text{A photo of }S_\ast\)</span> 能够重建输入图像： <span class="math display">\[v_\ast=\arg\min_v \mathbb E_{z\sim\mathcal E(x),\,y,\,\epsilon\sim\mathcal N(0,1),\,t}\left[\Vert\epsilon-\epsilon_\theta(z_t,t,c_\theta(y,v_\ast))\Vert_2^2\right]\]</span> 流程图如下：</p><p><img src="textual-inversion.png" width=80% /></p><p>Textual Inversion 的主体内容就是这些，简单有效，并且最后只需要保存 embedding vector，所以文件非常小；但是效果会比微调整个模型的 DreamBooth（见下文）和微调额外网络的 LoRA 更差一些。</p><div class="note note-secondary">            <details><summary><b>点击查看 Textual Inversion 的生成样例（摘自<a href="https://textual-inversion.github.io/">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://textual-inversion.github.io/static/images/editing/fluffy.JPG" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://textual-inversion.github.io/static/images/editing/elephant.JPG" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://textual-inversion.github.io/static/images/style/style.JPG" /></div></div></div></details>          </div><h2 id="dreambooth">DreamBooth</h2><p><span class="label label-primary">Google</span> <span class="label label-default">2022.08.25</span></p><p>与 Textual Inversion 几乎同时，DreamBooth<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ruiz, Nataniel, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. *arXiv preprint arXiv:2208.12242* (2022).">[2]</span></a></sup> 也想到了让预训练模型学习用户指定的概念来进行个性化生成。为此，作者考虑将用户给出的物体与一个特殊的 identifier 绑定，然后用包含这个特殊 identifier 的 prompt 和用户的图像来<strong>微调整个 UNet</strong>. 微调结束后，只要用户输入的 prompt 中包含这个 identifier，那么模型就能生成用户想要的物体。整体思路清晰易懂，但是实现上有些小细节需要说明。</p><p>首先，这个 identifier 要与描述其类别的词一起使用，即 a [identifier] [class noun]，这样预训练模型能借用其已有的关于那个类别的知识，训练更快速稳定、生成的效果也更好。</p><p>其次，identifier 的选取也有讲究，应该尽可能避免使用常用的单词（如 "unique"、"special" 等），否则模型还得学会分辨什么时候这个词是原来的意思，什么时候是新的意思。然而，直接用随机的字符串也不是一个好的选择，因为 tokenizer 可能会把它拆散，变成常见的 tokens. 因此，作者先查找罕见的 tokens，再把它们映射回 text space 来得到 identifier.</p><p>最后，直接微调可能会导致 language drift 问题，即模型遗忘了预训练时的知识；模型还可能失去多样性，即生成的物体都有类似的姿态、视角。为此，作者提出了 prior preservation loss，在微调的同时用模型自己生成的样例监督它自己，相当于一个正则项，如下图所示：</p><p><img src="dreambooth.png" width=60% /></p><div class="note note-secondary">            <details><summary><b>点击查看 DreamBooth 的生成样例（摘自<a href="https://dreambooth.github.io/">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://dreambooth.github.io/DreamBooth_files/results.png" /></div></div></div></details>          </div><h2 id="dreambooth-lora">DreamBooth + LoRA</h2><p>LoRA<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685* (2021).">[3]</span></a></sup> 是一种参数高效微调方法（PEFT），最早应用于微调语言大模型之中，后来由 <a href="https://github.com/cloneofsimo/lora">cloneofsimo</a> 引入到对 Stable Diffusion 模型的微调之中。LoRA 并不改变原模型的权重，而是在线性层旁边新增一个下采样-上采样的支路，通过训练这个支路来完成微调。因此，同一个基底 Stable Diffusion 模型可以搭载不同的 LoRA 使用，具有很高的灵活性。由于 LoRA 支路网络的参数量小，相比微调整个模型，对算力的需求更加友好，并且也能达到不错的效果，因此很快受到大家的热烈欢迎，成为了目前最流行的微调 Stable Diffusion 的方法之一。</p><p>特别地，我们可以按照 DreamBooth 的方式（即使用 a [identifier] [class noun] 的描述词 + 正则化图像）来训练 LoRA，相比原始 DreamBooth 微调整个模型，资源消耗大大减小并且灵活性更高。</p><h2 id="custom-diffusion">Custom Diffusion</h2><p><span class="label label-primary">CMU</span> <span class="label label-primary">THU</span> <span class="label label-primary">Adobe</span> <span class="label label-success">CVPR 2023</span> <span class="label label-default">2022.12.08</span></p><p>不同于 DreamBooth 微调整个模型，Custom Diffusion<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Kumari, Nupur, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 1931-1941. 2023.">[4]</span></a></sup> 只微调 cross-attention 层中的 K、V 投影矩阵以及一个输入 text token（如下图所示），微调时间大大减小的同时能取得与 DreamBooth 相当的效果。并且，Custom Diffusion 还支持合并两个微调的模型，实现<strong>多概念生成</strong>。</p><p><img src="custom-diffusion.png" width=80% /></p><p>为什么选择 K、V 投影矩阵微调呢？因为作者分析了微调整个模型后各模块权重的相对变化量，发现 cross-attention 层的 K、V 投影矩阵变化最大。进一步地，这部分参数只占全部参数的 5%，说明了它们在微调中的重要性。具体到微调过程，其实与 DreamBooth 区别不大，也是使用特殊标识符，并且也使用了正则化图像。</p><p>关于多概念生成，作者尝试了两种方法，都达到了比 DreamBooth 更优的结果：</p><ol type="1"><li>合并两个数据集同时训练两个概念；</li><li>分别训练之后合并模型。</li></ol><p>第一种方法没什么好说的。对于第二种方法，作者将其形式化为了一个优化问题。设一共有 <span class="math inline">\(N\)</span> 个概念，<span class="math inline">\(\mathbf c_i\in\mathbb R^{s_i\times d}\)</span> 表示描述第 <span class="math inline">\(i\)</span> 个概念的词汇 embeddings. 记原投影矩阵为 <span class="math inline">\(W_0\)</span>，微调后的投影矩阵为 <span class="math inline">\(W_i\)</span>，那么优化问题为： <span class="math display">\[\begin{align}&amp;\hat W=\mathop{\arg\min}_W\Vert WC_\text{reg}^{\mathsf T}-W_0 C_\text{reg}^{\mathsf T}\Vert_F\\\text{s.t.}\quad&amp;WC^{\mathsf T}=V\end{align}\]</span> 其中 <span class="math inline">\(C=[\mathbf c_1\cdots \mathbf c_N]^{\mathsf T}\in\mathbb R^{s\times d}\)</span> 包含了所有 <span class="math inline">\(N\)</span> 个概念一共 <span class="math inline">\(s\)</span> 个目标词汇 embeddings，<span class="math inline">\(V=[W_1\mathbf c_1^{\mathsf T}\cdots W_N\mathbf c_N^{\mathsf T}]^{\mathsf T}\)</span>. 直观而言，我们希望优化后的投影矩阵在个性化概念上的输出与分别微调的模型保持一致，同时在正则化图像上与原模型的输出差异最小。这个优化问题具有封闭解： <span class="math display">\[\hat W=W_0+\mathbf v^{\mathsf T}\mathbf d\]</span> 其中 <span class="math inline">\(\mathbf d=C(C^{\mathsf T}_\text{reg}C_\text{reg})^{-1},\,\mathbf v^{\mathsf T}=(V-W_0C^{\mathsf T})(\mathbf dC^{\mathsf T})^{-1}\)</span>.</p><div class="note note-secondary">            <details><summary><b>点击查看 Custom Diffusion 的生成样例（摘自<a href="https://www.cs.cmu.edu/~custom-diffusion/">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="custom-diffusion-ex.jpg" /></div></div></div></details>          </div><h2 id="suti">SuTI</h2><p><span class="label label-primary">Google</span> <span class="label label-default">2023.04.01</span></p><p>DreamBooth 等方法需要为每种个性化主体分别微调出一个专家模型，比较麻烦。SuTI<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Chen, Wenhu, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William W. Cohen. Subject-driven text-to-image generation via apprenticeship learning. *arXiv preprint arXiv:2304.00186* (2023).">[5]</span></a></sup> 希望只使用一个模型，输入若干张主体图片，就能够生成该主体的其他图片，这样的模型被称作 apprentice model. 为了训练这个 apprentice model，我们需要一个大规模的个性化主体数据集，而数据的获取方式极其粗暴——从网络上爬几百万张图像，训练大量的专家模型，用这些专家模型来产生数据，如下图所示。虽然 SuTI 的训练消耗极大，但推断的时候能比基于逐主体优化的方法快 20 倍，相当于把时间开销从推断转移到了训练，因此如果模型能够开源出来，对普通用户无疑是一个好消息。不过嘛，考虑到这是 Google，而且 SuTI 基于的是未开源的 Imagen……恐怕是“可远观而不可亵玩焉”。</p><p><img src="suti-pipeline.png" width=80% /></p><p>可以看出，SuTI 非常偏工程，比如怎么收集数据集、怎么把同一个主体聚在一起、怎么生成文本描述、怎么过滤质量差的图片等，对普通课题组没有太多的参考意义（而且也玩不起啊），这里便不再赘述。</p><h2 id="svdiff">SVDiff</h2><p><span class="label label-primary">Rutgers University</span> <span class="label label-primary">Google</span> <span class="label label-default">2023.05.20</span></p><p>SVDiff<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Han, Ligong, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. *arXiv preprint arXiv:2303.11305* (2023).">[6]</span></a></sup> 的动机与 LoRA、Custom Diffusion 类似，依旧是希望避免微调整个模型的所有参数，试图寻找一个更为紧凑的参数空间。具体而言，SVDiff 微调的是<strong>权重矩阵的奇异值</strong>，在 Stable Diffusion（全部参数占 3.66GB）上只需要微调 1.7MB 的参数，并且能够实现多概念生成。</p><p>微调权重矩阵的奇异值这一想法其实来自于 FSGAN<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="Robb, Esther, Wen-Sheng Chu, Abhishek Kumar, and Jia-Bin Huang. Few-shot adaptation of generative adversarial networks. *arXiv preprint arXiv:2010.11943* (2020).">[7]</span></a></sup>。对于卷积网络而言，我们首先将卷积层改写作全连接层：设卷积核为 <span class="math inline">\(W_\text{conv}\in\mathbb R^{c_\text{out}\times c_\text{in}\times h\times w}\)</span>，输入的图像 patch 为 <span class="math inline">\(\mathbf x_\text{conv}\in\mathbb R^{c_\text{in}\times h\times w}\)</span>，那么： <span class="math display">\[W_\text{conv}\otimes \mathbf x_\text{conv}\implies W\mathbf x\]</span> 其中 <span class="math inline">\(W=\text{reshape}(W_\text{tensor})\in\mathbb R^{c_\text{out}\times(c_\text{in}\times h\times w)}\)</span>，<span class="math inline">\(\mathbf x=\text{reshape}(\mathbf x_\text{conv})\in\mathbb R^{(c_\text{in}\times h\times w)\times 1}\)</span>. 对 <span class="math inline">\(W\)</span> 进行奇异值分解： <span class="math display">\[W=U\Sigma_\sigma V^{\mathsf T}\quad \text{where}\;\Sigma_\sigma=\text{diag}(\sigma),\;\sigma=[\sigma_1,\sigma_2,\ldots]\]</span> 我们通过训练 spectral shift <span class="math inline">\(\delta\)</span> 来微调权重矩阵 <span class="math inline">\(W\)</span>： <span class="math display">\[W_\delta=U\Sigma_\delta V^{\mathsf T}\quad\text{where}\;\Sigma_\delta=\text{diag}(\text{ReLU}(\sigma+\delta))\]</span> 对于两次微调的 spectral shift <span class="math inline">\(\delta_1,\delta_2\)</span>，我们可以设计一些方式将它们融合起来： <span class="math display">\[\begin{align}&amp;\Sigma_{\delta&#39;}=\text{diag}(\text{ReLU}(\sigma+\delta_1+\delta_2))&amp;&amp;\text{addition}\\&amp;\Sigma_{\delta&#39;}=\text{diag}(\text{ReLU}(\sigma+\alpha\delta_1+(1-\alpha)\delta_2)),\quad 0&lt;\alpha&lt;1&amp;&amp;\text{interpolation}\end{align}\]</span> 至于多概念生成，作者提出了一种数据增强方法 Cut-Mix-Unmix，通过左右拼接两个概念的图像作为数据来训练模型。另外，作者还通过只在一个图像文本对上微调的方式实现了单图像编辑。私以为这两点的做法并不优雅，这里就不赘述了。</p><div class="note note-secondary">            <details><summary><b>点击查看 SVDiff 的生成样例（摘自<a href="https://svdiff.github.io/">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://svdiff.github.io/images/teaser_large.png" /></div></div></div></details>          </div><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Gal, Rinon, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. <em>arXiv preprint arXiv:2208.01618</em> (2022). <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Ruiz, Nataniel, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. <em>arXiv preprint arXiv:2208.12242</em> (2022). <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. <em>arXiv preprint arXiv:2106.09685</em> (2021). <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Kumari, Nupur, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 1931-1941. 2023. <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Chen, Wenhu, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William W. Cohen. Subject-driven text-to-image generation via apprenticeship learning. <em>arXiv preprint arXiv:2304.00186</em> (2023). <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Han, Ligong, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. <em>arXiv preprint arXiv:2303.11305</em> (2023). <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Robb, Esther, Wen-Sheng Chu, Abhishek Kumar, and Jia-Bin Huang. Few-shot adaptation of generative adversarial networks. <em>arXiv preprint arXiv:2010.11943</em> (2020). <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>Diffusion Models</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>扩散模型应用·寻找语义空间</title>
    <link href="/blog-main/2023/02/11/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%C2%B7%E5%AF%BB%E6%89%BE%E8%AF%AD%E4%B9%89%E7%A9%BA%E9%97%B4/"/>
    <url>/blog-main/2023/02/11/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%C2%B7%E5%AF%BB%E6%89%BE%E8%AF%AD%E4%B9%89%E7%A9%BA%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\x}{\mathbf x}\]</span></p><blockquote><p>封面来自 <a href="https://civitai.com/images/1283664?modelVersionId=102828&amp;prioritizedUserIds=312503&amp;period=AllTime&amp;sort=Most+Reactions&amp;limit=20">CivitAI</a>.</p></blockquote><h2 id="diffusion-autoencoders">Diffusion Autoencoders</h2><p><span class="label label-success">CVPR 2022</span> <span class="label label-default">2021.11.30</span></p><p>尽管扩散模型的生成效果非常好，但是它缺乏有语义的隐空间，给一些下游应用带来了麻烦。即便是 DDIM 的确定性采样过程，其隐空间，即 <span class="math inline">\(\x_T\)</span> 所在空间也不理想，典型表现就是 DDIM 的插值结果并不是平滑变化的。Diffusion Autoencoders 希望构造一个像 GANs 和 VAEs 一样方便操纵的隐空间，能够平滑地插值、控制语义和编辑图像属性。为此，作者设计了如下架构：</p><p><img src="diffusionae.png" width=50% /></p><p>Semantic encoder 是一个卷积编码器，目的是提取输入图像的语义特征 <span class="math inline">\(z_\text{sem}\)</span>；Conditional DDIM 是以 <span class="math inline">\(z_\text{sem}\)</span> 为条件输入（通过 AdaGN 融入）的扩散模型，可以把 <span class="math inline">\(\x_0\to\x_T\)</span> 过程看作“编码器”，<span class="math inline">\(\x_T\to\x_0\)</span> 过程看作“解码器”；Latent DDIM 稍后再做解释。在这个架构下，<strong><span class="math inline">\(z_\text{sem}\)</span> 和 <span class="math inline">\(\x_T\)</span> 共同形成了输入图像的隐空间</strong>。前者编码语义信息，让我们能够操纵生成图像的各种属性（如人的性别、年龄、微笑）；后者编码了 <span class="math inline">\(z_\text{sem}\)</span> 遗留的其他信息，往往是一些随机细节。二者共同作用，既有扩散模型能够几乎完美地重建输入图像的优势，又得到了 high-level 的语义表征 <span class="math inline">\(z_\text{sem}\)</span> 供下游任务的使用。</p><p>现在，如果我们想用 Diffusion AE 做<strong>无条件</strong>生成，会发现一个问题——不知道 <span class="math inline">\(z_\text{sem}\)</span>，所以我们必须为 <span class="math inline">\(z_\text{sem}\)</span> 建模，这就是 Latent Diffusion 的用途。当然，任何生成模型都可以用来建模 <span class="math inline">\(z_\text{sem}\)</span>，只是作者觉得扩散模型更好罢了。因为 <span class="math inline">\(z_\text{sem}\in\mathbb R^{512}\)</span>，所以 Latent Diffusion 的模型 backbone 是一个十几层的 MLP，效果还不错。</p><p>说了这么多，这个隐空间究竟是不是像作者声称的这么好，还得实验来证明——</p><ol type="1"><li>在第一个实验中，作者固定 <span class="math inline">\(z_\text{sem}\)</span> 不变，随机采样 <span class="math inline">\(\x_T\)</span>，发现生成的结果也大体不变，只有细节改变，这证明了 <span class="math inline">\(z_\text{sem}\)</span> 和 <span class="math inline">\(\x_T\)</span> 确实一个编码图像语义、另一个编码随机细节。</li><li>在第二个实验中，作者在隐空间中插值（<span class="math inline">\(z_\text{sem}\)</span> 用的线性插值，<span class="math inline">\(\x_T\)</span> 用的球面线性插值），并与 DDIM 和 StyleGAN 做比较，发现 Diffusion AE 既能得到像 StyleGAN 一样平滑的插值过程，也能像 DDIM 一样完美地重建插值端点。</li><li>前两个实验证明作者设计的隐空间的确非常优秀，于是我们可以依靠它来编辑图像属性。通过在 <span class="math inline">\(z_\text{sem}\)</span> 空间中训练一个线性分类器，我们能得到某属性（如微笑）的方向向量，然后在 <span class="math inline">\(z_\text{sem}\)</span> 上加减方向向量即可。<span class="math inline">\(\x_T\)</span> 不用改动，因为第一个实验已经证明它和 high-level 语义关系不大。</li><li>第四个实验作者定量比较了 Diffusion AE 和其他生成模型的重构性能，并消融了不同大小的 <span class="math inline">\(z_\text{sem}\)</span> 和是否有 <span class="math inline">\(\x_T\)</span> 对重构的影响。</li><li>从扩散模型的角度，Diffusion AE 能够“加快”去噪。这里的加快不是指减小时间步，而是指 <span class="math inline">\(t\)</span> 时刻模型预测的 <span class="math inline">\(\hat\x_{0\vert t}\)</span> 更加准确。在原始的扩散模型中，我们用 <span class="math inline">\(p_\theta(\x_{t-1}\vert\x_t)\)</span> 近似 <span class="math inline">\(q(\x_{t-1}\vert\x_t,\x_0)\)</span>，是因为 <span class="math inline">\(\x_0\)</span> 是未知的——如果知道，我们就没必要捣鼓一个扩散模型来生成了。但是现在由于 <span class="math inline">\(z_\text{sem}\)</span> 提取了 <span class="math inline">\(\x_0\)</span> 的许多信息，所以用 <span class="math inline">\(p_\theta(\x_{t-1}\vert\x_t,z_\text{sem})\)</span> 来近似 <span class="math inline">\(q(\x_{t-1}\vert\x_t,\x_0)\)</span> 就相当于作了弊，当然能够更快的去噪了。</li><li>作者进一步探索了 few-shot conditional 生成。给定目标类别的分类器 <span class="math inline">\(p_\gamma(c\vert z_\text{sem})\)</span>，通过拒绝采样得到 <span class="math inline">\(z_\text{sem}\)</span>，然后生成样本。</li><li>最后测试无条件生成的性能，证明 Diffusion AE 相比一般的扩散模型并不会对图像质量带来损失。</li></ol><div class="note note-secondary">            <details><summary><b>点击查看 Diffusion Autoencoders 的生成样例（摘自<a href="https://diff-ae.github.io/">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://diff-ae.github.io/data/website_results/Smiling-ffhq-18072.png" /></div><div class="group-image-wrap"><img src="https://diff-ae.github.io/data/website_results/Smiling-ffhq-37879.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://diff-ae.github.io/data/website_results/Young-ffhq-4077.png" /></div><div class="group-image-wrap"><img src="https://diff-ae.github.io/data/website_results/Young-ffhq-41207.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://diff-ae.github.io/data/website_results/Wavy_Hair-ffhq-4253.png" /></div><div class="group-image-wrap"><img src="https://diff-ae.github.io/data/website_results/Wavy_Hair-ffhq-4730.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://diff-ae.github.io/data/website_results/Male-ffhq-3804.png" /></div><div class="group-image-wrap"><img src="https://diff-ae.github.io/data/website_results/Male-ffhq-4145.png" /></div></div><div class="group-image-row"></div></div><video src="https://diff-ae.github.io/data/image/interpolate_ffhq/img8.mp4" type="video/mp4" controls="controls" width="30%"></video><video src="https://diff-ae.github.io/data/image/interpolate_ffhq/img4.mp4" type="video/mp4" controls="controls" width="30%"></video><video src="https://diff-ae.github.io/data/image/interpolate_ffhq/img2.mp4" type="video/mp4" controls="controls" width="30%"></video></details>          </div><h2 id="asyrp">Asyrp</h2><p><span class="label label-success">ICLR 2023 notable top 25%</span> <span class="label label-default">2022.10.20</span></p><p>这篇论文的名字其实叫做 Diffusion Models already have a semantic latent space，也是相当直白了。具体而言，设隐变量为 <span class="math inline">\(h\)</span>，当我们做出变动 <span class="math inline">\(\Delta h\)</span> 时，要求满足下列性质：</p><ul><li>同质性：对于不同的样本，同样的 <span class="math inline">\(\Delta h\)</span> 会导致这些样本有着类似的改变（而不是各变各的）。</li><li>线性：改变 <span class="math inline">\(\Delta h\)</span> 的大小能够控制样本改变的程度；对多个 <span class="math inline">\(\Delta h\)</span> 的线性组合能够控制样本同时朝多个方向改变。</li><li>健壮性：<span class="math inline">\(\Delta h\)</span> 能够无损地编码原图。</li><li>时间一致性：对于扩散模型来说，各个时间步下 <span class="math inline">\(\Delta h_t\)</span> 应该大抵一致。</li></ul><p>之前的工作 Diffusion Autoencoders<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Preechakul, Konpat, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward a meaningful and decodable representation. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 10619-10629. 2022.">[1]</span></a></sup> 通过训练一个编码器来人为构造了一个隐空间，而本文作者指出——训练好的扩散模型本身就有一个有语义的隐空间了，不需要专门去学习一个，这个隐空间就是 U-Net 的 bottleneck 的输出，作者称之为 "h-space"（我猜取这个名字是因为大家在写代码的时候喜欢把中间层变量命名为 <span class="math inline">\(h\)</span> 吧～）。空口无凭，怎么证明这个隐空间确实有语义呢？结合文本编辑图像的思路，只要我们对特定的属性，如 smiling，找到它的 <span class="math inline">\(\Delta h\)</span>，那就说明隐空间中确实有一块区域表示“微笑脸”，任何一个隐变量只要加上 <span class="math inline">\(\Delta h\)</span> 就能生成微笑的人脸。为了找到 <span class="math inline">\(\Delta h\)</span>，作者提出以下方法。</p><p><img src="asyrp.png" width=50% /></p><p>如图所示，作者在 h-space（就是预训练扩散模型的 U-Net 的 bottleneck 输出）后面用一个可学习的小网络 <span class="math inline">\(f_t\)</span>（两层 1x1 卷积构成）来输出 <span class="math inline">\(\Delta h_t\)</span>，然后训练 <span class="math inline">\(f_t\)</span> 让生成的人脸笑起来，这样就能得到“微笑”这个语义的 <span class="math inline">\(\Delta h\)</span> 了。这其实也是一个文本编辑图像的过程，做法和 DiffusionCLIP 类似，只不过 DiffusionCLIP 是微调整个 U-Net，而本文只训练一个很轻量 <span class="math inline">\(f_t\)</span>. 损失函数如下： <span class="math display">\[\mathcal L^{(t)}=\lambda_\text{CLIP}\mathcal L_\text{direction}(P_t^\text{edit},y^\text{tar};P_t^\text{source},y^\text{source})+\lambda_\text{recon}|\x_t^\text{edit}-\x_t^\text{source}|\]</span> 其中 <span class="math inline">\(P_t^\text{edit}\)</span> 指每一步预测的 <span class="math inline">\(\x_0\)</span>（这是论文的记法，我常写作 <span class="math inline">\(\x_\theta(\x_t,t)\)</span> 或者 <span class="math inline">\(\hat\x_{0|t}\)</span>），重构损失作为正则项防止编辑过头。</p><p>本文另一个贡献是把逆向采样过程分成了三段：前期用改动的网络做 DDIM 采样来编辑；中期编辑得差不多了，就用原本的网络做 DDIM 采样；后期切换成 DDPM 采样来增加随机细节、提升图像质量。至于什么时候切换不同的采样方式，作者提出了两个指标分别量化编辑强度和采样质量，此处不再赘述，感兴趣的读者可以参看论文 Section 4.</p><p>最后，读者可能想问标题的 Asyrp 是什么、上图中的 <span class="math inline">\(D_t\)</span> 是什么，这其实是本文的一个败笔。它用了一个过于简单的假设证明了 guidance 技巧没用，也是遭到了所有审稿人的质疑。幸好寻找有语义的隐空间这一点做得十分突出，大家还是一致倾向于接收本文。</p><div class="note note-secondary">            <details><summary><b>点击查看 Asyrp 的生成样例（摘自<a href="https://kwonminki.github.io/Asyrp/">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://kwonminki.github.io/Asyrp/resrc/celeba_church2.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://kwonminki.github.io/Asyrp/resrc/out_of_domain_ver4.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://kwonminki.github.io/Asyrp/resrc/afhq_metfaces_bedroom.png" /></div></div></div></details>          </div><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Preechakul, Konpat, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward a meaningful and decodable representation. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 10619-10629. 2022. <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Kwon, Mingi, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic latent space. <em>arXiv preprint arXiv:2210.10960</em> (2022). <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>Diffusion Models</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>扩散模型应用·基于文本的图像编辑</title>
    <link href="/blog-main/2023/01/05/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%C2%B7%E5%9F%BA%E4%BA%8E%E6%96%87%E6%9C%AC%E7%9A%84%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/"/>
    <url>/blog-main/2023/01/05/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%C2%B7%E5%9F%BA%E4%BA%8E%E6%96%87%E6%9C%AC%E7%9A%84%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\x}{\mathbf x}\]</span></p><blockquote><p>封面来自 <a href="https://civitai.com/images/1113613?modelVersionId=94081&amp;prioritizedUserIds=53515&amp;period=AllTime&amp;sort=Most+Reactions&amp;limit=20">CivitAI</a>.</p></blockquote><h2 id="diffusionclip">DiffusionCLIP</h2><p><span class="label label-success">CVPR 2022</span> <span class="label label-default">2021.10.06</span></p><p>DiffusionCLIP<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Kim, Gwanghyun, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 2426-2435. 2022.">[1]</span></a></sup> 旨在用文本编辑图像，之前人们常使用 GAN inversion + CLIP 来做这个任务，但是作者指出 GAN inversion 的能力有限，难以重建原图中在训练集里不常见的部分（比如文章开篇大图展示了一个人把手放在了脸旁，但是 GAN inversion 会把手给去掉，因为大多数训练数据都没有手）。因此，作者希望利用 Diffusion Models 几近完美的重建能力（DDIM）来解决这个问题。</p><p>如下左图所示，作者首先用 DDIM inversion 将待编辑的图像转换到隐空间，然后用 CLIP 来微调 score function.</p><p><img src="diffusionclip.png" width=100% /></p><p>损失函数包含两部分：</p><ul><li><p>Directional CLIP loss： <span class="math display">\[\begin{align}&amp;\mathcal L_\text{directional}(\x_\text{gen},y_\text{tar};\x_\text{ref},y_\text{ref})=1-\frac{\langle\Delta I,\Delta T\rangle}{\Vert\Delta I\Vert\Vert\Delta T\Vert}\\\text{where}\quad&amp;\Delta T=E_T(y_\text{tar})-E_T(y_\text{ref}),\;\Delta I=E_I(\x_\text{gen})-E_I(\x_\text{ref})\end{align}\]</span> 其中 <span class="math inline">\(E_T\)</span> 和 <span class="math inline">\(E_I\)</span> 是 pretrained CLIP 的文本和图像编码器。使用 directional CLIP loss 而非直接对齐 <span class="math inline">\(\x_\text{gen}\)</span> 和 <span class="math inline">\(y_\text{tar}\)</span> 的好处是能防止生成图像多样性降低。</p></li><li><p>Identity loss： <span class="math display">\[\mathcal L_\text{id}(\hat\x_0(\hat\theta),\x_0)=\lambda_\text{L1}\Vert\x_0-\hat\x_0(\hat\theta)\Vert+\lambda_\text{face}\mathcal L_\text{face}(\hat\x_0(\hat\theta),\x_0)\]</span> 其中 <span class="math inline">\(\mathcal L_\text{face}\)</span> 是 face identity loss，来自 Arcface 论文。这一项 loss 的添加与否取决于用户的编辑需求。</p></li></ul><p>我们知道扩散模型是分时间步生成的，梯度传播过程可能不是那么直观，所以作者还贴心地绘制了梯度流以供参考，见上右图。在训练完成后，任何输入图像都能被编辑到 <span class="math inline">\(y_\text{tar}\)</span> 所表示的 domain 之中。另外，作者指出我们还能调整前向/逆向过程的步数来加速编辑过程，实验发现只需要对前 300~600 步，用 40 步前向过程和 6 步逆向过程就足以用来训练了，在推断时用 200 步前向和 40 步逆向来增加细节。</p><p>进一步地，如果源图像不在训练分布之中，我们还能用 DDPM 前向过程编码，然后用微调的 DDIM 逆向过程来生成；我们还能连接多个前向-逆向过程来编辑图像；还能直接融合多个不同微调模型的输出来同时编辑多个属性……总之根据不同的应用场景，怎么应用是比较灵活的，如下图所示。</p><p><img src="diffusionclip2.png" width=60%/></p><p>然而，DiffusionCLIP 有一个致命的缺点，即对于每个 <span class="math inline">\(y_\text{tar}\)</span>，我们都要微调一个模型，这实在是太费时费力了。</p><div class="note note-secondary">            <details><summary><b>点击查看 DiffusionCLIP 的生成样例（摘自<a href="https://github.com/gwang-kim/DiffusionCLIP">官方 repo</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://github.com/submission10095/DiffusionCLIP_temp/raw/master/imgs/main1.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://github.com/submission10095/DiffusionCLIP_temp/raw/master/imgs/main2.png" /></div></div></div></details>          </div><h2 id="blended-diffusion">Blended Diffusion</h2><p><span class="label label-success">CVPR 2022</span> <span class="label label-default">2021.11.29</span></p><p>Blended Diffusion<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Avrahami, Omri, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 18208-18218. 2022.">[2]</span></a></sup> 关注的是用文本指导修改指定区域的内容，可以看作是 inpainting 的升级版。具体而言，给定一张图像 <span class="math inline">\(\x\)</span> 和一段引导文本 <span class="math inline">\(d\)</span>，再给定一个二值掩码 <span class="math inline">\(m\)</span>，要求将掩码内的部分改为符合文本描述的内容，同时保持掩码外的部分不变。为了完成这个目标，作者的主要贡献可以分两点叙述：</p><ol type="1"><li><p>为了让文本引导图像生成，作者借鉴了 classifier guidance 的做法，但将引导项改成了： <span class="math display">\[D_\text{CLIP}(\x,d,m)=D_\text{cosine}\left(\text{E}_\text{I}(\x\odot m), \text{E}_\text{L}(d)\right)\]</span> 其中 <span class="math inline">\(\text{E}_\text{I}\)</span> 和 <span class="math inline">\(\text{E}_\text{L}\)</span> 分别是 CLIP 的图像和文本编码器。作者通过拉近掩码内图像特征与文本特征，达到生成符合文本描述的内容的效果。这种做法在后续的 SDG<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Liu, Xihui, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, and Trevor Darrell. More control for free! image synthesis with semantic diffusion guidance. In *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*, pp. 289-299. 2023.">[3]</span></a></sup>、GLIDE<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nichol, Alex, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. *arXiv preprint arXiv:2112.10741* (2021).">[4]</span></a></sup> 等工作中也有应用。然而，原始的 classifier guidance 要求对每一个 noise level 都做引导，意味着我们可能还需要在带噪数据上训练 CLIP——这对于小实验室来说并不可行。事实上，SDG 专门为此提出了一个自监督微调方法；而对于 GLIDE 后面的 OpenAI，训练 CLIP 完全是个小 case。关于这两个模型的详细介绍请见后文。相比而言，Blended Diffusion 的解决方案更简单：直接使用预测的 <span class="math inline">\(\hat\x_0\)</span>，规避对 CLIP 的任何训练或微调。</p><p>CLIP 引导使得掩码内图像符合文本描述，但并不能保证掩码外内容不变。因此，作者又设计了一个针对掩码外内容的引导项： <span class="math display">\[\begin{align}&amp;D_\text{bg}=(\x_1,\x_2,m)=\text{dis}(\x_1\odot(1-m),\x_2\odot(1-m))\\\text{where}\quad&amp;\text{dis}(\x_1,\x_2)=\frac{1}{2}(\text{MSE}(\x_1,\x_2)+\text{LPIPS}(\x_1,\x_2))\end{align}\]</span> 结合二者，算法总结如下：</p><p><img src="blended-alg1.png" width=60% /></p></li><li><p>Algorithm 1 分别保证了掩码内外内容都是我们希望的，但并没保证二者能很好地融合在一起。为此，作者充分利用了扩散模型的特点，设计了一个针对 inpainting 任务的采样模式：在每一个时间步，对于掩码内的部分，我们使用扩散模型给出的结果不变；对于掩码外的部分，我们用原图的加噪结果代替（事实上，这个做法早在宋飏刚提出 NCSN 的论文<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, Yang, and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. *Advances in neural information processing systems* 32 (2019).">[5]</span></a></sup>的附录 B.3 里就有所阐述）。这样，虽然当前步依旧不能保证掩码内外的一致连贯性，但下一步能迫使图像向一致连贯的方向生成。另外，这种模式自然满足了掩码外的图像不会被改变的要求，因此我们也不必使用 Algorithm 1 中的 <span class="math inline">\(D_\text{bg}\)</span> 了。总而言之，新的算法如下：</p><p><img src="blended-alg2.png" width=60% /></p></li></ol><div class="note note-info">            <p><strong>Mask Guidance</strong></p><p>有些人将这种针对 image inpainting 的采样模式称为 mask guidance，因为它和 classifier guidance 具有类似的形式。沿用上图记号，即 <span class="math inline">\(\x_\text{fg}\)</span> 表示扩散模型给出的结果，<span class="math inline">\(\x_\text{bg}\)</span> 表示原图的加噪结果，那么稍作改写： <span class="math display">\[\begin{align}&amp;\x_{t-1}=\x_\text{fg}\odot m+\x_\text{bg}\odot (1-m)\\\implies &amp;\x_{t-1}=\x_\text{fg}+(1-m)\odot (\x_\text{bg}-\x_\text{fg})\end{align}\]</span> 可见 <span class="math inline">\((1-m)\odot(\x_\text{bg}-\x_\text{fg})\)</span> 就是 guidance 技巧中的偏移项。</p>          </div><p>另外，论文还提出了 extending augmentations 来增加模型的鲁棒性，以及最后对结果依 CLIP 相似度排序，这里不过多赘述。</p><p>话说，这篇论文里展示的效果感觉一般，修改的地方好似贴图，有点违和。另外，其生成的区域甚至还带有噪点，可能是代码哪里有点 bug 吧。</p><div class="note note-secondary">            <details><summary><b>点击查看 Blended Diffusion 的生成样例（摘自<a href="https://github.com/omriav/blended-diffusion">官方 repo</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://github.com/omriav/blended-diffusion/raw/master/docs/different_prompts2.jpg" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://github.com/omriav/blended-diffusion/raw/master/docs/different_prompts1.jpg" /></div></div></div></details>          </div><h2 id="sdg-semantic-diffusion-guidance">SDG (Semantic Diffusion Guidance)</h2><p><span class="label label-success">WACV 2023</span> <span class="label label-default">2021.12.10</span></p><p>SDG<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Liu, Xihui, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, and Trevor Darrell. More control for free! image synthesis with semantic diffusion guidance. In *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*, pp. 289-299. 2023.">[3]</span></a></sup> 可以看作是 classifier guidance 的直接推广，支持用文本、图像或二者共同来指导模型的生成。具体而言，原始的 classifier guidance 是用 <span class="math inline">\(\log p_\phi(y\vert \x_t)\)</span> 做引导，而 SDG 将其推广为 <span class="math inline">\(F_\phi(\x_t,y,t)\)</span>，根据不同类型的条件设计不同的引导函数。</p><p>首先来看<strong>文本引导</strong>。作者利用 CLIP 模型，设置引导函数为图像、文本特征的点积： <span class="math display">\[F_\phi(\x_t,y,t)=\text{E}_\text{I}(\x_t)\cdot\text{E}_\text{L}(y)\]</span> 其中 <span class="math inline">\((\x_t,y)\)</span> 是图像文本对，<span class="math inline">\(\text{E}_\text{I}\)</span> 和 <span class="math inline">\(\text{E}_\text{L}\)</span> 分别是 CLIP 的图像和文本编码器。我们知道，CLIP 通过对比学习拉近了成对图像和文本的特征，所以该方法能引导生成的图像 <span class="math inline">\(\x_t\)</span> 去接近给定的文本 <span class="math inline">\(y\)</span>.</p><p>由于扩散模型中的每一个 noise level 都要做引导，所以这个 CLIP 模型的图像编码器需要在 noised data 上训练。为此，作者特别提出了一种<strong>自监督的微调方法</strong>，不需要使用文本数据。设预训练的图像编码器为 <span class="math inline">\(\text{E}_\text{I}\)</span>，作者将其中的 BN 层更改为了 adaptive BN 层，以便在 scale 和 bias 项中加入时间步 <span class="math inline">\(t\)</span>，这样得到了一个新的图像编码器 <span class="math inline">\(\text{E}&#39;_\text{I}\)</span>. 由于新编码器仅仅添加了 adaptive BN 层的参数，所以其他参数仍然可以用预训练权重初始化。接下来，我们通过对比学习对齐无噪图像和加噪图像的特征。具体而言，设一个 batch 里有 <span class="math inline">\(N\)</span> 对无噪-加噪图像对 <span class="math inline">\(\{\x_0^i,\x_{t_i}^i\}_{i=1}^N\)</span>，分别将 <span class="math inline">\(\x_0^i\)</span> 和 <span class="math inline">\(\x_{t_i}^i\)</span> 通过 <span class="math inline">\(\text{E}_\text{I}\)</span> 和 <span class="math inline">\(\text{E}&#39;_\text{I}\)</span> 提取特征后，用 CLIP 的对比损失函数拉近成对特征、推远不成对的特征。简单来说，作者巧妙地利用了已有的训练好的图像编码器，把图像-文本对比学习更换成了图像-图像对比学习，从而避开了文本数据。</p><p>关于<strong>图像引导</strong>，作者将其分为了两个方面：<strong>图像内容引导</strong>和<strong>图像风格引导</strong>。对于内容引导，设引导图像为 <span class="math inline">\(\x_0&#39;\)</span>，我们可以对其加噪得到 <span class="math inline">\(\x_t&#39;\)</span>，那么引导函数设置为： <span class="math display">\[F_\phi(\x_t,\x_t&#39;,t)=\text{E}_\text{I}(\x_t)\cdot\text{E}_\text{I}(\x_t&#39;)\]</span> 即在特征层面拉近生成图像与引导图像。如此，生成的图像在高层语义上能与引导图像保持一致。比如设引导图像是一只柯基犬，那么生成的图像也会包含一只类似的柯基犬，但也许有不同的位置、姿态等。如果我们想让柯基犬的位置、姿态和引导图像都保持一致，可以考虑在每个特征图上都做匹配： <span class="math display">\[F_\phi(\x_t,\x_t&#39;,t)=-\sum_j\frac{1}{C_jH_jW_j}\|\text{E}_\text{I}(\x_t)_j-\text{E}_\text{I}(\x_t&#39;)_j\|^2\]</span></p><p>这里的 <span class="math inline">\(\text{E}_\text{I}\)</span> 也都是在 noised data 上训练过的编码器。</p><p>对于风格引导，我们可以匹配各个特征图的 Gram 矩阵： <span class="math display">\[F_\phi(\x_t,\x_t&#39;,t)=-\sum_j\|G_I(\x_t)_j-G_I(\x_t&#39;)_j\|^2\]</span> 至于<strong>文本图像混合引导</strong>，将上述引导函数做一个加权和即可。</p><p>总而言之，SDG 的关键点就在于设计引导函数 <span class="math inline">\(F_\phi(\x_t,y,t)\)</span> 来达到预期的条件生成效果，如何设计则是比较自由的。</p><div class="note note-info">            <p><strong>SDG 与 ILVR 的联系</strong></p><p>我们在介绍 ILVR<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Choi, Jooyoung, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models. In *2021 IEEE/CVF International Conference on Computer Vision (ICCV)*, pp. 14347-14356. IEEE, 2021.">[6]</span></a></sup> 时说（见<a href="/blog-main/2023/01/05/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%C2%B7%E5%9B%BE%E7%94%9F%E5%9B%BE%E4%B8%8E%E5%9B%BE%E5%83%8F%E6%81%A2%E5%A4%8D/" title="扩散模型应用·图生图与图像恢复">扩散模型应用·图生图与图像恢复</a>），ILVR 本质和 classifier guidance 是一致的。那么，SDG 作为 classifier guidance 的推广形式，自然也能导出 ILVR. 事实上，对比二者的引导形式，它们的联系还是挺显然的： <span class="math display">\[F_\phi(\x_t,\x_t&#39;,t)=-\frac{1}{2}\|\phi_N(\x_t)-\phi_N(\x_t&#39;)\|^2\]</span> 那么： <span class="math display">\[\nabla_{\x_t} F_\phi(\x_t,\x_t&#39;,t)=\phi_N(\x_t&#39;)-\phi_N(\x_t)\]</span></p>          </div><div class="note note-secondary">            <details><summary><b>点击查看 SDG 的生成样例（摘自论文）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="sdg-ex2.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="sdg-ex3.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="sdg-ex4.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="sdg-ex5.png" /></div></div></div></details>          </div><h2 id="prompt-to-prompt">Prompt-to-Prompt</h2><p><span class="label label-primary">Google</span> <span class="label label-default">2022.08.02</span></p><p>GLIDE、DALL·E 2、Imagen 等 large-scale language-image models 展现出了令人惊叹的生成能力，但是用它们来做图像编辑却不是一件容易的事，因为对 prompt 的微小改动会导致生成完全不一样的图像。因此，这些模型往往需要用户给定一个 mask 指示需要更改的区域。然而，绘制 mask 一方面多少带来了一些不便，另一方面 mask 掉的区域将被完全无视，丢弃了可能是很重要的信息，导致应用场景受限，例如用户无法更改一个物体的纹理。因此，Prompt-to-Prompt<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hertz, Amir, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. *arXiv preprint arXiv:2208.01626* (2022).">[7]</span></a></sup> 致力于不需要 mask 的图像编辑方法。</p><p>作者探究发现，网络中的 cross-attention layer 能体现出文本和图像之间的语义联系，如下图所示：</p><p><img src="prompt-to-prompt.png" width=80% /></p><p>因此，我们可以通过操纵 attention map 来操纵图像语义。具体而言，设原文本描述为 <span class="math inline">\(P\)</span>，扩散模型生成过程为 <span class="math inline">\(\x_T\to\cdots\to\x_0=I\)</span>，编辑后的文本描述为 <span class="math inline">\(P^\ast\)</span>，我们希望得到编辑后的图像 <span class="math inline">\(I^\ast\)</span>. 在 cross-attention layer 中，图像特征 <span class="math inline">\(\phi(\x_t)\)</span> 经由线性映射后得到 <span class="math inline">\(Q\)</span>，文本 embedding 经由线性映射后得到 <span class="math inline">\(K\)</span> 和 <span class="math inline">\(V\)</span>，那么 attention map 和最终输出为 ： <span class="math display">\[\begin{align}&amp;M=\text{Softmax}\left(\frac{QK^T}{\sqrt d}\right)\\&amp;\hat\phi(\x_t)=MV\end{align}\]</span> 其中 <span class="math inline">\(M_{ij}\)</span> 就表示第 <span class="math inline">\(i\)</span> 个像素与第 <span class="math inline">\(j\)</span> 个文本 token 之间的关联程度，可视化出来即是上图。为了编辑图像，我们同时以 <span class="math inline">\(P\)</span> 和 <span class="math inline">\(P^\ast\)</span> 为条件进行生成，那么在时间步 <span class="math inline">\(t\)</span> 会有两个 attention map <span class="math inline">\(M_t\)</span> 和 <span class="math inline">\(M_t^\ast\)</span>，二者通过精心设计的编辑函数得到新的 attention map <span class="math inline">\(\hat M_t=\text{Edit}(M_t,M_t^\ast,t)\)</span>，用 <span class="math inline">\(\hat M_t\)</span> 覆盖原来的 attention map 即可达到编辑的目的。值得注意的是，二者的随机数种子要设置为一样的，这样 <span class="math inline">\(M_t\)</span> 与 <span class="math inline">\(M_t^\ast\)</span> 才能有所配对。算法流程如下图所示：</p><p><img src="prompt-to-prompt-alg.png" width=80% /></p><p>可见，问题的关键在于如何设计编辑函数 <span class="math inline">\(\text{Edit}(M_t,M_t^\ast,t)\)</span>. 作者分别对三种编辑方式——换词、加词、修改某词的权重——设计了操纵 attention map 的方法：</p><ol type="1"><li><p>换词：例如将 <span class="math inline">\(P=\text{a big red bicycle}\)</span> 变成 <span class="math inline">\(P^\ast=\text{a big red car}\)</span>. 这里存在的主要挑战是在生成新的内容的同时要保留原有的图像结构不变。为此，我们可以用原本的 attention map 覆写新的的 attention map，限制 car 出现的位置和大致形状与 bicycle 相同。然而，考虑到替换的物品之间可能有比较大的差异，我们的限制可以“软”一些——只在逆向过程前期覆写： <span class="math display">\[\text{Edit}(M_t,M_t^\ast,t)=\begin{cases}M_t^\ast&amp;&amp;\text{if}\ t&lt;\tau\\M_t&amp;&amp;\text{otherwise}\end{cases}\]</span></p></li><li><p>加词：例如将 <span class="math inline">\(P=\text{a castle next to a river}\)</span> 变成 <span class="math inline">\(P^\ast=\text{children drawing of a castle next to a river}\)</span>. 为了保留两个 prompt 的共同点，作者只覆写原本存在的词的 attention map，保留新词的 attention map 不变。形式化地说，设 <span class="math inline">\(A\)</span> 表示 <span class="math inline">\(P^\ast\)</span> 的 token 索引到 <span class="math inline">\(P\)</span> 的 token 索引的映射，如果不存在映射则为 None，那么： <span class="math display">\[[\text{Edit}(M_t,M_t^\ast,t)]_{i,j}=\begin{cases}[M_t^\ast]_{i,j}&amp;&amp;\text{if}\ A(j)=\text{None}\\ [M_t]_{i,A(j)}&amp;&amp;\text{otherwise}\end{cases}\]</span> 我们也可以用 <span class="math inline">\(\tau\)</span> 来控制编辑的程度。</p></li><li><p>修改某词的权重：例如 <span class="math inline">\(P=\text{a fluffy red ball}\)</span>，我们想让这个球更 fluffy 或者更不 fluffy 一些，那么可以更改 attention map 里 fluffy 这个 token 的权重： <span class="math display">\[[\text{Edit}(M_t,M_t^\ast,t)]_{i,j}=\begin{cases}c\cdot[M_{i,j}]&amp;&amp;\text{if}\ j=j^\ast\\M_{i,j}&amp;&amp;\text{otherwise}\end{cases}\]</span></p></li></ol><p>基于上述三种编辑方式，作者阐释了更多的应用场景：</p><ul><li><strong>Text-only localized editing</strong>：一般而言，换一个名词或者添加形容某物体的形容词就能达到局部修改的效果。</li><li><strong>Global editing</strong>：添加一个形容地点、天气、时间等的形容词就能达到全局修改的效果，但是画面的整体结构并不会改变。</li><li><strong>Fader control using attention re-weighing</strong>：有时候我们并不好描述一个形容词的程度，例如山上到底有多少雪。这时候修改词权重的方法就给出了很好的解决方案。</li><li><strong>Real image editing</strong>：首先使用 DDIM inversion 得到隐变量表示，然后操控 reconstruction 过程. 然而，作者发现在 classifier-free guidance parameter 比较大的时候 DDIM inversion 并不能很好的 reconstruct 原图。为了解决这个问题，作者用 attention map 自动生成 mask 来辅助编辑，但具体怎么做的也没多说。</li></ul><div class="note note-success">            <p>在 Prompt-to-Prompt 之前，人们要么采用微调（如 DiffusionCLIP）、要么采用 guidance 技巧（如 Blended Diffusion 和 SDG）来实现文本编辑图像，而 Prompt-to-Prompt 首次给出了一个新的思路——<strong>更改 backbone 网络的中间值</strong>。当然，为了有效地编辑，这种思路要求更改的值容易对应上图像的语义，比如本文的 attention map. 后续非常多的工作都受到了 Prompt-to-Prompt 的影响。</p>          </div><div class="note note-secondary">            <details><summary><b>点击查看 Prompt-to-Prompt 的生成样例（摘自<a href="https://prompt-to-prompt.github.io/">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://prompt-to-prompt.github.io/ptp_files/teaser.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://prompt-to-prompt.github.io/ptp_files/99_imagen_results_web-02.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://prompt-to-prompt.github.io/ptp_files/99_imagen_results_web-03.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://prompt-to-prompt.github.io/ptp_files/99_imagen_results_web-04.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://prompt-to-prompt.github.io/ptp_files/04_style_transfer_web.png" /></div></div></div></details>          </div><h2 id="diffuseit">DiffuseIT</h2><p><span class="label label-success">ICLR 2023</span> <span class="label label-default">2022.09.30</span></p><p>在 Image Translation 中，最大的挑战在于改变图像风格（语义）的同时，保持图像的内容（构图）不变。一种解决方案是为模型输入原始图像作为条件，但这要求我们重新训练或微调模型，而且不能应用于非成对数据的场景。而之前使用预训练无条件模型的方法，如 Blended Diffusion、SDEdit 等，无法保证内容的不变。</p><p>DiffuseIT<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="Kwon, Gihyun, and Jong Chul Ye. Diffusion-based image translation using disentangled style and content representation. *arXiv preprint arXiv:2209.15264* (2022).">[8]</span></a></sup> 作者认为，这是因为风格和内容的表征没有显式的解耦，所以改变风格往往也改变了内容。为此，作者想到了 splicing Vision Transformer 发现的一个现象——对于一个预训练 DINO ViT，其多头注意力的 keys <span class="math inline">\(k^l\)</span> 包含了结构信息，而 <span class="math inline">\(\text{[CLS]}\)</span> token 包含了语义信息，也就是说 keys 和 <span class="math inline">\(\text{[CLS]}\)</span> token 把内容和风格解耦开了。基于此，作者就根据设计了各种奇奇怪怪的 loss 为逆向过程做 guidance：</p><ol type="1"><li><p>拉近生成图像和源图像的结构信息： <span class="math display">\[\begin{align}&amp;l_{ssim}(\x_{src},\x)=\Vert S^l(\x_{src})-S^l(\x)\Vert_F \\\text{where}\quad&amp;[S^l(\x)]_{i,j}=\cos(k^l_i(\x),k^l_j(\x))\end{align}\]</span> 作者觉得还不够，又加了个对比损失： <span class="math display">\[l_{cont}(\x_{src},\x)=-\sum_i\log\left(\frac{\exp(\text{sim}(k^l_i(\x),k_i^l(\x_{src}))/\tau)}{\exp(\text{sim}(k^l_i(\x),k_i^l(\x_{src}))/\tau)+\sum_{j\neq i}\exp(\text{sim}(k^l_i(\x),k_j^l(\x_{src}))/\tau)}\right)\]</span></p></li><li><p>拉近生成图像和目标文本（如果有）的语义信息： <span class="math display">\[\begin{align}&amp;l_{CLIP}(\x;\mathbf d_{trg},\x_{src},\mathbf{d}_{src})=-\text{sim}(\mathbf v_{trg},\mathbf v_{src})\\\text{where}\quad&amp; \mathbf v_{trg}=E_T(\mathbf d_{trg})+\lambda_i E_I(\x_{src})-\lambda_s E_T(\mathbf d_{src}),\,\mathbf v_{src}=E_I(\text{aug}(\x))\end{align}\]</span> 这里 <span class="math inline">\(E_T\)</span> 和 <span class="math inline">\(E_I\)</span> 是 CLIP 的文本和图像编码器，而且甚至不止一个。</p></li><li><p>拉近生成图像和目标图像（如果有）的语义信息： <span class="math display">\[l_{sty}(\x_{trg},\x)=\Vert e^L_{[CLS]}(\x_{trg})-e^L_{[CLS]}(\x)\Vert_2+\lambda_{mse}\Vert\x_{trg}-\x\Vert_2\]</span></p></li><li><p>在生成过程的每一步，都推远当前步和上一步的语义信息： <span class="math display">\[l_{sem}(\x_t;\x_{t+1})=-\Vert e^L_\text{[CLS]}(\hat\x_0(\x_t))-e^L_\text{[CLS]}(\hat\x_0(\x_{t+1}))\Vert_2\]</span></p></li></ol><p>一图以蔽之：</p><p><img src="diffuseit.png" width=80% /></p><p>另外还有一些细节，比如在最开始做 resampling，还有一个正则化的引导项，都是借鉴了其他论文的提议，这里不细说了。总之，DiffuseIT 的做法其实没有什么新鲜的，但是看可视化效果，它确实做到了极大程度上保留构图而改变风格，不过这也归功于 splicing ViT 的发现。</p><p>虽然 DiffuseIT 把保留构图作为出发动机和核心卖点，但是个人觉得这种性质不一定适合所有的 Image-to-Image Translation. 比如，它非常适合冬天转夏天或者狮子转老虎，但是如果要把狗变成猫，那生成的猫就有个狗一样的长鼻子，长相着实怪异。如果能有个超参数让用户权衡风格与内容的保留程度就更好了。</p><div class="note note-secondary">            <details><summary><b>点击查看 DiffuseIT 的生成样例（摘自论文）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="./diffuseit-ex.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="./diffuseit-ex2.png" /></div><div class="group-image-wrap"><img src="./diffuseit-ex3.png" /></div></div></div></details>          </div><h2 id="imagic">Imagic</h2><p><span class="label label-success">CVPR 2023</span> <span class="label label-default">2022.10.17</span></p><h2 id="diffedit">DiffEdit</h2><p><span class="label label-success">ICLR 2023 spotlight</span> <span class="label label-default">2022.10.20</span></p><p>DiffEdit<sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><span class="hint--top hint--rounded" aria-label="Couairon, Guillaume, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. *arXiv preprint arXiv:2210.11427* (2022).">[9]</span></a></sup> 与 Prompt-to-Prompt<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hertz, Amir, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. *arXiv preprint arXiv:2208.01626* (2022).">[7]</span></a></sup> 的动机类似，即目前根据文本编辑图像的模型，如 Blended Diffusion、GLIDE 等都存在一些问题：</p><ol type="1"><li>需要提供一个 mask 指定需要更改区域，否则整张图的内容会完全改变；</li><li>mask 会丢弃掉原有内容的重要信息，这些语义信息在编辑时应保留。譬如，将狗变成猫不应该改变其颜色、姿态等。</li></ol><p>因此，DiffEdit 旨在只使用文本来编辑图像中需要改变的区域，而不需要用户提供 mask. 其解决问题的思路如下图所示：</p><p><img src="diffedit.png" width=80% /></p><p>如上图所示，DiffEdit 包含三个步骤：首先通过比较两次逆向过程（一次以 ref. text 为条件或不带条件，另一次以 query 为条件）的差异生成 mask，然后通过 DDIM encoding (或称 DDIM inversion、reverse DDIM) 得到隐空间表示，最后加入 query 为条件、并用 mask guidance 方法 decode 得到输出图像。下面我们分别对这三个步骤做阐释。</p><ol type="1"><li><p><strong>Compute Mask</strong></p><p>在对图像做一定程度的加噪后，用不同文本作为条件让扩散模型去噪，那么理论上两次噪声估计差异较大的地方就是需要编辑的地方。实际操作中，为了让效果更加稳定，作者加噪了 <span class="math inline">\(n=10\)</span> 次并取平均，然后 rescale 到 <span class="math inline">\([0,1]\)</span> 并以 <span class="math inline">\(0.5\)</span> 的阈值二值化得到 mask. 这个 mask 往往比需要编辑的区域略大一点，对模型生成平滑过渡的图片有好处。</p></li><li><p><strong>Encoding</strong></p><p>依 DDIM encoding 过程走 <span class="math inline">\(r\)</span> 步，得到原图的隐空间表示。作者称 <span class="math inline">\(r\)</span> 为 encoding ratio，其值越大，可供编辑的空间越大，结果就越贴近文本描述，但同时也越偏离原图，而这种偏离可能是不需要的。</p><p>另外，encoding 用的是无条件模型，或者说取条件为 <span class="math inline">\(\varnothing\)</span>.</p></li><li><p><strong>Decoding with mask guidance</strong></p><p>以 query 为条件 decode，并施以 mask guidance: <span class="math inline">\(\tilde{\mathbf y}_t=M\mathbf y_t+(1-M)\x_t\)</span>.</p><p>这里产生了两个问题：</p><ol type="1"><li>理论上我们要用无条件模型才能恢复原图，为什么 DiffEdit 还能够保证 mask 外内容不变？</li><li>为什么作者称 DiffEdit 在编辑的同时能够保留 mask 内原图的一部分语义信息（比如狗的颜色、姿态）？</li></ol><p>这两个问题的关键在于 DiffEdit 用的是 DDIM 确定性编/解码而非 DDPM 的随机编/解码。作者推导了二者解码结果与原图之间的差异的界，发现前者在 <span class="math inline">\(r\)</span> 小于某特定值（70% 左右）时更小，而小的关键在于噪声估计模型在有无条件下能产生相近的结果。</p></li></ol><div class="note note-secondary">            <details><summary><b>点击查看 DiffEdit 的生成样例（摘自论文）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="./diffedit-ex.png" /></div></div></div></details>          </div><h2 id="null-text-inversion">Null-text Inversion</h2><p><span class="label label-primary">Google</span> <span class="label label-default">2022.11.17</span></p><p>Null-text Inversion<sup id="fnref:10" class="footnote-ref"><a href="#fn:10" rel="footnote"><span class="hint--top hint--rounded" aria-label="Mokady, Ron, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text Inversion for Editing Real Images using Guided Diffusion Models. *arXiv preprint arXiv:2211.09794* (2022).">[10]</span></a></sup> 是 prompt-to-prompt 的续作，解决的是如何在真实图像上运用 prompt-to-prompt 的问题。相比合成图像，我们并不知道真实图像的 prompt、也不知道它对应的隐变量是什么，而这两点都是 prompt-to-prompt 需要事先知道的。对于第一个问题，使用现有的一些 image captioning 模型就行；对于第二个问题，最直接的想法就是 DDIM inversion. 然而，作者发现 DDIM inversion 在有 guidance 的情形下并不能较好地重建输入，因此作者希望找到更好的 inversion 方式。</p><p><img src="null-text.png" width=70% /></p><p>如上图上半部分所示，设 DDIM inversion 过程为 <span class="math inline">\(z_0^\ast\to z_1^\ast\to \cdots\to z_T^\ast\)</span>，往回采样的过程为 <span class="math inline">\(z_T^\ast\to \bar z_{T-1}\to\cdots\to\bar z_1\to\bar z_0\)</span>. 二者每一步都会有一定的误差，在没有 guidance（即 <span class="math inline">\(w=1\)</span>）的情形下，这些误差比较小，重建结果还是能几乎还原图像的。但是当 <span class="math inline">\(w&gt;1\)</span> 时，这些误差会被放大，导致重建结果与原图有明显的差异。而实践中我们常常依靠大 guidance scale（如 <span class="math inline">\(w=7.5\)</span>）来生成高质量图像，因此这个问题必须得到解决。</p><p>我们现在仔细考虑采样过程的每一步。如上图下半部分所示，classifier-free guidance 会让网络推理两次，分别对应有条件（上分支）和无条件（下分支），二者结果依据 <span class="math inline">\(w\)</span> 做线性组合得到这一步的最终预测值。由于 prompt-to-prompt 方法要利用文本条件和 attention map 之间的关系，所以我们并不想改变条件分支和网络权重，于是我们能改变的只有无条件分支的输入，也即 null-text embedding <span class="math inline">\(\varnothing_t\)</span>. 我们希望通过优化 <span class="math inline">\(\varnothing_t\)</span>，使得采样过程能够重建出原图，这就是该方法的名字 null-text inversion 的意义。考虑到 <span class="math inline">\(w=1\)</span> 下的 DDIM inversion 能较好的重建原图，我们自然想到以 <span class="math inline">\(z_{t-1}^\ast\)</span> 作为目标，让 <span class="math inline">\(\bar z_{t-1}\)</span> 去接近它，因此损失函数就是二者的 MSE.</p><p>实现上，由于采样过程是随时间步进行的，因此我们要按 <span class="math inline">\(t=T\to t=1\)</span> 的顺序逐步训练——当前一步训练好之后，在前一步的采样基础之上训练下一步。另外，虽然原本的 null-text 对应的 embedding 只有一个全局的 <span class="math inline">\(\varnothing\)</span>，但是作者发现为每一步都定义自己的 <span class="math inline">\(\varnothing_t\)</span> 能大幅提高性能。综上所述，训练算法如下图所示：</p><p><img src="null-text-alg.png" width=45% /></p><p>虽然对于每一个输入的真实图像都要跑一遍上述训练算法来做 inversion，但是作者称在 1 块 A100 上只需要 1min 就能得到较好的结果，还是能接受的。</p><p>总结一下，Null-text inversion 的亮点在于它揭示了 <strong>classifier-free guidance 下，结果受无条件分支的影响非常大</strong>，并成功地利用这种影响达到了重建输入图像的目的。换句话说，null-text inversion 可以看作是 DDIM inversion 在 classifier-free guidance 下的“修正”或“拓展”。另外，与 GAN inversion 不同，null-text inversion 改动的并不是传统意义上的隐变量 <span class="math inline">\(z_T\)</span>，而是逆向过程的每一步中无条件分支的输入，这一点着实出乎了我的意料，令人眼前一亮。事实上，正因此特点，null-text inversion 除了可以搭配 prompt-to-prompt，还能搭配 SDEdit 并提升其效果。</p><div class="note note-secondary">            <details><summary><b>点击查看 Null-text Inversion 的生成样例（摘自<a href="https://null-text-inversion.github.io/">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://null-text-inversion.github.io/files/teaser-01.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://null-text-inversion.github.io/files/multi_cap-01.png" /></div></div></div></details>          </div><h2 id="instructpix2pix">InstructPix2Pix</h2><p><span class="label label-primary">UCB</span> <span class="label label-default">2022.11.17</span></p><p>前文中，Prompt-to-Prompt<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hertz, Amir, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. *arXiv preprint arXiv:2208.01626* (2022).">[7]</span></a></sup> 和 DiffEdit<sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><span class="hint--top hint--rounded" aria-label="Couairon, Guillaume, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. *arXiv preprint arXiv:2210.11427* (2022).">[9]</span></a></sup> 虽然都实现了文本编辑图像的功能，但它们的模式是「把描述原图的文本编辑为描述目标图像的文本」，对用户而言这并不是很自然。最自然的方式应该是给指令，例如 "Color the cars pink"、"Make it lit by fireworks" 等。为此，UCB 的研究人员提出了 InstructPix2Pix<sup id="fnref:11" class="footnote-ref"><a href="#fn:11" rel="footnote"><span class="hint--top hint--rounded" aria-label="Brooks, Tim, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. *arXiv preprint arXiv:2211.09800* (2022).">[11]</span></a></sup>.</p><p>其实 InstructPix2Pix 的技术路线非常直接——构造一个（源图像、编辑指令、目标图像）三元组构成的数据集，然后拿预训练的 Stable Diffusion 在上面微调。构造数据集的方式如下图左侧所示：</p><p><img src="instructpix2pix.png" width=100% /></p><p>可以看到整体思路还是非常简单直接。先人为构造 700 条（源描述、编辑指令、目标描述）三元组构成的数据集，然后拿预训练的 GPT-3 在上面微调，使之能够根据源描述自动生成编辑指令和对应的目标描述。微调结束后，用 GPT-3 生成 450,000 多条数据，然后利用 Stable Diffusion 和 Prompt-to-Prompt 编辑方法分别生成源描述和目标描述对应的图像，这样就得到了用来训练的数据集。在这个数据集上微调 Stable Diffusion，将源图像和指令都作为模型的条件输入，即可让它遵循输入指令编辑图像了，如上图右侧所示。</p><p>众所周知，数据集的质量非常重要，因此作者用 Prompt-to-Prompt 时对每对文本描述都采用了不同的编辑强度生成了 100 对图像，然后用 directional CLIP similarity 做筛选。另外，由于模型输入有两个条件（源图像和编辑指令），所以采用 classifier-free guidance 时会有两个 guidace scale.</p><p>总而言之，InstructPix2Pix 虽然没有什么创新性 idea，但是非常实用。作者也在 HuggingFace Spaces 上放了一个 <a href="https://huggingface.co/spaces/timbrooks/instruct-pix2pix">demo</a>，可以随时玩耍。</p><div class="note note-secondary">            <details><summary><b>点击查看 InstructPix2Pix 的生成样例（摘自<a href="https://www.timothybrooks.com/instruct-pix2pix">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://instruct-pix2pix.timothybrooks.com/landscape.jpg" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://instruct-pix2pix.timothybrooks.com/adam.jpg" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://instruct-pix2pix.timothybrooks.com/abbey.jpg" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://instruct-pix2pix.timothybrooks.com/mona.jpg" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://instruct-pix2pix.timothybrooks.com/cityscape.jpg" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://instruct-pix2pix.timothybrooks.com/girl.jpg" /></div></div></div></details>          </div><h2 id="pix2pix-zero">Pix2pix-zero</h2><p><span class="label label-primary">CMU</span> <span class="label label-default">2023.02.06</span></p><p>Pix2pix-zero 的目的是基于预训练的扩散模型通过文本来编辑真实图像，与 Null-text Inversion + Prompt-to-Prompt 比较相像，其方法如下图所示：</p><p><img src="pix2pix-zero.png" width=90%/></p><p>由图可见，pix2pix-zero 分为几个步骤——首先做 inversion 得到隐变量 <span class="math inline">\(\x_\text{inv}\)</span>，然后重构原图并记录下采样过程的 cross-attention map，接下来在原始文本 embedding <span class="math inline">\(c\)</span> 上加上编辑方向 <span class="math inline">\(\Delta c_\text{edit}\)</span> 后重新采样，且对 cross-attention map 做 guidance，从而生成保留了原始图像构图的编辑图像。接下来我们分别对这些部分做详细叙述。</p><ol type="1"><li><p>要基于预训练扩散模型做编辑，就免不了要做隐空间 inversion. 最直接的方法显然是 DDIM inversion. 但是作者指出，网络的输出 <span class="math inline">\(\epsilon_\theta(\x_t,t,c)\)</span> 常常不符合标准正态分布，会损害编辑效果。因此，作者提出了 <strong>noise regularization</strong> 来约束它近似标准正态分布。</p><p>具体而言，作者用一个自相关目标函数 <span class="math inline">\(\mathcal L_\text{auto}=\mathcal L_\text{pair} + \lambda\mathcal L_\text{KL}\)</span> 来引导逆向过程。其中，<span class="math inline">\(\mathcal L_\text{pair}\)</span> 意图使得自相关函数符合 Kronecker delta function，但是考虑到计算各个像素两两之间的自相关系数过于费时，作者采用了一种巧妙的 pyramid 方式：设 <span class="math inline">\(\eta^0\in\mathbb R^{64\times64\times4}\)</span> 是原始大小的预测噪声图 <span class="math inline">\(\epsilon_\theta\)</span>，将其下采样（average pool，并乘 2 来保持方差）三次直到分辨率大小为 <span class="math inline">\(8\times 8\)</span>，记作 <span class="math inline">\(\{\eta^0,\eta^1,\eta^2,\eta^3\}\)</span>，然后定义： <span class="math display">\[\mathcal L_\text{pair}=\sum_p\frac{1}{S_p^2}\sum_{\delta=1}^{S_p-1}\sum_{x,y,c}\eta_{x,y,c}^p\left(\eta_{x-\delta,y,c}^{p}+\eta_{x,y-\delta,c}^{p}\right)\]</span> 另外，作者添加了一个 KL 正则项（类似于 VAE）约束 <span class="math inline">\(\epsilon_\theta\)</span> 为零均值、单位方差。两个损失函数通过超参数 <span class="math inline">\(\lambda\)</span> 平衡。</p></li><li><p>寻找编辑方向 <span class="math inline">\(\Delta c_\text{edit}\)</span>：给定一个原始词汇（cat）和编辑后的词汇（dog），随机生成两组分别包含它们的句子，计算两组的 CLIP embeddings 的平均差异，即得到编辑的方向。用一组句子而非一个句子能保证找到的 embedding direction 的健壮性。这个过程只需要大约 5 秒并且可以预处理，所以并不会降低效率。注意，这里原始词汇并不是描述输入图像的文本，只是想要编辑的东西。</p></li><li><p>提出 <strong>cross-attention guidance</strong> 保留不希望编辑的部分的构图：作者发现 cross-attention map 对应着生成图像的构图，所以通过保证编辑前后 cross-attention map 的一致性，可以保留原始构图不变。这个 idea 与 Prompt-to-Prompt 很像，但是本文作者是通过 L2 loss 来约束的，相比 Prompt-to-Prompt 的直接修改更“软”。</p></li></ol><p>算法伪代码如下：</p><p><img src="pix2pix-zero-alg.png" width=50%/></p><p>总的来说，由于之前已经看过不少 zero-shot image-to-image translation 的工作了，所以这篇文章带来的新鲜感没有那么强，但是也不失为一个很好的工作。</p><div class="note note-secondary">            <details><summary><b>点击查看 pix2pix-zero 的生成样例（摘自<a href="https://pix2pixzero.github.io/">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://pix2pixzero.github.io/assets/results_row_3.jpg" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://pix2pixzero.github.io/assets/results_row_4.jpg" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://pix2pixzero.github.io/assets/results_row_5.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://pix2pixzero.github.io/assets/synth_results_row_1.jpg" /></div></div></div></details>          </div><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Kim, Gwanghyun, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 2426-2435. 2022. <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Avrahami, Omri, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 18208-18218. 2022. <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Liu, Xihui, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, and Trevor Darrell. More control for free! image synthesis with semantic diffusion guidance. In <em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, pp. 289-299. 2023. <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Nichol, Alex, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. <em>arXiv preprint arXiv:2112.10741</em> (2021). <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Song, Yang, and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. <em>Advances in neural information processing systems</em> 32 (2019). <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Choi, Jooyoung, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models. In <em>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pp. 14347-14356. IEEE, 2021. <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Hertz, Amir, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. <em>arXiv preprint arXiv:2208.01626</em> (2022). <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Kwon, Gihyun, and Jong Chul Ye. Diffusion-based image translation using disentangled style and content representation. <em>arXiv preprint arXiv:2209.15264</em> (2022). <a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>Couairon, Guillaume, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. <em>arXiv preprint arXiv:2210.11427</em> (2022). <a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:10" class="footnote-text"><span>Mokady, Ron, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text Inversion for Editing Real Images using Guided Diffusion Models. <em>arXiv preprint arXiv:2211.09794</em> (2022). <a href="#fnref:10" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:11" class="footnote-text"><span>Brooks, Tim, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. <em>arXiv preprint arXiv:2211.09800</em> (2022). <a href="#fnref:11" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:12" class="footnote-text"><span>Parmar, Gaurav, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. <em>arXiv preprint arXiv:2302.03027</em> (2023). <a href="#fnref:12" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>Diffusion Models</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>扩散模型应用·文生图大模型</title>
    <link href="/blog-main/2023/01/05/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%C2%B7%E6%96%87%E7%94%9F%E5%9B%BE%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    <url>/blog-main/2023/01/05/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%C2%B7%E6%96%87%E7%94%9F%E5%9B%BE%E5%A4%A7%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\x}{\mathbf x}\]</span></p><blockquote><p>封面来自 <a href="https://civitai.com/images/1347346?modelVersionId=107343&amp;prioritizedUserIds=941582&amp;period=AllTime&amp;sort=Most+Reactions&amp;limit=20">CivitAI</a>.</p></blockquote><h2 id="glide">GLIDE</h2><p><span class="label label-primary">OpenAI</span> <span class="label label-default">2021.12.20</span></p><p>GLIDE<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nichol, Alex, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. *arXiv preprint arXiv:2112.10741* (2021).">[1]</span></a></sup> 是 OpenAI 在 2021 年底推出的文本引导图像生成的扩散模型。GLIDE 沿用了 ADM<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Dhariwal, Prafulla, and Alexander Nichol. Diffusion models beat gans on image synthesis. *Advances in Neural Information Processing Systems* 34 (2021): 8780-8794.">[2]</span></a></sup> 架构，但是更大，有 2.3 billion 参数。为了向其中注入文本条件，作者首先将输入文本通过 BPE tokenizer 编码成了 <span class="math inline">\(K\)</span> 个 tokens，然后经由一个有 1.2 billion 参数的 Transformer 得到 <span class="math inline">\(K\)</span> 个 token embeddings，它们被融入了 UNet 的每一个 Attention Block 之中（如下图所示）；另外，取最后一个 token embedding 经过维度映射后与 time embedding 相加，融入 UNet 的每一个 ResBlock 之中，相当于替换了 ADM 中的 class embedding. 更多细节可参阅<a href="https://github.com/openai/glide-text2im/blob/main/glide_text2im/text2im_model.py#L89">官方代码</a>。</p><p>简而言之，除了使用 AdaGN，GLIDE 还在每个注意力层融入了文本条件。另外，上述 3.5 billion 参数（2.3+1.2=3.5）的模型只生成 64x64 图像，作者还构建了另一个类似的、有 1.5 billion 参数的模型把图像上采样至 256x256.</p><p><img src="glide.png" width=100% alt="Text encoder and attention block in GLIDE. Green dots are text token embeddings and blue dots are image features. Drawn by me based on the official code." /></p><p>关于文本引导，作者尝试了两种方法——CLIP guidance 和 classifier-free guidance，实验发现后者效果更好。</p><ul><li><p><strong>CLIP guidance</strong> 其实就是将 classifier guidance 技巧中的分类器换做 CLIP 图像文本的匹配分数： <span class="math display">\[F_\phi(\x_t,y,t)=\text{E}_\text{I}(\x_t)\cdot\text{E}_\text{L}(y)\]</span> 其中 <span class="math inline">\((\x_t,y)\)</span> 是图像文本对，<span class="math inline">\(\text{E}_\text{I}\)</span> 和 <span class="math inline">\(\text{E}_\text{L}\)</span> 分别是 CLIP 的图像和文本编码器。同样的，CLIP 需要在 noised data 上训练，作者应该并没有像 SDG 那样设计自监督微调方式，而是直接训练的（毕竟 OpenAI 不差这点算力）。</p></li><li><p><strong>Classifier-free guidance</strong>：作者首先训练了支持文本条件的模型，然后在此基础上以 20% 的概率丢弃文本条件微调，从而使模型也具有了无条件生成能力。</p></li></ul><div class="note note-secondary">            <details><summary><b>点击查看 GLIDE 的生成样例（摘自论文）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="glide-ex1.png" width=100% /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="glide-ex2.png" width=100% /></div><div class="group-image-wrap"><img src="glide-ex3.png" width=100% /></div></div></div></details>          </div><h2 id="dalle-2-unclip">DALL·E 2 (unCLIP)</h2><p><span class="label label-primary">OpenAI</span> <span class="label label-default">2022.04.13</span></p><p>虽然有了 GLIDE，但 OpenAI 还不满足，四个月后又推出了另一个文本引导图像生成模型 unCLIP<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ramesh, Aditya, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. *arXiv preprint arXiv:2204.06125* (2022).">[3]</span></a></sup>，也称作 DALL·E 2.</p><p>DALL·E 2 是一个 two-stage 模型：首先使用一个 prior 从 text embedding 生成对应的 image embedding；然后使用一个 decoder 根据 image embedding 生成图像，如下图虚线以下部分所示：</p><p><img src="dalle2.png" width=70% /></p><p>下面我们分别就 prior 和 decoder 做进一步的说明。</p><ul><li><p><strong>Decoder</strong></p><p>Decoder 是一个以 CLIP image embedding 为条件的扩散模型，其融入条件的方式是在 GLIDE 的基础上修改而来：</p><ol type="1"><li>将 image embedding 投影后与 time embedding 相加；</li><li>将 image embedding 投影为四个额外的 tokens，concatenate 到 GLIDE text encoder 的输出序列之后。作者保留了 GLIDE 的 text conditioning pathway，希望能为模型带来 CLIP 不具备的性质（如 variable binding），但实验发现这并没有发挥作用。</li></ol><p>另外，作者也采取了 classifier-free guidance，在训练时以 10% 的概率将 image embedding 置零（或置为一个可学习的 embedding），并以 50% 的概率丢弃 text caption.</p><p>为了生成高分辨率图像，作者还用了两个上采样扩散模型，64x64 → 256x256 → 1024x1024. 同 SR3<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Saharia, Chitwan, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. *IEEE Transactions on Pattern Analysis and Machine Intelligence* (2022).">[4]</span></a></sup> 和 CDM<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ho, Jonathan, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded Diffusion Models for High Fidelity Image Generation. *J. Mach. Learn. Res.* 23 (2022): 47-1.">[5]</span></a></sup> 一样，作者先将低分辨率图略微退化后再给到超分模型。具体而言，作者对第一个上采样阶段使用高斯模糊，对第二个上采样阶段使用更多样的 BSR 退化。值得注意的是，这两个超分模型只用了纯卷积而没用 attention layers，所以训练时可以只对 1/4 大小的 random crops 训练来减小计算量并保证数值稳定性，推断时改用完整大小。</p><p>由于 decoder 可以看作是从 image embedding 得到图像，和 CLIP 从图像得到 image embedding 正好是相反的过程，所以作者将整个文生图模型命名为 unCLIP.</p></li><li><p><strong>Prior</strong></p><p>用户输入文本（text caption）后，我们可以通过 pretrained CLIP 得到 text embedding，但是由于 decoder 的输入是 image embedding，所以我们需要训练一个 prior 模型从 text embedding 预测 image embedding.</p><p>作者尝试了两种方案：</p><ol type="1"><li>自回归模型：将 image embedding 转换为一列离散编码，然后用自回归的方式逐个预测；</li><li>扩散模型：以 text embedding 为条件，用扩散模型对 image embedding 建模。</li></ol><p>实验发现使用扩散模型效果更佳。关于如何融入条件的具体细节有亿些繁琐，感兴趣的读者直接看论文吧。</p><p>一个自然的问题是，prior 是否是必要的？我们为什么不直接把 CLIP text embedding、甚至是 text caption 给到 decoder 做生成呢？其实这样做也没毛病，不过作者做了一个 ablation study，发现用 prior 来预测 image embedding 效果更好。</p></li></ul><p>论文列举了许多有趣的 image manipulations 应用，不过在介绍这些应用之前，我们先要明确一件事：每一张图像 <span class="math inline">\(\x\)</span> 都对应着一个二元组 <span class="math inline">\((z_i,\x_T)\)</span>，其中 <span class="math inline">\(z_i\)</span> 是 CLIP 给出的 image embedding，也是 decoder 的条件；<span class="math inline">\(\x_T\)</span> 是 decoder 给出的隐变量表示，可以通过 DDIM inversion 获得。换句话说，<span class="math inline">\((z_i,\x_T)\)</span> 共同组成了一张图像的隐变量。</p><ul><li><p><strong>Variations</strong></p><p>第一个应用称作 variations，指给定一张图像 <span class="math inline">\(\x\)</span>，生成与之类似但不同的图像。我们首先根据 CLIP 和 DDIM inversion 得到 <span class="math inline">\(\x\)</span> 对应的 <span class="math inline">\((z_i,\x_T)\)</span>，然后取 DDIM 中的逆向方差非零采样，即可赋予采样结果多样性。结果请查看节末截图的 Figure 3.</p></li><li><p><strong>Interpolations</strong></p><p>给定两张图像 <span class="math inline">\(\x_1,\x_2\)</span>，我们首先对 <span class="math inline">\({z_i}_1\)</span> 和 <span class="math inline">\({z_i}_2\)</span> 做球面插值：<span class="math inline">\({z_i}_\theta=\text{slerp}({z_i}_1,{z_i}_2,\theta)\)</span>. 接下来有两种选择：在 <span class="math inline">\({\x_T}_1\)</span> 和 <span class="math inline">\({\x_T}_2\)</span> 之间也做插值，或随机选定一个新的隐变量 <span class="math inline">\(\hat\x_T\)</span>. 前者能够保证插值的两个端点能够重构出给定的图像，而后者能够带来更多的随机性和可能性。作者在论文中展示了第二种选择的结果，见节末截图的 Figure 4.</p></li><li><p><strong>Text Diffs</strong></p><p>上面两个应用都是基于图像，但作为一个文本生成图像模型，怎么能缺少基于文本的图像编辑呢！受益于 CLIP 将文本和图像编码到了同一个隐空间之中，我们对 text embedding 的修改也能比较好地反映在 image embedding 上，从而让我们操纵图像。具体而言，给定两段文本，我们可以计算二者 text embedding 的差 <span class="math inline">\(z_d=\text{norm}({z_t}_1-{z_t}_2)\)</span>，然后在 <span class="math inline">\(z_i\)</span> 和 <span class="math inline">\(z_d\)</span> 之间插值得到新的 image embedding：<span class="math inline">\(z_\theta=\text{slerp}(z_i,z_d,\theta)\)</span>. 把 <span class="math inline">\(z_\theta\)</span> 给到 decoder 就能生成对应语义的图像了。结果请查看节末截图的 Figure 5.</p></li></ul><p>最后，作者也发现 DALL·E 2 的一些问题，比如在物体-属性的对应关系上往往不如 GLIDE. 例如，输入文本为“一个红色方块在一个蓝色方块之上”，DALL·E 2 生成的结果不是把位置搞错，就是把颜色搞错，但 GLIDE 就靠谱很多。作者推测这与 CLIP embedding 本身没有显式地绑定物体与属性有关。</p><div class="note note-secondary">            <details><summary><b>点击查看 DALL·E 2 的生成样例（摘自<a href="https://openai.com/dall-e-2/">官网</a>和论文）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://cdn.openai.com/dall-e-2/demos/text2im/astronaut/horse/photo/0.jpg"/></div><div class="group-image-wrap"><img src="https://cdn.openai.com/dall-e-2/demos/text2im/soup/portal/digital_art/0.jpg"/></div><div class="group-image-wrap"><img src="https://cdn.openai.com/dall-e-2/demos/text2im/teddy_bears/mad_scientists/steampunk/0.jpg"/></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://cdn.openai.com/dall-e-2/demos/inpainting/flamingo_pool/a/0.jpg"/></div><div class="group-image-wrap"><img src="https://cdn.openai.com/dall-e-2/demos/inpainting/flamingo_pool/b/0.jpg"/></div><div class="group-image-wrap"><img src="https://cdn.openai.com/dall-e-2/demos/inpainting/flamingo_pool/c/0.jpg"/></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="dalle2-fig3.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="dalle2-fig4.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="dalle2-fig5.png" /></div></div></div></details>          </div><h2 id="imagen">Imagen</h2><p><span class="label label-primary">Google</span> <span class="label label-default">2022.05.23</span></p><blockquote><p>这里有一篇关于 Imagen 的很细致的<a href="https://www.assemblyai.com/blog/how-imagen-actually-works/">博客</a>。</p></blockquote><p>看到 OpenAI 又是 GLIDE 又是 DALL·E 2 的，Google 这边终于坐不住了，推出了更强的文本生成图像大模型——Imagen<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Saharia, Chitwan, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour et al. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. *arXiv preprint arXiv:2205.11487* (2022).">[6]</span></a></sup>. 相比 DALL·E 2，Imagen 的整体思路更简单一些：先用一个大语言模型将输入文本编码为 text embedding，然后以此为条件并利用 classifier-free guidance 指导一个扩散模型生成 64x64 大小的图像，随后用两个上采样扩散模型（也加入了文本条件并使用了 classifier-free guidance）将图像上采样至 256x256 和 1024x1024，如下图所示：</p><p><img src="imagen.png" width=90% /></p><p>下面我们分别对各个组成部分做进一步说明：</p><ol type="1"><li><p><strong>Pretrained text encoders</strong></p><p>与 GLIDE 不同的是，Imagen 采用预训练好且固定不动的文本编码器而非从头训练。常见的 LLM，包括 BERT、CLIP、T5 都是可行的选择，作者发现 T5 效果最佳。有趣的是，作者发现<strong>扩大 text encoder 的规模比扩大 image diffusion model 的规模显著更有效</strong>。</p></li><li><p><strong>Dynamic thresholding</strong></p><p>我们知道，增加 classifier-free guidance weight 能够让模型更加符合条件——也就是生成的图像和文本更加贴合，但 weight 过大会导致图像失真，主要体现在过饱和的颜色上。这是因为 guidance 只用在 inference 阶段，与训练阶段存在 gap，导致预测的 <span class="math inline">\(\hat\x_0\)</span> 往往不在理论的范围 <span class="math inline">\([-1,1]\)</span> 内。我们知道即便是最早的 DDPM，实现时也会人为把 <span class="math inline">\(\hat\x_0\)</span> clip 到 <span class="math inline">\([-1,1]\)</span> 内，作者称这种做法为 static thresholding. 然而，static thresholding 不足以弥补 guidance 引入的 gap，因此作者提出了 dynamic thresholding：我们选择一个百分位，当发现位于该百分位的像素值 <span class="math inline">\(s\)</span> 大于 <span class="math inline">\(1\)</span> 后，就把所有像素都 clip 到 <span class="math inline">\([-s,s]\)</span> 之间，然后整体除以 <span class="math inline">\(s\)</span>. 与 static thresholding 相比，dynamic thresholding 让百分位以内的所有像素都有所减小，因此能缓解颜色饱和问题。</p><p><img src="imagen-thresholding.png" width=80% /></p></li><li><p><strong>Cascaded diffusion models</strong></p><p>同 SR3、CDM、DALL·E 2 等一样，Imagen 作者也发现对超分模型而言，将低分辨率图像做一定的增强（高斯噪声）后作为超分模型的条件，能让模型更鲁棒。</p></li><li><p><strong>Network architecture</strong></p><p>对于第一个扩散模型，作者除了将 text embedding 与 time embedding 相加来融入条件，还采用了 cross attention.</p><p>对于两个上采样扩散模型，作者提出了更简单、收敛更快、更 memory efficient 的 <strong>Efficient U-Net</strong>. 相比常用的 U-Net，Efficient U-Net 做了如下改变：</p><ul><li>在小分辨率添加更多的 residual blocks（如 8 个），将模型参数从大分辨率 blocks 转移到小分辨率 blocks.</li><li>在使用了很多 residual blocks 的小分辨率 blocks 中，将残差连接放缩到 <span class="math inline">\(1/\sqrt{2}\)</span>，能显著加快收敛。</li><li>在传统 U-Net 下采样 block 中，下采样操作在卷积之后；而上采样 block 中，上采样操作在卷积之前。Efficient U-Net 调换了二者的顺序，显著加快了前向传播的速度，并且没有性能损失。</li></ul><p>与 DALL·E 2 类似，第二个超分模型是在 image crops 上训练的，因此没有使用 self-attention layers，但是保留了 text cross-attention layers. 更多细节可以在论文的 Appendix F 中找到。</p></li></ol><p>以上就是 Imagen 的基本内容，更多细节请参阅原论文。事实上，作者还在论文中还提出了 DrawBench 评测指标，这里按下不表。</p><div class="note note-secondary">            <details><summary><b>点击查看 Imagen 的生成样例（摘自<a href="https://imagen.research.google/">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/a-robot-couple-fine-dining.jpg"/></div><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/a-dog-looking-curiously.jpg"/></div><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/the-toronto-skyline-with-google-brain-logo.jpg"/></div><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/a-photo-of-a-corgi-dog-riding-a-bike-in-times-square.jpg"/></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/an-extreme-angry-bird.jpg"/></div><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/sprouts-in-the-shape-of-text-imagen.jpg"/></div><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/cactus.jpg"/></div><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/a-single-beam-of.jpg"/></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/a-dragon-fruit-wearing-karate-belt.jpg"/></div><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/a-blue-jay-standing-on-a-large-basket-of-rainbow-macarons.jpg"/></div><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/a-chrome-plated-duck-with-a-golden-beak-arguing-with-an-angry-turtle.jpg"/></div><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/a-transparent-sculpture-of-a-duck-made-out-of-glass.jpg"/></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/a-bucket-bag.jpg"/></div><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/a-brain-riding-a-rocketship.jpg"/></div><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/corn-snake-on-farm.jpg"/></div><div class="group-image-wrap"><img src="https://imagen.research.google/main_gallery_images/a-bald-eagle-made-of-chocolate-powder.jpg"/></div></div></div></details>          </div><h2 id="stable-diffusion">Stable Diffusion</h2><p><span class="label label-primary">StabilityAI</span> <span class="label label-primary">Runway</span></p><p>Stable Diffusion 是 StabilityAI 和 Runway 在 Latent Diffusion<sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 10684-10695. 2022.">[9]</span></a></sup> 的基础上加大规模发展而来。受益于其开源性质，是目前最出圈、最火热的文生图模型，以迭代了若干版本。相关内容可见 <a href="/blog-main/2023/06/16/Stable-Diffusion-%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%A7%88/" title="[Stable Diffusion]模型概览">Stable Diffusion 模型概览</a>一文。</p><h2 id="ediff-i">eDiff-I</h2><p><span class="label label-primary">NVIDIA</span> <span class="label label-default">2022.11.02</span></p><p>在 OpenAI 和 Google 打得有来有回之际，NVIDIA 终于也参与了进来，推出了 eDiff-I<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="Balaji, Yogesh, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. *arXiv preprint arXiv:2211.01324* (2022).">[7]</span></a></sup>. 通过分析文生图模型的去噪过程，作者发现：在去噪前期，模型非常依赖于文本条件来生成符合描述的图像；而在去噪后期，模型会几乎忽略文本，关注于生成高质量图像。因此，现有的方法在不同去噪阶段都使用同一个 UNet 模型也许并不好（尽管 time embedding 指示了去噪时间步）。于是，eDiff-I 对不同去噪阶段采用了多个专家去噪模型。为了训练效率的考虑，作者先只训练一个模型，然后逐渐将其分解为各个阶段的专家模型。值得注意的是，尽管模型多了，但推理时间还是不变的。另外，作者还研究了不同条件的作用，包括 T5 text embedding、CLIP text embedding 和 CLIP image embedding. 其中，CLIP image embedding 可以用来迁移参考图像的风格。最后，作者还展示了 “paint-with-words” 功能，即在画布上标注区域和文字，那么模型能在指定区域上依照对应文字作图。</p><p><img src="ediffi.png" width=100% /></p><p>如图所示，eDiff-I 由一个基础模型和两个超分模型构成，这一点与 Imagen 完全一致。每个分辨率下的模型都由多个专家模型组成。</p><p>为了训练专家模型，作者的主要思路是采用二叉树的方式：首先训练一个适用于全部噪声强度分布 <span class="math inline">\(p(\sigma)\)</span> 的基础模型，然后将噪声强度平均分成两份 <span class="math inline">\(p_0^1(\sigma),p_1^1(\sigma)\)</span>，均以基础模型初始化并分别在各自的噪声分布上微调；反复这个过程，最终 <span class="math inline">\(p(\sigma)\)</span> 被分成了 <span class="math inline">\(2^l\)</span> 个区间 <span class="math inline">\(\{p_i^l(\sigma)\}_{i=0}^{2^l-1}\)</span>，每个区间上有一个专家模型 <span class="math inline">\(E_i\)</span>. 然而，这样要训练的模型成指数级增加，并不现实，所以作者只重点考虑 <span class="math inline">\(E_0^l\)</span> 和 <span class="math inline">\(E_{2^l-1}^l\)</span> 这两端的模型及它们的父节点，最后再用一个模型补齐中间的噪声区间。</p><p>由于文本并不好描述物体的位置，作者提出了 “paint-with-words” 技术，通过指定区域和对应文本来控制位置。这个方法并不需要训练，主要思路是修改 attention map，这其实与很多图像编辑工作（如 Prompt-to-Prompt<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hertz, Amir, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. *arXiv preprint arXiv:2208.01626* (2022).">[8]</span></a></sup>）的做法相同，具体如下图所示：</p><p><img src="ediffi-paint-with-words.png" width=50% /></p><div class="note note-secondary">            <details><summary><b>点击查看 eDiff-I 的生成样例（摘自<a href="https://research.nvidia.com/labs/dir/eDiff-I/">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://research.nvidia.com/labs/dir/eDiff-I/images/teaser/portal.jpg" /></div><div class="group-image-wrap"><img src="https://research.nvidia.com/labs/dir/eDiff-I/images/teaser/cat_witch.jpg" /></div><div class="group-image-wrap"><img src="https://research.nvidia.com/labs/dir/eDiff-I/images/teaser/landscape.jpg" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://research.nvidia.com/labs/dir/eDiff-I/images/comparison_multiple_entitles/ediffi/ours_teapots.jpg" /></div><div class="group-image-wrap"><img src="https://research.nvidia.com/labs/dir/eDiff-I/images/comparison_multiple_entitles/ediffi/ours_grizzly.jpg" /></div><div class="group-image-wrap"><img src="https://research.nvidia.com/labs/dir/eDiff-I/images/comparison_text/ediffi/ours_nvidia_rocks.jpg" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://research.nvidia.com/labs/dir/eDiff-I/images/comparison_text/ediffi/ours_two_monkeys.jpg" /></div><div class="group-image-wrap"><img src="https://research.nvidia.com/labs/dir/eDiff-I/images/comparison_long/ediffi/ours_restaurant.jpg" /></div><div class="group-image-wrap"><img src="https://research.nvidia.com/labs/dir/eDiff-I/images/comparison_long/ediffi/ours_cat_scooter.jpg" /></div></div></div></details>          </div><h2 id="deepfloyd-if">DeepFloyd IF</h2><p><span class="label label-primary">DeepFloyd</span> <span class="label label-primary">StabilityAI</span> <span class="label label-default">2023.04.27</span></p><p>DeepFloyd IF 是 <a href="https://deepfloyd.ai/">DeepFloyd Lab</a> 和 <a href="https://stability.ai/">StabilityAI</a> 开源的文生图大模型，整体沿用 Imagen 的技术路线，<strong>可以看作是 Imagen 的开源版本</strong>。因此本身没有什么值得多说的。</p><p><img src="https://raw.githubusercontent.com/deep-floyd/IF/develop/pics/deepfloyd_if_scheme.jpg" width=90% /></p><p>可以看到确实与 Imagen 是差不多的，不过 DeepFloyd IF 在每个阶段都有不同大小的模型可以选择。</p><p><img src="https://images.squarespace-cdn.com/content/v1/63a454eb31a56e23b7c98380/ababd1c5-1299-4a9e-b3a2-3229bf158170/fid_table.png?format=1500w" width=80% /></p><p>官方声称要发布论文，然而目前看来恐怕希望不大。在官网给出的定量指标中 IF-4.3B 表现非常优异，甚至超过了 Imagen.</p><div class="note note-secondary">            <details><summary><b>点击查看 DeepFloyd IF 的生成样例（摘自<a href="https://github.com/deep-floyd/IF">官方 repo</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://raw.githubusercontent.com/deep-floyd/IF/develop/pics/nabla.jpg" /></div></div></div></details>          </div><h2 id="raphael">RAPHAEL</h2><p><span class="label label-primary">HKU</span> <span class="label label-primary">SenseTime</span> <span class="label label-default">2023.05.29</span></p><ol type="1"><li>核心创新点：<ol type="1"><li>引入 MoEs 层：利用 Space-MoE 和 Time-MoE 实现上亿种从网络输入到输出的传播路径。其中，每个 MoE 层每次选择一个 expert，相当于专精于某种概念或某个时间步的绘制的小模块。</li><li>Edge-supervised Learning：以 attention map 为输入，使用一个卷积网络预测图像边缘，能有效提高生成质量。</li></ol></li><li>Space-MoE：对每个 text token，利用 cross-attention map 分割出对应区域的特征图，然后从 6 个 space experts 中自动选择一个来处理，所有处理后的特征图取平均。</li><li>Time-MoE：对当前时间步，从 4 个 time experts 中自动选择一个来处理。</li></ol><p><img src="raphael.png" width=80% /></p><h2 id="dalle-3">DALL·E 3</h2><p><span class="label label-primary">OpenAI</span> <span class="label label-default">2023.09.21</span></p><p>DALL·E 3 建立在 DALL·E 2 的基础之上，结合了 GPT-4 超强的文本理解能力，将作为新的图像生成组件加入 ChatGPT. 受益于 GPT-4 强大的自然语言理解能力，DALL·E 3 生成的图像显著比其他模型更加符合用户的描述，并且能够借助 ChatGPT 实现交互式生成——只需要用聊天一样的自然语言，再也不用费劲做 prompt engineering 了！</p><p>遗憾的是，OpenAI 并没有详细描述技术细节，只在 <a href="https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf">System Card</a> 中粗略地带了一句实现原理：用 GPT-4 将用户的自然语言描述转化为 DALL·E 3 的 prompt 输入。当用户的输入相对模糊时，GPT-4 会自动补全细节。</p><p>OpenAI 团队非常注重不当信息的处理，包括涩涩、偏见（种族、性别、文化）、带有误导性的拟真图片、公众人物、生化武器、版权商标、艺术家风格等。OpenAI 在 System Card 中着重阐述了对这些不当信息的发现与处理。</p><p>总的来说，DALL·E 3 大幅提高了文生图模型遵循文本描述的能力，是一个非常大的突破。不过，从官方或民间展示的图像来看，其生成质量本身并没有达到业界最顶尖（Midjourney）的水准。</p><div class="note note-secondary">            <details><summary><b>点击查看 DALL·E 3 的生成样例（摘自<a href="https://openai.com/dall-e-3">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://images.openai.com/blob/bc8a8a6b-36f2-4774-bc95-c563cb32dcdd/heart.png?trim=0,0,0,0&width=3200" /></div><div class="group-image-wrap"><img src="https://images.openai.com/blob/bc8a8a6b-36f2-4774-bc95-c563cb32dcdd/coffee.png?trim=0,0,0,0&width=3200" /></div><div class="group-image-wrap"><img src="https://images.openai.com/blob/bc8a8a6b-36f2-4774-bc95-c563cb32dcdd/monster.png?trim=0,0,0,0&width=3200" /></div><div class="group-image-wrap"><img src="https://images.openai.com/blob/bc8a8a6b-36f2-4774-bc95-c563cb32dcdd/crab.png?trim=0,0,0,0&width=3200" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://images.openai.com/blob/0303dc78-1b1c-4bbe-a24f-cb5f0ac95565/avocado-square.png?trim=0,0,0,0&width=3200" /></div><div class="group-image-wrap"><img src="https://images.openai.com/blob/b196df3a-6fea-4d86-87b2-f9bb50be64c7/leaf.png?trim=0,0,0,0&width=3200" /></div><div class="group-image-wrap"><img src="https://images.openai.com/blob/0303dc78-1b1c-4bbe-a24f-cb5f0ac95565/blackbackdrop-square.png?trim=0,0,0,0&width=3200" /></div><div class="group-image-wrap"><img src="https://images.openai.com/blob/b196df3a-6fea-4d86-87b2-f9bb50be64c7/lychee.png?trim=0,0,0,0&width=3200" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://images.openai.com/blob/bc8a8a6b-36f2-4774-bc95-c563cb32dcdd/captain.png?trim=0,0,0,0&width=3200" /></div><div class="group-image-wrap"><img src="https://images.openai.com/blob/fc03c354-2b1f-4d35-b2a3-9516f8f42a9f/IMG_9605.png?trim=0,0,0,0&width=3200" /></div><div class="group-image-wrap"><img src="https://images.openai.com/blob/bc8a8a6b-36f2-4774-bc95-c563cb32dcdd/hedgehog.png?trim=0,0,0,0&width=3200" /></div><div class="group-image-wrap"><img src="https://images.openai.com/blob/bc8a8a6b-36f2-4774-bc95-c563cb32dcdd/piano.png?trim=0,0,0,0&width=3200" /></div></div></div></details>          </div><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Nichol, Alex, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. <em>arXiv preprint arXiv:2112.10741</em> (2021). <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Dhariwal, Prafulla, and Alexander Nichol. Diffusion models beat gans on image synthesis. <em>Advances in Neural Information Processing Systems</em> 34 (2021): 8780-8794. <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Ramesh, Aditya, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. <em>arXiv preprint arXiv:2204.06125</em> (2022). <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Saharia, Chitwan, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (2022). <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Ho, Jonathan, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded Diffusion Models for High Fidelity Image Generation. <em>J. Mach. Learn. Res.</em> 23 (2022): 47-1. <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Saharia, Chitwan, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour et al. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. <em>arXiv preprint arXiv:2205.11487</em> (2022). <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Balaji, Yogesh, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. <em>arXiv preprint arXiv:2211.01324</em> (2022). <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Hertz, Amir, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. <em>arXiv preprint arXiv:2208.01626</em> (2022). <a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp. 10684-10695. 2022. <a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>Diffusion Models</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>扩散模型应用·图生图与图像恢复</title>
    <link href="/blog-main/2023/01/05/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%C2%B7%E5%9B%BE%E7%94%9F%E5%9B%BE%E4%B8%8E%E5%9B%BE%E5%83%8F%E6%81%A2%E5%A4%8D/"/>
    <url>/blog-main/2023/01/05/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%C2%B7%E5%9B%BE%E7%94%9F%E5%9B%BE%E4%B8%8E%E5%9B%BE%E5%83%8F%E6%81%A2%E5%A4%8D/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\x}{\mathbf x}\]</span></p><blockquote><p>封面来自 <a href="https://civitai.com/images/1490468?modelVersionId=114367&amp;prioritizedUserIds=26957&amp;period=AllTime&amp;sort=Most+Reactions&amp;limit=20">CivitAI</a>.</p></blockquote><p>图生图可以泛指基于输入图像生成新图像的过程，因此诸如 image restoration（超分、去噪、填充、上色等）、image-to-image translation、style transfer 等任务都可以归为其中。特别地，本文不包括基于文本的图像编辑方法，相关内容可在<a href="/blog-main/2023/01/05/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%C2%B7%E5%9F%BA%E4%BA%8E%E6%96%87%E6%9C%AC%E7%9A%84%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/" title="扩散模型应用·基于文本的图像编辑">扩散模型应用·基于文本的图像编辑</a>中查看。</p><h2 id="sr3">SR3</h2><p><span class="label label-primary">Google</span> <span class="label label-success">TPAMI 2022</span> <span class="label label-default">2021.04.15</span></p><p>SR3<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Saharia, Chitwan, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. *IEEE Transactions on Pattern Analysis and Machine Intelligence* (2022).">[1]</span></a></sup> 将 DDPM 应用在了图像超分辨率中，取得了 SOTA 的效果。超分辨率是一种以低分辨率图像为条件的生成任务，把低分辨率图像融入模型的方式很简单——将其 bicubic 插值后 concatenate 到输入即可（作者也尝试了更复杂的，如 FiLM，但发现效果无明显差别）。另外，作者发现在 concatenate 之前做一些高斯模糊扰动能够有效提升 FID 2 个点。</p><p>与 DDPM 相比，SR3 做了一些有趣的改变。训练时，对于时间步 <span class="math inline">\(t\)</span>，DDPM 用的是 <span class="math inline">\(\bar\alpha_t\)</span> 来计算相关变量和损失函数，但是 SR3 并没有直接使用 <span class="math inline">\(\bar\alpha_t\)</span>，而是从分布 <span class="math inline">\(\bar\alpha\sim p(\bar\alpha)=U(\bar\alpha_{t-1},\bar\alpha_t)\)</span> 之中随机取值；另外，模型直接接受 <span class="math inline">\(\bar\alpha\)</span> 作为输入而非时间步 <span class="math inline">\(t\)</span>. 这些改变允许我们在 inference 时在一定范围内灵活地调整 noise scale 和采样步数，从而提高效率。</p><p>最后，论文中零散地提及了三个 future research，可供参考：</p><ul><li>给前向过程加入条件</li><li>通过 <span class="math inline">\(p(\bar\alpha)\)</span> 来 scale 损失函数</li><li>引入类似于 BigGAN 的 quality-diversity trade-off，当然我们现在知道 classifier(-free) guidance 提供了一种解决方案。</li></ul><div class="note note-secondary">            <details><summary><b>点击查看 SR3 的生成样例（摘自论文）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="sr3-ex1.png" /></div><div class="group-image-wrap"><img src="sr3-ex2.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="sr3-ex3.png" /></div><div class="group-image-wrap"><img src="sr3-ex4.png" /></div></div></div></details>          </div><h2 id="sdedit">SDEdit</h2><p><span class="label label-primary">Stanford</span> <span class="label label-primary">CMU</span> <span class="label label-success">ICLR 2022</span> <span class="label label-default">2021.08.02</span></p><p>SDEdit<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Meng, Chenlin, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In *International Conference on Learning Representations*. 2021.">[2]</span></a></sup> 的思想一张图就能看明白：</p><p><img src="https://sde-image-editing.github.io/images/sde_stroke_generation.jpg" width=80% /></p><p>真实图像和我们画的图像分属两个分布，我们通过前向加噪过程让两个分布的支撑集越来越大，直到产生交集就停下。从交集中的一点出发，用在真实图像上训练好的扩散模型逆向去噪，就回到了真实分布。</p><p>从图像处理的角度，我们知道加噪过程首先破坏的是高频信息，然后才破坏低频信息。所以当我们加噪到一定程度时，就可以去掉不想要的细节纹理，但仍保留大体结构，于是生成出来的图像就既能遵循输入的引导，又显得真实。</p><p>显然，这里一个关键问题就是在什么时候停止加噪。如果停的太早，那么结果不够真实；如果停的太晚，那么可能丢掉了我们想要的引导。论文里称之为 <strong>realism-faithfulness trade-off</strong>.</p><p>遗憾的是，这个问题没有一个通用的解答。它跟用户究竟想要更真实还是更还原有关，还跟引导图像本身的质量有关——如果引导图像是一片白色，那怎么着也没法很还原。一般而言，作者说在 <span class="math inline">\([0.3,0.6]\)</span> 区间内应该都还行（设时间是从 <span class="math inline">\(0\)</span> 到 <span class="math inline">\(1\)</span>）。</p><p><img src="sdedit-tradeoff.jpeg" width=80% /></p><div class="note note-secondary">            <details><summary><b>点击查看 SDEdit 的生成样例（摘自<a href="https://sde-image-editing.github.io/">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://sde-image-editing.github.io/images/teaser.jpg" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://sde-image-editing.github.io/images/stroke_based_generation.jpg" /></div></div></div></details>          </div><h2 id="ilvr">ILVR</h2><p><span class="label label-success">ICCV 2021 (oral)</span> <span class="label label-default">2021.08.06</span></p><p>给定一张参考图像 <span class="math inline">\(\mathbf y\)</span>，ILVR<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Choi, Jooyoung, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models. In *2021 IEEE/CVF International Conference on Computer Vision (ICCV)*, pp. 14347-14356. IEEE, 2021.">[3]</span></a></sup> 通过精调 DDPM 逆向过程的每一步，让生成的图像 <span class="math inline">\(\x_0\)</span> 接近参考图像。所谓接近，即作者希望 DDPM 能够满足： <span class="math display">\[\phi_N(\x_t)=\phi_N(\mathbf y_t)\]</span> 其中 <span class="math inline">\(\phi_N\)</span> 是一个低通滤波器，将输入图像下采样 <span class="math inline">\(N\)</span> 倍后上采样回来（我们知道下采样会丢失图像的细节，即高频信息，所以操作后的图像只剩下了低频信息，是谓低通滤波）；<span class="math inline">\(\mathbf y_t\)</span> 是参考图像的加噪版本。上式的意义很明显：希望生成图像的低频信息与参考图像的低频信息相同。为了做到这一点，作者设计了如下逆向过程算法：</p><p><img src="ilvr-alg.png" width=50% /></p><p>可以看到，在 DDPM 给出的 <span class="math inline">\(\x&#39;_{t-1}\)</span> 的基础上，作者加上了一个偏移量 <span class="math inline">\(\phi_N(\mathbf y_{t-1})-\phi_N(\x&#39;_{t-1})\)</span>. 假若上/下采样是最近邻插值，那么 <span class="math inline">\(\phi_N(\phi_N(\x))=\phi_N(\x)\)</span>，则： <span class="math display">\[\begin{align}\phi_N(\x_{t-1})&amp;=\phi_N\left(\phi_N(\mathbf y_{t-1})+\x&#39;_{t-1}-\phi_N(\x&#39;_{t-1})\right)\\&amp;=\phi_N\left(\phi_N(\mathbf y_{t-1})\right)+\phi_N\left(\x&#39;_{t-1}\right)-\phi_N\left(\phi_N(\x&#39;_{t-1})\right)\\&amp;=\phi_N(\mathbf y_{t-1})+\phi_N(\x&#39;_{t-1})-\phi_N(\x&#39;_{t-1})\\&amp;=\phi_N(\mathbf y_{t-1})\end{align}\]</span> 即满足了条件。作者尝试了不同的上/下采样方法，发现效果其实差不多。</p><p>下采样率 <span class="math inline">\(N\)</span> 是一个超参数，<span class="math inline">\(N\)</span> 越小，保留细节越多，生成图像就越接近参考图像。作者还指出，我们不一定在整个逆向过程都用到参考图像，可以只选择某些步用。选的步数越多，留给随机采样的发挥空间就越少，生成图像也越接近参考图像。</p><div class="note note-info">            <p><strong>ILVR 与 classifier guidance 的联系</strong></p><p>是否有人觉得，ILVR 似曾相识？没错，ILVR 和 classifier guidance 的本质是一样的！</p><ul><li>ILVR：对采样的结果偏移了 <span class="math inline">\(\phi_N(\mathbf{y}_{t-1})-\phi_N(\x&#39;_{t-1})\)</span>；</li><li>Classifier guidance：对采样分布的均值偏移了 <span class="math inline">\(s\Sigma\nabla_\x \log p(y\vert \x)|_{\x=\mu}\)</span>.</li></ul><p>一方面，偏移结果（先采样再偏移）和偏移均值（先偏移再采样）显然是等价的；另一方面，<span class="math inline">\(\phi_N(\mathbf{y}_{t-1})-\phi_N(\x&#39;_{t-1})\)</span> 大致是 <span class="math inline">\(\x&#39;_{t-1}\)</span> 到 <span class="math inline">\(\mathbf{y}_{t-1}\)</span> 方向的向量，朝该方向走能够提升 <span class="math inline">\(p(\mathbf{y}_{t-1}\vert \x_{t-1})\)</span>，这与 <span class="math inline">\(\nabla_\x \log p(\mathbf{y}_{t-1}\vert \x_{t-1})\)</span> 的意义相吻合。</p><p>更有甚者，二者都有一个控制引导强度的超参数：下采样率 <span class="math inline">\(N\)</span> 和 guidance scale <span class="math inline">\(s\)</span>. 当 <span class="math inline">\(N\)</span> 增大时，满足 <span class="math inline">\(\phi_N(\x_t)=\phi_N(\mathbf{y}_t)\)</span> 的 <span class="math inline">\(\mathbf{y}_t\)</span> 更多，相当于 <span class="math inline">\(p(\mathbf{y}_{t-1}\vert \x_{t-1})\)</span> 分布变得更平缓，引导作用更弱；而 <span class="math inline">\(s\)</span> 减小时，<span class="math inline">\(p(y\vert\x)^s\)</span> 也是变得更平缓，引导作用也是更弱。完美！</p>          </div><div class="note note-success">            <p>事实上，后文的许多工作都可以与 classifier guidance 建立起类似的联系。这类方法的特点是无需训练，只需要加载预训练模型，对逆向采样过程施加引导即可，对算力要求十分友好。</p>          </div><p>ILVR 可以有许多有趣的应用，比如给一个在狗上面训练的模型以猫作为参考图像，并合理控制 <span class="math inline">\(N\)</span>，那么模型就会生成像这只猫的狗；又比如我们对图像做一些粗糙的涂改后当作参考图像，合理控制 <span class="math inline">\(N\)</span> 和步数范围，就能让模型既保留原有细节、又和谐化涂改的地方。</p><div class="note note-secondary">            <details><summary><b>点击查看 ILVR 的生成样例（摘自论文）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="ilvr-ex.png" /></div></div></div></details>          </div><h2 id="palette">Palette</h2><p><span class="label label-primary">Google</span> <span class="label label-success">SIGGRAPH 2022</span> <span class="label label-default">2021.11.10</span></p><p>Palette<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Saharia, Chitwan, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In *ACM SIGGRAPH 2022 Conference Proceedings*, pp. 1-10. 2022.">[4]</span></a></sup> 是 Google 提出的基于扩散模型做 Image-to-Image Translation 的统一框架，并分别在 colorization, inpainting, uncropping (outpainting) 和 JPEG restoration 四个任务上做了实验。Palette 并没有对不同任务做特别的调参、定制网络架构或任何辅助的损失，事实上，它也没对扩散模型做什么特别的改变，仅仅是把不同任务的退化后的图像当作条件给到模型而已。可能唯一需要提一下的就是对于 inpainting 和 uncropping (outpainting)，作者没有把 binary mask 给到模型，而是把缺失区域填上 Gaussian noise，这样的设置与扩散模型更搭。</p><p>作者做了两方面的 ablation study：</p><ol type="1"><li>常用 U-Net 架构里面的 (global) self-attention 真的有用。作者实验对比了 local self-attetion、无 attention 但用 dilated convolution、无 attention 但叠双倍的 resnet blocks，最后还是发现沿用 DDPM 就有的 attention 层最管用。</li><li>许多基于扩散模型的图像恢复或翻译任务都采用的是 L1 loss 而非 L2 loss（如上文的 SR3 就用的 L1），作者实验发现二者的采样质量差不多，但是 L2 loss 能给出更多样的结果。推测是 L1 loss 倾向于让模型丢掉更多的模式。</li></ol><div class="note note-secondary">            <details><summary><b>点击查看 Palette 的生成样例（摘自<a href="https://diffusion-palette.github.io/">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://diffusion-palette.github.io/colorization/17.jpg" /></div><div class="group-image-wrap"><img src="https://diffusion-palette.github.io/colorization/18.jpg" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://diffusion-palette.github.io/inpainting/18.jpg" /></div><div class="group-image-wrap"><img src="https://diffusion-palette.github.io/inpainting/19.jpg" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://diffusion-palette.github.io/uncrop_top/0.jpg" /></div><div class="group-image-wrap"><img src="https://diffusion-palette.github.io/uncrop_top/1.jpg" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://diffusion-palette.github.io/jpeg_decompression/0.jpg" /></div><div class="group-image-wrap"><img src="https://diffusion-palette.github.io/jpeg_decompression/1.jpg" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://diffusion-palette.github.io/panoramas/image6.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://diffusion-palette.github.io/panoramas/image13.png" /></div></div></div></details>          </div><h2 id="repaint">RePaint</h2><p><span class="label label-success">CVPR 2022</span> <span class="label label-default">2022.01.24</span></p><p>RePaint<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Lugmayr, Andreas, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 11461-11471. 2022.">[5]</span></a></sup> 是专注于 image inpainting 任务的模型，其基本思想与Blended Diffusion<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Avrahami, Omri, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 18208-18218. 2022.">[6]</span></a></sup> 的 mask guidance 是一样的，即对于采样过程的每一步，掩码内使用模型的预测结果，而掩码外使用真实图像的加噪结果，如下图所示：</p><p><img src="repaint.png" width=60% /></p><p>然而，上述采样模式会导致一个严重的问题——mask 内填充的内容与 mask 外已知的内容在语义上不能很好地匹配。这是因为针对已知部分的加噪过程并没有考虑 mask 内生成的部分。尽管在下一步模型会试图和谐化上一步的结果，但它本身又会带来的新的不和谐，所以这个问题会一直存在。更有甚者，我们的 noise level 是逐渐减小的，所以越往后越难更正前面各步带来的不和谐。</p><p>但是，考虑到 DDPM 的机制是生成符合数据分布的图像，它理应收敛到语义匹配的结果上，只是实际采样时每一步都引入的不和谐让收敛过程变得很慢。为此，作者提出了 <strong>resampling</strong> 技术，即不断地把生成的 <span class="math inline">\(\x_{t-1}\)</span> 重新扩散回 <span class="math inline">\(\x_t\)</span>，在马尔可夫链上“反复横跳”，以期模型能缓解不和谐之处。当然，往回走一步可能不够，作者引入了两个超参数：<span class="math inline">\(j\)</span> 表示每次往回走的步数，<span class="math inline">\(r\)</span> 表示往回走多少次。下图是 <span class="math inline">\(j=10,r=10\)</span> 下时间步的变化情况：</p><p><img src="repaint-time.png" width=70% alt="每一个尖峰高度是 j=10，每一段有 r=10 个尖峰。" /></p><p>显然，resampling 的代价就是采样时长变成了原本的 <span class="math inline">\(r\)</span> 倍。</p><p>相比传统的基于 Auto-Encoder + GAN 的模型，基于扩散模型的 RePaint 有一个显著的优势：仅一个预训练模型就能<strong>无缝适配任意形状的 mask</strong>. 这是因为 RePaint 只改动了扩散模型的采样过程，不必专门为填充任务训练。作者尝试了几种不同的 mask 设置：Wide、Narrow、SR 2x、Alt. Lines 、Expand、Half，都取得了不错的效果。</p><div class="note note-secondary">            <details><summary><b>点击查看 RePaint 的生成样例（摘自<a href="https://github.com/andreas128/RePaint">官方 repo</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://user-images.githubusercontent.com/11280511/150822917-737c00b0-b6bb-439d-a5bf-e73238d30990.png" /></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="https://user-images.githubusercontent.com/11280511/150803812-a4729ef8-6ad4-46aa-ae99-8c27fbb2ea2e.png" /></div></div></div></details>          </div><h2 id="ddrm">DDRM</h2><p><span class="label label-default">2022.01.27</span></p><h2 id="ddib">DDIB</h2><p><span class="label label-success">ICLR 2023</span> <span class="label label-default">2022.05.16</span></p><p>DDIB<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="Su, Xuan, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion implicit bridges for image-to-image translation. In *The Eleventh International Conference on Learning Representations*. 2022.">[8]</span></a></sup> 的理论写得非常 fancy，联系了 Schrödinger Bridge Problem、Optimal Transport 等理论知识，但其实做法很简单——把两个分别训练的 DDIM 用 latent space 连接起来，就可以完成 image-to-image translation 了，如下图所示：</p><p><img src="ddib.png" width=80% /></p><p>具体而言，设有两个分别在源域 A 和目标域 B 上预训练好的扩散模型，输入一张域 A 的图像，DDIB 首先在第一个扩散模型上用 ODESolver（如 DDIM）把输入图像转换为隐变量，再在第二个扩散模型上用 ODESolver（如 DDIM）把该隐变量转换到域 B，就完成了域 A 到域 B 的迁移。</p><p>尽管 DDIB 非常直观简单，相比其他 image-to-image translation 方法，它有几个优点：</p><ol type="1"><li>具有 cycle consistency：<span class="math inline">\(A_1 → l_1 → B → l_2 → A_2\)</span>，若忽略 ODESolver 的离散化误差，则 <span class="math inline">\(A_1=A_2\)</span>.</li><li>DDIB 用的两个扩散模型分别训练互不干扰，可以保证数据隐私性。</li><li>如果要在 <span class="math inline">\(n\)</span> 个域之间转换，只需要分别训练 <span class="math inline">\(n\)</span> 个模型，而如 CycleGAN 需要两两训练 <span class="math inline">\(O(n^2)\)</span> 个模型。</li></ol><p>但是，DDIB 毕竟没有显式地指导，所以转换过程也较难控制。特别地，当源域和目标域比较接近时（如狮子和老虎），DDIB 还能较好地工作；但是如果二者差距太大（如鸟和狗），那么源域图像中的主体的姿态就很难被保留下来。</p><div class="note note-secondary">            <details><summary><b>点击查看 DDIB 的生成样例（摘自<a href="https://suxuann.github.io/ddib/">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://suxuann.github.io/ddib/assets/grid.png" /></div></div></div></details>          </div><h2 id="ddnm">DDNM</h2><p><span class="label label-primary">PKU Shenzhen</span> <span class="label label-success">ICLR 2023 spotlight</span> <span class="label label-default">2022.12.01</span></p><blockquote><p>论文作者的知乎讲解：<a href="https://zhuanlan.zhihu.com/p/588663035">链接</a></p></blockquote><p>在图像恢复任务中，超分、填充、上色对应的退化模式（下采样、掩码、灰度化）都属于线性退化，可以表达为如下形式： <span class="math display">\[\mathbf y=\mathbf A\x\]</span> 其中，<span class="math inline">\(\x\in\mathbb R^{D\times 1}\)</span> 是原图像，<span class="math inline">\(\mathbf A\in\mathbb R^{d\times D}\)</span> 为退化矩阵，<span class="math inline">\(\mathbf y\in\mathbb R^{d\times 1}\)</span> 为退化图像。图像恢复就是已知 <span class="math inline">\(\mathbf y\)</span>、<span class="math inline">\(\mathbf A\)</span>，求 <span class="math inline">\(\x\)</span> 的过程（当然，在更难的盲图像恢复任务中，<span class="math inline">\(\mathbf A\)</span> 也是未知的）。</p><p>一般而言，同一个退化图像可以对应多个真实图像（称为 ill-posed problem），因此我们的目标其实是：</p><ol type="1"><li><strong>Consistency</strong>：求解 <span class="math inline">\(\hat\x\)</span>，使得 <span class="math inline">\(\mathbf y=\mathbf A\hat \x\)</span> 成立；</li><li><strong>Realness</strong>：<span class="math inline">\(\hat \x\sim q(\x)\)</span>，其中 <span class="math inline">\(q(\x)\)</span> 代表真实图像的分布。</li></ol><p>为解决这个问题，论文作者首先指出了 <strong>range-null space decomposition</strong>. 设 <span class="math inline">\(\mathbf A\)</span> 的伪逆为 <span class="math inline">\(\mathbf A^{\dagger}\in\mathbf R^{D\times d}\)</span>，即满足 <span class="math inline">\(\mathbf A\mathbf A^\dagger\mathbf A\equiv\mathbf A\)</span>，那么 <span class="math inline">\(\x\)</span> 可以分解为： <span class="math display">\[\x\equiv \mathbf A^\dagger\mathbf A\x+(\mathbf I-\mathbf A^\dagger\mathbf A)\x\]</span> 其中第一项 <span class="math inline">\(\mathbf A^\dagger\mathbf A\)</span> 是 <span class="math inline">\(\mathbf A\)</span> 的 range-space，因为 <span class="math inline">\(\mathbf A\mathbf A^\dagger\mathbf A\x=\mathbf A\x\)</span>；而第二项 <span class="math inline">\(\mathbf I-\mathbf A^\dagger\mathbf A\)</span> 是 <span class="math inline">\(\mathbf A\)</span> 的 null-space，因为 <span class="math inline">\(\mathbf A(\mathbf I-\mathbf A^\dagger\mathbf A)\x=\mathbf 0\)</span>.</p><p>将 range-null space decomposition 应用到图像恢复任务中，我们能直接构造出满足 consistency 的解： <span class="math display">\[\hat\x=\mathbf A^\dagger \mathbf y+(\mathbf I-\mathbf A^\dagger\mathbf A)\bar\x\]</span> 因为无论 <span class="math inline">\(\bar\x\)</span> 是什么，都有： <span class="math display">\[\mathbf A\hat\x=\mathbf A\mathbf A^\dagger\mathbf y+\mathbf A(\mathbf I-\mathbf A^\dagger\mathbf A)\bar\x=\mathbf y+\mathbf A\bar\x-\mathbf A\bar\x=\mathbf y\]</span> 但是 <span class="math inline">\(\bar\x\)</span> 会影响 realness. 所以我们现在希望找到 <span class="math inline">\(\bar\x\)</span>，使得 <span class="math inline">\(\hat\x\sim q(\x)\)</span>，这就是扩散模型的用武之地了。</p><p>我们知道，扩散模型在第 <span class="math inline">\(t\)</span> 步会预测一个 "clean" image <span class="math inline">\(\x_{0\vert t}=\x_\theta(\x_t,t)\)</span>，但它并不一定满足 consistency. 为此，作者做出修改： <span class="math display">\[\hat\x_{0\vert t}=\mathbf A^\dagger \mathbf y+(\mathbf I-\mathbf A^\dagger\mathbf A)\x_{0\vert t}\]</span> 那么根据上文所述，<span class="math inline">\(\hat\x_{0\vert t}\)</span> 是一定满足 consistency 的；又由于 <span class="math inline">\(\x_0\)</span> 其实就是 <span class="math inline">\(\hat\x_{0\vert1}\)</span>，所以我们能保证生成的结果满足 consistency. 接下来用修改后的结果计算 <span class="math inline">\(\x_{t-1}\)</span>： <span class="math display">\[\x_{t-1}=\frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar\alpha_t}\hat\x_{0\vert t}+\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}\x_t+\sigma_t\epsilon,\quad\epsilon\sim\mathcal N(\mathbf 0,\mathbf I)\]</span> 重复这个过程，算法和图示如下：</p><p><img src="ddnm.png" width=100% /></p><p>相比 DDPM，这里扩散模型只参与了 null-space 一项的计算，range-space 一项是我们直接计算的，所以作者将这种方法称为 <strong>Denoising Diffusion Null-Space Model (DDNM)</strong><sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><span class="hint--top hint--rounded" aria-label="Wang, Yinhuai, Jiwen Yu, and Jian Zhang. Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model. *arXiv preprint arXiv:2212.00490* (2022).">[9]</span></a></sup>.</p><p>可以看到，DDNM 要求我们知道 <span class="math inline">\(\mathbf A\)</span> 和 <span class="math inline">\(\mathbf A^\dagger\)</span>. 那么对于常见的退化，它们的 <span class="math inline">\(\mathbf A,\mathbf A^\dagger\)</span> 可以构造为：</p><ol type="1"><li>填充：mask 就是 <span class="math inline">\(\mathbf A\)</span>。又由于 <span class="math inline">\(\mathbf A\mathbf A\mathbf A=\mathbf A\)</span>，所以 <span class="math inline">\(\mathbf A^\dagger=\mathbf A\)</span>；</li><li>上色：<span class="math inline">\(\mathbf A=[1/3,1/3,1/3]\)</span>，即将各通道数值做平均。其伪逆为 <span class="math inline">\(\mathbf A^\dagger=[1,1,1]^\mathsf T\)</span>，可以由 <span class="math inline">\(\mathbf A\mathbf A^\dagger\equiv \mathbf I\)</span> 验证；</li><li>超分：与上色类似，取 <span class="math inline">\(\mathbf A=[1/n^2,\ldots,1/n^2]\)</span>，其中 <span class="math inline">\(n\)</span> 是 patch 大小，好比 average pooling 层。其伪逆为 <span class="math inline">\(\mathbf A^\dagger=[1,\ldots,1]^\mathsf T\)</span>，好比 nearest upsample 层。</li><li>复杂退化：以老照片修复为例，我们既需要填充划痕，又需要上色，还可能需要超分。因此 <span class="math inline">\(\mathbf A=\mathbf A_1\mathbf A_2\mathbf A_3\)</span>，对应伪逆为 <span class="math inline">\(\mathbf A^\dagger=\mathbf A_3^\dagger\mathbf A_2^\dagger\mathbf A_1^\dagger\)</span>.</li></ol><p><br/></p><p>DDNM 可以解决没有噪声干扰的图像恢复任务，但是不能很好地解决带噪声的任务。为此，作者提出了 DDNM+，做了两方面的改进：</p><ol type="1"><li><p>对于带噪声的任务，作者将其建模为： <span class="math display">\[\mathbf y=\mathbf A\x+\mathbf n,\quad \mathbf n\in\mathbb R^{d\times 1}\sim\mathcal N(\mathbf 0,\sigma_{\mathbf y}^2\mathbf I)\]</span> 如果直接应用 DDNM，有： <span class="math display">\[\hat\x_{0\vert t}=\mathbf A^\dagger(\mathbf A\x+\mathbf n)+(\mathbf I-\mathbf A^\dagger\mathbf A)\x_{0\vert t}=\x_{0\vert t}-\mathbf A^\dagger(\mathbf A\x_{0\vert t}-\mathbf A\x)+{\color{blue}{\mathbf A^\dagger\mathbf n}}\]</span> 可以看见噪声 <span class="math inline">\(\mathbf n\)</span> 会被保留下来并传递给 <span class="math inline">\(\x_{t-1}\)</span>，最终影响恢复效果。为了解决这个问题，考虑到扩散模型的逆向过程本身就是一个高斯分布，那么这个噪声 <span class="math inline">\(\mathbf n\)</span> 不正好可以利用进去吗？我们只需要调小方差，让 <span class="math inline">\(\mathbf n\)</span> 帮我们“补齐”调小的这一部分。于是，作者引入了两个参数 <span class="math inline">\({\color{blue}{\Sigma_t}}\)</span> 和 <span class="math inline">\({\color{blue}{\Phi_t}}\)</span>： <span class="math display">\[\begin{align}&amp;\hat\x_{0\vert t}=\x_{0\vert t}-{\color{blue}{\Sigma_t}}\mathbf A^\dagger(\mathbf A\x_{0\vert t}-\mathbf y)\\&amp;{\color{blue}{\hat p}}(\x_{t-1}\mid\x_t,\hat\x_{0\vert t})=\mathcal N(\x_{t-1};\mu_t(\x_t,\hat\x_{0\vert t}),{\color{blue}{\Phi_t}}\mathbf I)\end{align}\]</span> 特别地，如果取 <span class="math inline">\(\Sigma_t={\color{blue}{\lambda_t}}\mathbf I\)</span>，<span class="math inline">\(\Phi_t={\color{blue}{\gamma_t}}\mathbf I\)</span>，代入上式并都展开： <span class="math display">\[\begin{align}&amp;\hat\x_{0\vert t}={\color{blue}{\lambda_t}}\mathbf A^\dagger\mathbf y+(\mathbf I-{\color{blue}{\lambda_t}}\mathbf A^\dagger\mathbf A)\x_{0\vert t}\\&amp;\x_{t-1}=\frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar\alpha_t}\hat\x_{0\vert t}+\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}\x_t+\sqrt{\color{blue}{\gamma_t}}\epsilon,\quad\epsilon\sim\mathcal N(\mathbf 0,\mathbf I)\end{align}\]</span> 首先观察第一个式子，<span class="math inline">\(\hat\x_{0\vert t}\)</span> 的随机性完全来自于第一项。考虑到在超分和上色中，<span class="math inline">\(\mathbf A^\dagger\)</span> 都是复制操作，所以 <span class="math inline">\(\mathbf A^\dagger\mathbf y\)</span> 依旧可以用 <span class="math inline">\(\mathcal N(\mathbf 0,\sigma_\mathbf y^2)\)</span> 来近似，因此 <span class="math inline">\(\text{var}(\hat \x_{0\vert t})={\color{blue}{\lambda_t}}^2\sigma_\mathbf y^2\)</span>.</p><p>再看第二个式子，<span class="math inline">\(\x_{t-1}\)</span> 的随机性来自于第一项和第三项，为了方便叙述，记 <span class="math inline">\(\hat \x_{0\vert t}\)</span> 前面那一坨系数为 <span class="math inline">\(a_t=\frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar\alpha_t}\)</span>，那么第一项的方差就是 <span class="math inline">\(a_t^2\cdot\text{var}(\hat\x_{0\vert t})=a_t^2{\color{blue}{\lambda_t}}^2\sigma_\mathbf y^2\)</span>；第三项方差显然是 <span class="math inline">\({\color{blue}\gamma_t}\)</span>. 二者独立因此方差可加，所以 <span class="math inline">\(\text{var}(\x_{t-1})=(a_t{\color{blue}\lambda_t}\sigma_\mathbf y)^2+{\color{blue}\gamma_t}\)</span>.</p><p>前文说过，我们引入两个参数是希望 <span class="math inline">\(\mathbf n\)</span> 补齐故意调小的方差，所以应该让 <span class="math inline">\(\text{var}(\x_{t-1})\)</span> 与 <span class="math inline">\(q(\x_{t-1}\vert \x_t)\)</span> 的方差相等，即 <span class="math inline">\((a_t{\color{blue}{\lambda_t}}\sigma_{\mathbf y})^2+{\color{blue}{\gamma_t}}=\sigma_t^2\)</span>；另一方面，<span class="math inline">\(\Sigma_t\)</span> 其实是有害于 consistency 的，所以我们应该尽可能让它接近 <span class="math inline">\(\mathbf I\)</span>. 综上，作者最终选取： <span class="math display">\[{\color{blue}{\gamma_t}}=\sigma_t^2-(a_t{\color{blue}\lambda_t}\sigma_{\mathbf y})^2,\quad\quad {\color{blue}{\lambda_t}}=\begin{cases}1,&amp;&amp;\sigma_t\geq a_t\sigma_{\mathbf y}\\\sigma_t/a_t \sigma_{\mathbf y},&amp;&amp;\sigma_t&lt;a_t\sigma_{\mathbf y}\end{cases}\]</span></p></li><li><p>Time-Travel Trick. 作者发现，在大规模 average pooling 的超分、大 mask 填充、低采样率的压缩感知场景下，DDNM 的效果并不好。这是因为 <span class="math inline">\(\mathbf A^\dagger \mathbf y\)</span> 过于的 local，不能给逆向过程以有效的引导。作者给出的解决方案与 RePaint 的来回采样类似，即在 <span class="math inline">\(t\)</span> 时刻往回采样 <span class="math inline">\(\x_{t+l}\sim q(\x_{t+l}\vert\x_t)\)</span>，然后重新执行 DDNM 直到得到 <span class="math inline">\(\x_{t-1}\)</span>.</p></li></ol><p>DDNM+ 的算法如下所示：</p><p><img src="ddnm+.png" width=50% /></p><div class="note note-secondary">            <details><summary><b>点击查看 DDNM 的生成样例（摘自<a href="https://wyhuai.github.io/ddnm.io/">官网</a>）</b></summary><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="https://wyhuai.github.io/ddnm.io/images/front.png" /></div></div></div></details>          </div><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Saharia, Chitwan, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (2022). <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Meng, Chenlin, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In <em>International Conference on Learning Representations</em>. 2021. <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Choi, Jooyoung, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models. In <em>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pp. 14347-14356. IEEE, 2021. <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Saharia, Chitwan, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In <em>ACM SIGGRAPH 2022 Conference Proceedings</em>, pp. 1-10. 2022. <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Lugmayr, Andreas, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 11461-11471. 2022. <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Avrahami, Omri, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 18208-18218. 2022. <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>DDRM <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Su, Xuan, Jiaming Song, Chenlin Meng, and Stefano Ermon. Dual diffusion implicit bridges for image-to-image translation. In <em>The Eleventh International Conference on Learning Representations</em>. 2022. <a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>Wang, Yinhuai, Jiwen Yu, and Jian Zhang. Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model. <em>arXiv preprint arXiv:2212.00490</em> (2022). <a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>Diffusion Models</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>扩散模型条件引导生成</title>
    <link href="/blog-main/2022/12/29/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%9D%A1%E4%BB%B6%E5%BC%95%E5%AF%BC%E7%94%9F%E6%88%90/"/>
    <url>/blog-main/2022/12/29/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%9D%A1%E4%BB%B6%E5%BC%95%E5%AF%BC%E7%94%9F%E6%88%90/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\x}{\mathbf x}\newcommand{\calN}{\mathcal N}\newcommand{\coloneqq}{\mathrel{\mathrel{\vcenter{:}}=}}\]</span></p><h2 id="preface">Preface</h2><p>我们在之前的文章中关注的都是无条件生成，生成结果不受我们控制，特别是以 DDPM 为代表的采样过程本身就带有随机性的模型，即使用同样的初始变量也会得到完全不同的结果。但是，有条件的生成（受控生成）却又非常重要：以类别标签为条件可以让我们控制生成的类别；图像恢复、图像填充、图像编辑等任务可以视为以已知图像为条件的生成任务；另外，以文本为条件来跨模态地指导图像生成更是给予了我们无限的发挥创造力的空间。本文就梳理一下为扩散模型加入条件的一些方法。</p><h2 id="class-embeddings">Class Embeddings</h2><p>最简单的加入类别条件的方法就是给模型输入 class embeddings. 在论文<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Dhariwal, Prafulla, and Alexander Nichol. Diffusion models beat gans on image synthesis. *Advances in Neural Information Processing Systems* 34 (2021): 8780-8794.">[1]</span></a></sup>中，作者将 class embeddings 连同 time embeddings 一起给到了 AdaGN 层： <span class="math display">\[\text{AdaGN}(h,y)=y_s\ \text{GroupNorm}(h)+y_b\]</span> 其中 <span class="math inline">\(h\)</span> 是各个 residual block 第一个卷积之后的激活输出，<span class="math inline">\(y=[y_s,y_b]\)</span> 由 timestep 和 class embeddings 经过一个全连接层得到。相比 DDPM 把 time embeddings 直接加到激活上再做 GroupNorm，作者发现使用 AdaGN 能得到更好的效果。</p><h2 id="classifier-guidance">Classifier Guidance</h2><p>顾名思义，classifier guidance 引入额外的分类器 <span class="math inline">\(p(y\vert \x_t)\)</span> 来实现条件生成，其思想在论文<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, Yang, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. In *International Conference on Learning Representations*. 2020.">[2]</span></a></sup>中其实已经提出了，但在论文<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Dhariwal, Prafulla, and Alexander Nichol. Diffusion models beat gans on image synthesis. *Advances in Neural Information Processing Systems* 34 (2021): 8780-8794.">[1]</span></a></sup>中得到了进一步的研究。为了引入分类器，作者实际上做了两种推导，分别适用于 DDPM 和 DDIM（虽然我觉得后一种也可以用于 DDPM）。</p><ul><li><p><strong>Conditional Reverse Noising Process</strong></p><p>对于带条件的扩散模型，其逆向过程可以写作： <span class="math display">\[\begin{align}&amp;p(\x_t\vert\x_{t+1},y)=Z p(\x_t\vert\x_{t+1})p(y\vert \x_t) \tag{1}\label{conditional-markov}\\&amp;\log{p}(\x_t\vert\x_{t+1},y)=\log{p}(\x_t\vert\x_{t+1})+\log{p}(y\vert \x_t)\\\end{align}\]</span> 其中 <span class="math inline">\(Z\)</span> 是与 <span class="math inline">\(\x_t\)</span> 无关的常数项。这个式子看着简单，但其实并不是那么显然，证明过程见本节末尾。</p><p>上式右边包含两项，其中 <span class="math inline">\(p(\x_t\vert\x_{t+1})\)</span> 与无条件的逆向过程相同： <span class="math display">\[\begin{align}&amp;p(\x_t\vert\x_{t+1})=\calN(\x_t;\mu,\Sigma)\\&amp;\log{p}(\x_t\vert\x_{t+1})=-\frac{1}{2}(\x_t-\mu)^T\Sigma^{-1}(\x_t-\mu)+C\end{align}\]</span> 而 <span class="math inline">\(\log{p}(y\vert\x_t)\)</span> 一项可以在 <span class="math inline">\(\mu\)</span> 处进行一阶泰勒展开： <span class="math display">\[\begin{align}\log{p}(y\vert\x_t)&amp;\approx\log{p}(y\vert\x)\big|_{\x=\mu}+(\x_t-\mu)\nabla_{\x}\log{p}(y\vert\x)\big|_{\x=\mu}\\&amp;=(\x_t-\mu)g+C_1\end{align}\]</span> 其中 <span class="math inline">\(g=\nabla_{\x}\log{p}(y\vert \x)\big|_{\x=\mu}\)</span>. 于是， <span class="math display">\[\begin{align}\log{p}(\x_t\vert\x_{t+1},y)&amp;\approx-\frac{1}{2}(\x_t-\mu)^T\Sigma^{-1}(\x_t-\mu)+(\x_t-\mu)g+C_2\\&amp;=-\frac{1}{2}(\x_t-\mu-\Sigma g)^{-1}\Sigma^{-1}(\x_t-\mu-\Sigma g)+C_3\\\end{align}\]</span> 即： <span class="math display">\[p(\x_t\vert \x_{t+1},y)\sim\calN(\x_t;\mu+\Sigma g,\Sigma)\]</span> 这意味着有条件的逆向过程与无条件的唯一差别在于：<strong>前者的均值在后者的基础上偏移了一个 <span class="math inline">\(\Sigma g\)</span></strong>. 直观上，由于偏移的方向沿着分类器提供的梯度 <span class="math inline">\(g\)</span>，而这个梯度恰恰反映了应该如何移动才能让指定类别概率最大，所以偏移后的分布更能体现给定的类别条件，这就是分类器的引导作用。算法总结如下（其中 gradient scale 在后文解释）：</p><p><img src="classifier-alg1.png" width=80% alt="source: Diffusion models beat gans on image synthesis" /></p><div class="note note-danger">            <p>注意！根据我们的推导，我们应该在 <span class="math inline">\(\mu\)</span> 处取梯度，但是算法截图和<a href="https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/gaussian_diffusion.py#L365">官方代码</a>却是在 <span class="math inline">\(\x_t\)</span> 处求的梯度。为验证这确实是一个 bug，我 google 了许久，最后看到有个老哥提了相关的 <a href="https://github.com/openai/guided-diffusion/issues/51">issue</a>，并得到了原作者的肯定答复：</p><blockquote><p>Yes, this is indeed a slight bug, which we noticed shortly after releasing our work. However, we did try ablating using the correct formula, and found that it didn't noticeably change results.</p></blockquote><p>另外，这老哥还发了一个 <a href="https://www.youtube.com/watch?v=hAp7Lk7W4QQ">YouTube video</a> 讲这篇论文，挺不错的，推荐观看。</p>          </div></li><li><p><strong>Conditional Sampling for DDIM</strong></p><p>上述推导并不适用于 DDIM 的确定性采样过程（<span class="math inline">\(\Sigma=\mathbf 0\)</span>），因此我们需要另一种推导方式。</p><p>我们知道扩散模型的本质是在拟合 score function. 在无条件的扩散模型中，score function 为 <span class="math inline">\(\nabla_{\x_t}\log{p}(\x_t)\)</span>；那么若以类别标签 <span class="math inline">\(y\)</span> 为条件，则 score function 为： <span class="math display">\[\begin{align}\nabla_{\x_t}\log{p}(\x_t\vert y)&amp;=\nabla_{\x_t} \log\left(\frac{p(\x_t)p(y\vert\x_t)}{p(y)}\right)\\&amp;=\nabla_{\x_t}\log{p}(\x_t)+\nabla_{\x_t}\log{p}(y\vert \x_t)-\nabla_{\x_t}\log{p}(y)\\&amp;=\underbrace{\nabla_{\x_t}\log{p}(\x_t)}_{\text{unconditional score}}+\underbrace{\nabla_{\x_t}\log{p}(y\vert \x_t)}_\text{classifier guidance} \tag{2}\label{classifier-guidance}\end{align}\]</span> 可以看见，<strong>有条件的 score function，等于无条件的 score function 加上分类器关于输入的梯度</strong>。由于我们知道扩散模型的采样其实就是沿着 score function 的方向走，所以 unconditional score 一项是在增大 <span class="math inline">\(p(\x_t)\)</span>，即让样本越来越真实；classifier guidance 一项是在增大 <span class="math inline">\(p(y\vert \x_t)\)</span>，即让样本更能匹配上输入的条件 <span class="math inline">\(y\)</span>，从而实现条件生成。</p><p>由于 DDPM / DDIM 中的噪声 <span class="math inline">\(\epsilon(\x_t)\)</span> 和 score function 有如下联系（详见 <a href="/blog-main/2022/10/13/Score-Based-Generative-Models/" title="Score-Based Generative Models">Score Based Generative Models</a> 一文）： <span class="math display">\[\nabla_{\x_t}\log{p}(\x_t)=-\frac{1}{\sqrt{1-\bar\alpha_t}}\epsilon(\x_t)\]</span> 所以我们只需要定义一个新的噪声： <span class="math display">\[\hat\epsilon(\x_t)=\epsilon(\x_t)-\sqrt{1-\bar\alpha_t}\nabla_{\x_t}\log{p}(y\vert\x_t)\]</span> 那么把原本的所有 <span class="math inline">\(\epsilon\)</span> 换做 <span class="math inline">\(\hat\epsilon\)</span> 就可以实现条件生成了。</p><p><img src="classifier-alg2.png" width=80% alt="source: Diffusion models beat gans on image synthesis" /></p></li></ul><div class="note note-info">            <p><strong>两种推导的区别和联系</strong></p><p>不知道是否有人有和我一样的疑问，为什么作者要做两种推导，或者说，为什么不在第一种情形中也用第二种推导的结论？</p><p>如果我们把第二种推导也应用在 DDPM 里，由于原本的无条件 DDPM 的逆向过程是： <span class="math display">\[\begin{align}&amp;p(\x_{t-1}\vert\x_t)=\calN(\x_{t};\mu(\x_t),\sigma^2_t\mathbf I)\\\text{where}\quad&amp;\mu(\x_t)=\frac{1}{\sqrt{\alpha_t}}\left(\x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon(\x_t)\right)\end{align}\]</span> 代入新的噪声，那么新的均值为： <span class="math display">\[\begin{align}\hat\mu(\x_t)&amp;=\frac{1}{\sqrt{\alpha_t}}\left(\x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\hat\epsilon(\x_t)\right)\\&amp;=\frac{1}{\sqrt{\alpha_t}}\left(\x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\left(\epsilon(\x_t)-\sqrt{1-\bar\alpha_t}\nabla_{\x_t}\log{p}(y\vert\x_t)\right)\right)\\&amp;=\frac{1}{\sqrt{\alpha_t}}\left(\x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon(\x_t)\right)+\frac{1-\alpha_t}{\sqrt{\alpha_t}}\nabla_{\x_t}\log{p}(y\vert\x_t)\\&amp;=\mu(\x_t)+\frac{1-\alpha_t}{\sqrt{\alpha_t}}\nabla_{\x_t}\log{p}(y\vert\x_t)\end{align}\]</span></p><p>对比 Algorithm 1，可见主要区别是均值的偏移量从 <span class="math inline">\(\Sigma g\)</span> 变成了 <span class="math inline">\(\frac{1-\alpha_t}{\sqrt{\alpha_t}}\nabla_{\x_t}\log{p}(y\vert\x_t)\)</span>. 注意，前者是在 <span class="math inline">\(\mu(\x_t)\)</span> 处取的梯度，而后者是在 <span class="math inline">\(\x_t\)</span> 处取的梯度。考虑到 <span class="math inline">\(\mu(\x_t)\)</span> 和 <span class="math inline">\(\x_t\)</span> 差别不大，且 <span class="math inline">\(\Sigma\)</span>（常取 <span class="math inline">\(\beta_t\)</span> 或者 <span class="math inline">\(\tilde\beta_t\)</span>）与 <span class="math inline">\(\frac{1-\alpha_t}{\sqrt{\alpha_t}}=\frac{1}{\sqrt{1-\beta_t}}\beta_t\)</span> 的差别也不大，所以二者的性能可能相近吧，有待实验验证。</p>          </div><p>在实验中，作者发现直接按上述结论做条件生成的效果并不理想，如下左图所示：</p><p><img src="classifier-guidance.png" width=80% alt="source: Diffusion models beat gans on image synthesis" /></p><p>于是作者给分类器梯度项添加了一个权重系数 <span class="math inline">\(s\)</span>，权重越大，则生成的图像越靠近指定类别，但代价是多样性下降： <span class="math display">\[\nabla_{\x_t}\log{p}_s(\x_t\vert y)=\nabla_{\x_t}\log{p}(\x_t)+s \nabla_{\x_t}\log{p}(y\vert \x_t) \tag{3}\label{classifier-s}\]</span> 对应的噪声表示变成： <span class="math display">\[\hat\epsilon_{0t}=\epsilon_{0t}-s\cdot\sqrt{1-\bar\alpha_t}\nabla_{\x_t}\log{p}(y\vert\x_t)\]</span> 怎么理解权重系数发挥的作用呢？注意到 <span class="math inline">\(s\nabla_{\x_t}\log{p}(y\vert \x_t)=\nabla_{\x_t}\log \frac{1}{Z}p(y\vert \x_t)^s\)</span>，所以我们其实是人为把分类器的输出分布 <span class="math inline">\(p(y\vert\x_t)\)</span> 重新归一化成了正比于 <span class="math inline">\(p(y\vert\x_t)^s\)</span> 的分布。当 <span class="math inline">\(s&gt;1\)</span> 时，分布变得更加陡峭，也就让生成结果更靠近指定类别，但同时损失一定的多样性。所以对 <span class="math inline">\(s\)</span> 的调整体现出了 diversity-fidelity trade-off（类似 BigGAN 中的 truncation trick），其中 diversity 可以用 FID 来衡量，而 fidelity 用 IS 来衡量。</p><p>需要说明的是，classifier guidance 与输入 class embeddings 并不矛盾，二者共同作用能达到更好的效果。后者是加入条件的最基本做法，而无论模型是否带有条件，都可以用分类器做引导。<strong>也就是说，条件（condition）和引导（guidance）是两种不同但相辅相成的方法</strong>。</p><p>最后，虽然我们是视 <span class="math inline">\(y\)</span> 为类别标签来阐释的，但该方法也可以扩展到文本指导图像等其他条件生成场景之中。该方法还有一个好处是我们可以先在无标签的数据集上训练生成模型，事后再灵活地适应到有标签的场景下，鉴于无标签数据集往往比有标签数据集大很多，这有助于我们更好地训练模型。</p><div class="note note-secondary">            <details><summary><b>附录：关于 <span class="math inline">\(\eqref{conditional-markov}\)</span> 式的推导（原论文 Appendix H）</b></summary><br/>对标<b>无条件</b>马尔可夫链，我们用 <span class="math inline">\(\hat{q}\)</span> 来表示<b>带条件</b>马尔可夫链中的概率分布，包括： <span class="math display">\[\begin{align}\hat{q}(\x_0)&amp;\coloneqq q(\x_0)\\\hat{q}(y\vert \x_0)&amp;\coloneqq\text{Known labels per sample}\\\hat{q}(\x_{t+1}\vert \x_t,y) &amp;\coloneqq q(\x_{t+1}\vert \x_t)\\\hat{q}(\x_{1:T}\vert \x_0,y) &amp;\coloneqq \prod_{t=1}^T \hat{q}(\x_t\vert \x_{t-1},y)\end{align}\]</span> 其中最关键的是第三个式子，表示条件与扩散过程无关。在上述定义下，我们发现<b>当没有 <span class="math inline">\(y\)</span> 作为条件时，<span class="math inline">\(\hat{q}\)</span> 和 <span class="math inline">\(q\)</span> 其实是完全相同的。</b>具体而言，我们有： <span class="math display">\[\begin{align}\hat{q}(\x_{t+1}\vert \x_t)&amp;=\int_y \hat{q}(\x_{t+1},y\vert \x_t)\mathrm{d}y\\&amp;=\int_y \hat{q}(\x_{t+1}\vert \x_t,y)\hat{q}(y\vert \x_t)\mathrm{d}y\\&amp;=\int_y q(\x_{t+1}\vert \x_t)\hat{q}(y\vert \x_t)\mathrm{d}y\\&amp;=q(\x_{t+1}\vert \x_t)=\hat{q}(\x_{t+1}\vert \x_t,y)\end{align}\]</span> 即每一步转移概率分布都是相同的。类似的，有： <span class="math display">\[\begin{align}\hat{q}(\x_{1:T}\vert \x_0)&amp;=\int_y \hat{q}(\x_{1:T},y\vert \x_0)\mathrm{d}y\\&amp;=\int_y \hat{q}(\x_{1:T}\vert \x_0,y)\hat{q}(y\vert\x_0)\mathrm{d}y\\&amp;=\int_y \hat{q}(y\vert\x_0)\prod_{t=1}^T\hat{q}(\x_t\vert \x_{t-1},y)\mathrm{d}y\\&amp;=\int_y \hat{q}(y\vert\x_0)\prod_{t=1}^T q(\x_t\vert \x_{t-1})\mathrm{d}y\\&amp;=\prod_{t=1}^T q(\x_t\vert \x_{t-1})\int_y \hat{q}(y\vert\x_0)\mathrm{d}y\\&amp;=q(\x_{1:T}\vert \x_0)\end{align}\]</span> 结合 <span class="math inline">\(\hat{q}(\x_0)=q(\x_0)\)</span>，我们知道联合分布也是相同的。进一步： <span class="math display">\[\begin{align}\hat{q}(\x_t)&amp;=\int_{\x_{0:t-1}}\hat{q}(\x_0,\ldots,\x_t)\mathrm{d}\x_{0:t-1}\\&amp;=\int_{\x_{0:t-1}}\hat{q}(\x_1,\ldots,\x_t\vert\x_0)\hat{q}(\x_0)\mathrm{d}\x_{0:t-1}\\&amp;=\int_{\x_{0:t-1}} q(\x_1,\ldots,\x_t\vert\x_0)q(\x_0)\mathrm{d}\x_{0:t-1}\\&amp;=\int_{\x_{0:t-1}} q(\x_0,\ldots,\x_t)\mathrm{d}\x_{0:t-1}\\&amp;=q(\x_t)\end{align}\]</span> 即边缘分布也都是相同的。有了边缘分布和转移概率，根据贝叶斯公式： <span class="math display">\[\hat{q}(\x_t\vert \x_{t+1})=\frac{\hat{q}(\x_{t+1}\vert \x_t)\hat{q}(\x_t)}{\hat{q}(\x_{t+1})}=\frac{q(\x_{t+1}\vert \x_t)q(\x_t)}{q(\x_{t+1})}=q(\x_t\vert \x_{t+1})\]</span> 我们知道逆向过程的每一步也是相同的。另外，我们还有： <span class="math display">\[\begin{align}\hat{q}(y\vert \x_t,\x_{t+1})&amp;=\frac{\hat{q}(\x_{t+1}\vert \x_t,y)\hat{q}(y\vert \x_t)}{\hat{q}(\x_{t+1}\vert\x_t)}\\&amp;=\frac{q(\x_{t+1}\vert \x_t)\hat{q}(y\vert \x_t)}{q(\x_{t+1}\vert\x_t)}\\&amp;=\hat{q}(y\vert \x_t)\end{align}\]</span> 那么，加入条件后的逆向过程为： <span class="math display">\[\begin{align}\hat{q}(\x_t\vert \x_{t+1},y)&amp;=\frac{\hat{q}(y\vert\x_t,\x_{t+1})\hat{q}(\x_t\vert \x_{t+1})}{\hat{q}(y\vert \x_{t+1})}\\&amp;=\frac{\hat{q}(y\vert\x_{t})\hat{q}(\x_t\vert \x_{t+1})}{\hat{q}(y\vert \x_{t+1})}\\&amp;=\frac{\hat{q}(y\vert\x_{t})q(\x_t\vert \x_{t+1})}{\hat{q}(y\vert \x_{t+1})}\\&amp;=Z\hat{q}(y\vert\x_{t})q(\x_t\vert \x_{t+1})\end{align}\]</span> 其中 <span class="math inline">\(Z=\hat{q}(y\vert\x_{t+1})\)</span> 是与 <span class="math inline">\(\x_t\)</span> 无关的常数项。我们已经用 <span class="math inline">\(p(\x_t\vert\x_{t+1})\)</span> 来近似了 <span class="math inline">\(q(\x_t\vert\x_{t+1})\)</span>，所以只需要再训练一个分类器 <span class="math inline">\(p(y\vert \x_t)\)</span> 来近似 <span class="math inline">\(\hat{q}(y\vert\x_t)\)</span>，就得到了 <span class="math inline">\(\eqref{conditional-markov}\)</span> 式。</details>          </div><h2 id="classifier-free-guidance">Classifier-Free Guidance</h2><p>Classifier guidance 虽然成功地为扩散模型提供了引导，并引入了 diversity-fidelity tradeoff，但是其劣势也很明显：我们必须额外训练一个分类器，而且该分类器需要对所有 noise scale 都有分类能力，所以还不能直接加载常见的预训练模型。因此，classifier-free guidance<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ho, Jonathan, and Tim Salimans. Classifier-Free Diffusion Guidance. In *NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications*. 2021.">[3]</span></a></sup> 应运而生。</p><p>首先，我们重新排列 <span class="math inline">\(\eqref{classifier-guidance}\)</span> 式得到： <span class="math display">\[\nabla_{\x_t}\log{p}(y\vert \x_t)=\nabla_{\x_t}\log{p}(\x_t\vert y)-\nabla_{\x_t}\log{p}(\x_t)\]</span> 代入 <span class="math inline">\(\eqref{classifier-s}\)</span> 式得： <span class="math display">\[\begin{align}\nabla_{\x_t}\log{p}_s(\x_t\vert y)&amp;=\nabla_{\x_t}\log{p}(\x_t)+s\left(\nabla_{\x_t}\log{p}(\x_t\vert y)-\nabla_{\x_t}\log{p}(\x_t)\right)\\&amp;=\nabla_{\x_t}\log{p}(\x_t)+s\nabla_{\x_t}\log{p}(\x_t\vert y)-s\nabla_{\x_t}\log{p}(\x_t)\\&amp;=(1-s)\underbrace{\nabla_{\x_t}\log{p}(\x_t)}_\text{unconditional score}+s\underbrace{\nabla_{\x_t}\log{p}(\x_t\vert y)}_\text{conditional score}\end{align}\]</span> 这是无条件和有条件的 score function 的线性组合，<span class="math inline">\(s\)</span> 同样是控制条件引导力度的系数。当 <span class="math inline">\(s=0\)</span> 时，就是无条件模型；当 <span class="math inline">\(s=1\)</span> 时，就是标准的有条件模型；而当 <span class="math inline">\(s&gt;1\)</span> 时，不仅 conditional score 被进一步加强，而且 unconditional score 甚至变成了负向！同 classifier guidance 一样，我们可以通过调节 <span class="math inline">\(s\)</span> 实现 diversity-fidelity trade-off，<span class="math inline">\(s\)</span> 越大，条件指向性越强，生成结果越真实，但是多样性下降。</p><blockquote><p>注：上述权重记号与原论文有所不同，原论文写作： <span class="math display">\[\nabla_{\x_t}\log{p}_w(\x_t\vert y)=(1+w)\nabla_{\x_t}\log{p}(\x_t\vert y)-w\nabla_{\x_t}\log{p}(\x_t)\]</span> 即本文和原论文的记号有如下关系：<span class="math inline">\(w=s-1\)</span>.</p></blockquote><p><img src="free-ex.png" width=60% alt="source: Classifier-Free Diffusion Guidance" /></p><p>在实现上，我们用 <span class="math inline">\(p(\x_t\vert y)\)</span> 表示有条件模型，同时用 null token <span class="math inline">\(\varnothing\)</span> 表示无条件限制，即 <span class="math inline">\(p(\x_t\vert\varnothing)\)</span> 表示无条件模型。这样就可以只训练一个模型，同时处理有条件和无条件两种情况了。训练时随机以 <span class="math inline">\(p_\text{uncond}\)</span> 的概率去掉输入条件（换成 <span class="math inline">\(\varnothing\)</span>），即相当于同时训练了有条件和无条件情形，实验发现取 <span class="math inline">\(p_\text{uncond}\in\{0.1,0.2\}\)</span> 效果最好。</p><p>由于 classifier-free guidance 的效果非常优秀且训练方便，后续许多著名的模型，包括 GLIDE、DALL-E 2 (unCLIP)、Imagen 等都应用了 classifier-free guidance. 而至于这些让扩散模型火出圈的应用，我们留到以后的文章吧。</p><h2 id="summary">Summary</h2><p>在本文中，我们说明了为模型加入条件其实包括两方面：</p><ol type="1"><li><strong>Condition</strong>：我们可以直接把条件输入给模型，使之从 unconditional model 变成 conditional model；针对条件的不同形式，把条件融入模型架构的方式也不同，比如用 AdaGN 来融合 class embedding；</li><li><strong>Guidance</strong>：我们可以用显式或隐式的分类器为生成过程加入条件引导，即 classifier guidance 或 classifier-free guidance.</li></ol><p>需要注意的是，condition 和 guidance 是两种不同的技术，我们可以只用 conditional model 而没有 guidance，也可以对 unconditional model 做显式的 guidance，它们都能达到条件生成的目的。当然，二者同时使用往往能取得更好的效果。</p><p>在之后的文章中，我们将聚焦基于扩散模型的应用。这些应用几乎都离不开条件生成，譬如，超分、去模糊、填充等图像恢复任务可以视为<strong>以退化图像为条件</strong>的生成任务，图像编辑、图像翻译等任务可以视为<strong>以源图像/参考图像为条件</strong>的生成任务，根据文本描述生成或编辑图像显然是<strong>以文本为条件</strong>的生成任务……那么，针对这些任务，<strong>许多工作就是在如何加入 condition 和如何施加 guidance 两方面下功夫</strong>。</p><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Dhariwal, Prafulla, and Alexander Nichol. Diffusion models beat gans on image synthesis. <em>Advances in Neural Information Processing Systems</em> 34 (2021): 8780-8794. <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Song, Yang, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. In <em>International Conference on Learning Representations</em>. 2020. <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Ho, Jonathan, and Tim Salimans. Classifier-Free Diffusion Guidance. In <em>NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications</em>. 2021. <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Luo, Calvin. Understanding diffusion models: A unified perspective. <em>arXiv preprint arXiv:2208.11970</em> (2022). <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>扩散模型与受控图像生成-脉络梳理 - 中森的文章 - 知乎 https://zhuanlan.zhihu.com/p/585938939 <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Sander Dieleman. Guidance: a cheat code for diffusion models. https://benanne.github.io/2022/05/26/guidance.html <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>Diffusion Models</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>不止去噪！任意退化模式上的扩散模型</title>
    <link href="/blog-main/2022/12/17/%E4%B8%8D%E6%AD%A2%E5%8E%BB%E5%99%AA%EF%BC%81%E4%BB%BB%E6%84%8F%E9%80%80%E5%8C%96%E6%A8%A1%E5%BC%8F%E4%B8%8A%E7%9A%84%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"/>
    <url>/blog-main/2022/12/17/%E4%B8%8D%E6%AD%A2%E5%8E%BB%E5%99%AA%EF%BC%81%E4%BB%BB%E6%84%8F%E9%80%80%E5%8C%96%E6%A8%A1%E5%BC%8F%E4%B8%8A%E7%9A%84%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\x}{\mathbf x}\newcommand{\E}{\mathbb E}\newcommand{\calN}{\mathcal N}\newcommand{\I}{\mathbf I}\newcommand{\coloneqq}{\mathrel{\mathrel{\vcenter{:}}=}}\]</span></p><h2 id="cold-diffusion">Cold Diffusion</h2><p>站在 machine learning researcher 的角度，DDPM 或 SMLD 的「加噪-去噪」过程有着非常严谨的数学描述，使得我们能够用 variational inference 或 score matching 等方法来解决问题，让 Diffusion Models 建立在了坚实的数学基石上。倘若站在 computer vision researcher 的角度，特别是做 low-level vision 的，我们很快能想到「加噪-去噪」其实只是图像的一种<strong>退化模式</strong>，而常见的其他退化模式（例如「模糊-去模糊」「降低分辨率-超分」「遮挡-填充」等）能否用来做类似的扩散过程呢？答案是肯定的，Cold Diffusion<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Bansal, Arpit, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise. *arXiv preprint arXiv:2208.09392* (2022).">[1]</span></a></sup>一文就做了相关的探索。</p><h3 id="generalized-diffusion">Generalized Diffusion</h3><p>由于任意退化模式不一定有像高斯噪声那样简洁方便的数学表达，我们难以从底层数学开始为每种退化做严谨推导，而更多的是做一种高层的、启发式的思考。</p><p>首先我们需要定义退化过程。对于 DDPM 而言，这便是前向加噪过程： <span class="math display">\[q(\x_t\vert \x_0)=\calN\left(\x_t;\ \sqrt{\bar\alpha_t}\x_0,(1-\bar\alpha_t)\I\right)\]</span> 更一般地，我们定义退化算子 <span class="math inline">\(D\)</span> 来表示任意一种退化： <span class="math display">\[\x_t=D(\x_0,t)\tag{1}\label{D}\]</span> 退化可以是<strong>随机性</strong>的，比如 DDPM 就是随机噪声；也可以是<strong>确定性</strong>的，例如下采样、高斯模糊、甚至是事先确定好的噪声等。</p><p>值得注意的是，当扩散步数充分大时，DDPM 保证任何数据都能趋近于服从标准正态，这样我们在逆向生成时才知道要从标准正态中采样。同样地，我们定义的退化过程也需要让退化后的图像呈现出某种已知的分布（或方便建模的分布），以便从中采样。</p><p><br/></p><p>有了退化算子，我们还需要一个重构算子，近似为 <span class="math inline">\(D\)</span> 的逆运算。通常用一个神经网络为其建模，例如 DDPM 的： <span class="math display">\[\x_\theta(\x_t,t)\approx \x_0\]</span> 更一般地，定义重构算子 <span class="math inline">\(R\)</span>，以 <span class="math inline">\(\theta\)</span> 为参数： <span class="math display">\[R_\theta(\x_t,t)\approx \x_0 \tag{2}\label{R}\]</span> 理想情况下，<span class="math inline">\(R\)</span> 恰是 <span class="math inline">\(D\)</span> 的逆，即 <span class="math inline">\(R_{\theta^\ast}(D(\x_0,t),t)=\x_0\)</span> 或 <span class="math inline">\(D(R_{\theta^\ast}(\x_t,t),t)=\x_t\)</span>. 但由于退化算子会磨灭部分输入信息，这是无法做到的。</p><p><br/></p><p>为了训练重构网络，DDPM 从 ELBO 出发，经过一系列推导和简化，最终得到损失函数为： <span class="math display">\[\E_{\x_0,\epsilon,t}\left[\Vert\x_\theta(\x_t,t)-\x_0\Vert^2_2\right]\]</span> 对标这个结论，我们直接定义损失函数为： <span class="math display">\[\E_{\x_0,t}\left\|R_\theta(D(\x_0,t),t)-\x_0\right\|_1 \tag{3}\label{loss}\]</span> 其中将 2 范数替换为 1 范数应该也只是经验性的举措，因为一般图像恢复任务都倾向于使用 1 范数。</p><p><br/></p><p>最后，我们重点考虑逆向采样过程。在 DDPM 中，倘若给定 <span class="math inline">\(\x_0\)</span>，我们知道： <span class="math display">\[\begin{align}&amp;q(\x_{t-1}\vert\x_t,\x_0)=\calN(\x_{t-1};\ \mu_t(\x_t,\x_0),\tilde\beta_t\mathbf I)\\\text{where}\quad&amp;\mu_t(\x_t,\x_0)=\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}\x_t+\frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar\alpha_t}\x_0\\&amp;\tilde\beta_t=\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_t\\\end{align}\]</span> 但由于 <span class="math inline">\(\x_0\)</span> 是未知的，我们用模型近似之： <span class="math display">\[\begin{align}&amp;p_\theta(\x_{t-1}\vert\x_t)\coloneqq\calN\left(\x_{t-1};\ \mu_\theta(\x_t,t),\sigma_t^2\mathbf I\right)\approx\calN\left(\x_{t-1};\mu_t(\x_t,\x_0),\tilde\beta_t\mathbf I\right)=q(\x_{t-1}\vert \x_t,\x_0)\\\text{where}\quad&amp; \mu_\theta(\x_t,t)= \mu_t(\x_t,\x_\theta(\x_t,t))\approx \mu_t(\x_t,\x_0)\\&amp;\sigma_t^2=\tilde \beta_t\text{ or }\beta_t\end{align}\]</span> 更一般地，我们现在也希望用 <span class="math inline">\(\x_t\)</span> 和 <span class="math inline">\(R_\theta(\x_t,t)=\hat\x_0\approx\x_0\)</span> 得到 <span class="math inline">\(\x_{t-1}\)</span>. 但是由于缺乏理论支撑，我们只能启发式地构造。作者给出了两种算法：</p><ul><li>Algorithm 1：直接对 <span class="math inline">\(\hat\x_0\)</span> 做 <span class="math inline">\(t-1\)</span> 步退化得到 <span class="math inline">\(\x_{t-1}\)</span>；</li><li>Algorithm 2：分别对 <span class="math inline">\(\hat\x_0\)</span> 做 <span class="math inline">\(t-1\)</span> 步和 <span class="math inline">\(t\)</span> 步退化，通过二者的差分得到 <span class="math inline">\(\x_{t-1}\)</span>.</li></ul><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="alg1.png" /></div><div class="group-image-wrap"><img src="alg2.png" /></div></div></div><p>实验发现，Algorithm 1 只对噪声退化有较好的表现，而对模糊退化表现非常糟糕。其实这也在意料之中，因为当 <span class="math inline">\(t\)</span> 很大时，重构误差也是很大的，那么得到的 <span class="math inline">\(\x_{t-1}\)</span> 自然也不理想。但是为什么 Algorithm 2 能够有更好的表现呢？</p><p>考虑一个线性的退化：<span class="math inline">\(D(\x_0,t)=\x_0+t\cdot \mathbf e\)</span>，应用 Algorithm 2： <span class="math display">\[\begin{align}\x_{t-1}&amp;=\x_t-D(\hat\x_0,t)+D(\hat\x_0,t-1)\\&amp;=D(\x_0,t)-D(R_\theta(\x_t,t),t)+D(R_\theta(\x_t,t),t-1)\\&amp;=\x_0+t\cdot\mathbf e-R_\theta(\x_t,t)-t\cdot\mathbf e+R_\theta(\x_t,t)+(t-1)\cdot\mathbf e\\&amp;=\x_0+(t-1)\cdot\mathbf e\\&amp;=D(\x_0,t-1)\end{align}\]</span> 我们惊讶地发现结果与 <span class="math inline">\(R_\theta\)</span> 无关！所以无论重构误差有多大，在线性退化场景下，Algorithm 2 总能给出正确的值。而在 <span class="math inline">\(t\)</span> 足够小的时候，根据泰勒展开，任意退化都可用线性退化近似：<span class="math inline">\(D(\x_0,t)=\x_0+t\cdot \mathbf e+o(t)\)</span>，所以 Algorithm 2 能表现得比 Algorithm 1 更好。</p><div class="note note-warning">            <p><strong>疑问</strong>：上述分析只在 <span class="math inline">\(t\)</span> 足够小时成立，但是我们知道 <span class="math inline">\(R_\theta(\x_t,t)\)</span> 的近似误差在 <span class="math inline">\(t\)</span> 越大时越大，怎么解释 Algorithm 2 在 <span class="math inline">\(t\)</span> 较大时的表现呢？</p>          </div><p>现在我们已经描述出了一般的扩散过程，接下来我们尝试代入几个常见的退化模式。</p><h3 id="various-transformations">Various Transformations</h3><p>这一节中，我们将考虑几个具体的退化模式：「模糊-去模糊」「遮挡-填充」「下采样-超分」「雪花-去雪花」。值得注意的是，本节重点在于探索逆向过程能否恢复这些退化，即展示的结果是先做退化、再重构，而非直接生成。</p><ul><li><p><strong>Deblurring</strong></p><p>作者用一个高斯核在原图上做卷积来完成高斯模糊。设高斯核序列为 <span class="math inline">\(\{G_t\}_{t=1}^T\)</span>，则： <span class="math display">\[\x_t=G_t\ast \x_{t-1}=G_t\ast G_{t-1}\ast\cdots\ast G_1\ast\x_0=\bar G_t\ast\x_0=D(\x_0,t)\]</span> 其中 <span class="math inline">\(\ast\)</span> 表示卷积操作。我们按 <span class="math inline">\(\eqref{R}\)</span> 式定义重构网络，并用 <span class="math inline">\(\eqref{loss}\)</span> 式训练。采样时，依 Algorithm 2，每一步 <span class="math inline">\(\x_{t-1}\)</span> 会在 <span class="math inline">\(\x_t\)</span> 的基础上减去： <span class="math display">\[D(\hat\x_0,t)-D(\hat\x_0,t-1)=\bar G_t\ast\x_0-\bar G_{t-1}\ast\x_0\]</span> 由于高斯核可以视为滤波器，这个差值其实反映了 <span class="math inline">\(\x_{t-1}\)</span> 和 <span class="math inline">\(\x_t\)</span> 之间的频率差异。</p><p>作者在实验中对比了逆向过程逐层采样（Alg.）和一次性从 <span class="math inline">\(\x_T\)</span>​ 恢复（Direct）。逐层采样能够生成更多的细节，FID 值更优，但 SSIM 更差。考虑到 SSIM 比较的是两张图片的相似度，且去模糊本身具有多样性，视觉上更好的（有更多细节的）SSIM 反而更差是完全可以接受的。</p><p><img src="deblurring.png" width=70% /></p><p><img src="deblurring-table.png" width=70% /></p></li><li><p><strong>Inpainting</strong></p><p>作者使用一个高斯掩膜将掩盖的部分置为灰色。具体而言，设有一个递增序列 <span class="math inline">\(\{\beta_t\}_{t=1}^T\)</span>，以其中每一项为方差构建高斯分布，并将最大值（分布中心）归一化为 <span class="math inline">\(1\)</span>，这样就得到了一系列掩膜 <span class="math inline">\(\{z_{\beta_t}\}_{t=1}^T\)</span>. 通过累乘各级掩膜，我们就能够让掩盖的信息逐渐增多，即： <span class="math display">\[\x_t=D(\x_0,t)=\x_0\cdot\prod_{i=1}^tz_{\beta_i}\]</span> 定量结果与去模糊相似，逐层采样有着更优的 FID，直接恢复有更优的 SSIM.</p><p><img src="inpainting.png" width=70% /></p><p><img src="inpainting-table.png" width=70% /></p></li><li><p><strong>Super-Resolution</strong></p><p>对于超分而言，作者用下采样+最近邻上采样作为退化算子。</p><p>该任务的结果其实没有那么理想，可视化的图像看起来还算过得去，但是定量结果就有些不尽人意了。</p><p><img src="sr.png" width=70% /></p><p><img src="sr-table.png" width=70% /></p></li><li><p><strong>Snowification</strong></p><p>Snowification 似乎并不是一个常见的图像恢复任务，作者采用的是论文<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hendrycks, Dan, and Thomas Dietterich. Benchmarking Neural Network Robustness to Common Corruptions and Perturbations. In *International Conference on Learning Representations*. 2018.">[2]</span></a></sup>的<a href="https://github.com/hendrycks/robustness">官方实现</a>来做退化。</p><p>但是该任务的结果也不是很理想，仔细观察可以发现，逐层采样的重构图像有很严重的 artifacts.</p><p><img src="snow.png" width=70% /></p><p><img src="snow-table.png" width=70% /></p></li></ul><h3 id="cold-generation">Cold Generation</h3><p>上一小节我们考察的是不同退化下模型重构原始图片的能力，而这一节我们主要关注无条件生成。后者要求我们必须能够对退化后的分布建模，否则无法知道该从哪里采样 <span class="math inline">\(\x_T\)</span>.</p><ul><li><p><strong>Deterministic Noise Degradation</strong></p><p>所谓确定性噪声，其实就是事先采样好一个高斯噪声 <span class="math inline">\(z\sim\calN(\mathbf0,\mathbf I)\)</span>，且在生成过程中不发生变化。模仿 DDPM，定义退化模式为： <span class="math display">\[D(\x_0,t)=\sqrt{\bar\alpha_t}\x_0+\sqrt{1-\bar\alpha_t}z\]</span> 那么我们可以直接按照 Algorithm 2 采样。又或者，可以计算 <span class="math inline">\(z\)</span> 的估计值来代替之： <span class="math display">\[\hat z(\x_t,t)=\frac{\sqrt{\bar\alpha_t}R_\theta(\x_t,t)}{\sqrt{1-\bar\alpha_t}}\]</span> 作者称这其实对应了 DDIM 的采样过程。</p><p><img src="noise.png" width=80% /></p></li><li><p><strong>Deblurring</strong></p><p>模糊不仅是一个确定性退化，我们还能知道，当 <span class="math inline">\(T\)</span> 足够大时，<span class="math inline">\(\x_T\)</span> 所有像素的值都将等于各 channel 分别取平均的结果。所以我们可以用一个 3 维向量表达任意输入图像的退化结果。为了采样 <span class="math inline">\(\x_T\)</span>，作者使用 GMM 对退化得到的 3 维向量建模。这时作者发现了一个问题——由于所有像素的值都相同、网络和变换都是确定性的，所以得到的结果缺乏多样性。作者的解决方案是先对 <span class="math inline">\(\x_T\)</span> 的每个像素加上随机噪声，再拿去生成。</p><div class="note note-primary">            <p><strong>我的思考</strong>：个人觉得用类似 StyleGAN 中的 style modulation 更优雅一些。</p>          </div><p><img src="blur.png" width=80% /></p></li><li><p><strong>Gaussian Mask (Inpainting)</strong></p><p>当 <span class="math inline">\(T\)</span> 足够大时，<span class="math inline">\(\x_T\)</span> 的所有像素都会被遮挡完（一片黑），这时会产生一个和模糊一样的问题——采样将不具有任何多样性。作者的解决方案是让被遮盖区域变成一个随机颜色，而非黑色。如此，我们就可以从任意一种颜色开始生成了。</p><p><img src="mask.png" width=80% /></p></li><li><p><strong>Animorphosis</strong></p><p>如果我们再开一点脑洞，跳脱出「退化」的思维，Diffusion Models 到底做了一件什么事？把数据分布映射到了另一个分布。那这个另一个分布能不能是另一个数据集呢？当然可以！于是乎，作者展示了一个非常有意思的结果——把人脸转换成动物脸。具体而言，我们的“退化”过程就是人脸和动物脸的融合过程： <span class="math display">\[\x_t=D(\x_0,t)=\frac{T-t}{T}\x_0+\frac{t}{T}\mathbf y,\quad\mathbf y\sim \text{AFHQ Dataset}\]</span> <img src="animorph.png" width=80% /></p><p>如果说之前的退化设置分别对应去噪、去模糊、填充等图像恢复任务，那这个结果其实对应着图像分离任务。</p></li></ul><div class="note note-primary">            <p><strong>一点思考</strong>：Cold Diffusion 将随机噪声退化扩展到了任意退化模式，不拘泥于数学推导，让人眼前一亮。我们自然会想，能否用它来做相应的图像恢复任务？私以为，虽然 Cold Diffusion 提供了这样的可能性，但它的结果还远远不够，要能做出 SOTA 水平，还需要在其上设计更多其他的东西。拿图像填充举例，Cold Diffusion 的退化模式比较单一，不能满足填充任务中各种形状、各种面积占比的 mask 设置（当然这一点稍微设计一下就能搞定）。总而言之，Cold Diffusion 提供了一个新颖的方向，沿其思路做图像恢复任务应该是一条可行的路线。</p>          </div><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Bansal, Arpit, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise. <em>arXiv preprint arXiv:2208.09392</em> (2022). <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Hendrycks, Dan, and Thomas Dietterich. Benchmarking Neural Network Robustness to Common Corruptions and Perturbations. In <em>International Conference on Learning Representations</em>. 2018. <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>Diffusion Models</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DDIM：非马尔可夫过程与加速采样</title>
    <link href="/blog-main/2022/12/14/DDIM%EF%BC%9A%E9%9D%9E%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B%E4%B8%8E%E5%8A%A0%E9%80%9F%E9%87%87%E6%A0%B7/"/>
    <url>/blog-main/2022/12/14/DDIM%EF%BC%9A%E9%9D%9E%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B%E4%B8%8E%E5%8A%A0%E9%80%9F%E9%87%87%E6%A0%B7/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\x}{\mathbf x}\newcommand{\I}{\mathbf I}\newcommand{\calN}{\mathcal N}\newcommand{\E}{\mathbb E}\]</span></p><h2 id="ddpm">DDPM</h2><p>我们首先回顾一下 DDPM 的基础知识。</p><ul><li><p><strong>前向过程</strong></p><p>设有一列 noise schedule：<span class="math inline">\(\{\beta_t\}_{t=1}^T\)</span>，记 <span class="math inline">\(\alpha_t=1-\beta_t\)</span>，<span class="math inline">\(\bar\alpha_t=\prod_{i=1}^t\alpha_i\)</span>. 定义一个向输入逐渐添加噪声的马尔可夫过程如下： <span class="math display">\[q(\x_t\vert \x_{t-1})=\calN(\x_t;\sqrt{1-\beta_t}\x_{t-1},\beta_t \I) \tag{1}\label{ddpm-forward}\]</span> 则可以推得从输入到任意一步加噪的转移概率分布为： <span class="math display">\[q(\x_t\vert \x_0)=\calN\left(\x_t;\sqrt{\bar\alpha_t}\x_0,(1-\bar\alpha_t)\I\right) \tag{2}\label{ddpm-forward0}\]</span> 也就是说我们可以用 <span class="math inline">\(\x_0\)</span> 和一个高斯随机变量 <span class="math inline">\(\epsilon\)</span> 的线性组合来表达 <span class="math inline">\(\x_t\)</span>： <span class="math display">\[\x_t=\sqrt{\bar\alpha_t}\x_0+\sqrt{1-\bar\alpha_t}\epsilon,\quad \epsilon\sim\calN(\mathbf0,\I)\]</span></p></li><li><p><strong>逆向过程</strong></p><p>在给定 <span class="math inline">\(\x_0\)</span> 的条件下，从 <span class="math inline">\(\x_{t}\)</span> 生成 <span class="math inline">\(\x_{t-1}\)</span> 的概率分布为： <span class="math display">\[\begin{align}&amp;q(\x_{t-1}\vert\x_t,\x_0)=\calN\left(\x_{t-1};\ {\mu_t(\x_t,\x_0)},{\tilde\beta_t}\I\right)\\\text{where}\quad&amp;\mu_t(\x_t,\x_0)=\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}\x_t+\frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar\alpha_t}\x_0\\&amp;\tilde\beta_t=\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_t\\\end{align}\tag{3}\label{ddpm-reverse}\]</span> 但由于 <span class="math inline">\(\x_0\)</span> 实际未知，我们用 <span class="math inline">\(p_\theta(\x_{t-1}\vert \x_t)\)</span> 去近似 <span class="math inline">\(q(\x_{t-1}\vert \x_t,\x_0)\)</span>： <span class="math display">\[p_\theta(\x_{t-1}\vert \x_t)=\calN(\x_{t-1};\ \mu_\theta(\x_t,t),\sigma_t^2\I)\]</span> 其中均值 <span class="math inline">\(\mu_\theta(\x_t,t)\)</span> 参数化为： <span class="math display">\[\mu_\theta(\x_t,t)=\frac{1}{\sqrt{\alpha_t}}\left(\x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon_\theta(\x_t,t)\right)\]</span> 由于均值存在近似误差，所以方差 <span class="math inline">\(\sigma_t^2\)</span> 并不一定是要延续 <span class="math inline">\(q(\x_{t-1}\vert \x_t,\x_0)\)</span> 的方差 <span class="math inline">\(\tilde\beta_t\)</span>. DDPM 作者通过分析两种极端情况，人为指定了两种方差选择：<span class="math inline">\(\sigma_t^2=\tilde\beta_t, \sigma_t^2=\beta_t\)</span>.</p></li><li><p><strong>损失函数</strong></p><p>与 VAE 类似，我们优化 ELBO： <span class="math display">\[\max_\theta\quad\text{ELBO}=\E_{\x_{1:T}\sim q(\x_{1:T}\vert \x_0)}\left[\log p(\x_T)+\sum_{t=1}^T\log\frac{p_\theta(\x_{t-1}\vert\x_t)}{q(\x_t\vert \x_{t-1})}\right]\]</span> 该式可以变换为： <span class="math display">\[\min_\theta\quad \sum_{t=1}^T\E_{q(\x_t\vert \x_0)}\left[\mathrm{KL}\left({q(\x_{t-1}\vert \x_t,\x_0)}\ \Vert\ {p_\theta(\x_{t-1}\vert \x_t)}\right)\right]\]</span> 代入 <span class="math inline">\(q(\x_{t-1}\vert\x_t,\x_0)\)</span> 和 <span class="math inline">\(p_\theta(\x_{t-1}\vert \x_t)\)</span>，并简化系数，可得最终损失函数： <span class="math display">\[\mathcal L_\text{simple}=\E_{t,\x_0,\epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(\sqrt{\bar\alpha_t}\x_0+\sqrt{1-\bar\alpha_t}\epsilon,t\right) \right\|^2\right] \tag{4}\label{ddpm-loss}\]</span></p></li></ul><h2 id="shorter-path-faster-sampling">Shorter Path, Faster Sampling</h2><p>在 Improved DDPM<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Nichol, Alexander Quinn, and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In *International Conference on Machine Learning*, pp. 8162-8171. PMLR, 2021.">[1]</span></a></sup>中，作者阐述了一种直接简便的加速采样方法，即不要一步一步地走逆向过程，而是跳着走。受益于 DDPM 前向过程的特殊构造，运用从 <span class="math inline">\(\eqref{ddpm-forward}\)</span> 式推导出 <span class="math inline">\(\eqref{ddpm-forward0}\)</span> 式的思想，我们能轻松地导出任意两个 timestep 之间的转移概率： <span class="math display">\[q(\x_t\vert \x_s)=\calN\left(\x_t;\ \sqrt{\frac{\bar\alpha_t}{\bar\alpha_s}}\x_s,\left(1-\frac{\bar\alpha_t}{\bar\alpha_s}\right)\I\right),\quad  \quad 0\leq s&lt;t\leq T\]</span> 因此，我们可以选取一个子序列 <span class="math inline">\(0=\tau_0&lt;\tau_1&lt;\cdots&lt;\tau_K=N\)</span>，构建一个新的 <span class="math inline">\(K\)</span> 步马尔可夫链： <span class="math display">\[q(\x_{\tau_i}\vert \x_{\tau_{i-1}})=\calN\left(\x_{\tau_i};\ \sqrt{\frac{\bar\alpha_{\tau_i}}{\bar\alpha_{\tau_{i-1}}}}\x_{\tau_{i-1}},\left(1-\frac{\bar\alpha_{\tau_i}}{\bar\alpha_{\tau_{i-1}}}\right)\I\right),\quad 1\leq i\leq K\tag{5}\label{skip}\]</span> <img src="shorter.png" width=100% /></p><p>由于新马尔可夫链更短（<span class="math inline">\(K&lt;N\)</span>），所以在其上采样即可达到加速采样的目的。另外，由于新马尔可夫链是原马尔可夫链的一个子序列，原来的训练本就包括了对子序列的训练，所以我们可以直接加载训练好的 DDPM 模型。</p><p>对比 <span class="math inline">\(\eqref{skip}\)</span> 式与 <span class="math inline">\(\eqref{ddpm-forward}\)</span> 式，容易知道新马尔可夫链对应的 <span class="math inline">\(\beta\)</span> 序列为： <span class="math display">\[{\color{purple}\beta_{\tau_i}}=1-\frac{\bar\alpha_{\tau_i}}{\bar\alpha_{\tau_{i-1}}},\quad i=1,2,\ldots ,K\]</span> 为与原来的 <span class="math inline">\(\beta\)</span> 作区分，这里用紫色来表示新的 <span class="math inline">\(\beta\)</span>. 以新的 <span class="math inline">\(\beta\)</span> 为基础，重新定义一套新的参数： <span class="math display">\[\begin{align}&amp;{\color{purple}{\alpha_{\tau_i}}}=1-{\color{purple}{\beta_{\tau_i}}}\\&amp;{\color{purple}{\bar\alpha_{\tau_i}}}=\prod_{j=1}^i{\color{purple}{\alpha_{\tau_j}}}\\&amp;{\color{purple}{\tilde\beta_{\tau_i}}}=\frac{1-{\color{purple}\bar\alpha_{\tau_{i-1}}}}{1-{\color{purple}\bar\alpha_{\tau_i}}}{\color{purple}\beta_{\tau_i}}\end{align}\]</span> 那么原来的所有结论在新的参数下都适用。（其实 <span class="math inline">\(\color{purple}{\bar\alpha}\)</span> 和 <span class="math inline">\(\bar\alpha\)</span> 是一样的，毕竟新的 <span class="math inline">\(\color{purple}{\beta}\)</span> 就是用 <span class="math inline">\(\bar\alpha\)</span> 定义出来的。）</p><p>在具体实现上，我们有两种方案：</p><ol type="1"><li><p>第一种方案是 <a href="https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/respace.py#L63">Improved DDPM GitHub repo</a> 所采用的。我们继承原来的 DDPM 类，更新 <span class="math inline">\(\beta\)</span> 序列并重新初始化其他参数，就可以直接调用原本 DDPM 的方法了。唯一值得注意的一点是，给到模型的 timestep 要映射回原来的 timestep，见 repo 中的 <code>_WrappedModel</code> 类。</p><blockquote><p>事实上，Improved DDPM 还做了其他改进，如学习方差而非人为指定、更改 noise schedule 等，但它们不是本文关注的重点。</p></blockquote></li><li><p>第二种方案是 <a href="https://github.com/ermongroup/ddim/blob/main/functions/denoising.py#L35">DDIM GitHub repo</a> 所采用的，更加简单粗暴，即直接在采样函数里面重新计算 <span class="math inline">\(\beta\)</span> 等参数。相比方案一和 DDPM 的普遍实现，这样做每一次采样都会进行更多的计算，也许运行速度更慢一点吧（没实测过）。</p></li></ol><h2 id="ddim">DDIM</h2><h3 id="idea">Idea</h3><p>在 DDPM 中，我们先定义了 <span class="math inline">\(q(\x_t\vert\x_{t-1})\)</span>，然后推导出 <span class="math inline">\(q(\x_t\vert \x_0)\)</span>，最后利用贝叶斯公式推导出 <span class="math inline">\(q(\x_{t-1}\vert \x_t,\x_0)\)</span>： <span class="math display">\[q(\x_{t-1}\vert \x_t,\x_0)=\frac{q(\x_t\vert\x_{t-1},\x_0)q(\x_{t-1}\vert\x_0)}{q(\x_t\vert\x_0)}\]</span> 论文<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, Jiaming, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. In *International Conference on Learning Representations*. 2020.">[2]</span></a></sup>的作者敏锐地发现，DDPM 的损失函数和采样过程只依赖于 <span class="math inline">\(q(\x_t\vert\x_0)\)</span> 和 <span class="math inline">\(q(\x_{t-1}\vert\x_t,\x_0)\)</span>，与 <span class="math inline">\(q(\x_t\vert\x_{t-1})\)</span> 无关。因此，虽然 <span class="math inline">\(q(\x_t\vert\x_{t-1})\)</span> 是一切的根源，但是它最后并没有直接发挥作用。于是我们大胆地猜想——能否不从 <span class="math inline">\(q(\x_t\vert\x_{t-1})\)</span> 出发，而是直接定义 <span class="math inline">\(q(\x_t\vert \x_0)\)</span> 和 <span class="math inline">\(q(\x_{t-1}\vert \x_t,\x_0)\)</span> 呢？</p><p>首先，为了与 DDPM 对齐，我们保持 <span class="math inline">\(q(\x_t\vert\x_0)\)</span> 不变： <span class="math display">\[q(\x_t\vert\x_0)=\calN\left(\x_t;\ \sqrt{\bar\alpha_t}\x_0,(1-\bar\alpha_t)\I\right)\label{xtx0}\tag{6}\]</span> 其次，考虑 <span class="math inline">\(q(\x_{t-1}\vert \x_t,\x_0)\)</span>，我们发现仅从已定义的 <span class="math inline">\(q(\x_t\vert \x_0)\)</span> 是无法唯一确定下 <span class="math inline">\(q(\x_{t-1}\vert \x_t,\x_0)\)</span> 的（无视掉 <span class="math inline">\(\x_0\)</span> 的条件能看得更明显），只要它们俩满足以下关系式就行： <span class="math display">\[q(\x_{t-1}\vert \x_0)=\int q(\x_{t-1}\vert \x_t,\x_0)q(\x_t\vert \x_0)\mathrm d\x_t\]</span></p><p>论文给出的一族满足条件的解是： <span class="math display">\[q_\sigma(\x_{t-1}\vert\x_t,\x_0)=\calN\left(\x_{t-1};\ \sqrt{\bar\alpha_{t-1}}\x_0+\sqrt{1-\bar\alpha_{t-1}-\sigma_t^2}\cdot\frac{\x_t-\sqrt{\bar\alpha_t}\x_0}{\sqrt{1-\bar\alpha_t}},\sigma_t^2\I \right)\tag{7}\label{ddim-reverse}\]</span> 这里给 <span class="math inline">\(q\)</span> 加上了下标 <span class="math inline">\(\sigma=(\sigma_1,\ldots,\sigma_T)\)</span> 来明确地表示 <span class="math inline">\(q\)</span> 是关于 <span class="math inline">\(\sigma\)</span> 的一族解，改变 <span class="math inline">\(\sigma\)</span> 就能改变 <span class="math inline">\(q\)</span>.</p><blockquote><p>论文的发展顺序是先直接给出了 <span class="math inline">\(q_\sigma(\x_{t-1}\vert\x_t,\x_0)\)</span>，然后用数学归纳法证明 <span class="math inline">\(q(\x_t\vert \x_0)\)</span> 满足我们的定义。按这样的顺序写论文比较方便，但显然与人的正常思考过程相反，不具有参考意义。</p><p>参考资料<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="苏剑林. (Jul. 27, 2022). 《生成扩散模型漫谈（四）：DDIM = 高观点DDPM 》[Blog post]. Retrieved from https://kexue.fm/archives/9181">[3]</span></a></sup>使用了待定系数法求解 <span class="math inline">\(q_\sigma(\x_{t-1}\vert\x_t,\x_0)\)</span>，是一个不错的方案。</p></blockquote><p>根据 <span class="math inline">\(\eqref{xtx0},\eqref{ddim-reverse}\)</span> 式，我们从不同于 DDPM 的角度定义了一个新的扩散过程。在实际采样时，我们依旧用 <span class="math inline">\(p_{\theta,\sigma}(\x_{t-1}\vert \x_t)\)</span> 来近似 <span class="math inline">\(q_\sigma(\x_{t-1}\vert\x_t,\x_0)\)</span>： <span class="math display">\[\begin{align}&amp;p_{\theta,\sigma}(\x_{t-1}\vert \x_t,\x_0)=\calN\left(\x_{t-1};\sqrt{\bar\alpha_{t-1}}\ \x_\theta(\x_t,t)+\sqrt{1-\bar\alpha_{t-1}-\sigma_t^2}\cdot\epsilon_\theta(\x_t,t),\sigma_t^2\mathbf I\right)\\\text{where}\quad&amp;\x_\theta(\x_t,t)\xrightarrow{\text{approximates}}\x_0\\&amp;\epsilon_\theta(\x_t,t)\xrightarrow{\text{approximates}}\epsilon=\frac{\x_t-\sqrt{\bar\alpha_t}\x_0}{\sqrt{1-\bar\alpha_t}}\end{align}\]</span></p><p>当然这里也存在和 DDPM 相同的问题：<span class="math inline">\(\sigma_t^2\)</span> 并不是 <span class="math inline">\(p_{\theta,\sigma}(\x_{t-1}\mid\x_t,\x_0)\)</span> 最优的方差。这个问题一直遗留到了 Analytic-DPM 才得到解决。</p><h3 id="non-markovian">Non-Markovian</h3><p>对于新的扩散过程，其一步前向过程为： <span class="math display">\[q_\sigma(\x_t\vert\x_{t-1},\x_0)=\frac{q_\sigma(\x_{t-1}\vert\x_t,\x_0)q(\x_t\vert\x_0)}{q(\x_{t-1}\vert\x_0)}\]</span> 这依旧是一个高斯分布，具体参数是什么不重要，重要的是 <span class="math inline">\(\x_t\)</span> 同时依赖于 <span class="math inline">\(\x_{t-1}\)</span> 和 <span class="math inline">\(\x_0\)</span>，所以不再是一个马尔可夫过程了。因此，<strong>新的扩散过程放宽了马尔可夫条件这个约束，带来了一个新的自由变量 <span class="math inline">\(\sigma=(\sigma_1,\ldots,\sigma_T)\)</span>，即去噪过程的方差</strong>。</p><p><img src="ddim.png" width=80% /></p><p>虽然新的扩散过程把马尔可夫过程扩展为了非马尔可夫过程，但是它的损失函数与 DDPM 仍然一致（因为两同方差正态分布的 KL 散度正比于二者均值相减的平方，又两个均值有着相同的形式，所以最后都能归约到 <span class="math inline">\(\Vert \epsilon-\epsilon_\theta(\x_t,t)\Vert|^2\)</span>，仅仅系数略有差别），因此我们可以直接加载 DDPM 训练好的模型，无需重新训练。</p><h3 id="two-special-cases">Two Special Cases</h3><p>现在我们考虑两种特殊的方差选取方案：</p><p>第一种，取 <span class="math inline">\(\sigma_t^2=\tilde\beta_t=\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_t\)</span> . 代入 <span class="math inline">\(\eqref{ddim-reverse}\)</span> 式，会发现化简后的结果与 <span class="math inline">\(\eqref{ddpm-reverse}\)</span> 式完全一致，即得到了 DDPM. 因此，<strong>我们也许可以称新的扩散过程为 generalized DDPM</strong>.</p><p>第二种，取 <span class="math inline">\(\sigma_t^2=0\)</span>. 这意味着逆向过程变成了一个<strong>确定性过程</strong>： <span class="math display">\[\x_{t-1}=\sqrt{\bar\alpha_{t-1}}\x_0+\sqrt{1-\bar\alpha_{t-1}}\cdot\frac{\x_t-\sqrt{\bar\alpha_t}\x_0}{\sqrt{1-\bar\alpha_t}}\]</span> 代入模型，即： <span class="math display">\[\x_{t-1}=\sqrt{\bar\alpha_{t-1}}\x_\theta(\x_t,t)+\sqrt{1-\bar\alpha_{t-1}}\epsilon_\theta(\x_t,t)\label{ddim}\tag{8}\]</span> 确定性也意味着每个图像都唯一对应了一个隐变量，作者称之为 <strong>Denoising Diffusion Implicit Models (DDIM)</strong>.</p><p>确定性还意味着我们可以对隐变量插值来获取语义平滑变化的图像了。但是这里有一个小 trick：作者采用的不是一般的线性插值，而是球面线性插值： <span class="math display">\[\x_T^{(\alpha)}=\frac{\sin((1-\alpha)\theta)}{\sin(\theta)}\x_T^{(1)}+\frac{\sin(\alpha\theta)}{\sin(\theta)}\x_T^{(2)}\]</span> 其中，<span class="math inline">\(\theta\)</span> 是两向量 <span class="math inline">\(\x_T^{(1)}\)</span> 和 <span class="math inline">\(\x_T^{(2)}\)</span> 的夹角。</p><div class="note note-info">            <p><strong>球面线性插值（Spherical Linear Interpolation，Slerp）</strong></p><p>设有单位向量 <span class="math inline">\(\mathbf p,\mathbf q\)</span>，夹角为 <span class="math inline">\(\theta\)</span>，以 <span class="math inline">\(0&lt;t&lt;1\)</span> 为变量对夹角角度做插值，对应该角度的单位向量 <span class="math inline">\(\mathbf r\)</span> 即是插值结果：</p><p><img src="slerp.png" width=40% /></p><p><span class="math inline">\(\mathbf r\)</span> 可以表达为 <span class="math inline">\(\mathbf p,\mathbf q\)</span> 的线性组合 <span class="math inline">\(\mathbf r=a(t)\mathbf p+b(t)\mathbf q\)</span>，所以我们想解出系数 <span class="math inline">\(a(t),b(t)\)</span>. 两边同时点乘 <span class="math inline">\(\mathbf p\)</span> 得： <span class="math display">\[\begin{align}&amp;\mathbf p\cdot\mathbf r=a(t)\mathbf p\cdot\mathbf p+b(t)\mathbf p\cdot\mathbf q\\\implies&amp;\cos(t\theta)=a(t)+b(t)\cos(\theta)\end{align}\]</span> 同理，两边同时点乘 <span class="math inline">\(\mathbf q\)</span> 得： <span class="math display">\[\begin{align}&amp;\mathbf q\cdot\mathbf r=a(t)\mathbf q\cdot\mathbf p+b(t)\mathbf q\cdot\mathbf q\\\implies&amp;\cos((1-t)\theta)=a(t)\cos(\theta)+b(t)\end{align}\]</span> 联立两式，解得： <span class="math display">\[\begin{align}&amp;a(t)=\frac{\cos(t\theta)-\cos((1-t)\theta)\cos(\theta)}{1-\cos^2(\theta)}\\&amp;b(t)=\frac{\cos((1-t)\theta)-\cos(t\theta)\cos(\theta)}{1-\cos^2(\theta)}\\\end{align}\]</span> 以 <span class="math inline">\(a(t)\)</span> 为例化简分子： <span class="math display">\[\begin{align}&amp;\cos(t\theta)-\cos((1-t)\theta)\cos(\theta)\\=\ &amp;\cos(\theta-(1-t)\theta)-\cos((1-t)\theta)\cos(\theta)\\=\ &amp;\cos(\theta)\cos((1-t)\theta)+\sin(\theta)\sin((1-t)\theta)-\cos((1-t)\theta)\cos(\theta)\\=\ &amp;\sin(\theta)\sin((1-t)\theta)\end{align}\]</span> 因此： <span class="math display">\[a(t)=\frac{\sin(\theta)\sin((1-t)\theta)}{1-\cos^2(\theta)}=\frac{\sin((1-t)\theta)}{\sin(\theta)}\]</span> 同理可得： <span class="math display">\[b(t)=\frac{\sin(t\theta)}{\sin(\theta)}\]</span> 所以球面线性插值公式为： <span class="math display">\[\text{Slerp}(\mathbf p,\mathbf q)=\frac{\sin((1-t)\theta)}{\sin(\theta)}\mathbf p+\frac{\sin(t\theta)}{\sin(\theta)}\mathbf q\]</span> 特别地，当 <span class="math inline">\(\theta\to0\)</span> 时，Slerp 趋近于线性插值。</p>          </div><p>至于作者为什么要用球面线性插值而不是一般的线性插值，我其实也不是很清楚，作者似乎也没提……</p><p>Update 2023.03.10：在生成模型中使用 Slerp 应该最先是在论文<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="White, Tom. Sampling generative networks. *arXiv preprint arXiv:1609.04468* (2016).">[5]</span></a></sup>中阐述的，在后来的 StyleGAN 等生成模型里也有应用。论文认为 Slerp 插值出的隐变量更符合先验分布，但是并没有说服 ICLR 2017 的审稿人，最后也没中，可以在 <a href="https://openreview.net/forum?id=SypU81Ole">openreview</a> 上看审稿意见。但是就其引用量而言，论文提出的许多可视化方法还是受到了人们的广泛认可。</p><h3 id="faster-sampling">Faster Sampling</h3><p>截至目前，我们似乎并没有看出 DDIM 有什么加速之处。其实，DDIM 的加速采样思想和上一节是类似的，即取子序列 <span class="math inline">\(0= \tau_0&lt;\tau_1&lt;\cdots&lt;\tau_K=N\)</span>，只做 <span class="math inline">\(K\)</span> 步采样。具体而言，新的采样过程为： <span class="math display">\[p_{\theta,\sigma}(\x_{\tau_{i-1}}\vert \x_{\tau_i},\x_0)=\calN\left(\x_{\tau_{i-1}};\sqrt{\bar\alpha_{\tau_{i-1}}}\ \x_\theta(\x_{\tau_i},\tau_i)+\sqrt{1-\bar\alpha_{\tau_{i-1}}-\sigma_{\tau_i}^2}\cdot\epsilon_\theta(\x_{\tau_i},\tau_i),\sigma_{\tau_i}^2\mathbf I\right)\]</span> 可以看见，DDIM 的加速采样其实并没有什么新的东西。那为什么人们每每提到 DDIM，总会想到加速采样呢？因为实验发现，当采样步数小于 100 步时（假设训练是 1000 步），DDIM 的采样质量比 DDPM 显著更好；特别是在 50 到 100 步左右时，人眼其实不怎么看得出和 1000 步的区别。</p><h3 id="neural-ode-inversion">Neural ODE &amp; Inversion</h3><p>在「<a href="/blog-main/2022/12/04/%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94SDE%E4%B8%8EODE%E6%8F%8F%E8%BF%B0/" title="连续时间扩散模型——SDE与ODE描述">连续时间扩散模型——SDE与ODE描述</a>」一文中，我们介绍了如何将离散时间的 DDPM 连续化为 SDE 形式，同时还说明了每个 SDE 都对应着一个确定性的 ODE. 而 DDIM 恰是一种确定性情形，所以我们自然会想到——能不能用 ODE 来描述一个 DDIM 呢？答案是肯定的。</p><p>首先，我们把 <span class="math inline">\(\eqref{ddim}\)</span> 式中的 <span class="math inline">\(\x_\theta(\x_t,t)\)</span> 也换做 <span class="math inline">\(\epsilon_\theta(\x_t,t)\)</span> 表示： <span class="math display">\[\begin{align}\x_{t-1}&amp;=\sqrt{\bar\alpha_{t-1}}\x_\theta(\x_t,t)+\sqrt{1-\bar\alpha_{t-1}}\epsilon_\theta(\x_t,t)\\&amp;=\frac{\sqrt{\bar\alpha_{t-1}}}{\sqrt{\bar\alpha_t}}\left(\x_t-\sqrt{1-\bar\alpha_t}\epsilon_\theta(\x_t,t)\right)+\sqrt{1-\bar\alpha_{t-1}}\epsilon_\theta(\x_t,t)\\&amp;=\frac{1}{\sqrt{\alpha_t}}\x_t+\left(\sqrt{1-\bar\alpha_{t-1}}-\sqrt{\frac{1-\bar\alpha_t}{\alpha_t}}\right)\epsilon_\theta(\x_t,t)\end{align}\]</span></p><p>两边同时除以 <span class="math inline">\(\sqrt{\bar\alpha_{t-1}}\)</span>，得： <span class="math display">\[\frac{\x_{t-1}}{\sqrt{\bar\alpha_{t-1}}}=\frac{\x_t}{\sqrt{\bar\alpha_t}}+\left(\sqrt{\frac{1-\bar\alpha_{t-1}}{\bar\alpha_{t-1}}}-\sqrt{\frac{1-\bar\alpha_t}{\bar\alpha_t}}\right)\epsilon_\theta(\x_t,t)\]</span> 记 <span class="math inline">\(\bar\x(t)=\x_t/\sqrt{\bar\alpha_t}\)</span>，<span class="math inline">\(\sigma(t)=\sqrt{(1-\bar\alpha_t)/\bar\alpha_t}\)</span>，则上式对应于下述 ODE： <span class="math display">\[\mathrm d\bar\x(t)=\epsilon_\theta\left(\frac{\bar\x(t)}{\sqrt{\sigma(t)^2+1}},t\right)\mathrm d\sigma(t)\]</span> 这就是 DDIM 的 ODE 描述。</p><p><br/></p><p>前文提及，DDIM 使得每个图像都唯一对应了一个隐变量，但并没有说明如何求这个隐变量。现在有了 ODE，事情就变得简单了。如果说采样过程是沿着 <span class="math inline">\(T\to 0\)</span> 的时间求解 ODE，那么反过来，沿着 <span class="math inline">\(0\to T\)</span> 的时间解同一个 ODE 不就是逆过程了吗： <span class="math display">\[\begin{align}&amp;\bar\x(t+\Delta t)=\bar\x(t)+(\sigma(t+\Delta t)-\sigma(t))\epsilon_\theta\left(\frac{\bar\x(t)}{\sqrt{\sigma^2(t)+1}},t\right)\\\implies\ &amp;\frac{\x_{t+1}}{\sqrt{\bar\alpha_{t+1}}}=\frac{\x_t}{\sqrt{\bar\alpha_t}}+\left(\sqrt{\frac{1-\bar\alpha_{t+1}}{\bar\alpha_{t+1}}}-\sqrt{\frac{1-\bar\alpha_t}{\bar\alpha_t}}\right)\epsilon_\theta(\x_t,t)\\\implies\ &amp;\x_{t+1}=\sqrt{\bar\alpha_{t+1}}\x_\theta(\x_t,t)+\sqrt{1-\bar\alpha_{t+1}}\epsilon_\theta(\x_t,t)\end{align}\]</span></p><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Nichol, Alexander Quinn, and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In <em>International Conference on Machine Learning</em>, pp. 8162-8171. PMLR, 2021. <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Song, Jiaming, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. In <em>International Conference on Learning Representations</em>. 2020. <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>苏剑林. (Jul. 27, 2022). 《生成扩散模型漫谈（四）：DDIM = 高观点DDPM 》[Blog post]. Retrieved from https://kexue.fm/archives/9181 <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>四元数插值与均值（姿态平滑）. https://www.cnblogs.com/21207-iHome/p/6952004.html <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>White, Tom. Sampling generative networks. <em>arXiv preprint arXiv:1609.04468</em> (2016). <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>Diffusion Models</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>搭建个人深度学习工作站（环境篇）</title>
    <link href="/blog-main/2022/12/11/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%EF%BC%88%E7%8E%AF%E5%A2%83%E7%AF%87%EF%BC%89/"/>
    <url>/blog-main/2022/12/11/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%EF%BC%88%E7%8E%AF%E5%A2%83%E7%AF%87%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><ul><li><p><strong>硬件配置</strong></p><ul><li><p>主板：华硕 TUF GAMING B560M-PLUS WIFI</p></li><li><p>CPU：Intel i7-11700 @ 2.50GHz 8核16线程</p></li><li><p>内存：英睿达 美光32GB(16Gx2)套装 DDR4 3600MHz</p></li><li><p>GPU：NVIDIA RTX 3080Ti</p></li><li><p>硬盘：</p><ul><li>Samsung SSD 980 1TB x 2</li><li>WDC WD20EZBX-00A 2TB</li></ul></li><li><p>电源：长城 猎金部落 额定1100W G11金牌全模</p></li></ul></li><li><p><strong>操作系统</strong>：Ubuntu Server 22.04</p></li></ul><h2 id="概述">概述</h2><p>根据李沐老师的<a href="https://www.bilibili.com/video/BV1LT411F77M/">视频</a>，有三种环境安装方式：</p><ol type="1"><li>CUDA + conda PyTorch</li><li>NVIDIA driver + conda PyTorch</li><li>NVIDIA driver + NVIDIA Docker</li></ol><p><strong>第一种方案</strong>：首先从官网上下载 CUDA 大礼包进行系统级别的安装，随后在各个 conda 环境内安装 PyTorch. 大礼包的内容非常全，但如果嫌里面的驱动版本不够新，也可以先单独安装驱动，然后在安装大礼包时把驱动勾掉即可。这个方案的劣势是所有环境都用这个 CUDA，可能会产生一些版本问题。</p><p><strong>第二种方案（我选择使用的方案）</strong>：在系统层面上只安装驱动，在 conda 环境里安装 PyTorch 时顺便安装 CUDA. 好处是 CUDA 也在环境里，版本容易控制，坏处是多消耗一些空间，并且安装的 CUDA 可能不是那么全。</p><p><strong>第三种方案</strong>：和第二种差不多，在系统层面上只安装驱动，然后开 Docker 容器安装 PyTorch、CUDA 等。</p><h2 id="安装-nvidia-driver">安装 NVIDIA Driver</h2><ol type="1"><li><p>禁用 nouveau</p><p>Ubuntu 系统集成的显卡驱动程序是 nouveau，它是第三方为 NVIDIA 开发的开源驱动，我们需要先将其屏蔽才能安装 NVIDIA 官方驱动。 所以我们要先把驱动加到黑名单 blacklist.conf 里。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 修改属性</span><br>sudo <span class="hljs-built_in">chmod</span> 666 /etc/modprobe.d/blacklist.conf<br><span class="hljs-comment"># 用 vim 打开</span><br>sudo vim /etc/modprobe.d/blacklist.conf<br><span class="hljs-comment"># 在最后一行加入以下几句，保存退出</span><br>blacklist vga16fb<br>blacklist nouveau<br>blacklist rivafb<br>blacklist rivatv<br>blacklist nvidiafb<br><span class="hljs-comment"># 对刚才修改的文件进行更新</span><br>sudo update-initramfs -u<br><span class="hljs-comment"># 记得重启计算机，打开终端检查 nouveau 是否被禁用</span><br>lsmod | grep nouveau <br><span class="hljs-comment"># 若执行完该句，没有任何输出，则 nouveau 被成功禁用</span><br></code></pre></td></tr></table></figure></li><li><p>卸载旧的驱动</p><p>如果你是第一次安装 GPU 驱动，这一步可以省略。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt-get remove --purge nvidia*<br></code></pre></td></tr></table></figure></li><li><p>安装 gcc</p><p>必须先安装 gcc，才能安装 GPU 驱动。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt-get install build-essential <span class="hljs-comment"># 安装gcc，不要怀疑，就是 build-essential</span><br>gcc --version <span class="hljs-comment"># 检查gcc是否安装成功,Default GCC 9.3</span><br></code></pre></td></tr></table></figure><p>这里只安装了gcc，GPU 驱动就可以成功安装了，如果你执行到下面，提示你 make、g++ 没有安装的话，再回来执行以下语句进行安装即可。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt-get install g++<br>sudo apt-get install make<br></code></pre></td></tr></table></figure></li><li><p>下载对应版本的 GPU 驱动</p><p>根据你的显卡类型选择合适的 <code>.run</code> 文件下载，<a href="https://www.nvidia.cn/geforce/drivers/">官网下载链接</a>（速度较快，可直接使用）</p></li><li><p>安装驱动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># cd 到 .run 文件所在位置</span><br><span class="hljs-built_in">cd</span> xxx<br><span class="hljs-comment"># 修改权限</span><br>sudo <span class="hljs-built_in">chmod</span> a+x NVIDIA-Linux-x86_64-xxx.run<br><span class="hljs-comment"># 执行安装，过程中根据提示，选择 yes 等</span><br>sudo ./NVIDIA-Linux-x86_64-xxx.run -no-x-check -no-nouveau-check -no-opengl-files<br><span class="hljs-comment"># 执行此语句，出现显卡信息则证明安装成功。</span><br>nvidia-smi<br></code></pre></td></tr></table></figure></li></ol><h2 id="安装-anaconda">安装 Anaconda</h2><ol type="1"><li><p>从<a href="https://www.anaconda.com/products/distribution">官网</a>下载 <code>.sh</code> 文件后，<code>bash</code> 运行即可</p></li><li><p>[Optional] 区分 Ubuntu 自带 python 和 anaconda 中的 python</p><p>Ubuntu22.04 中自带 python3，为方便起见，我们可以做一定的区分。在 <code>~/.bashrc</code> 文件最后加入以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">alias</span> python3=<span class="hljs-string">&quot;/usr/bin/python3&quot;</span>  <span class="hljs-comment"># 给系统自带的python起一个别名，就叫python3</span><br><span class="hljs-built_in">export</span> PATH=<span class="hljs-string">&quot;/home/jason/anaconda3/bin:<span class="hljs-variable">$PATH</span>&quot;</span>  <span class="hljs-comment"># anaconda3中的python</span><br></code></pre></td></tr></table></figure><p>然后 source 一下让修改发生作用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure><p>这样，输入 <code>python3</code> 时会自动替换为 <code>/usr/bin/python3</code>，即使用系统自带的 python；而直接输入 <code>python</code> 则是用当前环境下的 python.</p></li><li><p>[Optional] 每次打开终端时，conda 会默认自动激活 base 环境，在命令提示符前有 <code>(base)</code> 标志。强迫症表示这太难看了，可以用以下命令禁止自动激活：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda config --<span class="hljs-built_in">set</span> auto_activate_base False<br></code></pre></td></tr></table></figure></li></ol><h2 id="安装-pytorch">安装 PyTorch</h2><p>激活 conda 环境，根据<a href="https://pytorch.org/get-started/locally/">官网</a>命令安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.6 -c pytorch -c conda-forge<br></code></pre></td></tr></table></figure><p>可以看到安装的东西除了 pytorch 以外，还有 cudatoolkit. 由于我们采用的是概述中的方案二，即没有在系统层面安装 CUDA，因此在该环境下我们将使用这时下载的 CUDA.</p><div class="note note-warning">            <p>我试过 pytorch 1.13 版本，命令行中的 <code>cudatoolkit</code> 变成了 <code>pytorch-cuda</code>，但安装之后没能成功运行。</p>          </div>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
      <tag>linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>搭建个人深度学习工作站（操作系统篇）</title>
    <link href="/blog-main/2022/12/11/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%EF%BC%88%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AF%87%EF%BC%89/"/>
    <url>/blog-main/2022/12/11/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E7%AB%99%EF%BC%88%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%AF%87%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<div class="note note-primary">            <p><strong>说明</strong>：由于是个人使用，我安装的是 Windows + Ubuntu Server 双系统。用 Ubuntu Server 训练，<del>用 Windows 摸鱼</del>。</p>          </div><h2 id="硬件配置">硬件配置</h2><ul><li>主板：华硕 TUF GAMING B560M-PLUS WIFI</li><li>CPU：Intel i7-11700 @ 2.50GHz 8核16线程</li><li>内存：英睿达 美光32GB(16Gx2)套装 DDR4 3600MHz</li><li>GPU：NVIDIA RTX 3080Ti</li><li>硬盘：<ul><li>Samsung SSD 980 1TB x 2</li><li>WDC WD20EZBX-00A 2TB</li></ul></li><li>电源：长城 猎金部落 额定1100W G11金牌全模</li></ul><h2 id="安装过程">安装过程</h2><p>这个 <a href="https://www.youtube.com/watch?v=yJeDV34GzqI">YouTube 视频</a>和我的需求完全一致（即先安装 Windows10，然后安装 Ubuntu Server），可作为直接参考。</p><h3 id="硬盘分区规划">硬盘分区规划</h3><p>由于我是安装双系统，因此要事先做好硬盘分区规划。经衡量，决定：</p><ol type="1"><li>一块 1TB 固态盘作为系统盘，分出 670GB 给 Ubuntu，剩下 260GB 给 Windows；</li><li>另一块 1TB 固态盘作为数据盘（存放数据集），全部给 Ubuntu；</li><li>2TB 机械硬盘分出 1.5TB 给 Ubuntu 做备份盘（数据集备份），剩下 300GB 给 Windows。</li></ol><h3 id="开机自动挂载硬盘">开机自动挂载硬盘</h3><p>如上文所言，我有一个 1T 固态盘存放数据，因此准备挂载到 <code>/data</code> 下。以此为例，开机自动挂载的步骤为：</p><ol type="1"><li><p>使用 <code>lsblk</code> 命令查看磁盘分区</p></li><li><p>使用 <code>blkid /dev/xxx</code> 查看相应分区的 UUID、TYPE 等</p></li><li><p>在 <code>/etc/fstab</code> 文件中添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># &lt;file system&gt;              &lt;mount point&gt;  &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt;</span><br>/dev/disk/by-uuid/xxxxxxxxx     /data        ext4   defaults   0      0<br></code></pre></td></tr></table></figure><p>事实上，第一个字段可以不用 uuid 而直接写名称（如 <code>/dev/nvme1n1p1</code>），用 uuid 的好处是如果磁盘顺序交换导致名称变了，依然能够挂载上正确的磁盘。</p></li></ol><h3 id="交换启动顺序">交换启动顺序</h3><p>在 BIOS boot 中把 Ubuntu 放到 Windows 前面，以通过 Ubuntu 的启动菜单选择要进入的系统，如下图所示：</p><p><img src="boot.png" width=70% /></p><h3 id="远程开机wakeonlan">远程开机——WakeOnLAN</h3><ol type="1"><li><p>本机安装 ethtool：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt install ethtool -y<br></code></pre></td></tr></table></figure></li><li><p>查看本机网卡接口名称和 MAC 地址：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ifconfig -a<br></code></pre></td></tr></table></figure><p>本机有线网卡接口名为 <code>enp5s0</code>（下面的 <code>wol.service</code> 中要用），MAC 地址保存在如下行内：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ether xx:xx:xx:xx:xx txqueuelen 100 (Ethernet)<br></code></pre></td></tr></table></figure></li><li><p>查看本机 WakeOnLAN 状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo ethtool enp5s0 | grep -i <span class="hljs-string">&quot;wake&quot;</span>   <span class="hljs-comment"># g 启用，d 未启用</span><br></code></pre></td></tr></table></figure></li><li><p>本机配置开机启动 ethtool 服务：</p><p>编辑 <code>/etc/systemd/system/wol.service</code> 文件如下：</p><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">[Unit]<br>Description=Configure Wake On LAN<br><br>[Service]<br>Type=oneshot<br>ExecStart=/sbin/ethtool -s INTERFACE wol g  //INTERFACE 改为 enp5s0<br><br>[Install]<br>WantedBy=basic.target<br></code></pre></td></tr></table></figure></p><p>然后设置开机启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo systemctl daemon-reload<br>sudo systemctl <span class="hljs-built_in">enable</span> wol.service<br>sudo systemctl start wol.service<br></code></pre></td></tr></table></figure></li></ol><h3 id="一些小问题">一些小问题</h3><ol type="1"><li><p>开机时在 <code>A start job is running for Wait for Network to be Configured</code> 消息提示下卡 2min</p><p>经搜索得知这是 <code>systemd-networkd-wait-online</code> 服务在检查网络接口，根据<a href="http://www.jinbuguo.com/systemd/systemd-networkd-wait-online.service.html">手册</a>所述：</p><blockquote><p><code>systemd-networkd-wait-online</code> 是一个一次性系统服务(参见 <a href="http://www.jinbuguo.com/systemd/systemd.service.html#">systemd.service(5)</a>)，用于等待网络连线成功(可以对外通信)。 默认情况下，它会一直等待到所有被其监视且由 <a href="http://www.jinbuguo.com/systemd/systemd-networkd.service.html#">systemd-networkd.service(8)</a> 管理的网络接口连线成功或者超时失败，并且至少有一个连接可以对外通信。</p></blockquote><p>由于我在装系统时配置了有线网络，但是暂时没有连接，所以系统会一直检查直至超时。</p></li><li><p>禁用 cloud-init</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo <span class="hljs-built_in">touch</span> /etc/cloud/cloud-init.disabled<br></code></pre></td></tr></table></figure></li></ol><h2 id="后续维护">后续维护</h2><h3 id="连接新网络有线-wifi">连接新网络（有线 / WiFi）</h3><blockquote><p>参考资料：</p><p>https://blog.csdn.net/u014752296/article/details/127784922</p><p>https://linuxconfig.org/ubuntu-22-04-connect-to-wifi-from-command-line</p><p>https://blog.csdn.net/hanweiwallywang/article/details/122646206</p></blockquote><p>早期的 Ubuntu 用的是 <code>/etc/network/interface</code> 文件来配置网络，从 17.10 版本开始使用 NetPlan，通过 <code>/etc/netplan/</code> 下的 yaml 文件来配置网络，非常方便。</p><ol type="1"><li><p>用 <code>ifconfig</code> 或 <code>ls /sys/class/net</code> 命令查看有线/无线接口名称，比如我的有线接口是 <code>enp5s0</code> ，无线接口是 <code>wlo1</code>；</p></li><li><p>[WiFi] [Optional] 搜索可连接的 WiFi，找要连接的 WiFi 名称（SSID）；</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo iw dev wlan scan  <span class="hljs-comment"># wlan 换成自己的无线接口名称</span><br></code></pre></td></tr></table></figure><p>当然，如果你本来就知道 WiFi 的名称可跳过这一步。</p></li><li><p>打开 <code>/etc/netplan/xxxxxxxxxxxx.yaml</code> 配置文件，如果之前在系统安装过程中已经配置有网络，文件中应该会有当时配置的内容。</p><p>以连接新 WiFi 为例，若使用 DHCP 动态获取 IP，则添加内容（或覆盖原来内容）：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">network:</span><br>  <span class="hljs-attr">version:</span> <span class="hljs-number">2</span><br>  <span class="hljs-attr">wifis:</span><br>    <span class="hljs-attr">w1o1:</span><br>      <span class="hljs-attr">access-points:</span><br>        <span class="hljs-attr">&quot;xxxxxxxx&quot;:</span>  <span class="hljs-comment"># WiFi Name (SSID)</span><br>          <span class="hljs-attr">password:</span> <span class="hljs-string">&quot;xxxxxxxxx&quot;</span><br>      <span class="hljs-attr">dhcp4:</span> <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><p>如果使用静态 IP，则添加内容（或覆盖原来内容）：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">network:</span><br>  <span class="hljs-attr">version:</span> <span class="hljs-number">2</span><br>  <span class="hljs-attr">wifis:</span><br>    <span class="hljs-attr">w1o1:</span><br>      <span class="hljs-attr">access-points:</span><br>        <span class="hljs-attr">&quot;xxxxxxxx&quot;:</span>  <span class="hljs-comment"># WiFi Name (SSID)</span><br>          <span class="hljs-attr">password:</span> <span class="hljs-string">&quot;xxxxxxxxx&quot;</span><br>      <span class="hljs-attr">dhcp4:</span> <span class="hljs-literal">no</span><br>      <span class="hljs-attr">addresses:</span> [<span class="hljs-string">&quot;xxxxxxxx/xx&quot;</span>]    <span class="hljs-comment"># IP地址/子网掩码位数</span><br>      <span class="hljs-attr">gateway4:</span> <span class="hljs-string">&quot;xxxxxxxxx&quot;</span>         <span class="hljs-comment"># 网关</span><br>      <span class="hljs-attr">nameservers:</span><br>        <span class="hljs-attr">addresses:</span> [<span class="hljs-string">&quot;xxxxxxxx&quot;</span>]     <span class="hljs-comment"># 域名解析服务器</span><br></code></pre></td></tr></table></figure></li><li><p>Debug 检查配置格式是否有错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo netplan --debug try<br></code></pre></td></tr></table></figure></li><li><p>若无错误，应用新的配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo netplan apply<br></code></pre></td></tr></table></figure></li><li><p><code>ifconfig</code> 或 <code>ip a</code> 查看配置结果，如果配置成功应该能看到 IP 地址。</p></li></ol><h3 id="添加新硬盘">添加新硬盘</h3><p>操作和我们安装系统时设置开机自动挂载硬盘是一样的。</p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
      <tag>linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>连续时间扩散模型——SDE与ODE描述</title>
    <link href="/blog-main/2022/12/04/%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94SDE%E4%B8%8EODE%E6%8F%8F%E8%BF%B0/"/>
    <url>/blog-main/2022/12/04/%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94SDE%E4%B8%8EODE%E6%8F%8F%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\x}{\mathbf x}\newcommand{\z}{\mathbf z}\newcommand{\E}{\mathbb E}\newcommand{\f}{\mathbf f}\newcommand{\w}{\mathbf w}\newcommand{\calN}{\mathcal N}\newcommand{\pdata}{p_\text{data}}\]</span></p><h2 id="brief-introduction">Brief Introduction</h2><p>我们已经用两篇文章<a href="/blog-main/2022/09/29/%E4%BB%8EVAE%E5%88%B0DDPM/" title="从VAE到DDPM">从VAE到DDPM</a>、<a href="/blog-main/2022/10/13/Score-Based-Generative-Models/" title="Score-Based Generative Models">Score-Based Generative Models</a> 分别介绍了 DDPM (Denoising Diffusion Probabilistic Models) 和 SMLD (Score Matching + Langevin Dynamics)，并说明了它们的本质其实是相同的。进一步，论文<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, Yang, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. In *International Conference on Learning Representations*. 2020.">[1]</span></a></sup>把它们扩展到了连续时间场景并使用<strong>随机微分方程（Stochastic Differential Equations, SDE）</strong>描述。在该视角下，DDPM 和 SMLD 是两个特殊形式 SDE 的离散化。DDPM / SMLD 的加噪过程对应 forward SDE，去噪过程对应 reverse SDE，解 reverse SDE 的过程就是生成过程。除了使用通用的 SDE 数值解法以外，作者根据问题特点提出了 Predictor-Corrector Samplers，统一并扩展了 DDPM / SMLD 的去噪过程。作者还构造了与 SDE 具有相同边缘分布的 ODE，从而得到确定性生成过程。</p><h3 id="ddpm-recap">DDPM Recap</h3><ol type="1"><li><p>加噪过程（前向过程）</p><p>DDPM 将前向过程构建为一个离散马尔可夫链 <span class="math inline">\(\{\x_0,\x_1,\ldots,\x_N\}\)</span>，转移概率为： <span class="math display">\[p(\x_i\vert \x_{i-1})=\calN\left(\x_i;\sqrt{1-\beta_i}\x_{i-1},\beta_i\mathbf I\right)\]</span> 其中 <span class="math inline">\(0&lt;\beta_1,\beta_2,\ldots,\beta_N&lt;1\)</span> 是每一步转移的正态分布的方差。上式等价写作： <span class="math display">\[\x_i=\sqrt{1-\beta_i}\x_{i-1}+\sqrt{\beta_i}\z_{i-1},\quad i=1,\ldots,N \tag{1}\label{ddpm}\]</span> 在该转移概率下，我们有一个优秀的性质： <span class="math display">\[p_{\alpha_i}(\x_i\vert \x_0)=\calN\left(\x_{i};\sqrt{\bar\alpha_i}\x_0,(1-\bar\alpha_i)\mathbf I\right)\tag{2}\label{ddpm0}\]</span> 其中 <span class="math inline">\(\bar\alpha_i=\prod_{j=1}^i(1-\beta_j)\)</span>.</p></li><li><p>去噪过程（逆向过程）</p><p>去噪过程是前向过程的逆向，若以 score function 为目标构建模型 <span class="math inline">\(s_\theta(\x_i,i)\)</span>，则： <span class="math display">\[\begin{align}p_\theta(\x_{i-1}\vert\x_i)=\calN\left(\x_{i-1};\tfrac{1}{\sqrt{1-\beta_i}}(\x_i+\beta_is_\theta(\x_i,i)),\beta_i\mathbf I\right)\end{align}\]</span> 或写作： <span class="math display">\[\x_{i-1}=\frac{1}{\sqrt{1-\beta_i}}(\x_i+\beta_i s_\theta(\x_i,i))+\sqrt{\beta_i}\z_i,\quad i=N,N-1,\ldots,1 \tag{3}\label{reverse-ddpm}\]</span></p></li><li><p>损失函数由 ELBO 推得： <span class="math display">\[\mathcal L=\sum_{i=1}^N(1-\alpha_i)\E_{\pdata(\x)}\left[\E_{p_{\alpha_i}(\tilde\x\vert \x)}\left[\left\|s_\theta(\tilde\x,i)-\nabla_{\tilde\x}\log p_{\alpha_i}(\tilde\x\vert\x)\right\|^2\right]\right]\]</span></p></li></ol><h3 id="smld-recap">SMLD Recap</h3><ol type="1"><li><p>加噪过程</p><p>与 DDPM 不同，SMLD 首先定义了从原数据到以 <span class="math inline">\(\sigma_i\)</span> 加噪的变换： <span class="math display">\[p_{\sigma_i}(\x_i\vert \x_0)=\calN(\x_i;\x_0,\sigma_i^2\mathbf I)\tag{4}\label{smld0}\]</span> 逆向思考，该分布可由如下相邻两个 noise scale 的转移得到： <span class="math display">\[p(\x_i\vert\x_{i-1})=\calN\left(\x_{i-1};\left(\sigma_i^2-\sigma_{i-1}^2\right)\mathbf I\right)\]</span> 或写作： <span class="math display">\[\x_i=\x_{i-1}+\sqrt{\sigma_i^2-\sigma_{i-1}^2}\ \z_{i-1},\quad i=1,\ldots,N \tag{5}\label{smld}\]</span></p></li><li><p>去噪过程（Annealed Langevin Dynamics）</p><p>在每个 noise scale 加噪的数据分布上，执行 <span class="math inline">\(M\)</span> 步 Langevin dynamics： <span class="math display">\[\x_i^m=\x_i^{m-1}+\epsilon_i s_\theta(\x_i^{m-1},\sigma_i)+\sqrt{2\epsilon_i}\ \z_i^m,\quad m=1,\ldots,M\]</span> 其中 <span class="math inline">\(\epsilon_i\)</span> 是步长相关参数。对 <span class="math inline">\(i=N,N-1\ldots,1\)</span> 均执行上述过程。</p></li><li><p>损失函数由在每个 noise scale 上做 denoising score matching 推得： <span class="math display">\[\mathcal L=\sum_{i=1}^N \sigma_i^2\E_{\pdata(\x)}\left[\E_{p_{\sigma_i}(\tilde\x\vert\x)}\left[\left\|s_\theta(\tilde\x,\sigma_i)-\nabla_{\tilde\x}\log p_{\sigma_i}(\tilde\x\vert\x) \right\|^2\right]\right]\]</span></p></li></ol><h2 id="forward-sde">Forward SDE</h2><p>DDPM 和 SMLD 都是在离散时间点扩散的，倘若将时间连续化，那么我们就能够得到对应的 SDE 表达。为了连续化，我们只需要将 <span class="math inline">\(\eqref{ddpm}\)</span> 式和 <span class="math inline">\(\eqref{smld}\)</span> 式写作差分形式，取 <span class="math inline">\(\Delta t\to0\)</span>，即得到微分形式。</p><h3 id="ddpm-vp-sde">DDPM: VP SDE</h3><p>为了方便叙述，定义 <span class="math inline">\(\bar\beta_i=N\beta_i\)</span>，那么 <span class="math inline">\(\eqref{ddpm}\)</span> 式改写作： <span class="math display">\[\x_i=\sqrt{1-\frac{\bar\beta_i}{N}}\ \x_{i-1}+\sqrt{\frac{\bar\beta_i}{N}}\ \mathbf z_{i-1},\quad i=1,\ldots,N\]</span> 设 <span class="math inline">\(\beta(\frac{i}{N})=\bar\beta_i,\,\x(\frac{i}{N})=\x_i,\,\z(\frac{i}{N})=\z_i\)</span>，那么对于 <span class="math inline">\(t=\left\{0,\frac{1}{N},\ldots,\frac{N-1}{N}\right\},\,\Delta t=\frac{1}{N}\)</span>，上式可写作： <span class="math display">\[\x(t+\Delta t)=\sqrt{1-\beta(t+\Delta t)\Delta t}\ \x(t)+\sqrt{\beta(t+\Delta t)\Delta t}\ \z(t)\]</span> 当 <span class="math inline">\(\Delta t\)</span> 很小时，根据近似 <span class="math inline">\((1+x)^\alpha\approx 1+\alpha x\)</span>，有： <span class="math display">\[\x(t+\Delta t)\approx\x(t)-\frac{1}{2}\beta(t+\Delta t)\Delta t\ \x(t)+\sqrt{\beta(t+\Delta t)\Delta t}\ \z(t)\]</span> 因此，令 <span class="math inline">\(\Delta t\to 0\)</span>，得到 ： <span class="math display">\[\mathrm  d\x=-\frac{1}{2}\beta(t)\x\ \mathrm dt+\sqrt{\beta(t)}\ \mathrm d\w\tag{6}\label{vp-sde}\]</span> 其中 <span class="math inline">\(\w\)</span> 是标准维纳过程（也称作布朗运动）。</p><div class="note note-info">            <p><strong>维纳过程（布朗运动）直观解释</strong></p><p>考虑一个马尔可夫随机过程 <span class="math inline">\(\w\)</span>，其以一年为时间单位，下一年相比这一年的变化量服从 <span class="math inline">\(\calN(\mathbf 0,\mathbf I)\)</span>，即： <span class="math display">\[\w(t+1)=\w(t)+\z(t),\quad \z(t)\sim\calN(\mathbf 0,\mathbf I)\]</span> 那么两年后，总变化量就是两个正态随机变量之和，服从 <span class="math inline">\(\calN(\mathbf 0,2\mathbf I)\)</span>： <span class="math display">\[\w(t+2)=\w(t)+\sqrt{2}\ \z(t),\quad \z(t)\sim\calN(\mathbf 0,\mathbf I)\]</span> 反过来想，一年的变化量是两个半年变化量之和，所以半年的变化量应该服从 <span class="math inline">\(\calN(\mathbf 0,0.5\ \mathbf I)\)</span>： <span class="math display">\[\w(t+0.5)=\w(t)+\sqrt{0.5}\ \z(t),\quad\z(t)\sim\calN(\mathbf 0,\mathbf I)\]</span> 以此类推，如果考虑 <span class="math inline">\(\Delta t\)</span> 的时间，变化量就应该服从 <span class="math inline">\(\calN(\mathbf 0,\Delta t\ \mathbf I)\)</span>： <span class="math display">\[\w(t+\Delta t)=\w(t)+\sqrt{\Delta t}\ \z(t),\quad\z(t)\sim \calN(\mathbf 0,\mathbf I)\]</span> 令 <span class="math inline">\(\Delta t\to 0\)</span>，则： <span class="math display">\[\mathrm d\w=\lim_{\Delta t\to0}\sqrt{\Delta t}\ \z(t)\]</span></p><p>这就是 <span class="math inline">\(\eqref{vp-sde}\)</span> 式后一项的由来。</p>          </div><p>论文作者称 <span class="math inline">\(\eqref{vp-sde}\)</span> 式为 Variance Preserving (VP) SDE，因为当初始分布有单位方差时，该 SDE 描述的随机过程也始终有固定的方差。为了说明这点，作者直接给出了如下关于方差演进过程的 ODE： <span class="math display">\[\frac{\mathrm d\Sigma_\text{VP}(t)}{\mathrm dt}=\beta(t)(\mathbf I-\Sigma_\text{VP}(t))\]</span> 解得： <span class="math display">\[\Sigma_\text{VP}(t)=\mathbf I+e^{\int_0^t -\beta(s)\mathrm ds}(\Sigma_\text{VP}(0)-\mathbf I)\]</span> 因此，当 <span class="math inline">\(\Sigma_\text{VP}(0)=\mathbf I\)</span> 时，始终有 <span class="math inline">\(\Sigma_\text{VP}(t)=\mathbf I\)</span>. 另外，即便 <span class="math inline">\(\Sigma_\text{VP}(0)\neq\mathbf I\)</span>，当 <span class="math inline">\(t\to\infty\)</span> 时，<span class="math inline">\(e^{\int_{0}^t-\beta(s)\mathrm ds}\to0\)</span>，也有 <span class="math inline">\(\Sigma_\text{VP}(t)\to\mathbf I\)</span>.</p><p>在上一篇文章的最后一节我们也对此做了解释——从 DDPM 的角度看，这无非就是说扩散过程最后会服从 <span class="math inline">\(\calN(\mathbf 0,\mathbf I)\)</span>.</p><blockquote><p>什么？你问这个 ODE 怎么来的？我也不会啊……</p></blockquote><h3 id="smld-ve-sde">SMLD: VE SDE</h3><p>类似地，设 <span class="math inline">\(\sigma(\frac{i}{N})=\sigma_i,\,\x(\frac{i}{N})=\x_i,\,\z(\frac{i}{N})=\z_i\)</span>，那么对于 <span class="math inline">\(t=\left\{0,\frac{1}{N},\ldots,\frac{N-1}{N}\right\},\,\Delta t=\frac{1}{N}\)</span>，<span class="math inline">\(\eqref{smld}\)</span> 式可写作： <span class="math display">\[\x(t+\Delta t)=\x(t)+\sqrt{\sigma^2(t+\Delta t)-\sigma^2(t)}\ \mathbf z(t)\]</span> 当 <span class="math inline">\(\Delta t\)</span> 很小时，根据一阶泰勒展开 <span class="math inline">\(f(t+\Delta t)\approx f(t)+\frac{\mathrm df(t)}{\mathrm dt}\Delta t\)</span>，有： <span class="math display">\[\x(t+\Delta t)\approx\x(t)+\sqrt{\frac{\mathrm  d[\sigma^2(t)]}{\mathrm dt}\Delta t}\ \z(t)\]</span> 令 <span class="math inline">\(\Delta t\to 0\)</span>，得到： <span class="math display">\[\mathrm d\x=\sqrt{\frac{\mathrm d[\sigma^2(t)]}{\mathrm dt}}\mathrm d\w \tag{7}\label{ve-sde}\]</span> 该 SDE 被作者称为 Variance Exploding (VE) SDE，因为随着 <span class="math inline">\(t\to\infty\)</span>，其描述的随机过程方差将无限制增大。</p><h3 id="general-form">General Form</h3><p>上文我们分别推导了 DDPM 和 SMLD 对应的 SDE 形式。更一般地，对于一个连续时间的扩散过程 <span class="math inline">\(\{\x(t)\}_{t=0}^T\)</span>，其中 <span class="math inline">\(\x(0)\sim p_0\)</span> 并且 <span class="math inline">\(\x(T)\sim p_T\)</span>，即 <span class="math inline">\(p_0\)</span> 表示数据分布，<span class="math inline">\(p_T\)</span> 表示先验分布，我们可以用 Itô diffusion SDE 来描述这个扩散过程： <span class="math display">\[\mathrm d\x= \f(\x,t)\mathrm dt+g(t)\mathrm d\w \tag{8}\label{sde}\]</span> 其中 <span class="math inline">\(\f(\x,t):\mathbb R^d\to\mathbb R^d\)</span> 是一个向量值函数，称作 drift coefficient；<span class="math inline">\(g(t):\mathbb R\to\mathbb R\)</span> 是一个标量函数，称作 diffusion coefficient. 特别地，对于 DDPM 而言，<span class="math inline">\(\f(\x,t)=-\frac{1}{2}\beta(t)\x,\ g(t)=\sqrt{\beta(t)}\)</span>；对于 SMLD 而言，<span class="math inline">\(\f(\x,t)=\mathbf 0,\ g(t)=\sqrt{\frac{\mathrm d[\sigma^2(t)]}{\mathrm dt}}\)</span>. 因此我们在引言里说，DDPM 和 SMLD 分别是两个特殊形式 SDE 的离散化。</p><h2 id="reverse-sde">Reverse SDE</h2><p>同 DDPM 类似，forward SDE 将数据 <span class="math inline">\(\x(0)\sim p_0\)</span> 变换为了先验分布 <span class="math inline">\(\x(T)\sim p_T\)</span>，而 reverse SDE 将该过程逆向，即可得到生成模型。</p><p>记 <span class="math inline">\(p_t(\x)\)</span> 为 <span class="math inline">\(\x(t)\)</span> 的概率分布，<span class="math inline">\(p_{st}(\x(t)\vert\x(s))\)</span> 为从 <span class="math inline">\(\x(s)\)</span> 到 <span class="math inline">\(\x(t)\)</span> 的转移概率分布。对于 <span class="math inline">\(\eqref{sde}\)</span> 式，根据论文<sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><span class="hint--top hint--rounded" aria-label="Anderson, Brian DO. Reverse-time diffusion equation models. *Stochastic Processes and their Applications* 12, no. 3 (1982): 313-326.">[9]</span></a></sup>的结论，其 reverse-time SDE 为： <span class="math display">\[\mathrm d\x=\left[\f(\x,t)-g^2(t)\nabla_\x\log p_t(\x)\right]\mathrm  dt+g(t)\mathrm d\bar\w \tag{9}\label{reverse-sde}\]</span> 其中 <span class="math inline">\(\bar\w\)</span> 是一个标准维纳过程（布朗运动）。</p><p><br/></p><p>我们又看到了老朋友 score function <span class="math inline">\(\nabla_\x\log p_t(\x)\)</span>，依旧使用 denoising score matching 来训练模型： <span class="math display">\[\E_t\left[\lambda(t)\E_{\x(0)}\E_{\x(t)\vert\x(0)}\left[\left\|s_\theta(\x(t),t)-\nabla_{\x(t)}\log p_{0t}(\x(t)\vert\x(0)) \right\|_2^2\right]\right] \tag{10}\label{loss}\]</span> 其中 <span class="math inline">\(\lambda:[0,T]\to \mathbb R_{&gt;0}\)</span> 是加权系数。与 DDPM / SMLD 类似，可以选取 <span class="math inline">\(\lambda(t)\propto1/\E[\Vert\nabla_{\x(t)}\log p_{0t}(\x(t)\vert\x(0)) \Vert^2]\)</span>.</p><p>在训练好模型之后，用模型替换 <span class="math inline">\(\eqref{reverse-sde}\)</span> 式中的 score function 即可： <span class="math display">\[\mathrm d\x=\left[\f(\x,t)-g^2(t)s_\theta(\x,t)\right]\mathrm  dt+g(t)\mathrm d\bar\w \tag{9&#39;}\label{reverse-sde-est}\]</span></p><h2 id="solving-the-reverse-sde">Solving the Reverse SDE</h2><p>为了生成数据，我们随机采样 <span class="math inline">\(\x(T)\sim p_T\)</span>，通过求解 <span class="math inline">\(\eqref{reverse-sde-est}\)</span> 式得到 <span class="math inline">\(\x(0)\)</span>. 作者首先指出，我们可以使用常见的通用数值求解方法解该 SDE；随后考虑到问题的特殊性，提出了 Predictor-Corrector 方法提高性能。</p><h3 id="numerical-sde-solvers">Numerical SDE Solvers</h3><p>存在一些广泛适用的 SDE 数值解法，例如 <a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Maruyama_method">Euler-Maruyama</a> 和 <a href="https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_method_(SDE)">stochastic Runge-Kutta</a>，它们分别是求解 ODE 的 Euler 法和 Runge-Kutta 法在 SDE 上的扩展。</p><div class="note note-info">            <p><strong>Euler 法回顾</strong>（摘自《计算方法》课程笔记）</p><p>对于初值问题： <span class="math display">\[\begin{cases}\frac{\mathrm dy}{\mathrm dx}=f(x,y)&amp;&amp;a\leq x\leq b\\y(a)=\eta\end{cases}\]</span> 数值求解方法是指：取定步长 <span class="math inline">\(h\)</span>，在一系列点 <span class="math inline">\(\{x_0=a,\,x_1=a+h,\,x_2=a+2h,\,\ldots\}\)</span> 上求 <span class="math inline">\(y_i=y(x_i)\)</span>.</p><p><strong>Euler 法</strong>是最简单的数值求解方法之一。考虑在 <span class="math inline">\([x_n,x_{n+1}]\)</span> 上对 <span class="math inline">\(y&#39;(x)=f(x,y(x))\)</span> 积分，有： <span class="math display">\[\begin{align}y(x_{n+1})&amp;=y(x_n)+\int_{x_n}^{x_{n+1}}f(x,y(x))\mathrm d x\\&amp;\approx y(x_n)+ hf(x_n,y(x_n))&amp;&amp;\text{左矩形近似积分}\\&amp;=y_n+ hf(x_n,y_n)\end{align}\]</span> 以此公式迭代求解即可。若做出图来，<span class="math inline">\(y_i\)</span> 其实就是不断沿当前导数方向走折线，因此也称作 <strong>Euler 折线法</strong>。</p><p><img src="euler.png" alt="source:https://www.researchgate.net/figure/Eulers-method-The-dashed-line-shows-the-solution-computed-by-successive-iterations-of_fig1_333678760" width=50% /></p><p>如果用更准确的梯形公式而非矩形来近似积分，可得： <span class="math display">\[y_{n+1}=y_n+\frac{h}{2}[f(x_n,y_n)+f(x_{n+1},y_{n+1})]\]</span> 但这是关于 <span class="math inline">\(y_{n+1}\)</span> 的隐格式，无法直接求解。故一般先使用 Euler 折线法求出一个初始值，再依梯形公式迭代。</p><p>特别地，如果只迭代一次，称作<strong>改进 Euler 法</strong>： <span class="math display">\[\begin{cases}y_{n+1}^{(0)}=y_n+h f(x_n,y_n)\\y_{n+1}=y_{n+1}^{(0)}+\frac{h}{2}\left[f(x_n,y_n)+f\left(x_{n+1},y_{n+1}^{(0)}\right)\right]\end{cases}\]</span></p>          </div><div class="note note-info">            <p><strong>Runge-Kutta 法回顾</strong>（摘自《计算方法）课程笔记）</p><p>理论上，<span class="math inline">\(y(x_{n+1})\)</span> 和 <span class="math inline">\(y(x_n)\)</span> 的差距由泰勒展开给出： <span class="math display">\[y(x_{n+1})=y(x_n)+h y&#39;(x_n)+\frac{h^2}{2!}y&#39;&#39;(x_n)+\cdots\]</span> 如果允许，我们可以截断到任意精度，让解更加准确。Euler 法只有 <span class="math inline">\(h f(x_n,y_n)\)</span> 一项，其实是截断到一阶；利用泰勒展开可以证明，改进 Euler 法截断到了二阶。然而直接计算高阶导数非常麻烦，Runge-Kutta 法提供了一种巧妙的构造，间接地利用了这种思想。</p><p>我们可以将 Euler 法写作： <span class="math display">\[\begin{cases}y_{n+1}=y_n+h K_1\\K_1=f(x_n,y_n)\end{cases}\]</span> 改进 Euler 法写作： <span class="math display">\[\begin{cases}y_{n+1}=y_n+\frac{1}{2}hK_1+\frac{1}{2}hK_2\\K_1=f(x_n,y_n)\\K_2=f(x_n+h,y_n+hK_1)\end{cases}\]</span> 以此类推，<span class="math inline">\(s\)</span> 阶 R-K 法写作： <span class="math display">\[\begin{cases}y_{n+1}=y_n+h\sum_{i=1}^s R_iK_i\\K_1=f(x_n,y_n)\\K_i=f(x_n+a_ih,y_n+h\sum_{j=1}^{i-1}b_{ij}K_j),&amp;&amp;i=2,3,\ldots,s\end{cases}\]</span> 其中 <span class="math inline">\(R_i,\,a_i,\,b_{ij}\)</span> 为参数，以使得 <span class="math inline">\(y(x_{n+1})-y_{n+1}\)</span> 泰勒展开后 <span class="math inline">\(h^i\)</span> 系数为 <span class="math inline">\(0\)</span> (<span class="math inline">\(i=1,2,\ldots,s\)</span>)，即截断到 <span class="math inline">\(s\)</span> 阶。</p>          </div><p>Euler-Maruyama 法和 Euler 法类似，只需要从初值开始，选定步长反复迭代即可。具体而言，对于 <span class="math inline">\(\eqref{reverse-sde-est}\)</span> 式，选择一个小步长 <span class="math inline">\(\Delta t\approx 0\)</span>，初始化 <span class="math inline">\(t\gets T\)</span>，然后迭代执行下述过程直至 <span class="math inline">\(t\approx 0\)</span>： <span class="math display">\[\begin{align}\Delta \mathbf{x} &amp;\gets \left[\mathbf{f}(\mathbf{x}, t) - g^2(t) \mathbf{s}_\theta(\mathbf{x}, t)\right]\Delta t + g(t) \sqrt{\vert \Delta t\vert }\mathbf{z}_t ,&amp;&amp;\z_t\sim \calN(\mathbf 0,\mathbf I)\\ \mathbf{x} &amp;\gets \mathbf{x} + \Delta \mathbf{x}\\ t &amp;\gets t + \Delta t\end{align}\]</span></p><h3 id="reverse-diffusion-samplers">Reverse Diffusion Samplers</h3><p>除了使用 Euler-Maruyama 等通用 SDE solver 以外，我们还有其他求解 reverse SDE <span class="math inline">\(\eqref{reverse-sde-est}\)</span> 式的做法吗？在第二节中，我们将前向过程连续化得到 forward SDE，那反过来，把 reverse SDE 离散化不就得到逆向过程了吗？ <span class="math display">\[\begin{align}&amp;\mathrm d\x=\left[\f(\x,t)-g^2(t)s_\theta(\x,t)\right]\mathrm  dt+g(t)\mathrm d\bar\w \tag{9&#39;}\\&amp;\text{(discretization) }\Bigg\Downarrow\\&amp;\x_i=\x_{i+1}-\f_{i+1}(\x_{i+1})+g_{i+1}^2 s_\theta(\x_{i+1},i+1)+g_{i+1}\z_{i+1} \tag{10}\label{reverse-diffusion}\end{align}\]</span> 作者将这种方法称作 reverse diffusion samplers.</p><h3 id="predictor-corrector-samplers">Predictor-Corrector Samplers</h3><p>与一般的 SDE 不同的是，我们现在有一个 score-based model <span class="math inline">\(s_\theta(\x,t)\approx\nabla_\x\log p_t(\x)\)</span>，可以利用 score-based MCMC 方法来改进通用 SDE solvers 的解。具体而言，每一步首先由通用 SDE solvers 给出一个解（predictor），然后由 MCMC 方法来修正解的分布（corrector），因此作者称该方法为 Predictor-Corrector (PC) samplers. 特别地，DDPM 相当于用 ancestral sampling 作为 predictor、identity function 作为 corrector；而 SMLD 相当于用 identity function 作为 predictor、Langevin dynamics 作为 corrector.</p><h2 id="probability-flow-ode">Probability Flow ODE</h2><p>上文中，我们已经构建了连续时间扩散模型的 SDE 描述。宋飏等人提出，对于任一 SDE 描述的扩散过程，都存在一个 ODE 描述的确定性过程与之有相同的边缘分布 <span class="math inline">\(\{p_t(\x)\}_{t=0}^T\)</span>. 具体地，<span class="math inline">\(\eqref{reverse-sde}\)</span> 式对应的 ODE 为： <span class="math display">\[\mathrm d\x=\left[\f(\x,t)-\frac{1}{2}g^2(t)\nabla_\x\log p_t(\x)\right]\mathrm dt \tag{11}\label{reverse-ode}\]</span> 同上一节类似，我们可以用常用的 RK45 来解上式，也可以将其离散化为 <span class="math inline">\(\x_{i}=\x_{i+1}-\f_{i+1}(\x_{i+1})+\frac{1}{2}g_{i+1}^2s_\theta(\x_{i+1},i+1)\)</span> 求解，predictor-corrector samplers 的思想也依然适用。但是作者指出，尽管解 ODE 比解 SDE 更快，其生成图片的质量往往不如 SDE.</p><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Song, Yang, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. In <em>International Conference on Learning Representations</em>. 2020. <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Yang Song. Generative Modeling by Estimating Gradients of the Data Distribution. https://yang-song.net/blog/2021/score/ <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>扩散模型与能量模型，Score-Matching和SDE，ODE的关系 - 中森的文章 - 知乎 https://zhuanlan.zhihu.com/p/576779879 <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>苏剑林. (Aug. 03, 2022). 《生成扩散模型漫谈（五）：一般框架之SDE篇 》[Blog post]. Retrieved from https://kexue.fm/archives/9209 <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>苏剑林. (Aug. 08, 2022). 《生成扩散模型漫谈（六）：一般框架之ODE篇 》[Blog post]. Retrieved from https://kexue.fm/archives/9228 <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Wiener Process and Ito's Lemma - 无情小超超的文章 - 知乎 https://zhuanlan.zhihu.com/p/148808235 <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Score-based Generative Models总结 - o00O00o的文章 - 知乎 https://zhuanlan.zhihu.com/p/583666759 <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Tanmaya Shekhar Dabral. Stochastic Differential Equations and Diffusion Models. https://www.vanillabug.com/posts/sde/ <a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>Anderson, Brian DO. Reverse-time diffusion equation models. <em>Stochastic Processes and their Applications</em> 12, no. 3 (1982): 313-326. <a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>Diffusion Models</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>保研回忆录</title>
    <link href="/blog-main/2022/11/30/2022%E4%BF%9D%E7%A0%94%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    <url>/blog-main/2022/11/30/2022%E4%BF%9D%E7%A0%94%E5%9B%9E%E5%BF%86%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<div class="note note-primary">            <p>话说我本来都想把保研回忆录给咕咕咕了，但是学弟突然邀请我做个保研分享，遂借此机会把坑给填上。</p>          </div><h2 id="个人情况">1 个人情况</h2><h3 id="基本情况">1.1 基本情况</h3><ul><li>本科学校：C9 计算机专业</li><li>专业排名：3/300+ (Top 1%)</li><li>荣誉奖项：一次国家奖学金</li><li>竞赛奖项：ICPC 区域赛银，CCPC 区域赛银，数学竞赛省一，数学建模国赛省一</li><li>科研经历：夏令营期间有两个课题正在进行之中，无论文产出</li><li><strong>最终去向：中科院计算所 学硕</strong></li></ul><h3 id="目标定位">1.2 目标定位</h3><ul><li>院校：冲清北本部，但自知概率较小；确保华五 / 人大 / 计算所 / 自动化所能拿到 offer；本校仅作兜底；不考虑同层次其他学校。</li><li>方向：AI，计算机视觉（没错，就是最卷的方向中最卷的领域……）</li><li>学位：学硕 / 强组直博</li><li>导师：人品最为重要，其次是方向和课题组的学术水平</li></ul><h2 id="关键时间点">2 关键时间点</h2><h3 id="准备基本材料">2.1 准备基本材料</h3><ol type="1"><li><p><strong>简历</strong>：一页 A4 纸，尽量简洁明了。具体内容因人而异，我分了以下几栏，<u>仅供参考</u>：</p><ul><li>基本信息（我甚至放了个人博客链接和 GitHub 链接，嘿嘿）</li><li>教育背景（学校、专业、成绩、排名、四六级）</li><li>个人荣誉（国家奖学金等）</li><li>竞赛获奖（挑重量级的，太拉垮的不好意思放……）</li><li>科研项目（可以附上一句话简述工作内容）</li><li>课程项目（<del>页面不够拿来凑数</del>）</li><li>社会实践（<del>页面不够拿来凑数</del>）</li></ul><p>我没有找模板，就自己用 word 随便画了画：</p><p><img src="cv.png" width=70% /></p></li><li><p><strong>个人陈述</strong>：介绍学术背景、科研兴趣方向以及对今后学习研究工作的设想和计划。</p><p>建议提前准备多个版本——1500字、1000字、500字。我是先写了 1500 字的，然后在此基础上删减得到其他版本。</p></li><li><p><strong>套磁邮件</strong>：注意给学校、导师姓名、导师方向留出空位，用其他颜色标出来方便改动。不建议用 qq 邮箱。</p></li><li><p><strong>推荐信</strong>：大多数学校要求 2 封副教授及以上职称的推荐信。平时有熟络的老师最好了，没有就厚着脸皮去要（逃）。一般自己先写一份大致的，然后给老师修改。</p></li><li><p><strong>自我介绍</strong>：面试用，中英文都要准备，2 min 左右吧。</p></li><li><p><strong>其他材料（成绩单、排名证明、证书扫描件等）</strong>：按照夏令营要求提供即可。</p></li><li><p><strong>复试 ppt</strong>：把简历 / 个人陈述的内容做成 ppt 就好。</p></li></ol><h3 id="套磁导师35月">2.2 套磁导师（3～5月）</h3><p>通过学校官网、Google scholar、导师评价网等多方面渠道查找感兴趣的导师信息，发邮件联系。如果导师对你感兴趣，可能会和你约时间聊聊天啥的。</p><p>不过就我的经验来说，发出去的邮件很有可能石沉大海。我发的有点晚，5 月份才开始发，发了 14 封左右，1 个导师加了微信、1 个让填申请表、2 个名额已满拒了、其他全都没消息。所以导师没回邮件的话大家不要气馁（才发现「馁」居然读 něi 而不是 lěi）。</p><p>一般认为，不要同时给同校的老师发邮件，不然可能发生如下对话：</p><blockquote><p>A 老师：诶，B 老师啊，一起吃饭去？</p><p>B 老师：好啊。话说今天收到个 xxx 学校学生的邮件，想读我的研究生。</p><p>A 老师：好巧，我也收到了一个 xxx 学校学生的邮件。</p><p>B 老师：啊？不会是同一个人吧？</p><p>A 老师：嘿！还真是同一个人诶！</p><p>（以上均为脑补，如有雷同，纯属巧合）</p></blockquote><p>这就有点尴尬……所以最好等到前一个老师拒绝了或者好几天都没回复，再给同校的另一个老师发。</p><p>题外话，同研究领域的圈子真的很小，你能想象我发现我联系过的 A 老师给另一个学校的 B 老师的朋友圈点赞之后的表情吗……</p><h3 id="夏令营报名56月">2.3 夏令营报名（5～6月）</h3><p>关注各院校官网的夏令营报名通知，按要求提交相应材料。</p><p>建议列一个 excel 表写明截止时间、通知链接、所需材料，按照截止时间排序以免漏掉。比如：</p><p><img src="table1.png" /></p><h3 id="夏令营进行78月">2.4 夏令营进行（7～8月）</h3><p>有些学校的时间会冲突，所以要做一定取舍。我们这届由于疫情原因全部线上进行，省去了线下到处跑的时间，所以冲突的不是很多。</p><p>机试 / 面试需要安静无人的环境，可以向学校借空教室，记得门口贴个告示以免有人误入。</p><p>同样建议列一个 excel 表，组织成日历的形式，这样冲突情况一目了然，也方便安排时间。比如：</p><p><img src="table2.png" /></p><p>详细情况放在下一节。</p><h3 id="预推免89月">2.5 预推免（8～9月）</h3><p>由于我在夏令营已经拿到了满意的 offer，并且计算所没有鸽学生的先例，所以预推免就直接躺平没有参加了。</p><p>不过听说预推免机会还是很多的，如果夏令营没有拿到满意的 offer 或者想再冲一冲更好的学校，还是建议参加。</p><h3 id="接受录取通知9月28日起">2.6 接受录取通知（9月28日起）</h3><p>大部分学校 9 月 28 日中午在推免系统上发出通知，然后你就能看到满天飞的鸽子。被鸽穿的学校只能沿着 waiting list 往下一个个打电话，对学生来说却是一个捡漏的好时机。</p><p>与大多数学校火急火燎形成鲜明对比，计算所非常佛系，一直到 30 日才发通知。群友都在发「我知道你很急，但先别急，先让我急」。可以说，留下来的都是真爱～</p><h2 id="夏令营详细情况">3 夏令营详细情况</h2><div class="note note-warning">            <p>部分学校要求保密夏令营考核的具体内容，我也无意违反。但如果有院校或同学认为以下涉及了保密事项，请联系我，我将第一时间删除相关内容。</p>          </div><table><thead><tr class="header"><th style="text-align: center;"><strong>学校</strong></th><th style="text-align: center;"><strong>院系</strong></th><th style="text-align: center;"><strong>入营</strong></th><th style="text-align: center;">结果</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">清华</td><td style="text-align: center;">叉院</td><td style="text-align: center;">否</td><td style="text-align: center;">-</td></tr><tr class="even"><td style="text-align: center;">清华</td><td style="text-align: center;">贵系</td><td style="text-align: center;">是</td><td style="text-align: center;">机试通过，未参加预推免</td></tr><tr class="odd"><td style="text-align: center;">清华</td><td style="text-align: center;">深研院</td><td style="text-align: center;">是</td><td style="text-align: center;">时间冲突，放弃</td></tr><tr class="even"><td style="text-align: center;">北大</td><td style="text-align: center;">计算机学院</td><td style="text-align: center;">是</td><td style="text-align: center;">寄</td></tr><tr class="odd"><td style="text-align: center;">北大</td><td style="text-align: center;">智能学院</td><td style="text-align: center;">否</td><td style="text-align: center;">-</td></tr><tr class="even"><td style="text-align: center;">人大</td><td style="text-align: center;">高瓴</td><td style="text-align: center;">是</td><td style="text-align: center;">优营，放弃</td></tr><tr class="odd"><td style="text-align: center;">南大</td><td style="text-align: center;">人工智能</td><td style="text-align: center;">是</td><td style="text-align: center;">寄</td></tr><tr class="even"><td style="text-align: center;">南大</td><td style="text-align: center;">计算机</td><td style="text-align: center;">否</td><td style="text-align: center;">-</td></tr><tr class="odd"><td style="text-align: center;">中科院</td><td style="text-align: center;">自动化所</td><td style="text-align: center;">是</td><td style="text-align: center;">优营，放弃</td></tr><tr class="even"><td style="text-align: center;">中科院</td><td style="text-align: center;">计算所</td><td style="text-align: center;">是</td><td style="text-align: center;">通过，<strong>最终去向</strong></td></tr><tr class="odd"><td style="text-align: center;">复旦</td><td style="text-align: center;">计算机</td><td style="text-align: center;">是</td><td style="text-align: center;">时间冲突，放弃</td></tr><tr class="even"><td style="text-align: center;">上交</td><td style="text-align: center;">电院计算机系</td><td style="text-align: center;">是</td><td style="text-align: center;">优营，放弃</td></tr></tbody></table><h3 id="清华贵系">3.1 清华贵系</h3><p>印象里清华要的材料非常少，什么推荐信啥的都不用。但是清华在夏令营是不发 offer 的，仅仅做一场机试，根据机考成绩决定是否获得直接参加9月推免复试的资格。推免复试时可以申请以夏令营的机考成绩代替推免复试的机考成绩。</p><p>机试 3 小时三道题，竞赛风格，所以 OIer / ACMer 有很大的优势。IOI 赛制，即有部分分，并且实时反馈结果，可以说是最友好的赛制了。第一题签到，第二题有难度，第三题只会拿暴力分（我太菜了～）。两天后就能收到反馈邮件。</p><p>值得注意的是，<strong>清深和本部的夏令营是同时进行的，只能二选一</strong>。</p><h3 id="北大计算机">3.2 北大计算机</h3><p>与清华形成鲜明对比的是，北大要的材料特别多。北大似乎是在入营之前，你报名的老师就会先来联系你，然后决定入不入营。我报名的老师（以下称作 Z 老师）本来没名额了，把我推给了另一位老师（P 老师）。结果和 P 老师视频聊过一次后，Z 老师也来视频聊了。聊一次就是两三个小时，内容主要集中在科研理想方面。通过这两次聊天，我真切地感受到北大老师眼界之高远，他们对科研的态度绝不只是发个顶会顶刊啥的，而是要做出真正有影响力、推动领域发展的工作。比如老师和我聊的时候就问我，你觉得人工智能领域最有影响力的工作是什么？给它们排一个 level？以及你想做什么 level 的工作？说实话，我之前还确实考虑过这个问题，但是视野局限在我比较熟悉的视觉领域。总而言之，这两次聊天可谓是「与君一席话，胜读十年书」。</p><p>聊完之后，Z 老师给我发了三篇论文，让我读一读。一篇是 Yoshua Bengio 关于下一代人工智能的高角度思考，一篇是自动驾驶的 BEV 感知，还有一篇是计算神经学的 GPU 计算架构。可惜三个领域我都不熟，确实花了我挺多精力的……最后做了个 ppt，但是也没有汇报。</p><p>后来顺利入营，然后只需要面试一场，没有笔试机试。面试不考察基础题，主要问一些对科研的看法，但是我表现得有点糟糕……感觉没有表达清楚就匆匆结束了，最后不出意料地寄了。</p><h3 id="南大人工智能">3.3 南大人工智能</h3><p>南大应该是只能入一个营，所以我计算机就没入。我的一个同学最后就去了南大的计算机。</p><p>南大人工智能学院有笔试和面试两场。笔试好像是一些数学基础题和专业题，面试是也是问一些数学题。众所周知，南大的机器学习非常强，所以重视数学基础也是很正常的。然而——我那时候没有复习啊！几道概率论直接给我干懵了（事后想起来有一道其实非常简单的……），现场一度很尴尬。果然很快就收到了「很遗憾」的邮件。</p><p>为避免重蹈覆辙，我遂花了两天时间把《概率导论》和《线性代数及其应用》给翻了一遍，试图收集起那些飘渺如烟的记忆……</p><p>据去了南大计算机的同学称，计算机那边笔试考的是很难的 408，得不得分看你运气……</p><h3 id="人大高瓴">3.4 人大高瓴</h3><p>报名的时候，高瓴给出的时间是 7.7～7.14 整整一周，害得我以为铁定和其他学校冲突，后来发现其实只是其中的四天、每天一个时间段而已。笔试内容签了保密协议，但是说实话我现在也记不清了……好像有一些常规的数学题和算法题啥的。面试也比较常规，英文问了个社会问题，然后问了一些简单的专业题，然后谈一谈做过的项目，然后就没了。</p><p>最值得吐槽的一点是，如果你排到了面试组的最后一个——恭喜你，你必须在镜头前啥事儿也不能干坐牢一整个上午，非常影响参营体验（虽然我排得还算靠前，但是我极其同情最后那名同学）！本来以为人大的人文关怀应该多一些的……</p><p>其实高瓴有一个我非常喜欢的老师，可惜我傻乎乎地等到优营名单放出来之后才去联系，然后他说他那边名额几乎已经没了，最后遗憾没有去成，悲。</p><h3 id="上交电院计算机系">3.5 上交电院计算机系</h3><p>非常神奇的是，上交直博只有面试，学硕反而有机试和面试。面试要做 ppt，自我介绍之后就谈了谈做过的项目（然后英文再讲一遍，大雾），好像就没了……我很好奇上交是怎么通过如此简练的面试筛人的。</p><p>优营通知发的挺晚，等它下来的时候我已经接了计算所的 offer 了，遂放弃。</p><h3 id="中科院自动化所">3.6 中科院自动化所</h3><p>自动化所会安排学长/学姐带一个小组，还会搞破冰活动（但是线上也就只能是开视频自我介绍一下了）。缺点就是各种各样的报告讲座属实有点多（不过线上嘛，懂得都懂，但是要截屏签到）。</p><p>最后面试也很常规，问一些数学题、专业题，英文问答一下，谈一谈项目，结束。</p><h3 id="中科院计算所">3.7 中科院计算所</h3><p>终于到计算所了。计算所是以课题组为单位招生的，组与组之间的考核完全独立，所以我只能讲我去的组（VIPL）。计算所入营的 bar 似乎不高，上至清北下至双非都能进。导师拥有很大的权力，<strong>听说</strong>如果事先有所交流并且满意的话，即便没有入营都能把你捞进来。我的经历正好相反，我没有事先联系任何一个老师，结果面完就立刻敲定了，非常奇幻。</p><p>VIPL 组报名的人有亿点点多，所以考核是先笔试、再机试、最后面试，逐层筛选。笔试内容分为数学、算法、专业（主要是机器学习）、中英文写作、综合五部分。数学题覆盖面广，幸好之前复习了一下，不过没复习到的曲面积分啥的还是空着了；算法题可能也比较常规吧（我不知道，我对算法题难度没有概念，逃）；机器学习有一道题手推 SVM，这我还真忘了；中英文写作是双向翻译以及总结文章；综合题就是一些找规律、看几何体这种快乐题。机考允许选择 C++ 或 Python，然后限时做几道编程小题，必须用 Visual Studio，非 online judge，考完打包发过去。面试的气氛甚至有些随意，介绍了一下科研项目，然后就开始问爱好特长、社会实践之类的，最后还问怎么看待疫情防控（好像问社会问题是计算所的传统项目）。</p><p>面试当晚就和老师加了微信互相确认，至此夏令营收官。</p><h2 id="瞎扯">4 瞎扯</h2><p>正如开头所言，这篇文章写得有点晚，很多细节已经不记得了，可能参考意义有限。如果有说得不对的地方，还望指正。</p><p>经过这几个月的折腾，我深刻地感受到了保研就是一场超大规模不完全信息博弈，学生常常手拿多个 offer 还去冲更好的学校，导师怕学生放了自己的鸽子导致招不到好学生。对于一些不好意思鸽导师而拒掉了其他 offer 的学生（比如说我），就天天担心最后没书读了……每个人都做出最有利于自己的选择，总体上却陷入纳什均衡，多少有点无奈，有点感慨。尘埃既定时，我们唯一能做的，就是沿着选择的路走下去，并认真走好未来的每一步。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Score-Based Generative Models</title>
    <link href="/blog-main/2022/10/13/Score-Based-Generative-Models/"/>
    <url>/blog-main/2022/10/13/Score-Based-Generative-Models/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\E}{\mathbb E}\newcommand{\pdata}{p_\text{data}}\newcommand{\x}{\mathbf x}\newcommand{\v}{\mathbf v}\newcommand{\R}{\mathbb R}\newcommand{\T}{\mathsf T}\]</span></p><h2 id="brief-introduction">Brief Introduction</h2><p>在<a href="/blog-main/2022/09/29/%E4%BB%8EVAE%E5%88%B0DDPM/" title="从VAE到DDPM">从VAE到DDPM</a>一文中，我们从 VAE 出发，通过变分推断得到了 DDPM 的理论框架；我们发现 DDPM 可以看作是具有人为指定的后验分布（即前向过程）的层次化 VAE。这一角度的代表论文是 2015 年的 Sohl-Dickstein et al.<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Sohl-Dickstein, Jascha, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In *International Conference on Machine Learning*, pp. 2256-2265. PMLR, 2015.">[1]</span></a></sup> 和 2020 年的 DDPM<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ho, Jonathan, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. *Advances in Neural Information Processing Systems* 33 (2020): 6840-6851.">[2]</span></a></sup>。与此同时，以<a href="https://yang-song.net/">宋飏</a>博士的几篇论文<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, Yang, and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. *Advances in Neural Information Processing Systems* 32 (2019).">[3]</span></a></sup><sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, Yang, and Stefano Ermon. Improved techniques for training score-based generative models. *Advances in neural information processing systems* 33 (2020): 12438-12448.">[4]</span></a></sup>为代表，人们从 score matching + Langevin dynamics (SMLD) 的角度出发得到了与 DDPM 本质相同的模型，称为 score-based generative models。两个角度出发点截然不同，但殊途同归，也是非常有意思了。</p><h2 id="score-function">Score Function</h2><p>所谓生成模型，就是希望对数据分布 <span class="math inline">\(\pdata(\x)\)</span> 建模，使得我们能够从该分布中随意采样。假设我们构建了一个以 <span class="math inline">\(\theta\)</span> 为参数的模型 <span class="math inline">\(f_\theta:\R^d\to\R\)</span>（一般称为 Energy-Based Model, EBM），那么可用以下形式获得概率分布： <span class="math display">\[p_\theta(\x)=\frac{e^{-f_\theta(\x)}}{Z_\theta}\tag{1}\label{p}\]</span> 其中 <span class="math inline">\(Z_\theta=\int_\x e^{-f_\theta(\x)}\mathrm{d}\x\)</span> 是归一化因子。</p><p>训练生成模型无非就是希望极大化数据的对数似然： <span class="math display">\[\max_\theta\quad \E_{\pdata(\x)}[\log p_\theta(\x)]\]</span> 但是直接计算对数似然存在一个问题：<span class="math inline">\(Z_\theta\)</span> 通常是 intractable 的。不同的生成模型采用了不同的思路来解决这个问题，而 score-based generative models 使用 <strong>score function</strong> 来巧妙地回避开 <span class="math inline">\(Z_\theta\)</span>.</p><p>Score function 定义为对数 pdf 对数据的梯度： <span class="math display">\[s(\x)=\nabla_\x\log p(\x)\]</span> 那么对于 <span class="math inline">\(\eqref{p}\)</span> 式来说，其 score function 为： <span class="math display">\[s_\theta(\x)=\nabla_\x \log p_\theta(\x)=\nabla_\x \log \frac{e^{-f_\theta(\x)}}{Z_\theta}=-\nabla_\x f_\theta(\x)\]</span> 可以看见<strong>由于 <span class="math inline">\(Z_\theta\)</span> 与 <span class="math inline">\(\x\)</span> 无关，对 <span class="math inline">\(\x\)</span> 求梯度后就直接变成零了</strong>！这启发我们直接对 score function 建模，即构建一个模型 <span class="math inline">\(s_\theta(\x)\)</span>，用它去近似真实数据分布的 score function： <span class="math display">\[s_\theta(\x)\xrightarrow{\text{approximate}} s_\text{data}(\x)=\nabla_\x\log \pdata(\x)\]</span> 此过程称作 <strong>score matching</strong>.</p><p>Score function 是一个向量场，指示了从当前点去到分布密度更大的地方的方向，如下图所示：</p><p><img src="https://yang-song.net/assets/img/score/score_contour.jpg" alt="source:https://yang-song.net/blog/2021/score/" width=40% /></p><p>因此模型学习到了 score function，也就相当于学到了数据分布。</p><h2 id="score-matching">Score Matching</h2><p>为了训练模型 <span class="math inline">\(s_\theta(\x)\)</span>，我们最小化它与 <span class="math inline">\(s_\text{data}(\x)\)</span> 的均方距离（也即是模型给出的分布和数据分布之间的 Fisher Divergence）： <span class="math display">\[\min_\theta\quad J_\text{ESM}(\theta)=\frac{1}{2}\E_{\pdata(\x)}\left[\|s_\text{data}(\x)-s_\theta(\x)\|_2^2\right]\tag{2}\label{esm}\]</span> 下标 ESM 是 Explicit Score Matching 的意思，explicit 是指该函数要求我们显式地知道 <span class="math inline">\(s_\text{data}(\x)\)</span>，但这恰恰是我们不知道的。我们可以通过 Parzen 窗对其做非参估计，但是论文<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hyvärinen, Aapo, and Peter Dayan. Estimation of non-normalized statistical models by score matching. *Journal of Machine Learning Research* 6, no. 4 (2005).">[5]</span></a></sup>的作者指出没有这个必要，我们可以使用一个分部积分去掉 <span class="math inline">\(s_\text{data}(\x)\)</span>。具体而言，将 <span class="math inline">\(\eqref{esm}\)</span> 式展开： <span class="math display">\[\begin{align}J_\text{ESM}(\theta)&amp;=\frac{1}{2}\E_{\pdata(\x)}\left[\|s_\text{data}(\x)-s_\theta(\x)\|_2^2\right]\\&amp;=\E_{\pdata(\x)}\left[\frac{1}{2}\|s_\text{data}(\x)\|^2-s_\text{data}(\x)^T s_\theta(\x)+\frac{1}{2}\|s_\theta(\x)\|^2\right]\end{align}\]</span> 第一项与 <span class="math inline">\(\theta\)</span> 无关可以略去，最后一项可以直接优化，唯一需要处理的是中间项： <span class="math display">\[\begin{align}-\E_{\pdata(\x)}[s_\text{data}(\x)^Ts_\theta(\x)]&amp;=-\int \pdata(\x) \sum_{i=1}^d s_\text{data}(\x)_is_\theta(\x)_i\mathrm{d}\x\\&amp;=-\sum_{i=1}^d \int \pdata(\x) \frac{\partial \log p_\text{data}(\x)}{\partial\x_i}s_\theta(\x)_i\mathrm{d}\x\\&amp;=-\sum_{i=1}^d\int \frac{\pdata(\x)}{\pdata(\x)}\frac{\partial\pdata(\x)}{\partial\x_i} s_\theta(\x)_i\mathrm{d}\x\\&amp;=-\sum_{i=1}^d\int \frac{\partial\pdata(\x)}{\partial\x_i} s_\theta(\x)_i \mathrm{d}\x\end{align}\]</span></p><div class="note note-info">            <p><strong>分部积分</strong></p><p>设 <span class="math inline">\(f(x),g(x)\)</span> 是关于 <span class="math inline">\(x\)</span> 的一元函数且具有连续导数，且下文出现的积分都存在，由于： <span class="math display">\[[f(x)g(x)]&#39;=f(x)g&#39;(x)+g(x)f&#39;(x)\]</span> 两边同时在 <span class="math inline">\((-\infty,+\infty)\)</span> 上积分，得到： <span class="math display">\[\lim_{x\to+\infty}f(x)g(x)-\lim_{x\to-\infty}f(x)g(x)=\int_{-\infty}^{+\infty}f(x)g&#39;(x)\mathrm{d}x+\int_{-\infty}^{+\infty}g(x)f&#39;(x)\mathrm{d}x\]</span> 即： <span class="math display">\[\int_{-\infty}^{+\infty}f(x)g&#39;(x)\mathrm{d}x=\left.[f(x)g(x)]\right|^{+\infty}_{-\infty}-\int_{-\infty}^{+\infty}g(x)f&#39;(x)\mathrm{d}x\]</span> <strong>扩展到多元函数</strong></p><p>设 <span class="math inline">\(f(\x),g(\x)\)</span> 是关于 <span class="math inline">\(\x\)</span> 的多元标量函数，固定除 <span class="math inline">\(\x_i\)</span> 外的其他变元，所有求导均为对 <span class="math inline">\(\x_i\)</span> 的偏导，那么上述推导依旧使用，因此有结论： <span class="math display">\[\begin{align}\int_{-\infty}^{+\infty}f(\x)\frac{\partial{g(\x)}}{\partial\x_i}\mathrm{d}\x&amp;=\lim_{\x_i\to+\infty}f(\ldots,\x_i,\ldots)g(\ldots,\x_i,\ldots)\\&amp;-\lim_{\x_i\to-\infty}f(\ldots,\x_i,\ldots)g(\ldots,\x_i,\ldots)\\&amp;-\int_{-\infty}^{+\infty}g(\x)\frac{\partial{f(\x)}}{\partial\x_i}\mathrm{d}\x\end{align}\]</span></p>          </div><p>应用分部积分，并且假设 <span class="math inline">\(\pdata(\ldots,\x_i,\ldots)s_\theta(\ldots,\x_i,\ldots)_i\to 0(\x_i\to\pm\infty)\)</span>，那么中间项变成： <span class="math display">\[\begin{align}-\sum_{i=1}^d\int \frac{\partial\pdata(\x)}{\partial\x_i} s_\theta(\x)_i \mathrm{d}\x&amp;=\sum_{i=1}^d\int \frac{\partial s_\theta(\x)_i}{\partial\x_i} \pdata(\x) \mathrm{d}\x\\&amp;=\int \pdata(\x) \sum_{i=1}^d \frac{\partial s_\theta(\x)_i}{\partial\x_i} \mathrm{d}\x\\&amp;=\E_{\x\sim\pdata}\left[\sum_{i=1}^d \frac{\partial s_\theta(\x)_i}{\partial\x_i}\right]\end{align}\]</span> 于是最终优化目标为： <span class="math display">\[J_\text{ISM}(\theta)=\E_{\pdata(\x)}\left[\sum_{i=1}^d \left(\frac{\partial s_\theta(\x)_i}{\partial\x_i}+\frac{1}{2}s_\theta(\x)_i^2\right)\right]\tag{3}\label{ism}\]</span> 下标 ISM 指 Implicit Score Matching，与 ESM 相对应。如果用 <span class="math inline">\(s_\theta(\x)\)</span> 的 Jacobian 矩阵 <span class="math inline">\(\nabla_\x s_\theta(\x)\)</span> 来表示，就是： <span class="math display">\[J_\text{ISM}(\theta)=\E_{\pdata(\x)}\left[\text{tr}\left({\nabla_\x s_\theta(\x)}\right)+\frac{1}{2}\|s_\theta(\x)\|^2\right]\tag{3&#39;}\label{ism&#39;}\]</span> 当然，由于 <span class="math inline">\(s_\theta(\x)\)</span> 本身是 <span class="math inline">\(\log p_\theta(\x)\)</span> 的梯度，所以前者的 Jacobian 矩阵，也是后者的 Hessian 矩阵，所以也可以写成： <span class="math display">\[J_\text{ISM}(\theta)=\E_{\pdata(\x)}\left[\text{tr}\left({\nabla^2_\x \log p_\theta(\x)}\right)+\frac{1}{2}\|\nabla_\x\log p_\theta(\x)\|^2\right]\tag{3&#39;&#39;}\label{ism&#39;&#39;}\]</span> 只是记号上的不同罢了。</p><p>可以看见，现在的优化目标期望里面已经没有 <span class="math inline">\(\pdata(\x)\)</span> 或 <span class="math inline">\(s_\text{data}(\x)\)</span> 了，而期望本身通过训练过程的采样来近似。</p><p>当然，上述推导过程要成立也有一些条件，比如用分部积分要求 <span class="math inline">\(s_\theta(\x)\)</span> 和 <span class="math inline">\(\pdata(\x)\)</span> 可导，出现的所有期望要存在，以及上文已经提到过的 <span class="math inline">\(\pdata(\x)s_\theta(\x)\to 0\ (\Vert\x\Vert \to\infty)\)</span>. 好在这些条件都容易满足。</p><h3 id="denoising-score-matching">Denoising Score Matching</h3><p>上文中，我们使用分部积分巧妙地去掉了 <span class="math inline">\(s_\text{data}(\x)=\nabla_\x \log\pdata(\x)\)</span>，避免了对数据分布做非参估计，但是代价是引入了 <span class="math inline">\(\text{tr}\left(\nabla_\x s_\theta(\x)\right)\)</span>，意味着我们还要对 score function 求导（即求 <span class="math inline">\(\log\pdata(\x)\)</span> 的 Hessian 矩阵）。虽然这个操作可以通过 <code>torch.autograd.grad()</code> 实现，但反向传播次数增加了 <span class="math inline">\(d\)</span> 倍（<span class="math inline">\(d\)</span> 为数据维数），在深度网络和高维数据的场景下效率很差。那如果我们做非参估计会怎样呢？考虑使用带高斯核的 Parzen 窗做非参估计（也称核密度估计，Kernel Density Estimation, KDE）： <span class="math display">\[\begin{align}&amp;q_\sigma(\tilde\x)=\frac{1}{N}\sum_{i=1}^N K_\sigma\left(\tilde\x-\x_i\right)\\&amp;K_\sigma(\tilde\x-\x_i)=\frac{1}{(2\pi)^{d/2}\sigma^d}e^{-\frac{1}{2\sigma^2}\|\tilde\x-\x_i\|_2^2}\end{align}\]</span> 其中，当 <span class="math inline">\(\sigma\to0\)</span> 时，高斯核趋向于 Dirac delta 函数，<span class="math inline">\(q_\sigma(\x)\to \pdata(\x)\)</span>.</p><p>由于上述 Parzen 窗的本质和用高斯噪声扰动数据是一致的： <span class="math display">\[\begin{align}&amp;q_\sigma(\tilde\x)=\int_\x \pdata(\x) q_\sigma(\tilde\x\vert \x) \mathrm{d}\x\\&amp;q_\sigma(\tilde\x\vert\x)=K_\sigma(\tilde\x-\x)=\frac{1}{(2\pi)^{d/2}\sigma^d}e^{-\frac{1}{2\sigma^2}\|\tilde\x-\x\|_2^2}\end{align}\]</span> 所以我们现在干的事其实和 denoising autoencoders 干的事是一样的，这也是为什么论文<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Vincent, Pascal. A connection between score matching and denoising autoencoders. *Neural computation* 23, no. 7 (2011): 1661-1674.">[6]</span></a></sup>的标题是「score matching 与 denoising autoencoders 之间的联系」。方便起见，下文都站在加噪扰动视角而非核密度估计的视角叙述。</p><p>我们在扰动后的数据分布上做 score matching，即将 <span class="math inline">\(\eqref{esm}\)</span> 式中的 <span class="math inline">\(\pdata\)</span> 换成 <span class="math inline">\(q_\sigma\)</span>： <span class="math display">\[\quad J_{\text{ESM}{q_\sigma}}(\theta)=\frac{1}{2}\E_{q_\sigma(\tilde\x)}\left[\|\nabla_\tilde\x\log q_\sigma(\tilde\x)-s_\theta(\tilde\x)\|_2^2\right]\]</span> 可是 <span class="math inline">\(\nabla_\tilde\x\log q_\sigma(\tilde\x)\)</span> 还是无法处理。注意到由于 <span class="math inline">\(q_\sigma(\tilde\x\vert\x)\)</span> 是高斯核，<span class="math inline">\(\nabla_\tilde\x\log q_\sigma(\tilde\x\vert \x)\)</span> 是可以直接计算的： <span class="math display">\[\nabla_{\tilde\x}\log q_\sigma(\tilde \x\vert\x)=\frac{1}{\sigma^2}(\x-\tilde \x)\tag{4}\label{score-gauss}\]</span> 所以我们希望能把期望里面的 <span class="math inline">\(\nabla_\tilde\x\log q_\sigma(\tilde\x)\)</span> 变换成 <span class="math inline">\(\nabla_\tilde\x\log q_\sigma(\tilde\x\vert \x)\)</span>. 把平方打开，第一个平方项与 <span class="math inline">\(\theta\)</span> 无关扔掉，另一个平方项保留，着重考虑中间项： <span class="math display">\[\begin{align}-\E_{q_\sigma(\tilde\x)}[\langle\nabla_{\tilde\x}\log q_\sigma(\tilde\x),s_\theta(\tilde\x)\rangle]&amp;=-\int_{\tilde\x} q_\sigma(\tilde\x) \left\langle \frac{\nabla_{\tilde\x}q_\sigma(\tilde\x)}{q_\sigma(\tilde\x)},s_\theta(\tilde\x) \right\rangle \mathrm{d}\tilde\x\\&amp;=-\int_{\tilde\x}\bigg\langle \nabla_{\tilde\x}{\color{purple}{q_\sigma(\tilde\x)}},s_\theta(\tilde\x) \bigg\rangle\mathrm{d}\tilde\x\\&amp;=-\int_{\tilde\x}\left\langle \nabla_{\tilde\x}{\color{purple}{\int_\x \pdata(\x)q_\sigma(\tilde\x\vert \x)\mathrm{d}\x}},s_\theta(\tilde\x) \right\rangle\mathrm{d}\tilde\x\\&amp;=-\int_{\tilde\x}\left\langle \int_\x \pdata(\x){\color{green}{\nabla_{\tilde\x}q_\sigma(\tilde\x\vert \x)}}\mathrm{d}\x,s_\theta(\tilde\x) \right\rangle\mathrm{d}\tilde\x\\&amp;=-\int_{\tilde\x}\left\langle \int_\x \pdata(\x){\color{green}{q_\sigma(\tilde\x\vert \x)\nabla_{\tilde\x}\log q_\sigma(\tilde\x\vert \x)}}\mathrm{d}\x,s_\theta(\tilde\x) \right\rangle\mathrm{d}\tilde\x\\&amp;=-\int_{\tilde\x}\int_\x \pdata(\x)q_\sigma(\tilde\x\vert \x)\bigg\langle {\nabla_{\tilde\x}\log q_\sigma(\tilde\x\vert \x)},s_\theta(\tilde\x) \bigg\rangle\mathrm{d}\x\mathrm{d}\tilde\x\\&amp;=-\E_{q_\sigma(\tilde \x,\x)}[\langle {\nabla_{\tilde\x}\log q_\sigma(\tilde\x\vert \x)},s_\theta(\tilde\x)\rangle]\\\end{align}\]</span> 这样我们就得到了变换后的目标函数： <span class="math display">\[J&#39;=\E_{q_\sigma(\tilde\x)}\left[\frac{1}{2}\|s_\theta(\tilde\x)\|^2\right]-\E_{q_\sigma(\tilde\x,\x)}\left[\left\langle\nabla_{\tilde\x}\log q_\sigma(\tilde\x\vert \x),s_\theta(\tilde\x)\right\rangle\right]\]</span> 看上去有点丑，不妨配凑回平方的形式，即是论文中提出的 Denoising Score Matching (DSM) 优化目标： <span class="math display">\[\begin{align}J_\text{DSM}(\theta)&amp;=\E_{q_\sigma(\tilde\x,\x)}\left[\frac{1}{2}\left\|\nabla_{\tilde\x}\log q_\sigma(\tilde\x\vert \x)-s_\theta(\tilde\x)\right\|^2 \right]\\&amp;=\E_{q_\sigma(\tilde\x,\x)}\left[\frac{1}{2}\left\|\frac{\x-\tilde\x}{\sigma^2} -s_\theta(\tilde\x)\right\|^2 \right]\end{align}\tag{5}\label{dsm}\]</span></p><p>直观上，<span class="math inline">\(\x-\tilde\x\)</span> 指出了加噪数据 <span class="math inline">\(\tilde\x\)</span> 回到无噪声数据的移动方向，这正与 score function 的定义相符。</p><h3 id="sliced-score-matching">Sliced Score Matching</h3><blockquote><p>本节其实与以去噪为基础的扩散模型关系不大，但为了完整性，同时考虑到它也是宋飏大佬的工作，就顺便读了读论文。</p></blockquote><p>Denoising score matching 虽然解决了原始 score matching 无法 scale 到深度网络和高维数据的问题，但是本身也有两个问题：</p><ul><li>只能学习带有噪声的分布</li><li>性能对 noise level 很敏感</li></ul><p>而宋飏等人在 2020 年提出的 sliced score matching<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, Yang, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In *Uncertainty in Artificial Intelligence*, pp. 574-584. PMLR, 2020.">[7]</span></a></sup> 是另一种解决方法，其思想是把 <span class="math inline">\(s_\theta(\x)\)</span> 随机投影到向量 <span class="math inline">\(\mathbf v\)</span> 上，只在投影的方向上做匹配。具体而言，<span class="math inline">\(\eqref{esm}\)</span> 式改写作： <span class="math display">\[J_\text{SSM(explicit)}=\frac{1}{2}\E_{p_{\v}(\v)}\E_{\pdata(\x)}\left[\left(\v^\T s_\text{data}(\x)-\v^\T s_\theta(\x)\right)^2\right]\]</span> 并要求 <span class="math inline">\(\E_{p_\v(\v)}[\v\v^\T]\succ 0\)</span> 且 <span class="math inline">\(\E_{p_\v(\v)}\left[\Vert\v\Vert_2^2\right]&lt;\infty\)</span>. 常用的 <span class="math inline">\(p_\v(\v)\)</span> 包括标准多元正态分布 <span class="math inline">\(\mathcal N(0,\mathbf I)\)</span>、多元 Rademacher 分布（<span class="math inline">\(\{\pm 1\}^d\)</span> 上的均匀分布）、超球面 <span class="math inline">\(\mathbb S^{d-1}\)</span> 上的均匀分布等。</p><p>与原始 score matching 类似，我们可以用分部积分去掉 <span class="math inline">\(s_\text{data}(\x)\)</span>. 同样只需着重考虑中间项： <span class="math display">\[\begin{align}-\E_{p_{\v}(\v)}\left[\E_{\pdata(\x)}\left[\left(\v^\T s_\text{data}(\x)\right)\left(\v^\T s_\theta(\x)\right)\right]\right]&amp;=-\E_{p_{\v}(\v)}\left[\int\pdata(\x)\left(\v^\T s_\text{data}(\x)\right)\left(\v^\T s_\theta(\x)\right)\mathrm{d}\x\right]\\&amp;=-\E_{p_{\v}(\v)}\left[\int\pdata(\x)\left(\v^\T\nabla_\x \log p_\text{data}(\x)\right)\left(\v^\T s_\theta(\x)\right)\mathrm{d}\x\right]\\&amp;=-\E_{p_{\v}(\v)}\left[\int\left(\v^\T\nabla_\x p_\text{data}(\x)\right)\left(\v^\T s_\theta(\x)\right)\mathrm{d}\x\right]\\&amp;=-\E_{p_{\v}(\v)}\left[\int\left(\sum_{i=1}^d v_i\frac{\partial\pdata(\x)}{\partial\x_i}\right)\left(\v^\T s_\theta(\x)\right)\mathrm{d}\x\right]\\&amp;=-\E_{p_{\v}(\v)}\left[\sum_{i=1}^d v_i\int\frac{\partial\pdata(\x)}{\partial\x_i}\left(\v^\T s_\theta(\x)\right)\mathrm{d}\x\right]\\\end{align}\]</span> 利用多元函数分部积分结论，有： <span class="math display">\[\begin{align}&amp;-\E_{p_{\v}(\v)}\left[\sum_{i=1}^d v_i{\color{dodgerblue}{\int\frac{\partial\pdata(\x)}{\partial\x_i}\left(\v^\T s_\theta(\x)\right)\mathrm{d}\x}}\right]\\=&amp;-\E_{p_{\v}(\v)}\left[\sum_{i=1}^d v_i\left({\color{dodgerblue}{\lim_{\x_i\to+\infty}\pdata(\x)\left(\v^\T s_\theta(\x)\right)-\lim_{\x_i\to-\infty}\pdata(\x)\left(\v^\T s_\theta(\x)\right)-\int\left(\frac{\partial\left(\v^\T s_\theta(\x)\right)}{\partial\x_i}\right)\pdata(\x)\mathrm{d}\x}}\right)\right]\\=&amp;-\E_{p_{\v}(\v)}\left[\sum_{i=1}^d v_i\left(\lim_{\x_i\to+\infty}\pdata(\x)\left(\v^\T s_\theta(\x)\right)-\lim_{\x_i\to-\infty}\pdata(\x)\left(\v^\T s_\theta(\x)\right)\right)\right]+\E_{p_\v(\v)}\left[\sum_{i=1}^d v_i\pdata(\x)\left(\int\v^\T\frac{\partial s_\theta(\x))}{\partial\x_i}\mathrm{d}\x\right)\right]\\=&amp;-\E_{p_{\v}(\v)}\left[\sum_{i=1}^d v_i\left(\lim_{\x_i\to+\infty}\pdata(\x)\left(\v^\T s_\theta(\x)\right)-\lim_{\x_i\to-\infty}\pdata(\x)\left(\v^\T s_\theta(\x)\right)\right)\right]+\E_{p_\v(\v)}\left[\int\pdata(\x)\left(\sum_{i=1}^d v_i\v^\T \frac{\partial s_\theta(\x))}{\partial\x_i}\right)\mathrm{d}\x\right]\\=&amp;-\E_{p_{\v}(\v)}\left[\sum_{i=1}^d v_i\left(\lim_{\x_i\to+\infty}\pdata(\x)\left(\v^\T s_\theta(\x)\right)-\lim_{\x_i\to-\infty}\pdata(\x)\left(\v^\T s_\theta(\x)\right)\right)\right]+\E_{p_\v(\v)}\left[\E_{\pdata(\x)}\left[\v^\T \nabla_\x s_\theta(\x)\v\right]\right]\\\end{align}\]</span> 后面那项是我们想要的，现在需要证明前面那项等于 0. 在原始 score matching 中，我们是通过假设 <span class="math inline">\(\pdata(\x)s_\theta(\x)\to 0\ (\Vert\x\Vert\to \infty)\)</span> 完成的，但是现在多了个 <span class="math inline">\(\v\)</span> 来搅局。不过好在最后要对 <span class="math inline">\(p_\v(\v)\)</span> 求期望，所以我们依旧能够期待前项可以等于 0. 利用绝对值不等式把 <span class="math inline">\(\v\)</span> 拆出来，并利用柯西不等式，我们有： <span class="math display">\[\begin{align}&amp;\left|\E_{p_{\v}(\v)}\left[\sum_{i=1}^d v_i\left(\lim_{\x_i\to+\infty}\pdata(\x)\left(\v^\T s_\theta(\x)\right)-\lim_{\x_i\to-\infty}\pdata(\x)\left(\v^\T s_\theta(\x)\right)\right)\right]\right|\\\leq&amp;\left|\E_{p_{\v}(\v)}\left[\sum_{i=1}^d v_i\left(\lim_{\x_i\to+\infty}\pdata(\x)\left(\v^\T s_\theta(\x)\right)\right)\right]\right|+\left|\E_{p_\v(\v)}\left[\sum_{i=1}^d v_i\left(\lim_{\x_i\to-\infty}\pdata(\x)\left(\v^\T s_\theta(\x)\right)\right)\right]\right|\\\leq&amp;\sum_{i=1}^d \lim_{\x_i\to+\infty}\sum_{j=1}^d\E_{p_\v(\v)}\left[\left| v_iv_j \right|\right]\left|\pdata(\x){s_\theta(\x)}_j\right|+\sum_{i=1}^d \lim_{\x_i\to-\infty}\sum_{j=1}^d\E_{p_\v(\v)}\left[\left| v_iv_j \right|\right]\left|\pdata(\x){s_\theta(\x)}_j\right|\\\leq&amp; \sum_{i=1}^d\lim_{\x_i\to+\infty}\sum_{j=1}^d\sqrt{\E_{p_\v(\v)}v_i^2\E_{p_\v(\v)}v_j^2}\left|\pdata(\x){s_\theta(\x)}_j\right|+\sum_{i=1}^d\lim_{\x_i\to-\infty}\sum_{j=1}^d\sqrt{\E_{p_\v(\v)}v_i^2\E_{p_\v(\v)}v_j^2}\left|\pdata(\x){s_\theta(\x)}_j\right|\\=&amp;0\end{align}\]</span> 柯西不等式的作用只是为了用上 <span class="math inline">\(\E_{p_\v(\v)}\left[\Vert\v\Vert^2_2\right]&lt;\infty\)</span> 的假设条件，进而有限项乘以无穷小依旧是无穷小。</p><p>于是我们就得到了 SSM 的 implicit 形式： <span class="math display">\[J_\text{SSM}=\E_{p_\v(\v)}\E_{\pdata(\x)}\left[\v^\T \nabla_\x s_\theta(\x)\v+\frac{1}{2}\left(\v^\T s_\theta(\x)\right)^2\right]\tag{6}\label{ssm}\]</span> 特别地，当 <span class="math inline">\(p_\v(\v)\)</span> 是标准多元正态分布或多元 Rademacher 分布时，后面一项可以直接积出来：<span class="math inline">\(\E_{p_\v(\v)}\left[\left(\v^\T s_\theta(\x)\right)^2\right]=\Vert s_\theta(\x)\Vert_2^2\)</span>，即： <span class="math display">\[J_\text{SSM-VR}=\E_{\pdata(\x)}\left[\E_{p_\v(\v)}\left[\v^\T \nabla_\x s_\theta(\x)\v\right]+\frac{1}{2}\left\|s_\theta(\x)\right\|^2_2\right]\tag{6&#39;}\label{ssm-vr}\]</span> 下标 VR 是 Variance Reduction 的意思，因为直接计算相比采样近似减小了方差，效果更好。</p><p><br/></p><p>说了这么多，<span class="math inline">\(\eqref{ssm}\)</span> 式中不还是有 <span class="math inline">\(\nabla_\x s_\theta(\x)\)</span> 吗，怎么就减少了计算量呢？注意 <span class="math inline">\(\v^\T \nabla_\x s_\theta(\x)\v=\nabla_\x(\v^\T s_\theta(\x))\v\)</span>，所以我们可以先计算出标量 <span class="math inline">\(\v^\T s_\theta(\x)\)</span>，然后对 <span class="math inline">\(\x\)</span> 求梯度，最后乘上 <span class="math inline">\(\v\)</span>，这样反向传播次数就是直接计算 <span class="math inline">\(\nabla_\x s_\theta(\x)\)</span> 的 <span class="math inline">\(1/d\)</span> 了。</p><p>另外，每次采样的投影向量个数也是影响效率的重要因素。采样越多，近似越准确，计算量也越大，所以这是一个 trade-off. 论文作者实验发现每次采样 1 个向量效果就不错了。</p><h2 id="langevin-dynamics">Langevin Dynamics</h2><p>在模型学到了 score function 之后，我们怎么从中采样呢？直观上，score function <span class="math inline">\(\nabla_\x\log p(\x)\)</span> 是当前点指向密度更大处的方向，所以我们可以先随机初始化一个位置，然后不断沿着 score function 的方向走，最后就能走到密度的极大值点——这个过程很像梯度下降，只不过梯度下降是在参数空间中进行，而采样是在数据空间中进行： <span class="math display">\[\x_{k}\gets \x_{k-1}+\alpha \nabla_\x \log p(\x_{k-1}),\quad k=1,\ldots,K\]</span> 然而这样做每次都到达 <span class="math inline">\(p(\x)\)</span> 的极大值点，并不是我们希望的从 <span class="math inline">\(p(\x)\)</span> 采样。不过基于类似的思想，<strong>Langevin dynamics</strong> 加入高斯噪声，可以让采样分布收敛到 <span class="math inline">\(p(\x)\)</span>： <span class="math display">\[\begin{align}&amp;\x_0\sim \pi(\x)\\&amp;\x_{k}\gets\x_{k-1}+\frac{\epsilon}{2}\nabla_\x \log p(\x_{k-1})+\sqrt{\epsilon} \mathbf z_k,\quad k=1,\ldots,K\end{align}\tag{7}\label{langevin}\]</span> 其中 <span class="math inline">\(\mathbf z_k\sim \mathcal N(0,\mathbf I)\)</span>. 可以证明，当 <span class="math inline">\(\epsilon\to 0\)</span> 且 <span class="math inline">\(K\to\infty\)</span> 时，<span class="math inline">\(\x_K\)</span> 的分布收敛到 <span class="math inline">\(p(\x)\)</span>.</p><p><img src="https://yang-song.net/assets/img/score/langevin.gif" alt="source:https://yang-song.net/blog/2021/score/" width=40% /></p><h2 id="noise-perturbations">Noise Perturbations</h2><p>截至目前，我们用 score matching 训练模型预测 score function，然后通过 Langevin dynamics 采样。但这个做法在实际中很难 work. 究其原因，罪魁祸首在于 <strong>manifold hypothesis</strong>，即真实情况下的数据分布在高维空间中的低维流形上。换句话说，空间中绝大部分都是低密度区域。这将导致两个问题：</p><p>首先，从训练层面上讲，低密度区域数据量少（甚至没有），无法较好地训练模型，导致模型预测的 score function 不准确：</p><p><img src="https://yang-song.net/assets/img/score/pitfalls.jpg" alt="source:https://yang-song.net/blog/2021/score/" width=80% /></p><p>又由于我们的初始化点极有可能落在低密度区域中，所以不准确的 score function 将对采样过程产生极大的影响。</p><p>其次，即便 score function 能被准确预测，但对于被低密度区域隔开的两个分布来说，Langevin dynamics 很难收敛到正确的分布（尽管 Langevin dynamics 在理论上能收敛到正确的分布，但可能需要很小的步长和很多的步数），如图所示：</p><p><img src="fig3.png" alt="source:Generative modeling by estimating gradients of the data distribution." width=80% /></p><p>真实情况是右上的正态分布权重更大，但中图的采样结果显示两个分布被予以了几乎相同的权重。</p><p>宋飏等人提出解决这两个问题的办法是：<strong>给数据添加高斯噪声，并且随着 Langevin dynamics 的进行逐渐减小 noise level（退火）</strong>。添加噪声后的数据遍布整个空间，填充了低密度区域，因此模型能够得到更多的监督信号，Langevin dynamics 也能更快的收敛到高密度区域；同时，逐渐减小的噪声使得带噪分布最终收敛到真实分布，因此也能保证采样质量不受影响。</p><p><img src="https://yang-song.net/assets/img/score/single_noise.jpg" alt="source:https://yang-song.net/blog/2021/score/" width=80% /></p><p>形式化地说，我们确定一个逐渐减小的噪声序列 <span class="math inline">\(\{\sigma_i\}_{i=1}^L\)</span>，那么对应 <span class="math inline">\(\sigma_i\)</span> 的噪声扰动后的数据分布为： <span class="math display">\[q_{\sigma_i}(\x)=\int\pdata(\mathbf t) \mathcal N\left(\x;\mathbf t,\sigma_i^2\mathbf I\right) \mathrm{d}\mathbf t\]</span> 我们用一个神经网络（称作 Noise Conditional Score Networks，NCSN）<strong>同时预测不同 noise level 的 score function</strong>，即： <span class="math display">\[s_\theta(\x, \sigma_i)\xrightarrow{\text{approximate}} \nabla_\x\log q_{\sigma_i}(\x)\]</span> 于是训练目标就是对每一个 noise level 做 score matching： <span class="math display">\[\mathcal L\left(\theta,\{\sigma_i\}_{i=1}^L\right)=\frac{1}{L}\sum_{i=1}^L \lambda(\sigma_i)\E_{q_{\sigma_i}(\x)}\left[\|\nabla_\x\log q_{\sigma_i}(\x)-s_\theta(\x,\sigma_i) \|^2\right]\]</span> 其中 <span class="math inline">\(\lambda(\sigma_i)\)</span> 是给不同 noise level 加的权重。</p><p>考虑到 denoising score matching 正好符合现在的加噪去噪设定，所以我们选择它而非 sliced score matching. 根据前文的叙述，训练目标变换为： <span class="math display">\[\begin{align}\mathcal L\left(\theta,\{\sigma_i\}_{i=1}^L\right)&amp;=\frac{1}{L}\sum_{i=1}^L \lambda(\sigma_i)\E_{\pdata(\x)}\E_{q_{\sigma_i}(\tilde\x\vert\x)}\left[\frac{1}{2}\left\|\nabla_{\tilde\x}\log q_{\sigma_i}(\tilde \x\vert\x) -s_\theta(\tilde\x,\sigma_i)\right\|^2 \right]\\&amp;=\frac{1}{L}\sum_{i=1}^L \lambda(\sigma_i)\E_{\pdata(\x)}\E_{q_{\sigma_i}(\tilde\x\vert\x)}\left[\frac{1}{2}\left\|\frac{\x-\tilde\x}{\sigma_i^2} -s_\theta(\tilde\x,\sigma_i)\right\|^2 \right]\end{align}\tag{8}\label{ncsn}\]</span> 其中 <span class="math inline">\(q_{\sigma_i}(\tilde\x\vert \x)=\mathcal N\left(\tilde \x;\x,\sigma_i^2\mathbf I\right)\)</span>.</p><p>怎么选取权重 <span class="math inline">\(\lambda(\sigma_i)\)</span> 呢？根据作者经验，一般有 <span class="math inline">\(\Vert s_\theta(\x,\sigma)\Vert_2\propto 1/\sigma\)</span>，所以为了各个 noise level 的损失大致相同，取 <span class="math inline">\(\lambda(\sigma_i)=\sigma_i^2\)</span>.</p><p>训练完成后，我们就可以随机初始化采样点，逐渐减小 noise level，在每个 level 下通过 Langevin dynamics 以步长 <span class="math inline">\(\alpha_i\)</span> 更新若干步，最终生成新的数据。算法流程图如下所示：</p><p><img src="alg.png" alt="source:Generative modeling by estimating gradients of the data distribution." width=40% /></p><p>其中步长设置为 <span class="math inline">\(\alpha_i=\epsilon\cdot\sigma_i^2/\sigma_L^2\propto \sigma_i^2\)</span> 的动机是保证不同 noise level 下 Langevin dynamics 的信噪比 <span class="math inline">\(\frac{\alpha_i s_\theta(\x,\sigma_i)}{2\sqrt{\alpha_i}\mathbf z}\)</span> 的模长固定。</p><p><br/></p><p>这一代的 NCSN 已然能与 GAN 媲美，但局限在 32x32 的大小。在后续的工作<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, Yang, and Stefano Ermon. Improved techniques for training score-based generative models. *Advances in neural information processing systems* 33 (2020): 12438-12448.">[4]</span></a></sup>中，宋飏等人探索了更多的技巧，进一步地提升了图像生成质量，并能稳定生成 256x256 的图片。这些技巧包括：</p><ul><li><p>选择 <span class="math inline">\(\sigma_1\)</span> 与训练数据中两两之间的最大欧氏距离差不多大。</p></li><li><p>选择 <span class="math inline">\(\{\sigma_i\}_{i=1}^L\)</span> 为公比为 <span class="math inline">\(\gamma\)</span> 的等比序列，且 <span class="math display">\[\Phi\left(\sqrt{2d}(\gamma-1)+3\gamma\right)-\Phi\left(\sqrt{2d}(\gamma-1)-3\gamma\right)\approx 0.5\]</span></p></li><li><p>将 NCSN 参数化为 <span class="math inline">\(s_\theta(\x,\sigma)=s_\theta(\x)/\sigma\)</span>，其中 <span class="math inline">\(s_\theta(\x)\)</span> 是一个无条件的网络。</p></li><li><p>选择 <span class="math inline">\(T\)</span>（每个 noise level 下 Langevin dynamics 的步数）在可承受范围尽可能大，并选择 <span class="math inline">\(\epsilon\)</span>（Langevin dynamics 里与步长有关的参数）使得一个复杂的式子（懒得抄了）约等于 <span class="math inline">\(1\)</span>.</p></li><li><p>在测试（采样）的时候使用 EMA.</p></li></ul><p>具体内容和分析本文不再叙述，感兴趣的读者可以去看论文<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, Yang, and Stefano Ermon. Improved techniques for training score-based generative models. *Advances in neural information processing systems* 33 (2020): 12438-12448.">[4]</span></a></sup>。</p><h2 id="connection-to-ddpm">Connection to DDPM</h2><p>在引言里，我们说 SMLD 和 DDPM 本质是相同的，现在来看看为什么。</p><h3 id="loss-function">Loss Function</h3><p>首先回顾一下 DDPM 的损失函数： <span class="math display">\[\E_{t,\x_0,\epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(\x_t,t\right) \right\|^2\right]\tag{9}\label{ddpm-eps}\]</span> 对 <span class="math inline">\(q(\x_t\vert \x_0)=\mathcal N\left(\x_t;\sqrt{\bar\alpha_t}\x_0,(1-\bar\alpha_t)\mathbf I\right)\)</span> 求 score function： <span class="math display">\[\begin{align}\nabla_{\x_t}\log q(\x_t\vert\x_0)&amp;=-\frac{1}{2(1-\bar\alpha_t)}\nabla_{\x_t}\left\|\x_t-\sqrt{\bar\alpha_t}\x_0\right\|^2\\&amp;=-\frac{1}{1-\bar\alpha_t}\left(\x_t-\sqrt{\bar\alpha_t}\x_0\right)\\&amp;=-\frac{1}{1-\bar\alpha_t}\left(\sqrt{\bar\alpha_t}\x_0+\sqrt{1-\bar\alpha_t}\epsilon-\sqrt{\bar\alpha_t}\x_0\right)\\&amp;=-\frac{\epsilon}{\sqrt{1-\bar\alpha_t}}\end{align}\tag{10}\label{qscore}\]</span> 代入 <span class="math inline">\(\eqref{ddpm-eps}\)</span> 式： <span class="math display">\[\begin{align}&amp;\E_{t,\x_0,\epsilon}\left[\left\|\epsilon-\epsilon_\theta\left(\x_t,t\right) \right\|^2\right]\\=&amp;\E_{t,\x_0,\epsilon}\left[\left\|-\sqrt{1-\bar\alpha_t}\nabla_{\x_t}\log q(\x_t\vert \x_0)-\epsilon_\theta\left(\x_t,t\right) \right\|^2\right]\\=&amp;\E_{t,\x_0,\epsilon}\left[(1-\bar\alpha_t)\left\|\nabla_{\x_t}\log q(\x_t\vert \x_0)-s_\theta\left(\x_t,t\right) \right\|^2\right]\end{align}\]</span> 其中做了一个变量替换：<span class="math inline">\(s_\theta(\x_t,t)=-\frac{1}{\sqrt{1-\bar\alpha_t}}\epsilon_\theta(\x_t,t)\)</span>.</p><p>为了方便比较，将其改写一下： <span class="math display">\[\frac{1}{T}\sum_{t=1}^T(1-\bar\alpha_t)\E_{\pdata(\x_0)}\E_{q(\x_t\vert \x_0)}\left[\left\|\nabla_{\x_t} \log q(\x_t\vert \x_0)-s_\theta\left(\x_t,t\right) \right\|^2 \right]\tag{9&#39;}\label{ddpm}\]</span> 对比 <span class="math inline">\(\eqref{ncsn}\)</span> 式： <span class="math display">\[\frac{1}{L}\sum_{i=1}^L \lambda(\sigma_i)\E_{\pdata(\x)}\E_{q_{\sigma_i}(\tilde\x\vert\x)}\left[\frac{1}{2}\left\|\nabla_{\tilde\x}\log q_{\sigma_i}(\tilde \x\vert\x) -s_\theta(\tilde\x,\sigma_i)\right\|^2 \right]\]</span> 可以看到，<span class="math inline">\(\eqref{ddpm}\)</span> 式和 <span class="math inline">\(\eqref{ncsn}\)</span> 式<strong>几乎</strong>完全相同！二者甚至连系数都保持了一致：</p><ul><li>根据 <span class="math inline">\(\eqref{qscore}\)</span> 式有：<span class="math inline">\(1-\bar\alpha_t\propto 1/\E[\Vert\nabla_{\x_t}\log q(\x_t\vert \x_0)\Vert^2]\)</span></li><li>根据 <span class="math inline">\(\eqref{score-gauss}\)</span> 式有：<span class="math inline">\(\lambda(\sigma_i)=\sigma_i^2\propto1/\E[\Vert\nabla_{\tilde\x}\log q_\sigma(\tilde \x\vert\x)\Vert^2]\)</span></li></ul><p>唯一能揪出来的一点不同是： <span class="math display">\[\begin{align}&amp;q(\x_t\vert \x_0)=\mathcal N\left(\x_t;{\color{darkred}{\sqrt{\bar\alpha_t}\x_0}},(1-\bar\alpha_t)\mathbf I\right)&amp;&amp;\text{DDPM}\\&amp;q_{\sigma_i}(\tilde\x\vert \x)=\mathcal N\left(\tilde \x;{\color{darkred}{\x}},\sigma_i^2\mathbf I\right)&amp;&amp;\text{SMLD}\end{align}\]</span> 它们对于均值处是否加权略有分歧。DDPM 为均值加权，效果就是随着扩散过程的进行，数据的均值往 <span class="math inline">\(\mathbf 0\)</span> 移动，并且一直维持着有限的方差且越来越接近 <span class="math inline">\(\mathbf I\)</span>，最后服从 <span class="math inline">\(\mathcal N(\mathbf 0,\mathbf I)\)</span>；而 SMLD 均值没有加权，所以无论怎么加噪数据的中心始终在原本的位置，因此加噪过程其实是依靠增大 <span class="math inline">\(\sigma_i^2\)</span> 来让数据弥散到整个空间。正因如此，在之后的文章中我们可以看到，宋飏等人将前者称为 Variance Preserving，而后者称为 Variance Exploding.</p><p>但其实这个差别不是很重要，如果把 SMLD 的均值也加上权重，并不影响任何推导过程。</p><h3 id="reverse-process">Reverse Process</h3><p>虽然二者在损失函数上保持了惊人的一致，但是在生成过程上还是有一些区别。对于 DDPM，我们构建逆向的马尔可夫链，每一步从 <span class="math inline">\(p_\theta(\x_{t-1}\vert\x_t)\)</span> 中采样： <span class="math display">\[\begin{align}&amp;p_\theta(\x_{t-1}\vert \x_t)=\mathcal N(\x_{t-1}; \mu_\theta(\x_t,t),\sigma_t^2\mathbf I)\\&amp;\mu_\theta(\x_t,t)=\frac{1}{\sqrt{\alpha_t}}\left(\x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon_\theta(\x_t,t)\right)=\frac{1}{\sqrt{\alpha_t}}\left(\x_t+(1-\alpha_t)s_\theta(\x_t,t)\right)\end{align}\]</span> 或写作： <span class="math display">\[\x_{t-1}=\frac{1}{\sqrt{\alpha_t}}(\x_t+(1-\alpha_t)s_\theta(\x_t,t))+\sigma_t\mathbf z_t\]</span> 而对于 SMLD，我们在每一个 noise level <span class="math inline">\(\sigma_t\)</span> 下都进行 <span class="math inline">\(K\)</span> 步的 Langevin dynamics 游走： <span class="math display">\[\x_t^{k}\gets\x_t^{k-1}+\frac{\epsilon_t}{2}s_\theta(\x_t^{k-1},\sigma_{t})+\sqrt{\epsilon_t} \mathbf z_t^k,\quad k=1,\ldots,K\]</span> 上述过程重复 <span class="math inline">\(T\)</span> 次。</p><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Sohl-Dickstein, Jascha, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In <em>International Conference on Machine Learning</em>, pp. 2256-2265. PMLR, 2015. <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Ho, Jonathan, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. <em>Advances in Neural Information Processing Systems</em> 33 (2020): 6840-6851. <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Song, Yang, and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. <em>Advances in Neural Information Processing Systems</em> 32 (2019). <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Song, Yang, and Stefano Ermon. Improved techniques for training score-based generative models. <em>Advances in neural information processing systems</em> 33 (2020): 12438-12448. <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Hyvärinen, Aapo, and Peter Dayan. Estimation of non-normalized statistical models by score matching. <em>Journal of Machine Learning Research</em> 6, no. 4 (2005). <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Vincent, Pascal. A connection between score matching and denoising autoencoders. <em>Neural computation</em> 23, no. 7 (2011): 1661-1674. <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Song, Yang, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In <em>Uncertainty in Artificial Intelligence</em>, pp. 574-584. PMLR, 2020. <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Yang Song. Generative Modeling by Estimating Gradients of the Data Distribution. https://yang-song.net/blog/2021/score/ <a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>Denoising Diffusion-based Generative Modeling: Foundations and Applications. https://cvpr2022-tutorial-diffusion-models.github.io <a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:10" class="footnote-text"><span>Chih-Sheng Chen. Score Matching 系列 (一) Non-normalized 模型估計. https://bobondemon.github.io/2022/01/08/Estimation-of-Non-Normalized-Statistical-Models-by-Score-Matching/ <a href="#fnref:10" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:11" class="footnote-text"><span>Chih-Sheng Chen. Score Matching 系列 (二) Denoising Score Matching (DSM) 改善效率並可 Scalable. https://bobondemon.github.io/2022/03/06/A-Connection-Between-Score-Matching-and-Denoising-Autoencoders/ <a href="#fnref:11" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:12" class="footnote-text"><span>Chih-Sheng Chen. Score Matching 系列 (三) Sliced Score Matching (SSM) 同時保持效率和效果. https://bobondemon.github.io/2022/03/06/Sliced-Score-Matching-A-Scalable-Approach-to-Density-and-Score-Estimation/ <a href="#fnref:12" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:13" class="footnote-text"><span>扩散模型与能量模型，Score-Matching和SDE，ODE的关系 - 中森的文章 - 知乎 https://zhuanlan.zhihu.com/p/576779879 <a href="#fnref:13" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:14" class="footnote-text"><span>Yang Song. Sliced Score Matching: A Scalable Approach to Density and Score Estimation. https://yang-song.net/blog/2019/ssm/ <a href="#fnref:14" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>Diffusion Models</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从VAE到DDPM</title>
    <link href="/blog-main/2022/09/29/%E4%BB%8EVAE%E5%88%B0DDPM/"/>
    <url>/blog-main/2022/09/29/%E4%BB%8EVAE%E5%88%B0DDPM/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\E}{\mathbb E}\newcommand{\KL}{\mathrm{KL}}\newcommand{\calN}{\mathcal N}\newcommand{\x}{\mathbf x}\newcommand{\z}{\mathbf z}\newcommand{\coloneqq}{\mathrel{\mathrel{\vcenter{:}}=}}\]</span></p><h2 id="vae-回顾">VAE 回顾</h2><p>在<a href="/blog-main/2022/09/17/VAE%E6%A2%B3%E7%90%86/" title="VAE梳理">之前的文章</a>中，我们详细地梳理了一遍 VAE，这里做一个简单回顾。</p><p><img src="1vae.png" alt="source:Angus Turner. Diffusion Models as a kind of VAE" width=20% /></p><p>在 VAE 中，为了最大化对数似然 <span class="math display">\[L(\theta)=\log p_\theta(x)=\log\left(\int_zp_\theta(x\vert z)p(z)\mathrm dz\right)\]</span> 我们引入变分后验 <span class="math inline">\(q_\phi(z\vert x)\)</span>： <span class="math display">\[\begin{align}L(\theta)&amp;=\log p_\theta(x)\\&amp;=\E_{z\sim q_\phi(z\vert x)}\left[\log p_\theta(x)\right]\\&amp;=\E_{z\sim q_\phi(z\vert x)}\left[\log\frac{p_\theta(x,z)}{p_\theta(z\vert x)}\right]\\&amp;=\E_{z\sim q_\phi(z\vert x)}\left[\log\left(\frac{p_\theta(x,z)}{p_\theta(z\vert x)}\cdot\frac{q_\phi(z\vert x)}{q_\phi(z\vert x)}\right)\right]\\&amp;=\underbrace{\E_{z\sim q_\phi(z\vert x)}\left[\log\frac{p_\theta(x,z)}{q_\phi(z\vert x)}\right]}_\text{ELBO}+\underbrace{\E_{z\sim q_\phi(z\vert x)}\left[\log\frac{q_\phi(z\vert x)}{p_\theta(z\vert x)}\right]}_{\KL(q_\phi(z\vert x)\| p_\theta(z\vert x))}\\&amp;\geq \text{ELBO}\end{align}\tag{1}\label{vae}\]</span> 得到证据下界 ELBO，通过最大化 ELBO 来最大化对数似然。进一步地，ELBO 还可以拆写成重构项和 KL 正则项： <span class="math display">\[\begin{align}\text{ELBO}&amp;=\E_{z\sim q_\phi(z\vert x)}\left[\log\frac{p_\theta(x,z)}{q_\phi(z\vert x)}\right]\label{elbo-vae}\tag{2}\\&amp;=\E_{z\sim q_\phi(z\vert x)}\left[\log\frac{p_\theta(x\vert z)p(z)}{q_\phi(z\vert x)}\right]\\&amp;=\underbrace{\E_{z\sim q_\phi(z\vert x)}[\log p_\theta(x\vert z)]}_{\text{reconstruction}}-\underbrace{\KL(q_\phi(z\vert x)\| p(z))}_{\text{regularization}}\\\end{align}\]</span> 为了计算上的方便，实践中常将 <span class="math inline">\(p(z),q_\phi(z\vert x),p_\theta(x\vert z)\)</span> 都取为正态分布，具体而言，它们分别是： <span class="math display">\[\begin{align}&amp;p(z)\coloneqq \calN(z;0,I)\\&amp;q_\phi(z\vert x)\coloneqq\calN\left(z;\mu_\phi(x),\mathrm{diag}(\sigma_\phi^2(x))\right)\\&amp;p_\theta(x\vert z)\coloneqq \calN(\mu_\theta(z),\sigma^2 I)&amp;&amp;\sigma\text{ is a constant}\\\end{align}\]</span> 代入 <span class="math inline">\(\eqref{elbo-vae}\)</span> 式即可得到损失函数： <span class="math display">\[\mathcal L=\underbrace{\frac{1}{2\sigma^2}\|x-\mu_\theta(z)\|^2}_\text{reconstruction}+\underbrace{\frac{1}{2}\sum_{i=1}^d\left(\mu_\phi^2(x)_i+\sigma_\phi^2(x)_i-\log \sigma_\phi^2(x)_i-1\right)}_\text{KL regularization}\]</span> 正态分布的假设虽然便于计算，但也限制了 VAE 的表达能力，导致生成的图片很模糊。毕竟，从隐变量一步到位映射到数据分布确实不是一件容易的事，那我们能不能把这个过程拆解成若干步呢？</p><h2 id="双层-vae">双层 VAE</h2><p>把 VAE 中的单个隐变量 <span class="math inline">\(z\)</span> 换成两个隐变量 <span class="math inline">\(z_1,z_2\)</span>，形成如下马尔可夫链：</p><p><img src="2vae.png" alt="source:Angus Turner. Diffusion Models as a kind of VAE" width=30% /></p><p>虽然有两个隐变量，但如果把它们视为一个整体，那证据下界 ELBO 的推导过程与 <span class="math inline">\(\eqref{vae}\)</span> 式没有什么本质不同，因此我们只需将 <span class="math inline">\(\eqref{elbo-vae}\)</span> 式中的 <span class="math inline">\(z\)</span> 换做 <span class="math inline">\(z_1,z_2\)</span> 就得到了双层 VAE 的 ELBO： <span class="math display">\[\begin{align}\text{ELBO}_\text{2-layers}&amp;=\E_{z_1,z_2\sim q_\phi(z_1,z_2\vert x)}\left[\log\frac{p_\theta(x,z_1,z_2)}{q_\phi(z_1,z_2\vert x)}\right]\\&amp;=\E_{z_1,z_2\sim q_\phi(z_1,z_2\vert x)}\left[\log\frac{p_\theta(x\vert z_1)p_\theta(z_1\vert z_2)p(z_2)}{q_\phi(z_2\vert z_1)q_\phi(z_1\vert x)}\right]&amp;&amp;\text{Markov chain}\label{elbo-2}\tag{3}\end{align}\]</span></p><p>我们依旧希望把 ELBO 拆解成重构项和正则项，为此需要做一些 fancy 的数学推导。重构项比较简单，只需要把 <span class="math inline">\(p_\theta(x\vert z_1)\)</span> 拆出来就是了，问题在于剩下的一坨应该如何处理。破局的关键在于利用马尔可夫性质把分母上的 <span class="math inline">\(q_\phi(z_2\vert z_1)\)</span> 改写作 <span class="math inline">\(q_\phi(z_2\vert z_1,x)\)</span>，然后用贝叶斯公式： <span class="math display">\[q_\phi(z_2\vert z_1)=q_\phi(z_2\vert z_1,x)=\frac{q_\phi(z_1\vert z_2,x)q_\phi(z_2\vert x)}{q_\phi(z_1\vert x)}\]</span> 代回 <span class="math inline">\(\eqref{elbo-2}\)</span> 式得： <span class="math display">\[\begin{align}\text{ELBO}_\text{2-layers}&amp;=\E_{z_1,z_2\sim q_\phi(z_1,z_2\vert x)}\left[\log\frac{p_\theta(x\vert z_1)p_\theta(z_1\vert z_2)p(z_2)}{q_\phi(z_2\vert z_1)q_\phi(z_1\vert x)}\right]\\&amp;=\E_{z_1\sim q_\phi(z_1\vert x)}[p_\theta(x\vert z_1)]+\E_{z_1,z_2\sim q_\phi(z_1,z_2\vert x)}\left[\log\frac{p_\theta(z_1\vert z_2)p(z_2)}{\frac{q_\phi(z_1\vert z_2,x)q_\phi(z_2\vert x)}{q_\phi(z_1\vert x)}q_\phi(z_1\vert x)}\right]\\&amp;=\E_{z_1\sim q_\phi(z_1\vert x)}[p_\theta(x\vert z_1)]+\E_{z_1,z_2\sim q_\phi(z_1,z_2\vert x)}\left[\log\frac{p_\theta(z_1\vert z_2)p(z_2)}{q_\phi(z_1\vert z_2,x)q_\phi(z_2\vert x)}\right]\\&amp;=\E_{z_1\sim q_\phi(z_1\vert x)}[p_\theta(x\vert z_1)]+\E_{z_2\sim q_\phi(z_2\vert x)}\left[\log\frac{p(z_2)}{q_\phi(z_2\vert x)}\right]+\E_{z_1,z_2\sim q_\phi(z_1,z_2\vert x)}\left[\log\frac{p_\theta(z_1\vert z_2)}{q_\phi(z_1\vert z_2,x)}\right]\\&amp;=\underbrace{\E_{z_1\sim q_\phi(z_1\vert x)}[p_\theta(x\vert z_1)]}_{\text{reconstruction}}-\underbrace{\KL(q_\phi(z_2\vert x)\| p(z_2))}_{\text{regularization}}-\underbrace{\E_{z_2\sim q_\phi(z_2\vert x)}[\KL(q_\phi(z_1\vert z_2,x)\|p_\theta(z_1\vert z_2))]}_{\text{matching}}\\\end{align}\]</span> 我们发现，除了重构项和正则项以外，双层 VAE 多了一个匹配项，用于匹配 <span class="math inline">\(q_\phi\)</span> 和 <span class="math inline">\(p_\theta\)</span> 在两个隐变量之间的关系。另外，式子中出现了 <span class="math inline">\(q_\phi(z_2\vert x)\)</span> 这种跳过了中间步的条件概率分布，对我们选取正态分布进行简便计算带来了一定的难度。在下一节中我们将看到，DDPM 将 <span class="math inline">\(q_\phi\)</span> 设计成了一种巧妙的形式解决了这个问题。</p><h2 id="ddpm">DDPM</h2><p>在双层 VAE 的基础上，我们能再多加几层吗？</p><p><img src="diffusion.png" alt="source:Angus Turner. Diffusion Models as a kind of VAE" width=40% /></p><p>如图所示，<span class="math inline">\(\x_1,\ldots,\x_T\)</span> 整体可以看作 VAE 中的隐变量 <span class="math inline">\(z\)</span>，<span class="math inline">\(\x_0\)</span> 是 VAE 中的 <span class="math inline">\(x\)</span>. 我们称从 <span class="math inline">\(\x_0\)</span> 到 <span class="math inline">\(\x_T\)</span> 的马尔可夫链为<strong>前向过程（forward process）</strong>，从 <span class="math inline">\(\x_T\)</span> 到 <span class="math inline">\(\x_0\)</span> 的马尔可夫链为<strong>逆向过程（reverse process）</strong>。换句话说，前向过程对应 VAE 的“后验概率”，逆向过程对应“生成模型”。</p><p>为方便表示，下文将 <span class="math inline">\(\x_l,\cdots,\x_r\)</span> 简写为 <span class="math inline">\(\x_{l:r}\)</span>.</p><h3 id="前向过程">前向过程</h3><p>在双层 VAE 一节中，我们面临着一个问题——选取怎样的分布形式能够简便地表示出 <span class="math inline">\(q_\phi(z_2\vert x)\)</span> 和 <span class="math inline">\(q_\phi(z_1\vert z_2,x)\)</span>？这个问题自然也延续到了 DDPM 之中。作者给出了如下巧妙的设计： <span class="math display">\[q(\x_t\vert \x_{t-1})=\calN(\x_t;\sqrt{1-\beta_t}\x_{t-1},\beta_t\mathbf{I})\tag{4}\label{q}\]</span> 其中 <span class="math inline">\(\beta_t\in(0,1)\)</span> 是事先指定的常量，代表从 <span class="math inline">\(\x_{t-1}\)</span> 到 <span class="math inline">\(\x_t\)</span> 这一步的方差。直观上理解，如果 <span class="math inline">\(\beta_t\)</span> 比较小，那么 <span class="math inline">\(q(\x_t\vert\x_{t-1})\)</span> 均值依旧在 <span class="math inline">\(\x_{t-1}\)</span> 附近，方差也不大，故 <span class="math inline">\(\x_t\)</span> 看起来就是在 <span class="math inline">\(\x_{t-1}\)</span> 的基础上加了一些噪声，所以前向过程就是“加噪过程”。值得注意的是，这个 <span class="math inline">\(q\)</span> <strong>不带可学习参数</strong>——这是 DDPM 与 VAE 不一样的地方。</p><p><img src="forward.png" alt="source:https://cvpr2022-tutorial-diffusion-models.github.io/" width=90% /></p><p><span class="math inline">\(\eqref{q}\)</span> 式的巧妙之处在于—— <span class="math inline">\(q(\x_t\vert\x_0)\)</span> 能写作封闭形式。换句话说，如果我们想知道 <span class="math inline">\(\x_0\)</span> 在第 <span class="math inline">\(t\)</span> 步加噪后变成什么样了，我们不需要真的一步一步采样走 <span class="math inline">\(t\)</span> 步，而是能直接得到结果。为了看得更清楚，记 <span class="math inline">\(\alpha_t=1-\beta_t,\,\bar\alpha_t=\prod_{i=1}^t\alpha_i\)</span>，那么根据 <span class="math inline">\(q(\x_t\vert \x_{t-1})=\calN(\x_t;\sqrt{\alpha_t}\x_{t-1},(1-\alpha_t)\mathbf{I})\)</span>，我们可以用如下方式从 <span class="math inline">\(\x_{t-1}\)</span> 得到 <span class="math inline">\(\x_t\)</span>：</p><p><span class="math display">\[\x_t=\sqrt{\alpha_t}\x_{t-1}+\sqrt{1-\alpha_t}\epsilon_{t-1},\quad \epsilon_{t-1}\sim\calN(\mathbf 0,\mathbf{I})\]</span> 类似地，我们可以用如下方式从 <span class="math inline">\(\x_{t-2}\)</span> 得到 <span class="math inline">\(\x_{t-1}\)</span>：</p><p><span class="math display">\[\x_{t-1}=\sqrt{\alpha_{t-1}}\x_{t-2}+\sqrt{1-\alpha_{t-1}}\epsilon_{t-2},\quad \epsilon_{t-2}\sim\calN(\mathbf 0,\mathbf{I})\]</span> 合并上面两个式子，从 <span class="math inline">\(\x_{t-2}\)</span> 直接得到 <span class="math inline">\(\x_t\)</span> 写作： <span class="math display">\[\x_t=\sqrt{\alpha_t\alpha_{t-1}}\x_{t-2}+{\color{green}\sqrt{\alpha_t(1-\alpha_{t-1})}\epsilon_{t-2}+\sqrt{1-\alpha_t}\epsilon_{t-1}}\]</span> 由于两个正态随机变量之和服从均值方差分别相加的正态分布，即： <span class="math display">\[{\color{green}\sqrt{\alpha_t(1-\alpha_{t-1})}\epsilon_{t-2}+\sqrt{1-\alpha_t}\epsilon_{t-1}}\sim\calN(\mathbf 0,(1-\alpha_t\alpha_{t-1})\mathbf{I})\]</span> 所以只用一个正态随机变量即可： <span class="math display">\[\x_t=\sqrt{\alpha_t\alpha_{t-1}}\x_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}\epsilon_{t-2},\quad \epsilon_{t-2}\sim\calN(\mathbf 0,\mathbf{I})\]</span> 以此类推，我们可以得到从 <span class="math inline">\(\x_0\)</span> 直接计算 <span class="math inline">\(\x_t\)</span> 的公式： <span class="math display">\[\x_t=\sqrt{\bar\alpha_t}\x_0+\sqrt{1-\bar\alpha_t}\epsilon,\quad \epsilon\sim\calN(\mathbf 0,\mathbf{I}) \tag{5}\label{xtx0}\]</span> 也就是说： <span class="math display">\[q(\x_t\vert \x_0)=\calN\left(\x_t;\sqrt{\bar\alpha_t}\x_0,(1-\bar\alpha_t)\mathbf{I}\right)\tag{6}\label{qxtx0}\]</span> <img src="forward2.png" alt="source:https://cvpr2022-tutorial-diffusion-models.github.io/" width=90% /></p><p>我们希望无论输入什么，前向过程最后得到的分布都趋近于各分量独立的标准正态分布，即 <span class="math inline">\(p(\x_\infty)=q(\x_\infty\vert\x_0)=\calN(\x_\infty;\mathbf 0,\mathbf{I})\)</span>，因此要求： <span class="math display">\[\lim_{t\to\infty}\sqrt{\bar\alpha_t}=0,\quad\lim_{t\to\infty}\sqrt{1-\bar\alpha_t}=1\]</span> 为满足这个要求，只需 <span class="math inline">\(\alpha_1&gt;\alpha_2&gt;\cdots&gt;\alpha_T\)</span>，也即 <span class="math inline">\(\beta_1&lt;\beta_2&lt;\cdots&lt;\beta_T\)</span> 即可。从加噪的角度来看，这意味着初期加噪较弱，后期加噪变强。在 DDPM 中，作者取 <span class="math inline">\(\beta_1,\ldots,\beta_T\)</span> 为从 <span class="math inline">\(0.0001\)</span> 到 <span class="math inline">\(0.02\)</span> 的线性递增序列。</p><p><br/></p><p>看到这里，不知读者心中是否有疑惑——为什么人为设置后验分布（即前向过程）是合理的？VAE 中 <span class="math inline">\(q\)</span> 不是要去拟合真实后验分布吗，现在人为设置好了怎么去拟合啊？私以为，这个问题揭示了 VAE 和 DDPM 出发点的不同。VAE 先定义生成模型 <span class="math inline">\(p_\theta(x\vert z)\)</span>，在这个定义下，存在所谓的“真实”后验分布 <span class="math inline">\(p_\theta(z\vert x)\)</span>，但是它不可解，所以拿一个 <span class="math inline">\(q_\phi(z\vert x)\)</span> 去近似。DDPM 则是反过来，先定义后验分布（即前向过程），然后根据后验去学习生成模型（即逆向过程）。</p><h3 id="逆向过程">逆向过程</h3><p>前向过程把输入图像一步步地加噪变成了高斯噪声，逆向过程就是把噪声转换回图像，也就是我们想要的生成模型。上一小节说过，我们可以直接从 <span class="math inline">\(\x_0\)</span> 得到 <span class="math inline">\(\x_T\)</span>，那反过来，我们可以直接从 <span class="math inline">\(\x_T\)</span> 得到 <span class="math inline">\(\x_0\)</span> 吗？有人可能要说，把 <span class="math inline">\(\eqref{xtx0}\)</span> 式移个项不就行了吗： <span class="math display">\[\x_0=\frac{1}{\sqrt{\bar\alpha_t}}\left(\x_t-\sqrt{1-\bar\alpha_t}\epsilon\right)\]</span> 但是小心，<span class="math inline">\(\eqref{xtx0}\)</span> 式中的 <span class="math inline">\(\epsilon\)</span> 是随机变量，意味着 <span class="math inline">\(\x_t\)</span> 也是随机<strong>变量</strong>，其具体取值由 <span class="math inline">\(\epsilon\)</span> 实际取值决定。现在我们有一个具体的 <span class="math inline">\(\x_T\)</span>，它对应着 <span class="math inline">\(\epsilon\)</span> 的某个取值，但是什么值我们并不知道（如果仅看逆向过程）！所以我们只能以前向过程的 <span class="math inline">\(\epsilon\)</span> 取值为标签，训练一个模型去估计它，即： <span class="math display">\[\x_\theta(\x_t,t)\coloneqq\frac{1}{\sqrt{\bar\alpha_t}}\left(\x_t-\sqrt{1-\bar\alpha_t}\epsilon_\theta(\x_t,t)\right)\tag{7}\label{x0xt}\]</span> 其中 <span class="math inline">\(\epsilon_\theta(\x_t,t)\)</span> 就是所谓的模型，用来近似真实的（即前向过程采样出来的） <span class="math inline">\(\epsilon\)</span>；相应地，<span class="math inline">\(\x_\theta(\x_t,t)\)</span> 就是 <span class="math inline">\(\x_0\)</span> 的近似。或者，你也可以无视 <span class="math inline">\(\epsilon\)</span>，直接把 <span class="math inline">\(\x_\theta(\x_t,t)\)</span> 视为模型去近似 <span class="math inline">\(\x_0\)</span>. 为了训练它，最直接的想法就是用 L2 损失 <span class="math inline">\(\Vert \epsilon-\epsilon_\theta(\x_t,t)\Vert^2\)</span>（或者 <span class="math inline">\(\Vert \x_0-\x_\theta(\x_t,t)\Vert^2\)</span>），虽然 DDPM 的损失函数还真就是这个，但是现在未免有点想当然了，下一小节我们会推导出这个 L2 损失。</p><p><img src="epsilon.png" width=70% /></p><p>现在，假设我们已经训练完模型了，那是不是按照 <span class="math inline">\(\eqref{x0xt}\)</span> 式生成 <span class="math inline">\(\x_\theta(\x_T,T)\approx \x_0\)</span> 就完事儿了呢？理论上没问题，但是实际效果很差，为什么呢？如果直接用 <span class="math inline">\(\x_\theta(\x_T,T)\)</span>，那么中间的 <span class="math inline">\(\x_2,\x_3,\ldots,\x_{T-1}\)</span> 都没用了，整个 DDPM 就退化成了 VAE 的结构。但是 VAE 的生成模型和后验都是自己学习出来的，二者双向奔赴共同优化去寻找最优解；而 DDPM 的后验是人为指定的（即 <span class="math inline">\(\eqref{qxtx0}\)</span> 式），并且由于 <span class="math inline">\(\bar\alpha_T\to 0\)</span>，<span class="math inline">\(q(\x_T\vert\x_0)\)</span> 基本上就是一个标准正态分布，磨灭掉了几乎所有的输入信息，生成模型根本无法一下恢复出 <span class="math inline">\(\x_0\)</span>！</p><p>如果我们一点一点来，先生成 <span class="math inline">\(\x_{T-1}\)</span>、然后 <span class="math inline">\(\x_{T-2}\)</span>……由于每一步的变化都比较小，保留了上一步足够的信息，生成模型的负担就轻了很多。所以现在我们希望求解 <span class="math inline">\(q(\x_{t-1}\vert \x_t)\)</span>. 因为我们知道前向过程 <span class="math inline">\(q(\x_t\vert \x_{t-1})\)</span>，所以自然想到使用贝叶斯公式：</p><p><span class="math display">\[q(\x_{t-1}\vert \x_t)=\frac{q(\x_t\vert\x_{t-1})q(\x_{t-1})}{q(\x_t)}\]</span></p><p>可惜 <span class="math inline">\(q(\x_t)\)</span> 和 <span class="math inline">\(q(\x_{t-1})\)</span> 是未知的。事情到这里似乎走入了僵局，但是我们敏锐地发现 <span class="math inline">\(q(\x_t\vert \x_0)\)</span> 和 <span class="math inline">\(q(\x_{t-1}\vert \x_0)\)</span> 是已知的，如果给上式加上 <span class="math inline">\(\x_0\)</span> 为条件，可以得到： <span class="math display">\[\begin{align}q(\x_{t-1}\vert \x_t,\x_0)&amp;=\frac{q(\x_t\vert \x_{t-1},\x_0)q(\x_{t-1}\vert \x_0)}{q(\x_t\vert\x_0)}\\&amp;=\frac{q(\x_t\vert \x_{t-1})q(\x_{t-1}\vert \x_0)}{q(\x_t\vert \x_0)}\\&amp;\propto\exp\left(-\frac{1}{2}\left(\frac{(\x_t-\sqrt{\alpha_t}\x_{t-1})^2}{\beta_t}+\frac{(\x_{t-1}-\sqrt{\bar\alpha_{t-1}}\x_0)^2}{1-\bar\alpha_{t-1}}-\frac{(\x_t-\sqrt{\alpha_t}\x_0)^2}{1-\bar\alpha_t}\right)\right)\\&amp;=\exp\left(-\frac{1}{2}\left(\underbrace{\frac{1-\bar\alpha_t}{\beta_t(1-\bar\alpha_{t-1})}}_A\x_{t-1}^2+\underbrace{\left(-\frac{2\sqrt{\alpha_t}\x_t}{\beta_t}-\frac{2\sqrt{\bar\alpha_{t-1}}\x_{0}}{1-\bar\alpha_{t-1}}\right)}_B\x_{t-1}+C(\x_t,\x_0)\right)\right)\end{align}\]</span> 也就是说，<span class="math inline">\(q(\x_{t-1}\vert \x_t,\x_0)\)</span> 是一个正态分布，并且均值和方差分别为： <span class="math display">\[\begin{align}&amp;\mu_t(\x_t,\x_0)=\frac{-B}{2A}=\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}\x_t+\frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar\alpha_t}\x_0\\&amp;\tilde\beta_t=\frac{1}{A}=\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_t\\\end{align}\]</span></p><div class="note note-warning">            <p><strong>注意</strong>：<span class="math inline">\(t&gt;1\)</span> 时上面的式子没问题，但需要特别考虑 <span class="math inline">\(t=1\)</span> 的情形。当 <span class="math inline">\(t=1\)</span> 时，<span class="math inline">\(q(\x_{t-1}\vert \x_t,\x_0)=q(\x_0\vert\x_1,\x_0)\)</span> 其实是一个确定性的分布，即一定取 <span class="math inline">\(\x_0\)</span>；如果我们合理地补充定义 <span class="math inline">\(\bar\alpha_0=1\)</span>（因为 <span class="math inline">\(\bar\alpha_i\)</span> 代表累乘，第零项设置为 <span class="math inline">\(1\)</span> 很合理），会发现 <span class="math inline">\(\mu_1(\x_1,\x_0)=\x_0,\,\tilde\beta_1=0\)</span>，正好符合预期，所以上面 <span class="math inline">\(\mu_t(\x_t,\x_0)\)</span> 和 <span class="math inline">\(\tilde\beta_t\)</span> 的表达式对 <span class="math inline">\(t\geq 1\)</span> 都适用。</p>          </div><p>那么 <span class="math inline">\(q(\x_{t-1}\vert \x_t,\x_0)\)</span> 和 <span class="math inline">\(q(\x_{t-1}\vert\x_t)\)</span> 有什么关系呢？DDPM 认为： <span class="math display">\[q(\x_{t-1}\vert\x_t,\x_0)\approx q(\x_{t-1}\vert\x_t,\x_\theta(\x_t,t))\coloneqq p_\theta(\x_{t-1}\vert\x_t)\]</span> 即用模型 <span class="math inline">\(\x_\theta(\x_t,t)\)</span> 代替 <span class="math inline">\(\x_0\)</span>，这样就摆脱了对 <span class="math inline">\(\x_0\)</span> 的依赖。由于 <span class="math inline">\(\x_\theta(\x_t,t)\)</span> 是用来近似 <span class="math inline">\(\mathbf x_0\)</span> 的，所以 <span class="math inline">\(p_\theta(\x_{t-1}\vert\x_t)\)</span> 其实就是在近似 <span class="math inline">\(q(\x_{t-1}\vert\x_t,\x_0)\)</span>. 鉴于 <span class="math inline">\(q(\x_{t-1}\vert\x_t,\x_0)=\calN(\x_{t-1};\mu_t(\x_t,\x_0),\tilde\beta_t\mathbf{I})\)</span>，我们可以将 <span class="math inline">\(p_\theta(\x_{t-1}\vert\x_t)\)</span> 也写作类似的形式：</p><p><span class="math display">\[p_\theta(\x_{t-1}\vert\x_t)=\calN(\x_{t-1};\mu_\theta(\x_t,t),\sigma_t^2\mathbf{I})\]</span> 其中：</p><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">真实式</th><th style="text-align: center;">近似式</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">概率记号</td><td style="text-align: center;"><span class="math inline">\(q(\x_{t-1}\vert\x_t,\x_0)\)</span></td><td style="text-align: center;"><span class="math inline">\(p_\theta(\x_{t-1}\vert\x_t)=q(\x_{t-1}\vert\x_t,\x_\theta(\x_t,t))\)</span></td></tr><tr class="even"><td style="text-align: center;">正态分布表达式</td><td style="text-align: center;"><span class="math inline">\(\calN(\x_{t-1};\mu_t(\x_t,\x_0),\tilde\beta_t)\)</span></td><td style="text-align: center;"><span class="math inline">\(\calN\left(\x_{t-1};\ \mu_\theta(\x_t,t),{\sigma_t^2}\mathbf{I}\right)\)</span></td></tr><tr class="odd"><td style="text-align: center;">正态分布均值</td><td style="text-align: center;"><span class="math inline">\(\mu_t(\x_t,\x_0)\)</span></td><td style="text-align: center;"><span class="math inline">\(\mu_\theta(\x_t,t)=\mu_t(\x_t,\x_\theta(\x_t,t))\)</span></td></tr><tr class="even"><td style="text-align: center;">正态分布方差</td><td style="text-align: center;"><span class="math inline">\(\tilde\beta_t\)</span></td><td style="text-align: center;"><span class="math inline">\(\sigma_t^2=\tilde\beta_t\text{ or }\beta_t\)</span></td></tr></tbody></table><p>并且 <span class="math inline">\(\mu_\theta(\x_t,t),\,\x_\theta(\x_t,t),\,\epsilon_\theta(\x_t,t)\)</span> 之间有线性关系： <span class="math display">\[\begin{align}&amp;\mu_\theta(\x_t,t)=\frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}\x_t+\frac{\sqrt{\bar\alpha_{t-1}}\beta_t}{1-\bar\alpha_t}\x_\theta(\x_t,t)\\&amp;\x_\theta(\x_t,t)=\frac{1}{\sqrt{\bar\alpha_t}}\left(\x_t-\sqrt{1-\bar\alpha_t}\epsilon_\theta(\x_t,t)\right)\\&amp;\mu_\theta(\x_t,t)=\frac{1}{\sqrt{\alpha_t}}\left(\x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon_\theta(\x_t,t)\right)\end{align}\]</span> 所以它们中的任一个都可以定义为我们真正用于训练的模型。在训练结束之后，我们就能根据 <span class="math inline">\(p_\theta(\x_{t-1}\vert\x_t)\)</span> 逆向采样了。</p><div class="note note-success">            <p><strong>关于 <span class="math inline">\(p_\theta(\x_{t-1}\vert \x_t)=\calN(\x_{t-1};\mu_\theta(\x_t,t),\sigma_t^2\mathbf{I})\)</span> 中方差 <span class="math inline">\(\sigma_t^2\)</span> 的注解</strong></p><p>读过论文<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ho, Jonathan, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. *Advances in Neural Information Processing Systems* 33 (2020): 6840-6851.">[1]</span></a></sup>的朋友可能注意到了这么一句话：</p><blockquote><p>Experimentally, both <span class="math inline">\(\sigma_t^2=\beta_t\)</span> and <span class="math inline">\(\sigma_t^2 = \tilde\beta_t =\frac{1−\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_t\)</span> had similar results. The first choice is optimal for <span class="math inline">\(\x_0\sim\calN(\mathbf 0,\mathbf{I})\)</span>, and the second is optimal for <span class="math inline">\(\x_0\)</span> deterministically set to one point.</p></blockquote><p>这里的第二个方差选择不难理解，就是沿用了 <span class="math inline">\(q(\x_{t-1}\vert\x_t,\x_0)\)</span> 的方差。但是第一个方差选择是怎么回事？论文甚至还说方差可以是可学习的，又是怎么回事？</p><p>实话实说，我被这个问题困扰了很久，后来终于想通了。<span class="math inline">\(q(\x_{t-1}\vert \x_t,\x_0)\)</span> 的方差确实就是 <span class="math inline">\(\tilde\beta_t\)</span>，我们现在想用 <span class="math inline">\(p_\theta(\x_{t-1}\vert\x_t)\)</span> 来近似它。最直观的选择就是让后者沿用前者的方差，这样我们只需要近似均值就行了。但是这样做不一定最合理，毕竟均值的近似存在误差，我们也许可以通过改变方差来对均值的预测误差进行弥补。因此，两种方差的选择都不算“错”。</p><p>现在让我们追根溯源一下：为什么有误差？因为近似。哪里有近似？<span class="math inline">\(\x_\theta(\x_t,t)\to \x_0\)</span>. 为什么要近似 <span class="math inline">\(\x_0\)</span>？因为要计算 <span class="math inline">\(q(\x_{t-1}\vert\x_t,\x_0)\)</span>. 为什么算它？因为 <span class="math inline">\(q(\x_{t-1}\vert \x_t)=\frac{q(\x_t\vert\x_{t-1})q(\x_{t-1})}{q(\x_t)}\)</span> 算不了。为什么算不了？因为不知道 <span class="math inline">\(q(\x_t)\)</span> 和 <span class="math inline">\(q(\x_{t-1})\)</span>！</p><p>如果我们引入一些假设让 <span class="math inline">\(q(\x_t)\)</span> 是可计算的，那不就不用近似了吗？</p><ul><li><p>假设一：<span class="math inline">\(\x_0\sim\calN(\mathbf 0,\mathbf{I})\)</span>，那么根据 <span class="math inline">\(\eqref{xtx0}\)</span> 式，容易知道： <span class="math display">\[q(\x_t)=\calN(\x_t;\mathbf 0,\mathbf{I})\]</span> 于是 <span class="math inline">\(q(\x_{t-1}\vert \x_t)=q(\x_t\vert \x_{t-1})=\calN(\x_t;\sqrt{1-\beta_t}\x_{t-1},\beta_t\mathbf{I})\)</span>. 这告诉我们，此时的方差应该取 <span class="math inline">\(\sigma_t^2=\beta_t\)</span>.</p></li><li><p>假设二：<span class="math inline">\(\x_0\sim \delta(\mathbf 0)\)</span>，即数据集只有一个样本 <span class="math inline">\(\x_0=\mathbf 0\)</span>，那么根据 <span class="math inline">\(\eqref{xtx0}\)</span> 式，容易知道： <span class="math display">\[q(\x_t)=\calN(\x_t;\mathbf 0,(1-\bar\alpha_t)\mathbf{I})\]</span> 后续的推导和上文其实没有什么区别，只要把所有的 <span class="math inline">\(\x_0\)</span> 换成 <span class="math inline">\(\mathbf 0\)</span> 即可。所以最后的结论是：<span class="math inline">\(\sigma_t^2=\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_t\)</span>.</p></li></ul><p>通过这两个极端的假设，我们得到了论文里说的两种选择。实验发现，二者效果无显著差异。</p><p>当然，<strong>这些假设本身也不合理</strong>——谁的数据是一堆高斯噪声啊！只是为了寻找方差可能的选择而做的一些试验性假设罢了。所以我们也看到了 DDPM 的改进空间——有没有办法不依赖于这些奇怪的假设而找到<strong>最优</strong>的方差呢？根据前文的讨论，我们甚至能猜到这样的方差一定与均值存在某种关系。事实上，ICLR 2022 的 Analytic-DPM 就给出了答案，不过相关内容只能留到以后的文章了。</p>          </div><p><img src="reverse.png" alt="source:https://cvpr2022-tutorial-diffusion-models.github.io/" width=90% /></p><p>纵观整个过程，我们说 <span class="math inline">\(\eqref{x0xt}\)</span> 式不能直接拿来生成，但是在每一小步中又用到了 <span class="math inline">\(\eqref{x0xt}\)</span> 式，这怎么理解？其实 <span class="math inline">\(\eqref{x0xt}\)</span> 式可以理解为大方向，我们沿着大方向走一小步，然后重新看看大方向在哪里，再走下一小步。打个比方，我想从成都走到深圳，我知道大致要朝东南 45° 方向走，但是“差之毫厘，谬以千里”，直接走可能一不小心就登陆台湾了，所以我先走一小步到重庆；然后再看地图，大方向变成了东南 50°，于是又走一小步，但是拐弯过猛到了贵阳；没关系，再看地图，大方向变成了东南 30°……这样每走一小步都对大方向做一点修正，最后就能平稳地到达目的地了。</p><h3 id="损失函数">损失函数</h3><p>上一小节提到过，损失函数其实就是 <span class="math inline">\(\Vert \epsilon-\epsilon_\theta(\x_t,t)\Vert^2\)</span>，这一小节我们来详细推导一下。（其实上一小节多少有点马后炮，是在 DDPM 出来之后尝试去理解它，因此有些地方显得比较奇怪。要做真正的研究，还得直接从 ELBO 下手。）</p><p>同双层 VAE 一样的道理，直接改写 <span class="math inline">\(\eqref{elbo-vae}\)</span> 式（<span class="math inline">\(\z\)</span> 换成 <span class="math inline">\(\x_{1:T}\)</span>，<span class="math inline">\(\x\)</span> 换成 <span class="math inline">\(\x_0\)</span>）就可以得到 DDPM 的 ELBO： <span class="math display">\[\begin{align}\text{ELBO}&amp;=\E_{\x_{1:T}\sim q(\x_{1:T}\vert \x_0)}\left[\log\frac{p_\theta(\x_{0:T})}{q(\x_{1:T}\vert \x_0)}\right]\\&amp;=\E_{\x_{1:T}\sim q(\x_{1:T}\vert \x_0)}\left[\log\frac{p(\x_T)\prod_{t=1}^Tp_\theta(\x_{t-1}\vert\x_t)}{\prod_{t=1}^T q(\x_t\vert \x_{t-1})}\right]\\\end{align}\tag{8}\label{obj}\]</span> 接下来的推导技巧和双层 VAE 如出一辙，即将分母中的 <span class="math inline">\(q(\x_t\vert\x_{t-1})\)</span> 写作 <span class="math inline">\(q(\x_t\vert\x_{t-1},\x_0)\)</span>，然后使用贝叶斯公式，即可进行<strong>大量的消元</strong>： <span class="math display">\[\begin{align}\prod_{t=1}^T q(\x_t\vert \x_{t-1})&amp;=q(\x_1\vert\x_0)\prod_{t=2}^T q(\x_t\vert \x_{t-1})\\&amp;=q(\x_1\vert\x_0)\prod_{t=2}^T q(\x_t\vert \x_{t-1},\x_0)\\&amp;=q(\x_1\vert\x_0)\prod_{t=2}^T \frac{q(\x_t\vert\x_0) q(\x_{t-1}\vert\x_t,\x_0)}{q(\x_{t-1}\vert\x_0)}\\&amp;=q(\x_T\vert\x_0)\prod_{t=2}^T q(\x_{t-1}\vert\x_t,\x_0)\end{align}\]</span> 代回 <span class="math inline">\(\eqref{obj}\)</span> 式得： <span class="math display">\[\begin{align}\text{ELBO}&amp;=\E_{\x_{1:T}\sim q(\x_{1:T}\vert \x_0)}\left[\log\frac{p(\x_T)\prod_{t=1}^Tp_\theta(\x_{t-1}\vert\x_t)}{\prod_{t=1}^T q(\x_t\vert \x_{t-1})}\right]\\&amp;=\E_{\x_{1:T}\sim q(\x_{1:T}\vert \x_0)}\left[\log\frac{p(\x_T)p_\theta(\x_0\vert\x_1)\prod_{t=2}^Tp_\theta(\x_{t-1}\vert\x_t)}{q(\x_T\vert\x_0)\prod_{t=2}^T q(\x_{t-1}\vert\x_t,\x_0)}\right]\\&amp;=\E_{\x_1\sim q(\x_1\vert\x_0)}[\log p_\theta(\x_0\vert\x_1)]+\E_{\x_T\sim q(\x_T\vert \x_0)}\left[\log\frac{p(\x_T)}{q(\x_T\vert\x_0)}\right]+\sum_{t=2}^T\E_{\x_{t-1},\x_t\sim q(\x_{t-1},\x_t\vert \x_0)}\left[\log\frac{p_\theta(\x_{t-1}\vert\x_t)}{q(\x_{t-1}\vert\x_t,\x_0)}\right]\\&amp;=\underbrace{\E_{\x_1\sim q(\x_1\vert\x_0)}[\log p_\theta(\x_0\vert\x_1)]}_{\text{reconstruction}}-\underbrace{\KL(q(\x_T\vert \x_0)\|p(\x_T))}_{\text{regularization}}-\sum_{t=2}^T\underbrace{\E_{\x_t\sim q(\x_t\vert\x_0)}\left[\KL(q(\x_{t-1}\vert\x_t,\x_0)\|p_\theta(\x_{t-1}\vert\x_t))\right]}_{\text{matching}}\end{align}\]</span> 出现了重构项、正则项和匹配项。直观上，匹配项的意义是让 <span class="math inline">\(p_\theta(\x_{t-1}\vert \x_t)\)</span> 逼近“真实标签” <span class="math inline">\(q(\x_{t-1}\vert \x_t,\x_0)\)</span>，这也证实了上一节我们的讨论。特别地，由于我们的 <span class="math inline">\(q\)</span> 是特殊设计的，<span class="math inline">\(q(\x_T\vert\x_0)\)</span> 在 <span class="math inline">\(T\)</span> 较大时趋近于标准正态分布，所以正则项可以忽略。对于匹配项而言，由于 <span class="math inline">\(q(\x_{t-1}\vert\x_t,\x_0)=\calN(\x_{t-1};\mu_t(\x_t,\x_0),\tilde\beta_t\mathbf{I})\)</span>、<span class="math inline">\(p_\theta(\x_{t-1}\vert \x_t)=\calN(\x_{t-1};\mu_\theta(\x_t,t),\sigma^2_t\mathbf{I})\)</span>，根据两个正态分布的 KL 散度计算公式，<strong>假设取 <span class="math inline">\(\sigma_t^2=\tilde\beta_t\)</span></strong>，那么有： <span class="math display">\[\mathrm{KL}(q(\x_{t-1}\vert \x_t,\x_0)\ \|\ p_\theta(\x_{t-1}\vert\x_t))=\frac{1}{2\sigma_t^2}\|\mu_t(\x_t,\x_0)-\mu_\theta(\x_t,t)\|^2\]</span> 类似的，把正态分布代入重构项： <span class="math display">\[\log p_\theta(\x_0\vert\x_1)=\text{constant}-\frac{1}{2\sigma_1^2}\|\x_0-\mu_\theta(\x_1,1)\|^2\]</span> 其中 <span class="math inline">\(\x_0\)</span> 可以统一格式写作 <span class="math inline">\(\mu_1(\x_1,\x_0)\)</span>，因此总的损失函数为： <span class="math display">\[\mathcal L(\theta)=\sum_{t=1}^T\frac{1}{2\sigma_t^2}\E_{\x_t\sim q(\x_t\vert\x_0)}\left[\|\mu_t(\x_t,\x_0)-\mu_\theta(\x_t,t)\|^2\right]\]</span> 这就出现了 L2 损失。</p><p>基于 <span class="math inline">\(\mu_\theta,\x_\theta,\epsilon_\theta\)</span> 之间的线性关系，如果以 <span class="math inline">\(\x_\theta(\x_t,t)\)</span> 为模型，那么代入关系式，对应的损失函数为： <span class="math display">\[\mathcal L(\theta)=\sum_{t=1}^T\frac{\bar\alpha_{t-1}\beta_t^2}{2\sigma_t^2(1-\bar\alpha_t)^2}\E_{\x_t\sim q(\x_t\vert\x_0)}\left[\left\|\x_0-\x_\theta(\x_t,t)\right\|^2\right]\]</span> 如果以 <span class="math inline">\(\epsilon_\theta(\x_t,t)\)</span> 为模型，那么代入关系式，对应的 KL 散度为： <span class="math display">\[\mathcal L(\theta)=\sum_{t=1}^T\frac{\beta_t^2}{2\sigma_t^2{\alpha_t}(1-\bar\alpha_t)}\E_{\x_t\sim q(\x_t\vert\x_0)}\left[\left\|{\epsilon}-\epsilon_\theta(\x_t,t)\right\|^2\right]\]</span> DDPM 作者发现 <span class="math inline">\(\epsilon_\theta(\x_t,t)\)</span> 的实践效果最好并且系数可以丢掉，所以最终的简化版损失函数为： <span class="math display">\[\begin{align}\mathcal L_\text{simple}(\theta)&amp;=\E_{t,\x_0,\epsilon}\left[\left\|\epsilon-\epsilon_\theta(\x_t,t)\right\|^2\right]\\&amp;=\E_{t,\x_0,\epsilon}\left[\left\|\epsilon-\epsilon_\theta({\sqrt{\bar\alpha_t}\x_0+\sqrt{1-\bar\alpha_t}\epsilon},t)\right\|^2\right]\end{align}\]</span> 相应算法流程如下：</p><p><img src="DDPM-algo.png" alt="source:[Ho et al. 2020](https://arxiv.org/abs/2006.11239)" width=100% /></p><p>可见 DDPM 虽然推导有些复杂，但最后得到的算法流程却异常简单，效果也很好，难怪很快成为了研究的热点。</p><h2 id="代码实现">代码实现</h2><p>Github repo: <a href="https://github.com/xyfJASON/Diffusion-Models-Implementations" class="uri">https://github.com/xyfJASON/Diffusion-Models-Implementations</a></p><h3 id="结果展示">结果展示</h3><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="result-mnist.png" width=300 /></div><div class="group-image-wrap"><img src="result-cifar10.png" width=300 /></div></div></div><p>更多内容请查看代码仓库。</p><h3 id="关于-clipping-的注解">关于 Clipping 的注解</h3><p>在官方代码<sup id="fnref:16" class="footnote-ref"><a href="#fn:16" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://github.com/hojonathanho/diffusion">[16]</span></a></sup>和若干其他实现中，我发现大家普遍喜欢使用 clipping，即对于逆向过程的<strong>每一步</strong>，在预测 <span class="math inline">\(\epsilon_\theta(\x_t,t)\)</span> 之后，不直接算 <span class="math inline">\(\mu_\theta(\x_t,t)\)</span>，而是先算 <span class="math inline">\(\x_\theta(\x_t,t)\)</span>，然后 clip 到 <span class="math inline">\([-1,1]\)</span> 之间，再算 <span class="math inline">\(\mu_\theta(\x_t,t)\)</span>. 这样做为什么合理呢？Clipping 本质上是对模型误差的人工修正——<span class="math inline">\(\x_\theta(\x_t,t)\)</span> 是用来估计 <span class="math inline">\(\x_0\)</span> 的，本就应该在 <span class="math inline">\([-1,1]\)</span> 之间，只是出于模型误差而跳脱了这个范围，所以强行把它 clip 回来并不违背理论；另外，clipping 只影响逆向过程，并不需要重新训练模型。</p><h3 id="色调偏移问题">色调偏移问题</h3><p>早期的实现版本在 MNIST 上 work 得很好，但是在 CelebA-HQ 上训练时出现了<strong>色调偏移</strong>（color shifting）问题。具体而言，我发现各个 epoch 之间的图片色调会发生明显偏移，比如前一个 epoch 图片都偏红，后一个 epoch 图片都偏蓝，有时候甚至亮/暗得根本看不清人脸，如下图所示：</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="result-red.png" /></div><div class="group-image-wrap"><img src="result-blue.png" /></div><div class="group-image-wrap"><img src="result-oversaturate.png" /></div><div class="group-image-wrap"><img src="result-undersaturate.png" /></div></div></div><p>本以为是模型还没收敛，但是 300 多个 epochs 之后仍然是这样，这就不得不重视起来。一番排查后，发现是我偷懒没有实现 EMA 导致的，特别是原作者把 decay rate 设置为 0.9999，意味着参数更新其实是很慢的。EMA 的本质是对历史权重做了加权平均，可以看作若干历史模型的集成。从这个角度来说，那些色调发生不同偏移的模型互相“抵消”，从而缓解了色调偏移问题。（注意只是缓解，并没有消除！）</p><p>后来我读到其实宋飏在论文<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Song, Yang, and Stefano Ermon. Improved techniques for training score-based generative models. *Advances in neural information processing systems* 33 (2020): 12438-12448.">[3]</span></a></sup>里面就提到了这一现象，这也是他引入 EMA 的原因。说到底，色调偏移就是模型还没有收敛到真实分布的一个表现而已，只不过视觉上给人的冲击比较强烈罢了。</p><p>[update 2022.11.27] 虽然 EMA 的 decay rate 设置为 0.9999，但 tensorflow 的官方实现其实是这样的： <span class="math display">\[\text{decay}=\min\left(\text{decay}_\max,\frac{1+\text{num_updates}}{10+\text{num_updates}}\right)\]</span> 随着 num_updates 增加，对应的 decay 序列是 <span class="math inline">\(0.1818,0.2500,0.3077,0.3571,0.4000,\ldots\)</span>，一直到 90000 步左右 decay 才会固定在 0.9999. 这样做能减小初始化的随机权重对整体权重的影响，模型见效更快。</p><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Ho, Jonathan, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. <em>Advances in Neural Information Processing Systems</em> 33 (2020): 6840-6851. <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Sohl-Dickstein, Jascha, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In <em>International Conference on Machine Learning</em>, pp. 2256-2265. PMLR, 2015. <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Song, Yang, and Stefano Ermon. Improved techniques for training score-based generative models. <em>Advances in neural information processing systems</em> 33 (2020): 12438-12448. <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Luo, Calvin. Understanding diffusion models: A unified perspective. <em>arXiv preprint arXiv:2208.11970</em> (2022). <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Lilian Weng. What are Diffusion Models?. https://lilianweng.github.io/posts/2021-07-11-diffusion-models/. <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Angus Turner. Diffusion Models as a kind of VAE. https://angusturner.github.io/generative_models/2021/06/29/diffusion-probabilistic-models-I.html <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Denoising Diffusion-based Generative Modeling: Foundations and Applications. https://cvpr2022-tutorial-diffusion-models.github.io <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>苏剑林. (Jul. 06, 2022). 《生成扩散模型漫谈（二）：DDPM = 自回归式VAE 》[Blog post]. Retrieved from https://kexue.fm/archives/9152 <a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>苏剑林. (Jul. 19, 2022). 《生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪 》[Blog post]. Retrieved from https://kexue.fm/archives/9164 <a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:10" class="footnote-text"><span>由浅入深了解Diffusion Model - ewrfcas的文章 - 知乎 https://zhuanlan.zhihu.com/p/525106459 <a href="#fnref:10" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:11" class="footnote-text"><span>扩散模型之DDPM - 小小将的文章 - 知乎 https://zhuanlan.zhihu.com/p/563661713 <a href="#fnref:11" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:12" class="footnote-text"><span>【54、Probabilistic Diffusion Model概率扩散模型理论与完整PyTorch代码详细解读】 https://www.bilibili.com/video/BV1b541197HX?share_source=copy_web&amp;vd_source=a43b4442e295a96065c7ae919b4866d3 <a href="#fnref:12" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:13" class="footnote-text"><span>【Diffusion Model：比“GAN”还要牛逼的图像生成模型！公式推导+论文精读，迪哥打你从零详解扩散模型！】 https://www.bilibili.com/video/BV1pD4y1179T?share_source=copy_web&amp;vd_source=a43b4442e295a96065c7ae919b4866d3 <a href="#fnref:13" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:14" class="footnote-text"><span>https://huggingface.co/blog/annotated-diffusion <a href="#fnref:14" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:15" class="footnote-text"><span>https://github.com/lucidrains/denoising-diffusion-pytorch <a href="#fnref:15" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:16" class="footnote-text"><span>https://github.com/hojonathanho/diffusion <a href="#fnref:16" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:17" class="footnote-text"><span>https://github.com/openai/improved-diffusion <a href="#fnref:17" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:18" class="footnote-text"><span>https://github.com/lucidrains/imagen-pytorch <a href="#fnref:18" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:19" class="footnote-text"><span>【炼丹技巧】指数移动平均（EMA）的原理及PyTorch实现 - Nicolas的文章 - 知乎 https://zhuanlan.zhihu.com/p/68748778 <a href="#fnref:19" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:20" class="footnote-text"><span>https://github.com/tqch/ddpm-torch <a href="#fnref:20" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:21" class="footnote-text"><span>https://github.com/abarankab/DDPM <a href="#fnref:21" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:22" class="footnote-text"><span>https://github.com/w86763777/pytorch-ddpm <a href="#fnref:22" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>Diffusion Models</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VAE梳理</title>
    <link href="/blog-main/2022/09/17/VAE%E6%A2%B3%E7%90%86/"/>
    <url>/blog-main/2022/09/17/VAE%E6%A2%B3%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\R}{\mathbb R}\newcommand{\N}{\mathcal N}\]</span></p><p>虽然早在看 CS231n 时就学过 VAE 了，但当时学的可谓是不明不白、一塌糊涂，假装懂了的样子就把它放过去了。其实倒也无可厚非，毕竟那时候刚刚入门，如果硬攻的话不知要浪费多少时间。而今我痛定思痛，决定好好学它一遭，不把 VAE 搞懂就誓不……咳咳，打住，打住……</p><h2 id="核心思想">核心思想</h2><p>VAE 的目标是训练一个生成器，将随机向量 <span class="math inline">\(z\in\R^d\)</span>（一般采自正态分布）映射到 <span class="math inline">\(x\in\R^D\)</span>，并要求 <span class="math inline">\(x\)</span> 的分布尽可能接近真实数据的分布，如图所示：</p><p><img src="1.png" alt="source: https://spaces.ac.cn/archives/5253" width=70% /></p><p>注意，<span class="math inline">\(z\)</span> 到 <span class="math inline">\(x\)</span> 并不是一一映射，<strong>事实上，在 VAE 中同一个输入 <span class="math inline">\(z\)</span> 可以映射到多个输出 <span class="math inline">\(x\)</span></strong>。</p><p>既然是衡量两个分布的相似度，我们能否直接用各种散度（如 KL 散度）作为损失函数呢？很遗憾不行，因为我们不知道分布的表达式，只知道从分布中采出的样本，而散度的计算需要具体的表达式。</p><p>熟悉 GANs 数学原理的朋友可能已经发现了，GANs 引入判别器，本质上是用一种<strong>隐式</strong>的方式优化了 KL 散度（或者 JS 散度、Wasserstein distance 等各种衡量分布相似度的指标）。</p><p>回到 VAE 来，一个生成网络可以表达为 <span class="math inline">\(p_\theta(x\vert z)\)</span>——其中 <span class="math inline">\(\theta\)</span> 是模型参数，<span class="math inline">\(z\)</span> 是模型输入，并且模型对同一个输入有不止一个可能的输出，因此写作概率分布。那么它能够生成的数据的概率分布为： <span class="math display">\[p_\theta(x)=\int_zp_\theta(x\vert z)p(z)\mathrm dz\]</span></p><p>我们希望 VAE 生成真实样本的概率变大。考虑用极大似然法，我们采样一些真实样本 <span class="math inline">\(x\)</span> ，并最大化它们的对数似然： <span class="math display">\[L(\theta)=\log p_\theta(x)=\log\left(\int_zp_\theta(x\vert z)p(z)\mathrm dz\right)\]</span> <span class="math inline">\(\log\)</span> 里有个积分的形式阻碍了我们继续求解，这是否让你想起了什么——没错，EM 算法！</p><blockquote><p>EM 算法回顾（基于<a href="/blog-main/2022/08/23/EM%E7%AE%97%E6%B3%95/" title="EM算法">EM算法</a> 1.6 节）：引入概率分布 <span class="math inline">\(q(z)\)</span>，则： <span class="math display">\[\begin{align}L(\theta)&amp;=\log p_\theta(x)\\&amp;=\int_z q(z)\log p_\theta(x)\mathrm dz\\&amp;=\int_z q(z)\log \left(\frac{p_\theta(x\vert z)p(z)}{p_\theta(z\vert x)}\cdot\frac{q(z)}{q(z)}\right)\mathrm dz\\&amp;=\int_z q(z)\left[\log\frac{p(z)}{q(z)}+\log p_\theta(x\vert z)+\log\frac{q(z)}{p_\theta(z\vert x)}\right]\mathrm dz\\&amp;=\underbrace{\mathbb E_{z\sim q(z)}[\log p_\theta(x\vert z)]-\mathrm {KL}(q(z)\|p(z))}_{\mathrm{ELBO}}+\underbrace{\mathrm{KL}(q(z)\|p_\theta(z\vert x))}_{\mathrm {KL}}\label{em}\tag{1}\end{align}\]</span> 优化过程是迭代执行 E-step 和 M-step：</p><ul><li>E-step：固定 <span class="math inline">\(\theta\)</span>，取 <span class="math inline">\(q(z)=p_\theta(z\vert x)\)</span>，即使得 <span class="math inline">\(\mathrm{KL}(q(z)\Vert p_\theta(z\vert x))=0\)</span>，也即让 ELBO 增大到与 <span class="math inline">\(L(\theta)\)</span> 相等。</li><li>M-step：固定 <span class="math inline">\(q(z)\)</span> 不变，最大化 ELBO，从而达到优化 <span class="math inline">\(L(\theta)\)</span> 的目的。</li></ul></blockquote><p>非常可惜的是，EM 算法无法直接应用于此，因为 E-step 要求我们能够表达出后验分布 <span class="math inline">\(p_\theta(z\vert x)\)</span>，但在 VAE 中，后验分布是 <strong>intractable</strong> 的。所以我们陷入了这样的境地：我们知道 <span class="math inline">\(p_\theta(z\vert x)\)</span> 是 <span class="math inline">\(q(z)\)</span> 的最优解，但是表达不出来。这就好比我们知道一个方程有解，但是无法显式地把解写出来一样尴尬。怎么办呢？解析走不通，数值法来凑！就好比用数值方法来求方程的根，我们现在用数值优化的方式——比如梯度下降——让 <span class="math inline">\(q(z)\)</span> 去逼近 <span class="math inline">\(p_\theta(z\vert x)\)</span>，即最小化 <span class="math inline">\(\mathrm{KL}(q(z)\Vert p_\theta(z\vert x))\)</span>. 当然，为了用上梯度下降，我们需要把 <span class="math inline">\(q(z)\)</span> 参数化为 <span class="math inline">\(q_\phi(z\vert x)\)</span>，这样对 <span class="math inline">\(\phi\)</span> 求梯度就好了.</p><blockquote><p>为什么是 <span class="math inline">\(q_\phi(z\vert x)\)</span> 而不是 <span class="math inline">\(q_\phi(z)\)</span>？因为 EM 算法中找到的最优 <span class="math inline">\(q(z)\)</span>，即 <span class="math inline">\(q^\ast(z)=p_\theta(z\vert x)\)</span>，其实是依赖于 <span class="math inline">\(x\)</span> 的，即不同的数据的最优 <span class="math inline">\(q(z)\)</span> 是不一样的，只是没在记号中体现出来而已。</p></blockquote><p>即便如此，计算 KL 不也得知道 <span class="math inline">\(p_\theta(z\vert x)\)</span> 表达式才行吗？注意，现在我们一直待在 E-step 里，所以 <span class="math inline">\(L(\theta)\)</span> 是定值，观察 <span class="math inline">\(\eqref{em}\)</span> 式，最小化 KL 项，等价于最大化 ELBO 项！所以我们可以通过<strong>最大化 ELBO 来间接地最小化 KL</strong><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="原来VAE是这么回事（从EM到VAE） - 市井小民的文章 - 知乎 https://zhuanlan.zhihu.com/p/368959795">[3]</span></a></sup>！（妙啊～</p><p>行文至此，原本的 EM 算法已经变成了<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="EM的升级打怪之路：EM-变分EM-VAE（part1） - Young Zicon的文章 - 知乎 https://zhuanlan.zhihu.com/p/418203971">[4]</span></a></sup>：</p><ul><li>E-step：固定 <span class="math inline">\(\theta\)</span>，梯度下降优化 ELBO</li><li>M-step：固定 <span class="math inline">\(q_\phi(z\vert x)\)</span>，优化 ELBO</li></ul><p>鉴于 E-step 一定用的是梯度下降，我们干脆在 M-step 也用梯度下降（而非算解析解），那就没有必要迭代了，直接两步合一步优化 ELBO 即可： <span class="math display">\[\max_{\theta,\phi}\quad\mathrm{ELBO}=\mathbb E_{z\sim q_\phi(z\vert x)}[\log p_\theta(x\vert z)]-\mathrm{KL}(q_\phi(z\vert x)\|p(z))\]</span> 取个负号就是 VAE 的损失函数： <span class="math display">\[\mathcal L=\mathbb E_{z\sim q_\phi(z\vert x)}[-\log p_\theta(x\vert z)]+\mathrm{KL}(q_\phi(z\vert x)\|p(z))\tag{2}\label{obj}\]</span></p><p><br/></p><p>我们看到，VAE 的损失函数由两部分构成：</p><ol type="1"><li><span class="math inline">\(\mathbb E_{z\sim q_\phi(z\vert x)}[-\log p_\theta(x\vert z)]\)</span> 是重构项，最大化 <span class="math inline">\(x\)</span> 被重构的似然；</li><li><span class="math inline">\(\mathrm{KL}(q_\phi(z\vert x)\Vert p(z))\)</span> 可以视作正则项，让估计的后验分布逼近先验分布。</li></ol><p>怎么理解呢？假设只有重构项，可以想见为了更好的重构，网络会尽可能地减小不确定性——一方面让分布 <span class="math inline">\(q_\phi(z\vert x)\)</span> 的方差很小，基本集中在一个点上；另一方面对不同的 <span class="math inline">\(x\)</span> 让分布 <span class="math inline">\(q_\phi(z\vert x)\)</span> 均值差异很大，以便更好地区分不同 <span class="math inline">\(x\)</span> 编码出来的 <span class="math inline">\(z\)</span><sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="半小时理解变分自编码器 - 多头注意力的文章 - 知乎 https://zhuanlan.zhihu.com/p/144649293">[8]</span></a></sup>（如下左图所示）。如此一来，VAE 就退化成一般的 auto-encoder 了；而正则项强制让 <span class="math inline">\(q_\phi(z\vert x)\)</span> 逼近 <span class="math inline">\(p(z)\)</span>，一个我们预先设定的分布，就可以约束上述两点的发生（如下右图所示）。所以二者之间存在一种“对抗”的感觉<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="苏剑林. (Mar. 18, 2018). 《变分自编码器（一）：原来是这么一回事 》[Blog post]. Retrieved from https://spaces.ac.cn/archives/5253">[1]</span></a></sup><sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="苏剑林. (Mar. 28, 2018). 《变分自编码器（二）：从贝叶斯观点出发 》[Blog post]. Retrieved from https://spaces.ac.cn/archives/5343">[2]</span></a></sup>。如此训练，VAE 被迫在具有一定随机性的噪声下重构出真实数据，自然就比 auto-encoder 有更好的泛化性。</p><p><img src="reg.png" alt="source: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" width=80% /></p><h2 id="具体化">具体化</h2><p>我们现在得到了 VAE 的损失函数 <span class="math inline">\(\eqref{obj}\)</span>，但其中的 <span class="math inline">\(p(z)\)</span>、<span class="math inline">\(p_\theta(x\vert z)\)</span>、<span class="math inline">\(q_\phi(z\vert x)\)</span> 具体是什么并没有说明。要真正落地，还需要把它们“实例化”。</p><h3 id="encoder-network">Encoder network</h3><p>首先考虑损失函数中 <span class="math inline">\(\mathrm{KL}(q_\phi(z\vert x)\Vert p(z))\)</span> 一项。由于正态分布的 KL 散度相对来说好算一些，我们希望 <span class="math inline">\(p(z)\)</span> 和 <span class="math inline">\(q_\phi(z\vert x)\)</span> 都是正态分布：</p><ul><li><span class="math inline">\(p(z)\)</span>：简便起见，直接取为 <span class="math inline">\(\N(0, I)\)</span> 标准正态分布；</li><li><span class="math inline">\(q_\phi(z\vert x)\)</span>：考虑到它依赖于 <span class="math inline">\(x\)</span>，所以应该是 <span class="math inline">\(\N(\mu_\phi(x),\Sigma_\phi(x))\)</span> 的形式。可是 <span class="math inline">\(\mu_\phi(x),\Sigma_\phi(x)\)</span> 用怎样的函数才好呢？在深度学习的时代，这种开放性问题就无脑上神经网络呗！这就是 VAE 中的 encoder network。</li></ul><p>于是<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="Doersch, Carl. Tutorial on variational autoencoders. *arXiv preprint arXiv:1606.05908* (2016).">[7]</span></a></sup>： <span class="math display">\[\begin{align}&amp;\mathrm{KL}(q_\phi(z\vert x)\|p(z))\\=&amp;\mathrm{KL}(\N(\mu_\phi(x),\Sigma_\phi(x))\|\N(0,I))\\=&amp;\frac{1}{2}\left[\mathrm{tr}(\Sigma_\phi(x))+\mu_\phi(x)^T\mu_\phi(x)-d-\log\det(\Sigma_\phi(x)) \right]\end{align}\]</span> 实操时，我们一般会做简化——取 <span class="math inline">\(\Sigma_\phi(x)=\mathrm{diag}(\sigma_\phi^2(x))\)</span>，即各分量独立，协方差矩阵只有对角线有值，那么： <span class="math display">\[\begin{align}&amp;\mathrm{KL}(q_\phi(z\vert x)\|p(z))\\=&amp;\mathrm{KL}(\N(\mu_\phi(x),\mathrm{diag}(\sigma_\phi^2(x)))\| \N(0,I))\\=&amp;\frac{1}{2}\sum_{i=1}^d\left(\mu_\phi^2(x)_i+\sigma_\phi^2(x)_i-\log \sigma_\phi^2(x)_i-1\right)\end{align}\]</span> 另外，为了避免正负的麻烦，我们可以视 encoder 的输出为 <span class="math inline">\(\log \sigma_\phi^2(x)\)</span> 而非 <span class="math inline">\(\sigma_\phi^2(x)\)</span>，这样就不需要在最后加一个激活函数了。</p><p><img src="vae-enc.png" alt="source: cs231n" width=40% /></p><h3 id="decoder-network">Decoder network</h3><p>接下来考虑损失函数中 <span class="math inline">\(\mathbb E_{z\sim q_\phi(z\vert x)}[-\log p_\theta(x\vert z)]\)</span> 一项，也就是生成模型 <span class="math inline">\(p_\theta(x\vert z)\)</span> 的形式，一般有两种选择<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="苏剑林. (Mar. 28, 2018). 《变分自编码器（二）：从贝叶斯观点出发 》[Blog post]. Retrieved from https://spaces.ac.cn/archives/5343">[2]</span></a></sup>：</p><ul><li><p><strong>伯努利分布</strong>：输出只有 0/1，所以只适用于生成二值数据（比如黑白图像）。设伯努利分布的参数为 <span class="math inline">\(\rho_\theta(z)\in\mathbb [0,1]^D\)</span>，那么： <span class="math display">\[p_\theta(x_i\vert z)=\begin{cases}\rho_\theta(z)_i,&amp;x_i=1\\1-\rho_\theta(z)_i,&amp;x_i=0\end{cases}\]</span> 于是 <span class="math display">\[-\log p_\theta(x\vert z)=-\sum_{i=1}^D\log p_\theta(x_i\vert z)=\sum_{i=1}^D\left[-x_i\log \rho_\theta(z)_i-(1-x_i)\log(1-\rho_\theta(z)_i)\right]\]</span> 即 <strong>BCELoss</strong>.</p></li><li><p><strong>正态分布</strong>：设参数为 <span class="math inline">\(\mu_\theta(z)\in\mathbb R^D,\Sigma_\theta(z)\in\mathbb R^{D\times D}\)</span>，那么： <span class="math display">\[p_\theta(x\vert z)=\frac{1}{(2\pi)^{D/2}(\det\Sigma_\theta(z))^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_\theta(z))^T\Sigma_\theta^{-1}(z)(x-\mu_\theta(z))\right)\]</span> 于是 <span class="math display">\[-\log p_\theta(x\vert z)=\frac{D}{2}\log(2\pi)+\frac{1}{2}\log\det\Sigma_\theta(z)+\frac{1}{2}(x-\mu_\theta(z))^T\Sigma_\theta^{-1}(z)(x-\mu_\theta(z))\]</span> 实操时，我们一般会取 <span class="math inline">\(\Sigma_\theta(z)=\sigma^2I\)</span>，即各分量独立且方差固定为某常数。那么： <span class="math display">\[-\log p_\theta(x\vert z)=\frac{D}{2}\log(2\pi)+\frac{D}{2}\log\sigma^2+\frac{1}{2\sigma^2}\|x-\mu_\theta(z)\|^2\]</span> 前两项是定值，与优化无关，所以优化目标就是 <span class="math display">\[\frac{1}{2\sigma^2}\|x-\mu_\theta(z)\|^2\]</span> 即 <strong>MSELoss</strong>.</p><blockquote><p>注意：上式中 <span class="math inline">\(\Vert\bullet\Vert^2\)</span> 是欧氏距离，如果直接用 <code>nn.MSELoss</code> 会对 CHW 维也取平均（假设在图像上训练），结果是实际欧氏距离的 <span class="math inline">\(1/CHW\)</span>，导致重构项和 KL 项权重失衡。所以实现时要么只对 mini-batch 取平均、CHW 维求和，要么全取平均，但是 KL 项加个系数缩小。</p></blockquote></li></ul><p>与 encoder network 同理，<span class="math inline">\(\rho_\theta(z)\)</span> 或者 <span class="math inline">\(\mu_\theta(z)\)</span> 直接由一个 decoder network 得到。</p><p><img src="vae-dec.png" alt="source: cs231n" width=40% /></p><p>至此我们算出了 <span class="math inline">\(-\log p_\theta(x\vert z)\)</span>，但是损失函数 <span class="math inline">\(\eqref{obj}\)</span> 里还要对它取期望，所以理论上，我们应该对每一条数据都采大量 <span class="math inline">\(z\)</span> 算平均。但实践中人们发现只采一个就能 work，毕竟我们会训练多个 epochs.</p><h3 id="loss-权重">Loss 权重</h3><p>考虑实践中最常用的设置：</p><ul><li><span class="math inline">\(p(z)\)</span> 取 <span class="math inline">\(\N(0,I)\)</span>；</li><li><span class="math inline">\(q_\phi(z\vert x)\)</span> 取 <span class="math inline">\(\N(\mu_\phi(x),\Sigma_\phi(x))\)</span>，且 <span class="math inline">\(\Sigma_\phi(x)=\mathrm{diag}(\sigma_\phi^2(x))\)</span>；</li><li><span class="math inline">\(p_\theta(x\vert z)\)</span> 取 <span class="math inline">\(\N(\mu_\theta(z),\Sigma_\theta(z))\)</span>，且 <span class="math inline">\(\Sigma_\theta(z)=\sigma^2I\)</span>，其中 <span class="math inline">\(\sigma^2\)</span> 是<strong>事先取定的一个超参数</strong>。</li></ul><p>那么根据前两小节的推导，损失函数是： <span class="math display">\[\mathcal L=\underbrace{\frac{1}{2\sigma^2}\|x-\mu_\theta(z)\|^2}_\text{Reconstruction}+\underbrace{\frac{1}{2}\sum_{i=1}^d\left(\mu_\phi^2(x)_i+\sigma_\phi^2(x)_i-\log \sigma_\phi^2(x)_i-1\right)}_\text{KL Regularization},\quad z\sim\mathcal N(\mu_\phi(x),\text{diag}(\sigma_\phi^2(x)))\]</span> 可以看到，重构项和 KL 正则项由超参数 <span class="math inline">\(\sigma^2\)</span> 加权。<span class="math inline">\(\sigma^2\)</span> 越小，重构项权重越大，意味着结果更真实，但泛化性下降。一般直接取 <span class="math inline">\(\sigma^2=1\)</span> 即可。</p><h3 id="重参数化技巧">重参数化技巧</h3><p>重参数化技巧在<a href="/blog-main/2022/06/22/%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96%E6%8A%80%E5%B7%A7-The-Reparameterization-Trick/" title="重参数化技巧 The Reparameterization Trick">之前的文章</a>中已经介绍过了，并且正是用 VAE 举的例子，所以这里不再赘述。简单说来，就是现在 <span class="math inline">\(z\)</span> 是从 <span class="math inline">\(q_\phi(z\vert x)\sim\N(\mu_\phi(x),\mathrm{diag}(\sigma_\phi^2(x)))\)</span> 中采样的，但梯度无法经过采样传播到参数 <span class="math inline">\(\phi\)</span>。解决方法很简单，先从 <span class="math inline">\(\N(0,I)\)</span> 中采样 <span class="math inline">\(z&#39;\)</span>，再计算 <span class="math inline">\(z=\mu_\phi(x)+z&#39;\ast\sigma_\phi(x)\)</span> 就好了。</p><h2 id="代码实现">代码实现</h2><p>Github repo: <a href="https://github.com/xyfJASON/VAEs-Implementations" class="uri">https://github.com/xyfJASON/VAEs-Implementations</a></p><p>放个结果：</p><p><img src="vae-celeba-random.png" width=60% /></p><h2 id="参考资料">参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>苏剑林. (Mar. 18, 2018). 《变分自编码器（一）：原来是这么一回事 》[Blog post]. Retrieved from https://spaces.ac.cn/archives/5253 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>苏剑林. (Mar. 28, 2018). 《变分自编码器（二）：从贝叶斯观点出发 》[Blog post]. Retrieved from https://spaces.ac.cn/archives/5343 <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>原来VAE是这么回事（从EM到VAE） - 市井小民的文章 - 知乎 https://zhuanlan.zhihu.com/p/368959795 <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>EM的升级打怪之路：EM-变分EM-VAE（part1） - Young Zicon的文章 - 知乎 https://zhuanlan.zhihu.com/p/418203971 <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>VAE 的前世今生：从最大似然估计到 EM 再到 VAE - AI科技评论的文章 - 知乎 https://zhuanlan.zhihu.com/p/443540253 <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Weng, Lilian. From Autoencoder to Beta-VAE. https://lilianweng.github.io/posts/2018-08-12-vae/ <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Doersch, Carl. Tutorial on variational autoencoders. <em>arXiv preprint arXiv:1606.05908</em> (2016). <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>半小时理解变分自编码器 - 多头注意力的文章 - 知乎 https://zhuanlan.zhihu.com/p/144649293 <a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>生成模型</category>
      
      <category>VAEs</category>
      
    </categories>
    
    
    <tags>
      
      <tag>generative models</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机视觉常用数据集</title>
    <link href="/blog-main/2022/09/14/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <url>/blog-main/2022/09/14/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
    
    <content type="html"><![CDATA[<p>本文记录计算机视觉中常用的数据集，包括它们的官网、下载链接、目录结构、文件大小、加载方式等等。其中「本地目录结构」为我个人组织数据的方式，仅供参考。</p><h2 id="afhq">AFHQ</h2><p><a href="https://github.com/clovaai/stargan-v2">官网</a> | <a href="https://paperswithcode.com/dataset/afhq">Paper with Code</a> | <a href="https://www.dropbox.com/s/vkzjokiwof5h8w6/afhq_v2.zip?dl=0">Dropbox</a></p><p><strong>简要介绍</strong>：Animal FacesHQ (AFHQ) 是一个高质量动物面部图像的数据集，包含猫、狗和野生动物三个域。所有图像都已经过水平和垂直对齐，以确保将眼睛置于图像中心。低质量图像已由人工剔除。</p><p><strong>基本信息</strong>：</p><ul><li>数量：15,803</li><li>划分（train / test）：<ul><li>Total：14,336 / 1,467</li><li>Cat：5,065 / 493</li><li>Dog：4,678 / 491</li><li>Wild：4,593 / 483</li></ul></li><li>分辨率：512×512</li></ul><p><strong>注意</strong>：上述信息对应第二个版本（AFHQv2），其在 v1 版本上更换了更好的重采样方式（Nearest neighbor =&gt; Lanczos）、删除了大约 2% 的图片、以及改用 png 格式保存。</p><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── AFHQ<br>    ├── afhq_v2.zip    (7.0 GB)<br>    ├── train          (extracted from afhq_v2.zip)<br>    │   ├── cat        (contains 5065 images)<br>    │   ├── dog        (contains 4678 images)<br>    │   └── wild       (contains 4593 images)<br>    └── test           (extracted from afhq_v2.zip)<br>        ├── cat        (contains 493 images)<br>        ├── dog        (contains 491 images)<br>        └── wild       (contains 483 images)<br></code></pre></td></tr></table></figure><p><strong>需要自定义 <code>Dataset</code> 加载数据集</strong>。</p><p><br/></p><h2 id="celeba">CelebA</h2><p><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">官网</a> | <a href="https://paperswithcode.com/dataset/celeba">Papers with Code</a> | <a href="https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8?resourcekey=0-5BR16BdXnb8hVj6CNHKzLg">Google drive</a> | <a href="https://pan.baidu.com/share/init?surl=CRxxhoQ97A5qbsKO7iaAJg">Baidu drive</a> (password: rp0s)</p><p><strong>简要介绍</strong>：CelebFaces Attribute (CelebA) 数据集包含来自 10,177 位名人的 202,599 张面部图像，每张图像的尺寸为 178×218 像素，并且用 40 个二进制标签进行注释，这些标签指示了面部属性，如头发颜色、性别和年龄。</p><p><strong>基本信息</strong>：</p><ul><li>数量：202,599</li><li>划分：162,770 / 19,867 / 19,962 (train / valid / test)</li><li>分辨率：<ul><li>align 处理后：178×218</li><li>原始图片：不一致，200+ 到 2000+ 都有</li></ul></li><li>标注：每张图像有<ul><li>40 个 binary labels，指示发色、性别、年龄等信息</li><li>人脸 bounding box</li><li>5 个位置的 landmark（左眼、右眼、鼻尖、左嘴角、右嘴角）</li><li>identity（名人身份 id，共 10,177 人）</li></ul></li></ul><p><strong>官方目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs text">.<br>├── Anno<br>│   ├── identity_CelebA.txt<br>│   ├── list_attr_celeba.txt<br>│   ├── list_bbox_celeba.txt<br>│   ├── list_landmarks_align_celeba.txt<br>│   └── list_landmarks_celeba.txt<br>├── Eval<br>│   └── list_eval_partition.txt<br>├── Img<br>│   ├── img_align_celeba_png.7z<br>│   ├── img_celeba.7z<br>│   └── img_align_celeba.zip<br>└── README.txt<br></code></pre></td></tr></table></figure><blockquote><p>注：三个图片压缩包中，一般用 <code>img_align_celeba.zip</code> 即可.</p></blockquote><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── CelebA<br>    └── celeba<br>        ├── identity_CelebA.txt<br>        ├── img_align_celeba.zip       (1.44 GB)<br>        ├── img_celeba.7z              (10.19 GB)<br>        ├── img_align_celeba           (extracted from img_align_celeba.zip)<br>        │   ├── 000001.jpg<br>        │   ├── ...<br>        │   └── 202599.jpg<br>        ├── img_celeba                 (extracted from img_celeba.7z)<br>        │   ├── 000001.jpg<br>        │   ├── ...<br>        │   └── 202599.jpg<br>        ├── list_attr_celeba.txt<br>        ├── list_bbox_celeba.txt<br>        ├── list_eval_partition.txt<br>        ├── list_landmarks_align_celeba.txt<br>        ├── list_landmarks_celeba.txt<br>        └── README.txt<br></code></pre></td></tr></table></figure><p><strong>使用 torchvision 加载数据集</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torchvision.datasets <span class="hljs-keyword">as</span> dset<br><span class="hljs-meta">&gt;&gt;&gt; </span>celeba = dset.CelebA(root=<span class="hljs-string">&#x27;/data/CelebA&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(celeba)<br><span class="hljs-number">162770</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>celeba = dset.CelebA(root=<span class="hljs-string">&#x27;/data/CelebA&#x27;</span>, split=<span class="hljs-string">&#x27;valid&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(celeba)<br><span class="hljs-number">19867</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>celeba = dset.CelebA(root=<span class="hljs-string">&#x27;/data/CelebA&#x27;</span>, split=<span class="hljs-string">&#x27;test&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(celeba)<br><span class="hljs-number">19962</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>celeba = dset.CelebA(root=<span class="hljs-string">&#x27;/data/CelebA&#x27;</span>, split=<span class="hljs-string">&#x27;all&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(celeba)<br><span class="hljs-number">202599</span><br></code></pre></td></tr></table></figure><p><br/></p><h2 id="celeba-hq">CelebA-HQ</h2><p><a href="https://github.com/tkarras/progressive_growing_of_gans">官网</a> | <a href="https://paperswithcode.com/dataset/celeba-hq">Papers with Code</a></p><p><strong>简要介绍</strong>：CelebA-HQ 数据集是 CelebA 数据集的高质量版本，由 30,000 张 1024×1024 分辨率的图像组成。</p><p><strong>基本信息</strong>：</p><ul><li>数量：30,000，是 CelebA 的子集</li><li>遵从 CelebA 原始划分：24,183 / 2,993 / 2,824 (train / valid / test)</li><li>分辨率：1024×1024</li></ul><p><strong>官方生成方式</strong>：下载 img_celeba.7z 和 delta 文件，使用 <code>dataset_tool.py</code> 生成高清图片。</p><p><strong>其他生成方式</strong>：用网上其他人魔改的 <code>h5tool.py</code> 生成高清图片。问题：生成的图片可能有噪点或质量不高。</p><p><strong>推荐方式</strong>：下载 <strong>CelebAMask-HQ</strong> 数据集（见下文），然后根据 <code>CelebA-HQ-to-CelebA-mapping.txt</code> 把文件名映射回原 id，例如，以下是一个简单的映射脚本 <code>map_index.py</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><br>mapping = pd.read_table(<span class="hljs-string">&#x27;CelebA-HQ-to-CelebA-mapping.txt&#x27;</span>, sep=<span class="hljs-string">&#x27;\s+&#x27;</span>, index_col=<span class="hljs-number">0</span>)<br>mapping_dict = <span class="hljs-built_in">dict</span>()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">30000</span>):<br>mapping_dict.update(&#123;<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;i&#125;</span>.jpg&#x27;</span>: mapping.iloc[i][<span class="hljs-string">&#x27;orig_file&#x27;</span>]&#125;)<br><br><span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> mapping_dict.items():<br><span class="hljs-keyword">assert</span> os.path.isfile(os.path.join(<span class="hljs-string">&#x27;CelebA-HQ-img&#x27;</span>, key))<br>os.rename(os.path.join(<span class="hljs-string">&#x27;CelebA-HQ-img&#x27;</span>, key), os.path.join(<span class="hljs-string">&#x27;CelebA-HQ-img&#x27;</span>, value))<br></code></pre></td></tr></table></figure><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── CelebA-HQ<br>    ├── CelebA-HQ-img.zip     (2.74 GB)<br>    ├── CelebA-HQ-img         (extracted from CelebA-HQ-img.zip)<br>    │   ├── 000004.jpg<br>    │   ├── ...<br>    │   └── 202591.jpg<br>    ├── CelebA-HQ-to-CelebA-mapping.txt<br>    └── map_index.py<br></code></pre></td></tr></table></figure><p><strong>需要自定义 <code>Dataset</code> 加载数据集</strong>，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CelebA_HQ</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    The downloaded 30,000 images should be stored under `root/CelebA-HQ-img/`.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    The file names should be the same as their counterparts in the original CelebA dataset.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    The train/valid/test sets are split according to the original CelebA dataset,</span><br><span class="hljs-string">    resulting in 24,183 training images, 2,993 validation images, and 2,824 test images.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, root, split=<span class="hljs-string">&#x27;train&#x27;</span>, transform=<span class="hljs-literal">None</span></span>):<br>        image_root = os.path.join(os.path.expanduser(root), <span class="hljs-string">&#x27;CelebA-HQ-img&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> os.path.isdir(image_root), <span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;image_root&#125;</span> is not a valid directory&#x27;</span><br>        <span class="hljs-keyword">assert</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;valid&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;all&#x27;</span>]<br><br>        self.transform = transform<br><br>        img_ext = [<span class="hljs-string">&#x27;.png&#x27;</span>, <span class="hljs-string">&#x27;.jpg&#x27;</span>, <span class="hljs-string">&#x27;.jpeg&#x27;</span>]<br>        self.img_paths = []<br>        <span class="hljs-keyword">for</span> curdir, subdirs, files <span class="hljs-keyword">in</span> os.walk(image_root):<br>            <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:<br>                <span class="hljs-keyword">if</span> os.path.splitext(file)[<span class="hljs-number">1</span>].lower() <span class="hljs-keyword">in</span> img_ext:<br>                    self.img_paths.append(os.path.join(curdir, file))<br>        self.img_paths = <span class="hljs-built_in">sorted</span>(self.img_paths)<br><br>        celeba_splits = [<span class="hljs-number">1</span>, <span class="hljs-number">162771</span>, <span class="hljs-number">182638</span>, <span class="hljs-number">202600</span>]<br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">filter_func</span>(<span class="hljs-params">p</span>):<br>            <span class="hljs-keyword">if</span> split == <span class="hljs-string">&#x27;all&#x27;</span>:<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>            k = <span class="hljs-number">0</span> <span class="hljs-keyword">if</span> split == <span class="hljs-string">&#x27;train&#x27;</span> <span class="hljs-keyword">else</span> (<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> split == <span class="hljs-string">&#x27;valid&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-number">2</span>)<br>            <span class="hljs-keyword">return</span> celeba_splits[k] &lt;= <span class="hljs-built_in">int</span>(os.path.splitext(os.path.basename(p))[<span class="hljs-number">0</span>]) &lt; celeba_splits[k+<span class="hljs-number">1</span>]<br><br>        self.img_paths = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(filter_func, self.img_paths))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.img_paths)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, item</span>):<br>        X = Image.<span class="hljs-built_in">open</span>(self.img_paths[item])<br>        <span class="hljs-keyword">if</span> self.transform <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            X = self.transform(X)<br>        <span class="hljs-keyword">return</span> X<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    dataset = CelebA_HQ(root=<span class="hljs-string">&#x27;/data/CelebA-HQ/&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br>    dataset = CelebA_HQ(root=<span class="hljs-string">&#x27;/data/CelebA-HQ/&#x27;</span>, split=<span class="hljs-string">&#x27;valid&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br>    dataset = CelebA_HQ(root=<span class="hljs-string">&#x27;/data/CelebA-HQ/&#x27;</span>, split=<span class="hljs-string">&#x27;test&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br>    dataset = CelebA_HQ(root=<span class="hljs-string">&#x27;/data/CelebA-HQ/&#x27;</span>, split=<span class="hljs-string">&#x27;all&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br></code></pre></td></tr></table></figure><p><br/></p><h2 id="celebamask-hq">CelebAMask-HQ</h2><p><a href="https://github.com/switchablenorms/CelebAMask-HQ">官网</a> | <a href="https://drive.google.com/open?id=1badu11NqxGf6qM3PTTooQDJvQbejgbTv">Google drive</a> | <a href="https://pan.baidu.com/s/1wN1E-B1bJ7mE1mrn9loj5g">Baidu drive</a></p><p><strong>简要介绍</strong>：在 CelebA-HQ 数据集的基础上增加了对应 CelebA 面部属性的 segmentation mask.</p><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>├── CelebAMask-HQ.zip       (3.15 GB)<br>└── CelebAMask-HQ           (extracted from CelebAMask-HQ.zip)<br>    ├── CelebA-HQ-img<br>    │   ├── 0.jpg<br>    │   ├── ...<br>    │   └── 29999.jpg<br>    ├── CelebA-HQ-to-CelebA-mapping.txt<br>    ├── CelebAMask-HQ-attribute-anno.txt<br>    ├── CelebAMask-HQ-mask-anno<br>    │   ├── 0<br>    │   │   ├── 00000_hair.png<br>    │   │   └── ...<br>    │   └── ...<br>    ├── CelebAMask-HQ-pose-anno.txt<br>    └── README.txt<br></code></pre></td></tr></table></figure><p><strong>需要自定义 <code>Dataset</code> 加载数据集</strong>。</p><p><br/></p><h2 id="cifar-10">CIFAR-10</h2><p><a href="https://www.cs.toronto.edu/~kriz/cifar.html">官网</a> | <a href="https://paperswithcode.com/dataset/cifar-10">Papers with Code</a></p><p><strong>简要介绍</strong>：CIFAR-10 数据集是 Tiny Images 数据集的子集，包括 60,000 张 32×32 像素的彩色图像。这些图像被标记为10个相互独立的类别：飞机、汽车（但不包括卡车或皮卡车）、鸟、猫、鹿、狗、青蛙、马、船和卡车（但不包括皮卡车）。每个类别有 6,000 张图像，每个类别包含 5,000 张训练图像和 1,000 张测试图像。</p><p><strong>基本信息</strong>：</p><ul><li>数量：60,000</li><li>划分：50,000 / 10,000 (train / test)</li><li>分辨率：32×32</li><li>标注：10 类</li></ul><p><strong>存储为 png</strong>：原文件并非图片格式，如果有 png 格式的需求，可用以下脚本转换 <code>makepng.py</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">import</span> torchvision.datasets <span class="hljs-keyword">as</span> dsets<br><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>os.makedirs(<span class="hljs-string">f&#x27;./png/train/<span class="hljs-subst">&#123;i&#125;</span>/&#x27;</span>, exist_ok=<span class="hljs-literal">True</span>)<br>os.makedirs(<span class="hljs-string">f&#x27;./png/test/<span class="hljs-subst">&#123;i&#125;</span>/&#x27;</span>, exist_ok=<span class="hljs-literal">True</span>)<br><br>ids = &#123;k: <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)&#125;<br>cifar10 = dsets.CIFAR10(root=<span class="hljs-string">&#x27;.&#x27;</span>, train=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> tqdm(cifar10):<br>X.save(<span class="hljs-string">f&#x27;./png/train/<span class="hljs-subst">&#123;y&#125;</span>/<span class="hljs-subst">&#123;ids[y]&#125;</span>.png&#x27;</span>)<br>ids[y] += <span class="hljs-number">1</span><br><br>ids = &#123;k: <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)&#125;<br>cifar10 = dsets.CIFAR10(root=<span class="hljs-string">&#x27;.&#x27;</span>, train=<span class="hljs-literal">False</span>)<br><span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> tqdm(cifar10):<br>X.save(<span class="hljs-string">f&#x27;./png/test/<span class="hljs-subst">&#123;y&#125;</span>/<span class="hljs-subst">&#123;ids[y]&#125;</span>.png&#x27;</span>)<br>ids[y] += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── CIFAR-10<br>    ├── cifar-10-python.tar.gz   (170.5 MB)<br>    └── cifar-10-batches-py      (extracted from cifar-10-python.tar.gz)<br>        ├── batches.meta<br>        ├── data_batch_1<br>        ├── data_batch_2<br>        ├── data_batch_3<br>        ├── data_batch_4<br>        ├── data_batch_5<br>        ├── readme.html<br>        └── test_batch<br></code></pre></td></tr></table></figure><p><strong>使用 torchvision 加载数据集</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torchvision.datasets <span class="hljs-keyword">as</span> dset<br><span class="hljs-meta">&gt;&gt;&gt; </span>cifar10 = dset.CIFAR10(root=<span class="hljs-string">&#x27;/data/CIFAR-10&#x27;</span>, train=<span class="hljs-literal">True</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(cifar10)<br><span class="hljs-number">50000</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>cifar10 = dset.CIFAR10(root=<span class="hljs-string">&#x27;/data/CIFAR-10&#x27;</span>, train=<span class="hljs-literal">False</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(cifar10)<br><span class="hljs-number">10000</span><br></code></pre></td></tr></table></figure><p><br/></p><h2 id="cifar-100">CIFAR-100</h2><p><a href="https://www.cs.toronto.edu/~kriz/cifar.html">官网</a> | <a href="https://paperswithcode.com/dataset/cifar-100">Papers with Code</a></p><p><strong>简要介绍</strong>：CIFAR-100 数据集是 Tiny Images 数据集的子集，包括 60,000 张 32×32 像素的彩色图像。CIFAR-100 的 100 个类别被分成 20 个超类别。每个图像都带有一个“细”标签（它所属的类别）和一个“粗”标签（它所属的超类别）。每个类别有 600 张图像——500 张训练图像和 100 张测试图像。</p><p><strong>基本信息</strong>：</p><ul><li>数量：60,000</li><li>划分：50,000 / 10,000 (train / test)</li><li>分辨率：32×32</li><li>标注：100 类，分组为 20 个 superclass</li></ul><p><strong>存储为 png</strong>：原文件并非图片格式，如果有 png 格式的需求，可用以下脚本转换 <code>makepng.py</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">import</span> torchvision.datasets <span class="hljs-keyword">as</span> dsets<br><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>os.makedirs(<span class="hljs-string">f&#x27;./png/train/<span class="hljs-subst">&#123;i&#125;</span>/&#x27;</span>, exist_ok=<span class="hljs-literal">True</span>)<br>os.makedirs(<span class="hljs-string">f&#x27;./png/test/<span class="hljs-subst">&#123;i&#125;</span>/&#x27;</span>, exist_ok=<span class="hljs-literal">True</span>)<br><br>ids = &#123;k: <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>)&#125;<br>cifar100 = dsets.CIFAR100(root=<span class="hljs-string">&#x27;.&#x27;</span>, train=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> tqdm(cifar100):<br>X.save(<span class="hljs-string">f&#x27;./png/train/<span class="hljs-subst">&#123;y&#125;</span>/<span class="hljs-subst">&#123;ids[y]&#125;</span>.png&#x27;</span>)<br>ids[y] += <span class="hljs-number">1</span><br><br>ids = &#123;k: <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>)&#125;<br>cifar100 = dsets.CIFAR100(root=<span class="hljs-string">&#x27;.&#x27;</span>, train=<span class="hljs-literal">False</span>)<br><span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> tqdm(cifar100):<br>X.save(<span class="hljs-string">f&#x27;./png/test/<span class="hljs-subst">&#123;y&#125;</span>/<span class="hljs-subst">&#123;ids[y]&#125;</span>.png&#x27;</span>)<br>ids[y] += <span class="hljs-number">1</span><br><br></code></pre></td></tr></table></figure><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>├── cifar-100-python.tar.gz   (169 MB)<br>└── cifar-100-python          (extracted from cifar-100-python.tar.gz)<br>    ├── file.txt~<br>    ├── meta<br>    ├── test<br>    └── train<br></code></pre></td></tr></table></figure><p><strong>使用 torchvision 加载数据集</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torchvision.datasets <span class="hljs-keyword">as</span> dset<br><span class="hljs-meta">&gt;&gt;&gt; </span>cifar100 = dset.CIFAR100(root=<span class="hljs-string">&#x27;/data/CIFAR-100&#x27;</span>, train=<span class="hljs-literal">True</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(cifar100)<br><span class="hljs-number">50000</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>cifar100 = dset.CIFAR100(root=<span class="hljs-string">&#x27;/data/CIFAR-100&#x27;</span>, train=<span class="hljs-literal">False</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(cifar100)<br><span class="hljs-number">10000</span><br></code></pre></td></tr></table></figure><p><br/></p><h2 id="fashion-mnist">Fashion-MNIST</h2><p><a href="https://github.com/zalandoresearch/fashion-mnist">官网</a> | <a href="https://paperswithcode.com/dataset/fashion-mnist">Papers with Code</a></p><p><strong>简要介绍</strong>：Fashion-MNIST 是一个由 10 个类别、每个类别 7,000 张 28×28 像素的灰度图像组成数据集，总计70,000 张时尚产品图像。训练集包含 60,000 张图像，测试集包含 10,000 张图像。Fashion-MNIST 与原始的 MNIST 数据集具有相同的图像大小、数据格式以及训练和测试集的划分结构。</p><p><strong>基本信息</strong>：</p><ul><li>数量：70,000</li><li>划分：60,000 / 10,000 (train / test)</li><li>分辨率：28×28</li><li>标注：10 类</li></ul><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── Fashion-MNIST<br>    └── FashionMNIST<br>        └── raw<br>            ├── t10k-images-idx3-ubyte<br>            ├── t10k-images-idx3-ubyte.gz<br>            ├── t10k-labels-idx1-ubyte<br>            ├── t10k-labels-idx1-ubyte.gz<br>            ├── train-images-idx3-ubyte<br>            ├── train-images-idx3-ubyte.gz<br>            ├── train-labels-idx1-ubyte<br>            └── train-labels-idx1-ubyte.gz<br></code></pre></td></tr></table></figure><p><strong>使用 torchvision 加载数据集</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torchvision.datasets <span class="hljs-keyword">as</span> dset<br><span class="hljs-meta">&gt;&gt;&gt; </span>fashion = dset.FashionMNIST(root=<span class="hljs-string">&#x27;/data/Fashion-MNIST&#x27;</span>, train=<span class="hljs-literal">True</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(fashion)<br><span class="hljs-number">60000</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>fashion = dset.FashionMNIST(root=<span class="hljs-string">&#x27;/data/Fashion-MNIST&#x27;</span>, train=<span class="hljs-literal">False</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(fashion)<br><span class="hljs-number">10000</span><br></code></pre></td></tr></table></figure><p><br/></p><h2 id="ffhq">FFHQ</h2><p><a href="https://github.com/NVlabs/ffhq-dataset">官网</a> | <a href="https://paperswithcode.com/dataset/ffhq">Papers with Code</a> | <a href="https://drive.google.com/drive/folders/1u2xu7bSrWxrbUxk-dT-UvEJq8IjdmNTP">Google drive</a></p><p><strong>简要介绍</strong>：Flickr-Faces-HQ (FFHQ) 由 70,000 张高质量 PNG 图像组成，分辨率为 1024×1024，具有年龄、种族和图像背景等方面的相当大的变化。它还涵盖了各种饰品，如眼镜、太阳镜、帽子等。这些图像是从 Flickr 爬取的，因此继承了该网站的所有偏见，并使用 dlib 自动对齐和裁剪。只收集了许可证宽松的图像。使用各种自动过滤器对数据集进行了修剪，最后使用 Amazon Mechanical Turk 删除了偶尔出现的雕像、绘画或照片等。</p><p><strong>基本信息</strong>：</p><ul><li>数量：70,000</li><li>划分：60,000 / 10,000 (train / valid)</li><li>分辨率：1024×1024</li></ul><p><strong>官方目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs text">.<br>├── LICENSE.txt<br>├── README.txt<br>├── ffhq-dataset-v2.json<br>├── images1024x1024         (89.1 GB)<br>├── in-the-wild-images      (955 GB)<br>├── tfrecords               (273 GB)<br>├── thumbnails128x128       (1.95 GB)<br>└── zips                    (1.28 TB)<br></code></pre></td></tr></table></figure><blockquote><p>注：一般而言，我们只需要用 images1024x1024 图片。</p></blockquote><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── FFHQ<br>    ├── LICENSE.txt<br>    ├── README.txt<br>    ├── ffhq-dataset-v2.json<br>    ├── images1024x1024.zip      (95.73 GB)<br>    └── images1024x1024          (extracted from images1024x1024.zip)<br>        ├── LICENSE.txt<br>        ├── 00000<br>        │   ├── 00000.png<br>        │   ├── ...<br>        │   └── 00999.png<br>        ├── ...<br>        └── 69000<br>            ├── 69000.png<br>            ├── ...<br>            └── 69999.png<br></code></pre></td></tr></table></figure><p><strong>需要自定义 <code>Dataset</code> 加载数据集</strong>，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">FFHQ</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    The downloaded 70,000 images should be organized in the following structure:</span><br><span class="hljs-string"></span><br><span class="hljs-string">    - root/</span><br><span class="hljs-string">        - image1024x1024/</span><br><span class="hljs-string">            - 00000/</span><br><span class="hljs-string">                - 00000.png</span><br><span class="hljs-string">                - 00001.png</span><br><span class="hljs-string">                - ...</span><br><span class="hljs-string">                - 00999.png</span><br><span class="hljs-string">            - ...</span><br><span class="hljs-string">            - 69000/</span><br><span class="hljs-string">                - 69000.png</span><br><span class="hljs-string">                - 69001.png</span><br><span class="hljs-string">                - ...</span><br><span class="hljs-string">                - 69999.png</span><br><span class="hljs-string"></span><br><span class="hljs-string">    If `official_split` is True, the first 60,000 images will be the training set and the remaining 10,000 images will be the test set.</span><br><span class="hljs-string">    Otherwise, the 10,000 images in the test set are further divided into a 5,000-image validation set and a 5,000-image test set.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, root, split=<span class="hljs-string">&#x27;train&#x27;</span>, original_size=<span class="hljs-number">1024</span>, transform=<span class="hljs-literal">None</span>, official_split=<span class="hljs-literal">True</span></span>):<br>        image_root = os.path.join(os.path.expanduser(root), <span class="hljs-string">f&#x27;images<span class="hljs-subst">&#123;original_size&#125;</span>x<span class="hljs-subst">&#123;original_size&#125;</span>&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> os.path.isdir(image_root), <span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;image_root&#125;</span> is not a valid directory&#x27;</span><br>        <span class="hljs-keyword">assert</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;valid&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;all&#x27;</span>]<br><br>        self.transform = transform<br><br>        img_ext = [<span class="hljs-string">&#x27;.png&#x27;</span>, <span class="hljs-string">&#x27;.jpg&#x27;</span>, <span class="hljs-string">&#x27;.jpeg&#x27;</span>]<br>        self.img_paths = []<br>        <span class="hljs-keyword">for</span> curdir, subdirs, files <span class="hljs-keyword">in</span> os.walk(image_root):<br>            <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:<br>                <span class="hljs-keyword">if</span> os.path.splitext(file)[<span class="hljs-number">1</span>].lower() <span class="hljs-keyword">in</span> img_ext:<br>                    self.img_paths.append(os.path.join(curdir, file))<br>        self.img_paths = <span class="hljs-built_in">sorted</span>(self.img_paths)<br><br>        <span class="hljs-keyword">if</span> official_split:<br>            <span class="hljs-keyword">if</span> split == <span class="hljs-string">&#x27;train&#x27;</span>:<br>                self.img_paths = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: <span class="hljs-string">&#x27;00000&#x27;</span> &lt;= (os.path.dirname(p)).split(<span class="hljs-string">&#x27;/&#x27;</span>)[-<span class="hljs-number">1</span>] &lt; <span class="hljs-string">&#x27;60000&#x27;</span>, self.img_paths))<br>            <span class="hljs-keyword">elif</span> split == <span class="hljs-string">&#x27;test&#x27;</span>:<br>                self.img_paths = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: <span class="hljs-string">&#x27;60000&#x27;</span> &lt;= (os.path.dirname(p)).split(<span class="hljs-string">&#x27;/&#x27;</span>)[-<span class="hljs-number">1</span>] &lt; <span class="hljs-string">&#x27;70000&#x27;</span>, self.img_paths))<br>            <span class="hljs-keyword">elif</span> split == <span class="hljs-string">&#x27;valid&#x27;</span>:<br>                <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&#x27;FFHQ official split does not have a validation set.&#x27;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> split == <span class="hljs-string">&#x27;train&#x27;</span>:<br>                self.img_paths = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: <span class="hljs-string">&#x27;00000&#x27;</span> &lt;= (os.path.dirname(p)).split(<span class="hljs-string">&#x27;/&#x27;</span>)[-<span class="hljs-number">1</span>] &lt; <span class="hljs-string">&#x27;60000&#x27;</span>, self.img_paths))<br>            <span class="hljs-keyword">elif</span> split == <span class="hljs-string">&#x27;valid&#x27;</span>:<br>                self.img_paths = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: <span class="hljs-string">&#x27;60000&#x27;</span> &lt;= (os.path.dirname(p)).split(<span class="hljs-string">&#x27;/&#x27;</span>)[-<span class="hljs-number">1</span>] &lt; <span class="hljs-string">&#x27;65000&#x27;</span>, self.img_paths))<br>            <span class="hljs-keyword">elif</span> split == <span class="hljs-string">&#x27;test&#x27;</span>:<br>                self.img_paths = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: <span class="hljs-string">&#x27;65000&#x27;</span> &lt;= (os.path.dirname(p)).split(<span class="hljs-string">&#x27;/&#x27;</span>)[-<span class="hljs-number">1</span>] &lt; <span class="hljs-string">&#x27;70000&#x27;</span>, self.img_paths))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.img_paths)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, item</span>):<br>        X = Image.<span class="hljs-built_in">open</span>(self.img_paths[item])<br>        <span class="hljs-keyword">if</span> self.transform <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            X = self.transform(X)<br>        <span class="hljs-keyword">return</span> X<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    dataset = FFHQ(root=<span class="hljs-string">&#x27;/data/FFHQ/&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, official_split=<span class="hljs-literal">False</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br>    dataset = FFHQ(root=<span class="hljs-string">&#x27;/data/FFHQ/&#x27;</span>, split=<span class="hljs-string">&#x27;valid&#x27;</span>, official_split=<span class="hljs-literal">False</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br>    dataset = FFHQ(root=<span class="hljs-string">&#x27;/data/FFHQ/&#x27;</span>, split=<span class="hljs-string">&#x27;test&#x27;</span>, official_split=<span class="hljs-literal">False</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br>    dataset = FFHQ(root=<span class="hljs-string">&#x27;/data/FFHQ/&#x27;</span>, split=<span class="hljs-string">&#x27;all&#x27;</span>, official_split=<span class="hljs-literal">False</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br>    <span class="hljs-built_in">print</span>()<br>    dataset = FFHQ(root=<span class="hljs-string">&#x27;/data/FFHQ/&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, official_split=<span class="hljs-literal">True</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br>    dataset = FFHQ(root=<span class="hljs-string">&#x27;/data/FFHQ/&#x27;</span>, split=<span class="hljs-string">&#x27;test&#x27;</span>, official_split=<span class="hljs-literal">True</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br>    dataset = FFHQ(root=<span class="hljs-string">&#x27;/data/FFHQ/&#x27;</span>, split=<span class="hljs-string">&#x27;all&#x27;</span>, official_split=<span class="hljs-literal">True</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br></code></pre></td></tr></table></figure><p><br/></p><h2 id="flare7k">Flare7K</h2><p><a href="https://github.com/ykdai/Flare7K">官网</a> | <a href="https://paperswithcode.com/dataset/flare7k">Papers with Code</a></p><p><strong>简要介绍</strong>：Flare7K 是第一个夜间光晕去除数据集，它是基于真实世界夜间镜头光晕的观察和统计生成的。该数据集包括 5,000 张散射光晕图像和 2,000 张反射光晕图像，涵盖了 25 种散射光晕类型和 10 种反射光晕类型。这 7,000 个光晕图案可以随机添加到无光晕的图像中，形成光晕污染和无光晕的图像对。</p><p><strong>下载</strong>：官方 github 页面包含了以下三个下载链接：</p><table><thead><tr class="header"><th>内容</th><th>文件名</th><th>说明</th><th>大小</th></tr></thead><tbody><tr class="odd"><td>Flares</td><td><code>Flare7k.zip</code></td><td>5,000 张散射光晕和 2,000 张反射光晕图像</td><td>3.01 GB</td></tr><tr class="even"><td>Background Images</td><td><code>Flickr24K.zip</code></td><td>23,949 张背景图像</td><td>1.12 GB</td></tr><tr class="odd"><td>Flare-corrupted images</td><td><code>flare_corrupted_test_imgs.zip</code></td><td>额外 645 张无 ground-truth 的有光晕图像</td><td>212.4 MB</td></tr></tbody></table><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── Flare7K<br>    ├── flare_corrupted_test_imgs.zip     (222.7 MB)<br>    ├── Flare7k.zip                       (3.24 GB)<br>    ├── Flickr24K.zip                     (1.2 GB)<br>    ├── flare_corrupted_test_imgs         (extracted from flare_corrupted_test_imgs.zip)<br>    │   ├── test_00000.png<br>    │   ├── ...<br>    │   └── test_00644.png<br>    ├── Flare7k                           (extracted Flare7k.zip)<br>    │   ├── Reflective_Flare              (contains 2000 png images)<br>    │   │   ├── reflective_img_000000.png<br>    │   │   ├── ...<br>    │   │   └── reflective_img_001999.png<br>    │   ├── Scattering_Flare<br>    │   │   ├── Compound_Flare            (contains 5000 png images)<br>    │   │   ├── Glare_with_shimmer        (contains 5000 png images)<br>    │   │   ├── Light_Source              (contains 5000 png images)<br>    │   │   └── Streak                    (contains 5000 png images)<br>    │   └── test_data<br>    │       ├── real<br>    │       │   ├── gt                    (contains 100 png images)<br>    │       │   └── input                 (contains 100 png images)<br>    │       └── synthetic<br>    │           ├── gt                    (contains 100 png images)<br>    │           └── input                 (contains 100 png images)<br>    └── Flickr24K                         (extracted from Flickr24K.zip, contains 23949 jpg images)<br>        ├── 2.jpg<br>        ├── ...<br>        └── r (13746).jpg<br></code></pre></td></tr></table></figure><p><br/></p><h2 id="imagenet-21k-imagenet-22k">ImageNet-21K (ImageNet-22K)</h2><p><a href="https://image-net.org/download-images.php">官网</a> | <a href="https://paperswithcode.com/dataset/imagenet">Papers with Code</a></p><p><strong>简要介绍</strong>：完整的 ImageNet 数据集包含 14,197,122 张按 WordNet 层次结构注释的图像，共有 21,841 类，因此也被称作 ImageNet-21K 或 ImageNet-22K. 自 2010 年以来，该数据集被用于 ImageNet 大规模视觉识别挑战赛（ILSVRC），成为了图像分类和目标检测的基准。其中，ILSVRC2012 比赛所采用的子集成为了研究者最常用的版本，详见下文 ImageNet-1K.</p><p><strong>基本信息</strong>：</p><ul><li>数量：14,197,122</li><li>标注：21,814 类</li></ul><p><strong>下载</strong>：用教育邮箱登录官网，选择下载页面，可以在 Winter 2021 release 处看到链接，有若干版本：</p><ul><li>原始全部数据</li><li>可以只下载某一类</li><li>使用论文 "ImageNet-21K pretraining for the masses" 的脚本处理后的版本</li><li>ImageNet10K from ECCV2010</li></ul><p><strong>WIP</strong>：尚未下载和使用过完整的 ImageNet-21K，因此无法给出更多的细节。</p><p><br/></p><h2 id="imagenet-1k-ilsvrc2012">ImageNet-1K (ILSVRC2012)</h2><p><a href="https://image-net.org/challenges/LSVRC/2012/2012-downloads.php">官网</a></p><p><strong>简要介绍</strong>：ILSVRC2012 是最常用的 ImageNet 子集，包含 1000 类物体，因此也被称作 ImageNet-1K.</p><p><strong>基本信息</strong>：</p><ul><li>数量：1,431,167</li><li>划分：1,281,167 / 50,000 / 100,000 (train / valid / test)</li><li>分辨率：各图片不一致，据说平均 469×387</li><li>标注：1000 类。<a href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a">此链接</a>有标签到类别名称的映射。</li></ul><p><strong>下载</strong>：用教育邮箱登录官网，选择 ILSVRC2012 下载页面可看到链接，包含：</p><table><tr><th colspan="2">内容</th><th>文件名</th><th>大小</th></tr><tr><th rowspan="2">Development Kit</th><td>Development Kit (Task 1 &amp; 2)</td><td>ILSVRC2012_devkit_t12.tar.gz</td><td>2.5MB</td></tr><tr><td>Development Kit (Task 3)</td><td>ILSVRC2012_devkit_t3.tar.gz</td><td>22MB</td></tr><tr><th rowspan="4">Images</th><td>Training images (Task 1 &amp; 2)</td><td>ILSVRC2012_img_train.tar</td><td>138GB</td></tr><tr><td>Training images (Task 3)</td><td>ILSVRC2012_img_train_t3.tar</td><td>728MB</td></tr><tr><td>Validation images (all tasks)</td><td>ILSVRC2012_img_val.tar</td><td>6.3GB</td></tr><tr><td>Test images (all tasks)</td><td>ILSVRC2012_img_test_v10102019.tar</td><td>13GB</td></tr><tr><th rowspan="4">Bounding Boxes</th><td>Training bounding box annotations (Task 1 &amp; 2 only)</td><td>ILSVRC2012_bbox_train_v2.tar.gz</td><td>20MB</td></tr><tr><td>Training bounding box annotations (Task 3 only)</td><td>ILSVRC2012_bbox_train_dogs.tar.gz</td><td>1MB</td></tr><tr><td>Validation bounding box annotations (all tasks)</td><td>ILSVRC2012_bbox_val_v3.tgz</td><td>2.2MB</td></tr><tr><td>Test bounding box annotations (Task 3 only)</td><td>ILSVRC2012_bbox_test_dogs.zip</td><td>33MB</td></tr></table><p>其中 Task 1 是粗粒度分类任务、Task 2 是分类并定位任务、Task 3 是细粒度分类任务。</p><blockquote><p>注：一般使用的（以及「基本信息」里面描述的）是 Task 1 &amp; 2 任务的训练数据。</p></blockquote><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── ImageNet<br>    ├── ILSVRC2012_bbox_train_v2.tar.gz         (20.9 MB)<br>    ├── ILSVRC2012_bbox_train_dogs.tar.gz       (726 KB)<br>    ├── ILSVRC2012_bbox_val_v3.tgz              (2.2 MB)<br>    ├── ILSVRC2012_bbox_test_dogs.zip           (34.6 MB)<br>    ├── ILSVRC2012_devkit_t12.tar.gz            (2.6 MB)<br>    ├── ILSVRC2012_devkit_t3.tar.gz             (22.4 MB)<br>    ├── ILSVRC2012_img_train.tar                (147.9 GB)<br>    ├── ILSVRC2012_img_train_t3.tar             (762.5 MB)<br>    ├── ILSVRC2012_img_val.tar                  (6.74 GB)<br>    ├── ILSVRC2012_img_test_v10102019.tar       (13.69 GB)<br>    ├── meta.bin                                (created by torchvision)<br>    ├── train                                   (extracted and organized by torchvision)<br>    │   ├── n01440764<br>    │   ├── ...<br>    │   └── n15075141<br>    ├── val                                     (extracted and organized by torchvision)<br>    │   ├── n01440764<br>    │   ├── ...<br>    │   └── n15075141<br>    └── test                                    (extracted from ILSVRC2012_img_test_v10102019.tar)<br>        ├── ILSVRC2012_test_00000001.JPEG<br>        ├── ...<br>        └── ILSVRC2012_test_00100000.JPEG<br></code></pre></td></tr></table></figure><blockquote><p>注：第一次使用 torchvision 加载 ImageNet 数据集时，它会自动帮你解压并组织文件，最终得到上述目录结构，这个过程耗时较长。这个目录结构与直接解压不同，例如 <code>ILSVRC2012_img_val.tar</code> 中其实是直接包含了所有图片，但 torchvision 将它组织成和训练集一致的结构便于加载。</p></blockquote><p><strong>使用 torchvision 加载数据集（不支持测试集，因为测试集没有 ground-truth 标签）</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torchvision.datasets <span class="hljs-keyword">as</span> dset<br><span class="hljs-meta">&gt;&gt;&gt; </span>imagenet = dset.ImageNet(root=<span class="hljs-string">&#x27;/data/ImageNet&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(imagenet)<br><span class="hljs-number">1281167</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>imagenet = dset.ImageNet(root=<span class="hljs-string">&#x27;/data/ImageNet&#x27;</span>, split=<span class="hljs-string">&#x27;val&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(imagenet)<br><span class="hljs-number">50000</span><br></code></pre></td></tr></table></figure><p><strong>对于不需要标签的任务，可以自定义 <code>Dataset</code> 加载测试集</strong>。</p><p><br/></p><h2 id="imagenet-downsampled">ImageNet-Downsampled</h2><p><a href="https://image-net.org/download-images.php">官网</a> | <a href="https://patrykchrabaszcz.github.io/Imagenet32/">官方博客</a></p><p><strong>简要介绍</strong>：论文 <a href="https://arxiv.org/abs/1707.08819">“A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets”</a> 提出的 ImageNet-1K 数据集的下采样版本，包括 8x8, 16x16, 32x32 和 64x64 四种分辨率版本。</p><p><strong>基本信息</strong>：</p><ul><li>划分：1,281,167 / 50,000 (train / valid)</li><li>标注：1000 类</li></ul><p><strong>下载</strong>：用教育邮箱登录官网，选择下载页面，可以在 Download downsampled image data (32x32, 64x64) 处看到链接。</p><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── ImageNet64<br>    ├── Imagenet64_train_part1.zip  (6.9 GB)<br>    ├── Imagenet64_train_part2.zip  (6.9 GB)<br>    ├── Imagenet64_val.zip          (534 MB)<br>    ├── train_data_batch_1          (extracted from Imagenet64_train_part1.zip)<br>    ├── train_data_batch_2          (extracted from Imagenet64_train_part1.zip)<br>    ├── train_data_batch_3          (extracted from Imagenet64_train_part1.zip)<br>    ├── train_data_batch_4          (extracted from Imagenet64_train_part1.zip)<br>    ├── train_data_batch_5          (extracted from Imagenet64_train_part1.zip)<br>    ├── train_data_batch_6          (extracted from Imagenet64_train_part2.zip)<br>    ├── train_data_batch_7          (extracted from Imagenet64_train_part2.zip)<br>    ├── train_data_batch_8          (extracted from Imagenet64_train_part2.zip)<br>    ├── train_data_batch_9          (extracted from Imagenet64_train_part2.zip)<br>    ├── train_data_batch_10         (extracted from Imagenet64_train_part2.zip)<br>    └── val_data                    (extracted from Imagenet64_val.zip)<br></code></pre></td></tr></table></figure><p><strong>自定义 <code>Dataset</code> 加载数据集</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> T<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ImageNet64</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, root, split=<span class="hljs-string">&#x27;train&#x27;</span>, transform=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-keyword">assert</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;valid&#x27;</span>]<br>        <span class="hljs-keyword">assert</span> os.path.isdir(root), <span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;root&#125;</span> is not an existing directory&#x27;</span><br><br>        self.transform = transform<br><br>        <span class="hljs-keyword">if</span> split == <span class="hljs-string">&#x27;train&#x27;</span>:<br>            self.data, self.labels = [], []<br>            <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>):<br>                filepath = os.path.join(root, <span class="hljs-string">f&#x27;train_data_batch_<span class="hljs-subst">&#123;idx&#125;</span>&#x27;</span>)<br>                <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>                    d = pickle.load(f)<br>                self.data.append(d[<span class="hljs-string">&#x27;data&#x27;</span>])<br>                self.labels.extend(d[<span class="hljs-string">&#x27;labels&#x27;</span>])<br>            self.data = np.concatenate(self.data, axis=<span class="hljs-number">0</span>)<br>            self.labels = [i-<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> self.labels]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(root, <span class="hljs-string">f&#x27;val_data&#x27;</span>), <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>                d = pickle.load(f)<br>            self.data = d[<span class="hljs-string">&#x27;data&#x27;</span>]<br>            self.labels = [i-<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> d[<span class="hljs-string">&#x27;labels&#x27;</span>]]<br>        self.data = np.dstack((self.data[:, :<span class="hljs-number">64</span>*<span class="hljs-number">64</span>], self.data[:, <span class="hljs-number">64</span>*<span class="hljs-number">64</span>:<span class="hljs-number">2</span>*<span class="hljs-number">64</span>*<span class="hljs-number">64</span>], self.data[:, <span class="hljs-number">2</span>*<span class="hljs-number">64</span>*<span class="hljs-number">64</span>:]))<br>        self.data = self.data.reshape((self.data.shape[<span class="hljs-number">0</span>], <span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>))  <span class="hljs-comment"># HWC</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.data)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, item</span>):<br>        img, label = self.data[item], self.labels[item]<br>        img = Image.fromarray(img)<br>        <span class="hljs-keyword">if</span> self.transform <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            img = self.transform(img)<br>        <span class="hljs-keyword">return</span> img, label<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    dataset = ImageNet64(root=<span class="hljs-string">&#x27;/data/ImageNet64/&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br>    dataset = ImageNet64(root=<span class="hljs-string">&#x27;/data/ImageNet64/&#x27;</span>, split=<span class="hljs-string">&#x27;valid&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br></code></pre></td></tr></table></figure><h2 id="istd">ISTD</h2><p><a href="https://github.com/DeepInsight-PCALab/ST-CGAN">官网</a> | <a href="https://paperswithcode.com/dataset/istd">Papers with Code</a></p><p><strong>简要介绍</strong>：Image Shadow Triplets Dataset (ISTD) 是一个用于阴影理解（检测/去除）的数据集，包含 1,870 个图像三元组，每个三元组由阴影图像、阴影掩膜和无阴影图像构成。</p><p><strong>基本信息</strong>：</p><ul><li>数量：1,870</li><li>划分：1,330 / 540 (train / test)</li><li>分辨率：640×480</li><li>标注：三元组（A：阴影图像；B：阴影掩膜；C：无阴影图像）</li></ul><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── ISTD<br>    ├── ISTD_Dataset.rar       (2.14 GB)<br>    ├── train                  (extracted from ISTD_Dataset.rar)<br>    │   ├── train_A            (contains 1330 images)<br>    │   ├── train_B            (contains 1330 images)<br>    │   └── train_C            (contains 1330 images)<br>    └── test                   (extracted from ISTD_Dataset.rar)<br>        ├── test_A             (contains 540 images)<br>        ├── test_B             (contains 540 images)<br>        └── test_C             (contains 540 images)<br></code></pre></td></tr></table></figure><p><strong>需要自定义 <code>Dataset</code> 加载数据集</strong>。</p><p><br/></p><h2 id="logo">LOGO</h2><p><a href="https://github.com/vinthony/deep-blind-watermark-removal">官网</a> | <a href="https://uofmacau-my.sharepoint.com/:f:/g/personal/yb87432_umac_mo/Ek27dEFECGJKqYlZ1vxf7QMBTp3LuEAo-24Sfq_6vGxgaw">OneDrive</a></p><p><strong>简要介绍</strong>：LOGO 是一个用于水印去除的数据集。背景图像选自 MSCOCO 数据集的 VAL2014 子集；水印收集自互联网上 1000 多个不同的著名标志。将水印（标志）在不同位置、半透明度和大小随机放置在自然图像上来生成水印样本。每个训练/测试样本都包含合成的水印图像、原始背景、水印以及水印掩膜以进行监督。所有水印和背景图像在训练和验证分区中都没有重叠。按照水印的不同透明度和大小，整个数据集分为四种设置：LOGO-L、LOGO-H、LOGO-Gray、LOGO30K.</p><p><strong>基本信息</strong>：</p><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">LOGO-L</th><th style="text-align: center;">LOGO-H</th><th style="text-align: center;">LOGO-Gray</th><th style="text-align: center;">LOGO30K</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">transparency</td><td style="text-align: center;">35% - 60%</td><td style="text-align: center;">60% - 85%</td><td style="text-align: center;">35% - 85%</td><td style="text-align: center;">35% - 85%</td></tr><tr class="even"><td style="text-align: center;">Percentage of the watermarks size</td><td style="text-align: center;">35% - 60%</td><td style="text-align: center;">60% - 85%</td><td style="text-align: center;">35% - 85%</td><td style="text-align: center;">35% - 85%</td></tr><tr class="odd"><td style="text-align: center;">Numbers of samples (train : val)</td><td style="text-align: center;">12k : 2k</td><td style="text-align: center;">12k : 2k</td><td style="text-align: center;">12k : 2k</td><td style="text-align: center;">28k : 4k</td></tr><tr class="even"><td style="text-align: center;">Colorful watermark?</td><td style="text-align: center;">yes</td><td style="text-align: center;">yes</td><td style="text-align: center;">no</td><td style="text-align: center;">yes</td></tr></tbody></table><ul><li>数量：<ul><li>LOGO-L、LOGO-H、LOGO-Gray：14,176</li><li>LOGO30K：32,403</li></ul></li><li>划分（train / valid）：<ul><li>LOGO-L、LOGO-H、LOGO-Gray：12,151 / 2,025</li><li>LOGO30K：28,352 / 4,051</li></ul></li><li>分辨率：对应 COCO 数据集的原始图片大小，另提供 resize 到 256×256 的验证集。</li></ul><p><strong>本地目录结构</strong>：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs scss">data<br>└── LOGO<br>    ├── <span class="hljs-number">10</span>kgray<span class="hljs-selector-class">.zip</span>               (<span class="hljs-number">14.99</span> GB)<br>    ├── <span class="hljs-number">10</span>khigh<span class="hljs-selector-class">.zip</span>               (<span class="hljs-number">15.18</span> GB)<br>    ├── <span class="hljs-number">10</span>kmid<span class="hljs-selector-class">.zip</span>                (<span class="hljs-number">15</span> GB)<br>    ├── <span class="hljs-number">27</span>kpng<span class="hljs-selector-class">.zip</span>                (<span class="hljs-number">24.11</span> GB)<br>    ├── <span class="hljs-number">10</span>kgray                   (extracted from <span class="hljs-number">27</span>kpng.zip)<br>    │   ├── train_images<span class="hljs-selector-class">.txt</span><br>    │   ├── train_wm<span class="hljs-selector-class">.txt</span><br>    │   ├── val_images<span class="hljs-selector-class">.txt</span><br>    │   ├── val_wm<span class="hljs-selector-class">.txt</span><br>    │   ├── natural               (contains <span class="hljs-number">40504</span> jpg images selected from COCO_val2014)<br>    │   ├── train_images<br>    │   │   ├── image             (contains <span class="hljs-number">12151</span> png images)<br>    │   │   ├── <span class="hljs-attribute">mask</span>              (contains <span class="hljs-number">12151</span> png images)<br>    │   │   └── wm                (contains <span class="hljs-number">12151</span> png images)<br>    │   ├── val_images<br>    │   │   ├── image             (contains <span class="hljs-number">2025</span> png images)<br>    │   │   ├── <span class="hljs-attribute">mask</span>              (contains <span class="hljs-number">2025</span> png images)<br>    │   │   └── wm                (contains <span class="hljs-number">2025</span> png images)<br>    │   ├── val_input_256         (contains <span class="hljs-number">2025</span> png images, resized to <span class="hljs-number">256</span>x256)<br>    │   └── val_target_256        (contains <span class="hljs-number">2025</span> png images, resized to <span class="hljs-number">256</span>x256)<br>    ├── <span class="hljs-number">10</span>khigh                   (extracted from <span class="hljs-number">10</span>khigh.zip)<br>    │   └── same as <span class="hljs-number">10</span>kgray<br>    ├── <span class="hljs-number">10</span>kmid                    (extracted from <span class="hljs-number">10</span>kmid.zip)<br>    │   └── same as <span class="hljs-number">10</span>kgray<br>    └── <span class="hljs-number">27</span>kpng                    (extracted from <span class="hljs-number">27</span>kpng.zip)<br>        ├── natural               (contains <span class="hljs-number">32403</span> jpg images selected from COCO_val2014)<br>        ├── train_images<br>        │   ├── image             (contains <span class="hljs-number">28352</span> png images)<br>        │   ├── <span class="hljs-attribute">mask</span>              (contains <span class="hljs-number">28352</span> png images)<br>        │   └── wm                (contains <span class="hljs-number">28352</span> png images)<br>        ├── val_images<br>        │   ├── image             (contains <span class="hljs-number">4055</span> png images)<br>        │   ├── <span class="hljs-attribute">mask</span>              (contains <span class="hljs-number">4051</span> png images)<br>        │   └── wm                (contains <span class="hljs-number">4051</span> png images)<br>        └── val_target_256        (contains <span class="hljs-number">4051</span> png images, cropped to <span class="hljs-number">256</span>x256)<br></code></pre></td></tr></table></figure><p><strong>需要自定义 <code>Dataset</code> 加载数据集</strong>。</p><p><br/></p><h2 id="mnist">MNIST</h2><p><a href="http://yann.lecun.com/exdb/mnist/">官网</a></p><p><strong>简要介绍</strong>：MNIST 手写数字数据集包含 60,000 个训练样本和 10,000 个测试样本。数字的大小已标准化且放置在图像中心。</p><p><strong>基本信息</strong>：</p><ul><li>数量：70,000</li><li>划分：60,000 / 10,000 (train / test)</li><li>分辨率：28×28</li><li>标注：10 类</li></ul><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── MNIST<br>    └── MNIST<br>        └── raw<br>            ├── t10k-images-idx3-ubyte<br>            ├── t10k-images-idx3-ubyte.gz<br>            ├── t10k-labels-idx1-ubyte<br>            ├── t10k-labels-idx1-ubyte.gz<br>            ├── train-images-idx3-ubyte<br>            ├── train-images-idx3-ubyte.gz<br>            ├── train-labels-idx1-ubyte<br>            └── train-labels-idx1-ubyte.gz<br></code></pre></td></tr></table></figure><p><strong>使用 torchvision 加载数据集</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torchvision.datasets <span class="hljs-keyword">as</span> dset<br><span class="hljs-meta">&gt;&gt;&gt; </span>mnist = dset.MNIST(root=<span class="hljs-string">&#x27;/data/MNIST&#x27;</span>, train=<span class="hljs-literal">True</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(mnist)<br><span class="hljs-number">60000</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>mnist = dset.MNIST(root=<span class="hljs-string">&#x27;/data/MNIST&#x27;</span>, train=<span class="hljs-literal">False</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(mnist)<br><span class="hljs-number">10000</span><br></code></pre></td></tr></table></figure><p><br/></p><h2 id="nvidia-irregular-mask-dataset">NVIDIA Irregular Mask Dataset</h2><p><a href="https://nv-adlr.github.io/publication/partialconv-inpainting">官网</a> | <a href="https://www.dropbox.com/s/qp8cxqttta4zi70/irregular_mask.zip?dl=0">Dropbox (irregular_mask.zip)</a> | <a href="https://www.dropbox.com/s/01dfayns9s0kevy/test_mask.zip?dl=0">Dropbox (test_mask.zip)</a></p><p><strong>简要介绍</strong>：随机掩码来源于估计视频中两个相邻帧之间遮挡/非遮挡掩码。数据集提出者采用这种方式生成了 55,116 个用于训练的掩码，训练时首先从 55,116 个掩码中随机采样一个掩码，然后进行随机膨胀、旋转和裁剪来增强掩码数据集。用同样的方式生成了 24,866 个测试掩码，经过随机增强后按照面积比例划分为 6 组：(0.01, 0.1], (0.1, 0.2], (0.2, 0.3], (0.3, 0.4], (0.4, 0.5], (0.5, 0.6]，每组 2,000 个掩码，其中 1,000 个没有靠近边缘的缺失像素，另外 1,000 个有靠近边缘的缺失像素，最终形成大小为 12,000 的测试集。</p><p><strong>基本信息</strong>：</p><ul><li>划分（官方）：55,116 / 12,000 (train / test)</li><li>划分（个人用法，见「使用说明」）：76,800 / 19,200 (train / test)</li><li>分辨率：<ul><li>训练集（增强前）：960×640</li><li>测试集：512×512</li></ul></li></ul><p><strong>使用说明（重要！）</strong>：从「简要介绍」可以看到，NVIDIA 官方提供的训练集并不是最终的训练数据，需要在训练时做增强操作。但考虑到数据增强的耗时，很多研究只采用官方测试集中的图片训练和测试，因此需要重新制作和划分训练、测试集。我这里采用类似于 EdgeConnect 的做法重新划分训练、测试集，具体方法为：每张官方测试集中的掩码图片在翻转和旋转后可以生成 8 张图片，因此原来的测试集大小被扩张到 96,000，再随机按 4:1 划分新的训练/测试集（详见 <a href="https://github.com/knazeri/edge-connect/issues/28#issuecomment-456440064">knazeri/edge-connect#28 (comment)</a>）。划分脚本如下 <code>_make_irregular_dataset.py</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> T<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    irregular_mask = <span class="hljs-string">&#x27;/data/NVIDIAIrregularMaskDataset/downloaded_test/&#x27;</span><br>    save_path_train = <span class="hljs-string">&#x27;/data/NVIDIAIrregularMaskDataset/train/&#x27;</span><br>    save_path_test = <span class="hljs-string">&#x27;/data/NVIDIAIrregularMaskDataset/test/&#x27;</span><br><br>    masks = os.listdir(irregular_mask)<br>    <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">6</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(os.path.join(save_path_train, <span class="hljs-built_in">str</span>(k))):<br>            os.mkdir(os.path.join(save_path_train, <span class="hljs-built_in">str</span>(k)))<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(os.path.join(save_path_test, <span class="hljs-built_in">str</span>(k))):<br>            os.mkdir(os.path.join(save_path_test, <span class="hljs-built_in">str</span>(k)))<br><br>        tests = np.random.choice(np.arange(<span class="hljs-number">2000</span>), size=<span class="hljs-number">400</span>, replace=<span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">for</span> idx, fileid <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(<span class="hljs-built_in">range</span>(k * <span class="hljs-number">2000</span>, (k + <span class="hljs-number">1</span>) * <span class="hljs-number">2000</span>))):<br>            mask = Image.<span class="hljs-built_in">open</span>(os.path.join(irregular_mask, <span class="hljs-built_in">str</span>(fileid).zfill(<span class="hljs-number">5</span>)+<span class="hljs-string">&#x27;.png&#x27;</span>))<br>            mask = T.ToTensor()(mask)<br>            <span class="hljs-comment"># binarization</span><br>            mask = torch.where(mask &lt; <span class="hljs-number">0.5</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>)<br>            <span class="hljs-comment"># rotation &amp; flipping</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>                <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):<br>                    tmp = T.RandomRotation((i * <span class="hljs-number">90</span>, i * <span class="hljs-number">90</span>))(mask)<br>                    tmp = T.RandomHorizontalFlip(p=j)(tmp)<br>                    tmp = T.ToPILImage()(tmp)<br><br>                    <span class="hljs-keyword">if</span> idx <span class="hljs-keyword">in</span> tests:<br>                        tmp.save(os.path.join(save_path_test, <span class="hljs-built_in">str</span>(k), <span class="hljs-built_in">str</span>(fileid).zfill(<span class="hljs-number">5</span>)+<span class="hljs-string">f&#x27;_<span class="hljs-subst">&#123;i*<span class="hljs-number">2</span>+j&#125;</span>.png&#x27;</span>))<br>                    <span class="hljs-keyword">else</span>:<br>                        tmp.save(os.path.join(save_path_train, <span class="hljs-built_in">str</span>(k), <span class="hljs-built_in">str</span>(fileid).zfill(<span class="hljs-number">5</span>)+<span class="hljs-string">f&#x27;_<span class="hljs-subst">&#123;i*<span class="hljs-number">2</span>+j&#125;</span>.png&#x27;</span>))<br></code></pre></td></tr></table></figure><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── NVIDIAIrregularMaskDataset<br>    ├── _make_irregular_dataset.py<br>    ├── downloaded_train.zip          (1.33 GB, renamed)<br>    ├── downloaded_test.zip           (44.9 MB, renamed)<br>    ├── downloaded_train              (extracted from downloaded_train.zip, renamed)<br>    │   ├── 00001.png<br>    │   ├── ...<br>    │   └── 55116.png<br>    ├── downloaded_test               (extracted from downloaded_test.zip, renamed)<br>    │   ├── 00000.png<br>    │   ├── ...<br>    │   └── 11999.png<br>    ├── train.zip                     (285 MB)<br>    ├── test.zip                      (71.3 MB)<br>    ├── train                         (created by _make_irregular_dataset.py)<br>    └── test                          (created by _make_irregular_dataset.py)<br></code></pre></td></tr></table></figure><p><strong>需要自定义 <code>Dataset</code> 加载数据集</strong>。</p><p><br/></p><h2 id="oxford-102-flower">Oxford 102 Flower</h2><p><a href="https://www.robots.ox.ac.uk/~vgg/data/flowers/102/">官网</a> | <a href="https://paperswithcode.com/dataset/oxford-102-flower">Papers with Code</a></p><p><strong>简要介绍</strong>：Oxford 102 Flower 是一个包含 102 个花卉类别的图像分类数据集。所选花卉是英国常见的花卉。每个类别包含 40 至 258 张图像。这些图像在大小、姿态和光照上都有很大的变化。此外，有些类别内部变化很大，还有一些类别非常相似。</p><p><strong>基本信息</strong>：</p><ul><li>数量：8,189</li><li>划分：1,020 / 1,020 / 6,149 (train / valid / test)</li><li>分辨率：不一致，但最小边长均为 500</li><li>标注：102 类</li></ul><p><strong>下载</strong>：官网提供了 6 个下载链接，分别是：</p><table><thead><tr class="header"><th>内容</th><th>文件名</th><th>大小</th></tr></thead><tbody><tr class="odd"><td>Dataset images</td><td><code>102flowers.tgz</code></td><td>344.9 MB</td></tr><tr class="even"><td>Image segmentations</td><td><code>102segmentations.tgz</code></td><td>203.6 MB</td></tr><tr class="odd"><td>&amp;Chi2 distances</td><td><code>distancematrices102.mat</code></td><td>1.98 GB</td></tr><tr class="even"><td>The image labels</td><td><code>imagelabels.mat</code></td><td>502 B</td></tr><tr class="odd"><td>The data splits</td><td><code>setid.mat</code></td><td>15 KB</td></tr><tr class="even"><td>README</td><td><code>README.txt</code></td><td>/</td></tr></tbody></table><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── Oxford-102-Flower<br>    └── flowers-102<br>        ├── README.txt<br>        ├── imagelabels.mat<br>        ├── setid.mat<br>        ├── distancematrices102.mat<br>        ├── 102flowers.tgz            (344.9 MB)<br>        ├── 102segmentations.tgz      (203.6 MB)<br>        ├── jpg                       (extracted from 102flowers.tgz)<br>        │   ├── image_00001.jpg<br>        │   ├── ...<br>        │   └── image_08189.jpg<br>        └── segmim                    (extracted from 102segmentations.tgz)<br>            ├── segmim_00001.jpg<br>            ├── ...<br>            └── segmim_08189.jpg<br></code></pre></td></tr></table></figure><p><strong>使用 torchvision 加载数据集</strong>。</p><p><br/></p><h2 id="paris-streetview">Paris StreetView</h2><p><strong>官网与下载</strong>：需要联系作者，见 https://github.com/pathak22/context-encoder/issues/24</p><p><strong>基本信息</strong>：</p><ul><li>数量：15,000</li><li>划分：14,900 / 100 (train / test)</li><li>分辨率：<ul><li>训练集：936×537</li><li>测试集：227×227</li></ul></li></ul><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── ParisStreetView<br>    ├── paris_eval_75876.zip                  (16.2 MB)<br>    ├── paris_train_original.zip              (1.22 GB)<br>    ├── paris_eval_gt                         (extracted from paris_eval_75876.zip)<br>    │   ├── 001_im.png<br>    │   ├── ...<br>    │   └── 100_im.png<br>    ├── paris_eval_corrupted                  (extracted from paris_eval_75876.zip)<br>    │   ├── 001_im.png<br>    │   ├── ...<br>    │   └── 100_im.png<br>    └── paris_train_original                  (extracted from paris_train_original.zip)<br>        ├── 48.842502_2.344968_90_-004.JPG<br>        ├── ...<br>        └── 48.867048_2.348918_270_-004.JPG<br></code></pre></td></tr></table></figure><p><strong>需要自定义 <code>Dataset</code> 加载数据集</strong>，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ParisStreetView</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    The downloaded data should be organized in the following structure:</span><br><span class="hljs-string"></span><br><span class="hljs-string">    - root/</span><br><span class="hljs-string">        - paris_train_original/ (14,900 images extracted from paris_train_original.zip)</span><br><span class="hljs-string">            - 48.842502_2.344968_90_-004.JPG</span><br><span class="hljs-string">            - ...</span><br><span class="hljs-string">        - paris_eval_gt/ (100 images extracted from paris_eval_75876.zip)</span><br><span class="hljs-string">            - 001_im.png</span><br><span class="hljs-string">            - ...</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, root, split=<span class="hljs-string">&#x27;train&#x27;</span>, transform=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-keyword">assert</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;all&#x27;</span>]<br>        train_root = os.path.join(root, <span class="hljs-string">&#x27;paris_train_original&#x27;</span>)<br>        eval_root = os.path.join(root, <span class="hljs-string">&#x27;paris_eval_gt&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> os.path.isdir(root) <span class="hljs-keyword">and</span> os.path.isdir(train_root) <span class="hljs-keyword">and</span> os.path.isdir(eval_root)<br><br>        self.transform = transform<br><br>        img_ext = [<span class="hljs-string">&#x27;.png&#x27;</span>, <span class="hljs-string">&#x27;.jpg&#x27;</span>, <span class="hljs-string">&#x27;.jpeg&#x27;</span>]<br>        self.img_paths = []<br>        <span class="hljs-keyword">if</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;all&#x27;</span>]:<br>            <span class="hljs-keyword">for</span> curdir, subdirs, files <span class="hljs-keyword">in</span> os.walk(train_root):<br>                <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:<br>                    <span class="hljs-keyword">if</span> os.path.splitext(file)[<span class="hljs-number">1</span>].lower() <span class="hljs-keyword">in</span> img_ext:<br>                        self.img_paths.append(os.path.join(curdir, file))<br>        <span class="hljs-keyword">if</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;all&#x27;</span>]:<br>            <span class="hljs-keyword">for</span> curdir, subdirs, files <span class="hljs-keyword">in</span> os.walk(eval_root):<br>                <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> files:<br>                    <span class="hljs-keyword">if</span> os.path.splitext(file)[<span class="hljs-number">1</span>].lower() <span class="hljs-keyword">in</span> img_ext:<br>                        self.img_paths.append(os.path.join(curdir, file))<br>        self.img_paths = <span class="hljs-built_in">sorted</span>(self.img_paths)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.img_paths)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, item</span>):<br>        X = Image.<span class="hljs-built_in">open</span>(self.img_paths[item])<br>        <span class="hljs-keyword">if</span> self.transform <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            X = self.transform(X)<br>        <span class="hljs-keyword">return</span> X<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    dataset = ParisStreetView(root=<span class="hljs-string">&#x27;/data/ParisStreetView&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br>    dataset = ParisStreetView(root=<span class="hljs-string">&#x27;/data/ParisStreetView&#x27;</span>, split=<span class="hljs-string">&#x27;test&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br>    dataset = ParisStreetView(root=<span class="hljs-string">&#x27;/data/ParisStreetView&#x27;</span>, split=<span class="hljs-string">&#x27;all&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))<br></code></pre></td></tr></table></figure><p><br/></p><h2 id="pascal-voc">PASCAL-VOC</h2><p><a href="http://host.robots.ox.ac.uk/pascal/VOC/">官网</a> | <a href="https://paperswithcode.com/dataset/pascal-voc">Papers with Code</a></p><p><strong>详细统计数据</strong>：http://host.robots.ox.ac.uk/pascal/VOC/voc2007/dbstats.html</p><p><strong>说明</strong>：最常用的两个版本是 2007 和 2012，检测和分割可用的数据量也有所不同</p><ul><li><p>数量及划分：</p><ul><li><p>2007 Detection</p><p>9,963 images，24,640 objects</p><p>2,501 / 2,510 / 4,952 (train / valid / test)</p></li><li><p>2007 Segmentation</p><p>632 images</p><p>209 / 213 / 210 (train / valid / test)</p></li><li><p>2012 Detection</p><p>11,540 images，27,450 objects</p><p>5,717 / 5,823 (train / valid)</p></li><li><p>2012 Segmentation</p><p>2,913 images</p><p>1,464 / 1,449 (train / valid)</p></li></ul></li><li><p>分辨率：各图片不一致，大多 500×375 左右</p></li><li><p>标注：Bounding boxes、语义/实例分割 mask</p></li></ul><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── PASCAL-VOC<br>    ├── VOCtrainval_06-Nov-2007.tar      (460 MB)<br>    ├── VOCtest_06-Nov-2007.tar          (451 MB)<br>    ├── VOCtrainval_11-May-2012.tar      (2 GB)<br>    └── VOCdevkit                        (extracted from *.tar)<br>        ├── VOC2007<br>        │   ├── Annotations<br>        │   │   ├── 000001.xml<br>        │   │   ├── ...<br>        │   │   └── 009963.xml<br>        │   ├── ImageSets<br>        │   │   ├── Layout<br>        │   │   │   ├── test.txt<br>        │   │   │   ├── train.txt<br>        │   │   │   ├── trainval.txt<br>        │   │   │   └── val.txt<br>        │   │   ├── Main<br>        │   │   │   ├── aeroplane_test.txt<br>        │   │   │   ├── ...<br>        │   │   │   └── val.txt<br>        │   │   └── Segmentation<br>        │   │       ├── test.txt<br>        │   │       ├── train.txt<br>        │   │       ├── trainval.txt<br>        │   │       └── val.txt<br>        │   ├── JPEGImages<br>        │   │   ├── 000001.jpg<br>        │   │   ├── ...<br>        │   │   └── 009963.jpg<br>        │   ├── SegmentationClass<br>        │   │   ├── 000032.jpg<br>        │   │   ├── ...<br>        │   │   └── 009950.jpg<br>        │   └── SegmentationObject<br>        │       ├── 000032.jpg<br>        │       ├── ...<br>        │       └── 009950.jpg<br>        └── VOC2012                      (same as VOC2007)<br></code></pre></td></tr></table></figure><p><strong>使用 torchvision 加载数据集</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torchvision.datasets <span class="hljs-keyword">as</span> dset<br><span class="hljs-meta">&gt;&gt;&gt; </span>voc = dset.VOCDetection(root=<span class="hljs-string">&#x27;/data/PASCAL-VOC&#x27;</span>, year=<span class="hljs-string">&#x27;2007&#x27;</span>, image_set=<span class="hljs-string">&#x27;train&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(voc)<br><span class="hljs-number">2501</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>voc = dset.VOCDetection(root=<span class="hljs-string">&#x27;/data/PASCAL-VOC&#x27;</span>, year=<span class="hljs-string">&#x27;2007&#x27;</span>, image_set=<span class="hljs-string">&#x27;val&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(voc)<br><span class="hljs-number">2510</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>voc = dset.VOCDetection(root=<span class="hljs-string">&#x27;/data/PASCAL-VOC&#x27;</span>, year=<span class="hljs-string">&#x27;2007&#x27;</span>, image_set=<span class="hljs-string">&#x27;trainval&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(voc)<br><span class="hljs-number">5011</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>voc = dset.VOCDetection(root=<span class="hljs-string">&#x27;/data/PASCAL-VOC&#x27;</span>, year=<span class="hljs-string">&#x27;2007&#x27;</span>, image_set=<span class="hljs-string">&#x27;test&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(voc)<br><span class="hljs-number">4952</span><br><br><span class="hljs-meta">&gt;&gt;&gt; </span>voc = dset.VOCSegmentation(root=<span class="hljs-string">&#x27;/data/PASCAL-VOC&#x27;</span>, year=<span class="hljs-string">&#x27;2007&#x27;</span>, image_set=<span class="hljs-string">&#x27;train&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(voc)<br><span class="hljs-number">209</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>voc = dset.VOCSegmentation(root=<span class="hljs-string">&#x27;/data/PASCAL-VOC&#x27;</span>, year=<span class="hljs-string">&#x27;2007&#x27;</span>, image_set=<span class="hljs-string">&#x27;val&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(voc)<br><span class="hljs-number">213</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>voc = dset.VOCSegmentation(root=<span class="hljs-string">&#x27;/data/PASCAL-VOC&#x27;</span>, year=<span class="hljs-string">&#x27;2007&#x27;</span>, image_set=<span class="hljs-string">&#x27;trainval&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(voc)<br><span class="hljs-number">422</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>voc = dset.VOCSegmentation(root=<span class="hljs-string">&#x27;/data/PASCAL-VOC&#x27;</span>, year=<span class="hljs-string">&#x27;2007&#x27;</span>, image_set=<span class="hljs-string">&#x27;test&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(voc)<br><span class="hljs-number">210</span><br><br><span class="hljs-meta">&gt;&gt;&gt; </span>voc = dset.VOCDetection(root=<span class="hljs-string">&#x27;/data/PASCAL-VOC&#x27;</span>, year=<span class="hljs-string">&#x27;2012&#x27;</span>, image_set=<span class="hljs-string">&#x27;train&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(voc)<br><span class="hljs-number">5717</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>voc = dset.VOCDetection(root=<span class="hljs-string">&#x27;/data/PASCAL-VOC&#x27;</span>, year=<span class="hljs-string">&#x27;2012&#x27;</span>, image_set=<span class="hljs-string">&#x27;val&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(voc)<br><span class="hljs-number">5823</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>voc = dset.VOCDetection(root=<span class="hljs-string">&#x27;/data/PASCAL-VOC&#x27;</span>, year=<span class="hljs-string">&#x27;2012&#x27;</span>, image_set=<span class="hljs-string">&#x27;trainval&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(voc)<br><span class="hljs-number">11540</span><br><br><span class="hljs-meta">&gt;&gt;&gt; </span>voc = dset.VOCSegmentation(root=<span class="hljs-string">&#x27;/data/PASCAL-VOC&#x27;</span>, year=<span class="hljs-string">&#x27;2012&#x27;</span>, image_set=<span class="hljs-string">&#x27;train&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(voc)<br><span class="hljs-number">1464</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>voc = dset.VOCSegmentation(root=<span class="hljs-string">&#x27;/data/PASCAL-VOC&#x27;</span>, year=<span class="hljs-string">&#x27;2012&#x27;</span>, image_set=<span class="hljs-string">&#x27;val&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(voc)<br><span class="hljs-number">1449</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>voc = dset.VOCSegmentation(root=<span class="hljs-string">&#x27;/data/PASCAL-VOC&#x27;</span>, year=<span class="hljs-string">&#x27;2012&#x27;</span>, image_set=<span class="hljs-string">&#x27;trainval&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(voc)<br><span class="hljs-number">2913</span><br></code></pre></td></tr></table></figure><p><br/></p><h2 id="places365">Places365</h2><p><a href="http://places2.csail.mit.edu/index.html">官网</a> | <a href="https://paperswithcode.com/dataset/places365">Papers with Code</a> | <a href="http://places2.csail.mit.edu/download-private.html">下载链接</a></p><p><strong>简要介绍</strong>：Places365 数据集是一个场景识别数据集，由 1000 万张图像组成，包括 434 个场景类别。数据集有两个版本：Places365-Standard 包括 180 万个训练图像和 36000 个验证图像，涵盖了 365 个场景类别；Places365-Challenge-2016 在训练集中增加了 620 万张额外的图像，包括了 69 个新的场景类别（总共涵盖了 434 个场景类别），达到了 800 万张的训练图像。</p><p><strong>基本信息</strong>：</p><ul><li><strong>Places365-Standard</strong><ul><li>数量：2,168,460</li><li>划分：1,803,460 / 36,500 / 328,500 (train / valid / test)</li><li>分辨率：<ul><li>High-resolution：按照短边 512、保留长宽比缩放，若原图小于 512 则不变</li><li>Small：直接缩放至 256×256，不管原始长宽比</li></ul></li><li>标注：365 类场景</li></ul></li><li><strong>Places365-Challenge-2016</strong><ul><li>数量：训练集在 Standard 的基础上额外添加了 6.2 million；验证集和测试集不变</li><li>划分：8,026,628 / 36,500 / 328,500 (train / valid / test)</li><li>标注：365 类场景</li></ul></li><li><strong>Places-Extra69</strong><ul><li>划分：98,721 / 6,600 (train / test)</li><li>标注：额外 69 类场景</li></ul></li></ul><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── Places365<br>    ├── categories_places365.txt<br>    ├── places365_train_standard.txt<br>    ├── places365_train_challenge.txt<br>    ├── places365_val.txt<br>    ├── places365_test.txt<br>    ├── train_256_places365standard.tar    (26.1 GB, MD5: 53ca1c756c3d1e7809517cc47c5561c5)<br>    ├── train_large_places365standard.tar  (112.6 GB, MD5: 67e186b496a84c929568076ed01a8aa1)<br>    ├── train_256_places365challenge.tar   (108 GB, MD5: 741915038a5e3471ec7332404dfb64ef)<br>    ├── train_large_places365challenge.tar (477 GB, MD5: 605f18e68e510c82b958664ea134545f)<br>    ├── val_256.tar                        (525.2 MB, MD5: e27b17d8d44f4af9a78502beb927f808)<br>    ├── val_large.tar                      (2.27 GB, MD5: 9b71c4993ad89d2d8bcbdc4aef38042f)<br>    ├── test_256.tar                       (4.74 GB, MD5: f532f6ad7b582262a2ec8009075e186b)<br>    ├── test_large.tar                     (20.48 GB, MD5: 41a4b6b724b1d2cd862fb3871ed59913)<br>    ├── data_256_standard                  (extracted from train_256_places365standard.tar)<br>    │   ├── a<br>    │   ├── ...<br>    │   └── z<br>    ├── data_large_standard                (extracted from train_large_places365standard.tar)<br>    │   ├── a<br>    │   ├── ...<br>    │   └── z<br>    ├── val_256                            (extracted from val_256.tar)<br>    │   ├── Places365_val_00000001.jpg<br>    │   ├── ...<br>    │   └── Places365_val_00036500.jpg<br>    ├── val_large                          (extracted from val_large.tar)<br>    │   ├── Places365_val_00000001.jpg<br>    │   ├── ...<br>    │   └── Places365_val_00036500.jpg<br>    ├── test_256                           (extracted from test_256.tar)<br>    │   ├── Places365_test_00000001.jpg<br>    │   ├── ...<br>    │   └── Places365_test_00328500.jpg<br>    └── test_large                         (extracted from test_large.tar)<br>        ├── Places365_test_00000001.jpg<br>        ├── ...<br>        └── Places365_test_00328500.jpg<br></code></pre></td></tr></table></figure><blockquote><p>注：我目前只下载了 Places365-Standard.</p></blockquote><p><strong>使用 torchvision 加载数据集（不支持测试集）</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torchvision.datasets <span class="hljs-keyword">as</span> dset<br><span class="hljs-meta">&gt;&gt;&gt; </span>places = dset.Places365(root=<span class="hljs-string">&#x27;/data/Places365&#x27;</span>, split=<span class="hljs-string">&#x27;train-standard&#x27;</span>, small=<span class="hljs-literal">True</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(places)<br><span class="hljs-number">1803460</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>places = dset.Places365(root=<span class="hljs-string">&#x27;/data/Places365&#x27;</span>, split=<span class="hljs-string">&#x27;val&#x27;</span>, small=<span class="hljs-literal">True</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(places)<br><span class="hljs-number">36500</span><br></code></pre></td></tr></table></figure><p><strong>可以自定义 <code>Dataset</code> 加载测试集</strong>。</p><p><br/></p><h2 id="raindrop">Raindrop</h2><p><a href="https://github.com/rui1996/DeRaindrop">官网</a> | <a href="https://paperswithcode.com/dataset/raindrop">Papers with Code</a> | <a href="https://drive.google.com/open?id=1e7R76s6vwUJxILOcAsthgDLPSnOrQ49K">Google Drive</a></p><p><strong>简要介绍</strong>：Raindrop 是一个真实拍摄（而非合成）的用于去雨滴任务的数据集，由若干图像对构成。每对图像包含完全相同的背景，但其中一个图像有雨滴，而另一个图像则没有。为此，使用两片完全相同的玻璃——一片喷有水、另一片保持干净——附在相机镜头上拍摄。拍摄设备为 Sony A6000 和 Canon EOS 60.</p><p><strong>基本信息</strong>：</p><ul><li>数量：1,110</li><li>划分：861 / 249 (train / test_b)；另外，取 test_b 中对齐比较好的 58 对图片构成 test_a.</li><li>分辨率：多数 720×480，有少量例外，但也很接近。</li></ul><p><strong>本地目录结构</strong>：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs text">data<br>└── Raindrop<br>    ├── train.zip               (1.07 GB)<br>    ├── test_a.zip              (63.7 MB)<br>    ├── test_b.zip              (154.4 MB)<br>    ├── train                   (extracted from train.zip)<br>    │   ├── data                (contains 861 png images)<br>    │   ├── gt                  (contains 861 png images)<br>    │   └── preview.html<br>    ├── test_a                  (extracted from test_a.zip)<br>    │   ├── data                (contains 58 png images)<br>    │   └── gt                  (contains 58 png images)<br>    └── test_b                  (extracted from test_b.zip)<br>        ├── data                (contains 249 jpg images)<br>        └── gt                  (contains 249 jpg images)<br></code></pre></td></tr></table></figure><p><strong>需要自定义 <code>Dataset</code> 加载数据集</strong>。</p><p><br/></p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>计算机视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>k-means探究（二）soft k-means</title>
    <link href="/blog-main/2022/09/04/k-means%E6%8E%A2%E7%A9%B6%EF%BC%88%E4%BA%8C%EF%BC%89soft-k-means/"/>
    <url>/blog-main/2022/09/04/k-means%E6%8E%A2%E7%A9%B6%EF%BC%88%E4%BA%8C%EF%BC%89soft-k-means/</url>
    
    <content type="html"><![CDATA[<p>在 k-means 聚类中，每一个数据点隶属于一个类，这是一种 hard 的模式。与之相对的，soft clustering 不把一个数据点硬分给一类，而是给出它属于各个类的“置信度”，表示它属于各个类的程度。在有些场景下，我们也许更希望使用 soft 模式。本文试从两种角度推导 soft 版本的 k-means 算法。</p><h2 id="角度-1hard-k-means-soft-k-means">角度 1：hard k-means → soft k-means</h2><p>在之前的文章<a href="/blog-main/2022/08/12/k-means%E6%8E%A2%E7%A9%B6/" title="k-means探究">k-means探究</a>中，我们知道 k-means 是用迭代的方式优化下述目标： <span class="math display">\[\quad\sum_{l=1}^k\sum_{C(i)=l}\|x_i-m_l\|^2\]</span></p><p>其中 <span class="math inline">\(C\)</span> 表示划分，<span class="math inline">\(C(i)=l\)</span> 表示样本 <span class="math inline">\(x_i\)</span> 被划分到类 <span class="math inline">\(l\)</span> 中。我们定义指示变量： <span class="math display">\[\gamma_{il}=\begin{cases}1,&amp;\text{if }C(i)=l\\0,&amp;\text{otherwise}\end{cases}\]</span> 那么优化目标可以改写为： <span class="math display">\[\quad\sum_{l=1}^k\sum_{i=1}^n\gamma_{il}\|x_i-m_l\|^2\tag{1}\label{target}\]</span> K-means 硬就硬在 <span class="math inline">\(\gamma_{il}\)</span> 是一个 0/1 变量。<span class="math inline">\(\gamma_{il}=1\)</span> 表示样本 <span class="math inline">\(x_i\)</span> 被划分给了类 <span class="math inline">\(l\)</span>，这意味着在 <span class="math inline">\(x_i\)</span> 到所有聚类中心的距离里面，它到 <span class="math inline">\(m_l\)</span> 最近，写作数学语言即： <span class="math display">\[l=\arg\min_{j}\|x_i-m_j\|^2\]</span> 进而 <span class="math inline">\(\gamma_{il}\)</span> 可以写作： <span class="math display">\[\gamma_{il}=\text{onehot}\left(\arg\min_j \|x_i-m_j\|^2\right)_l=\text{onehot}\left(\arg\max_j \left(-\|x_i-m_j\|^2\right)\right)_l\]</span> 根据<a href="/blog-main/2022/07/25/%E5%90%84%E7%A7%8D%E5%87%BD%E6%95%B0%E7%9A%84hard%E4%B8%8Esoft%E5%BD%A2%E5%BC%8F/" title="各种函数的hard与soft形式">各种函数的hard与soft形式</a>一文，<span class="math inline">\(\text{onehot}(\arg\max)\)</span> 的平滑近似是 <span class="math inline">\(\text{softmax}\)</span>，所以： <span class="math display">\[\gamma_{il}\approx\hat\gamma_{il}=\text{softmax}\left(-\|x_i-m_j\|^2;\tau\right)_l=\frac{e^{-\|x_i-m_l\|^2/\tau}}{\sum_{j=1}^ke^{-\|x_i-m_j\|^2/\tau}}\]</span> 并且 <span class="math inline">\(\hat\gamma_{il}\)</span> 可以解释为 <span class="math inline">\(x_i\)</span> 属于第 <span class="math inline">\(l\)</span> 类的概率。将其代回 <span class="math inline">\(\eqref{target}\)</span> 式就得到 soft 版本的优化目标： <span class="math display">\[\quad\sum_{l=1}^k\sum_{i=1}^n\hat\gamma_{il}\|x_i-m_l\|^2\tag{2}\label{target-soft}\]</span> 和 hard k-means 一样，我们用迭代的方式来优化 <span class="math inline">\(\eqref{target-soft}\)</span> 式：</p><ol type="1"><li><p>随机选择 <span class="math inline">\(k\)</span> 个样本作为中心 <span class="math inline">\((m_1,\ldots,m_k)\)</span>.</p></li><li><p>对给定的中心，计算样本属于各类的概率 <span class="math inline">\(\hat\gamma_{il}\)</span>： <span class="math display">\[\hat\gamma_{il}=\frac{e^{-\|x_i-m_l\|^2/\tau}}{\sum_{j=1}^ke^{-\|x_i-m_j\|^2/\tau}}\tag{3}\label{estep}\]</span></p></li><li><p>固定 <span class="math inline">\(\hat\gamma_{il}\)</span>，求各类最优中心，即： <span class="math display">\[\min_{m_1,\ldots,m_k}\quad\sum_{l=1}^k\sum_{i=1}^n\hat\gamma_{il}\|x_i-m_l\|^2\]</span> 求偏导并令为零，容易解得： <span class="math display">\[m_l=\frac{\sum_{i=1}^n\hat\gamma_{il}x_i}{\sum_{j=1}^n\hat\gamma_{jl}}\tag{4}\label{mstep}\]</span> 即对 <span class="math inline">\(x_i\)</span> 计算加权平均（weighted means）。</p></li><li><p>迭代执行 2、3 步直至收敛。</p></li></ol><h2 id="角度-2gmm-soft-k-means">角度 2：GMM → soft k-means</h2><p>熟悉 GMM 的朋友可能已经发现了，soft k-means 和 GMM 的形式非常相似。我们先回顾一下 GMM 的优化步骤（详见<a href="/blog-main/2022/08/23/EM%E7%AE%97%E6%B3%95/" title="EM算法">EM算法</a>）：</p><ol type="1"><li><p>随机初始化模型参数 <span class="math inline">\(\alpha_k,\mu_k,\Sigma_k,\,k=1,\ldots,K\)</span></p></li><li><p><strong>E-step</strong>：计算隐变量的概率分布： <span class="math display">\[p_{ik}=\frac{\alpha_k^{(t)}\phi(x_i\mid \mu_k^{(t)},\Sigma_k^{(t)})}{\sum_{j=1}^K\alpha_j^{(t)}\phi(x_i\mid \mu_j^{(t)},\Sigma_j^{(t)})}\]</span> 其中 <span class="math inline">\(\phi(x\mid \mu,\Sigma)\)</span> 是高斯分布的 pdf： <span class="math display">\[\phi(x\mid \mu,\Sigma)=\frac{1}{ {(\sqrt{2\pi})}^d|\Sigma|^{1/2} }\exp\left(-\frac{(x-\mu)^T\Sigma^{-1}(x-\mu)}{2}\right)\]</span></p></li><li><p><strong>M-step</strong>：计算新参数： <span class="math display">\[\begin{align}&amp;\alpha_k^{(t+1)}=\frac{\sum_{i=1}^np_{ik}}{n}&amp;&amp; k=1,\ldots,K\\&amp;\mu_k^{(t+1)}=\frac{\sum_{i=1}^np_{ik}x_i}{\sum_{i=1}^n p_{ik}}&amp;&amp; k=1,\ldots,K\\&amp;\Sigma_k^{(t+1)}=\frac{\sum_{i=1}^np_{ik}(x_i-\mu_k)^T(x_i-\mu_k)}{\sum_{i=1}^np_{ik}}&amp;&amp; k=1,\ldots,K\end{align}\]</span></p></li><li><p>迭代执行第 2、3 步</p></li></ol><p>如果我们取： <span class="math display">\[\begin{align}&amp;\alpha_1=\cdots=\alpha_K=\frac{1}{K}\\&amp;\Sigma_k=I\end{align}\]</span> 那么 E-step 将简化为： <span class="math display">\[p_{ik}=\frac{e^{-(x_i-\mu_k)^T(x_i-\mu_k)/2}}{\sum_{j=1}^Ke^{-(x_i-\mu_j)^T(x_i-\mu_j)/2}}\]</span> 这不就是 <span class="math inline">\(\tau=2\)</span> 的 <span class="math inline">\(\eqref{estep}\)</span> 式嘛！如果 <span class="math inline">\(\tau\)</span> 是其他数也没关系，给协方差矩阵乘一个倍数即可。</p><p>M-step 中只需要更新 <span class="math inline">\(\mu\)</span>： <span class="math display">\[\mu=\frac{\sum_{i=1}^np_{ik}x_i}{\sum_{i=1}^np_{ik}}\]</span> 与 <span class="math inline">\(\eqref{mstep}\)</span> 式完全一致！所以，<strong>soft k-means 是 GMM 在各高斯分布选择概率相同且协方差矩阵为单位矩阵（或单位矩阵的倍数）下的特殊情况</strong>。</p><p><br/></p><p>现在，让我们回过头来考虑 hard k-means。要从 soft 变回 hard，只需要让 <span class="math inline">\(\text{softmax}\)</span> 里的温度系数 <span class="math inline">\(\tau\)</span> 趋近于 <span class="math inline">\(0\)</span>。对应到 GMM 中，相当于让协方差矩阵 <span class="math inline">\(\Sigma\)</span> 趋近于 <span class="math inline">\(0\)</span>，即高斯分布趋近 Dirac delta 函数（分布）。因此，<strong>k-means 是 GMM 在各高斯分布趋近 Dirac delta 函数且选择概率相同下的特殊情况</strong>。</p><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Bauckhage, Christian. <em>Lecture notes on data science: Soft k-means clustering</em>. Technical Report, Univ. Bonn, DOI: https://doi.org/10.13140/RG. 2.1. 3582.6643, 2015. <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Hart, Peter E., David G. Stork, and Richard O. Duda. <em>Pattern classification</em>. Hoboken: Wiley, 2000. <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Bishop, Christopher M., and Nasser M. Nasrabadi. <em>Pattern recognition and machine learning</em>. Vol. 4, no. 4. New York: springer, 2006. <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EM算法</title>
    <link href="/blog-main/2022/08/23/EM%E7%AE%97%E6%B3%95/"/>
    <url>/blog-main/2022/08/23/EM%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p><strong>EM 算法是极大似然法的推广</strong>，用于解决存在<strong>隐变量（hidden variables / latent factors）</strong>的参数估计问题。</p><h2 id="em-算法">1 EM 算法</h2><h3 id="理论推导">1.1 理论推导</h3><blockquote><p>本节主要参考资料<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Borman, Sean. The expectation maximization algorithm-a short tutorial. https://www.lri.fr/~sebag/COURS/EM_algorithm.pdf">[1]</span></a></sup><sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="李航.统计学习方法">[2]</span></a></sup>，记号略有不同。</p></blockquote><p>设观测样本是 <span class="math inline">\(x\)</span>，隐变量为 <span class="math inline">\(z\)</span>，模型参数为 <span class="math inline">\(\theta\)</span>，那么对数似然为： <span class="math display">\[L(\theta)=\log P(x\vert \theta)=\log\left(\sum_{z} P(x,z\vert\theta)\right)\tag{1}\label{original}\]</span> <strong>由于隐变量的存在，我们无法直接写出 <span class="math inline">\(P(x\vert \theta)\)</span> 而必须表示为 <span class="math inline">\(P(x,z\vert \theta)\)</span> 求和（或积分）的形式</strong>，这导致 <span class="math inline">\(L(\theta)\)</span> 的导数非常复杂，阻碍我们用极大似然法求解（可能无法写出封闭形式的解）。</p><p>EM 算法采用迭代的方式求解，即先初始化一个 <span class="math inline">\(\theta^{(0)}\)</span>，然后不断迭代得到 <span class="math inline">\(\theta^{(1)},\theta^{(2)},\ldots\)</span>。假设当前的参数估计值为 <span class="math inline">\(\theta^{(t)}\)</span>，那么：</p><p><span class="math display">\[\begin{align}L(\theta)-L(\theta^{(t)})&amp;=\log\left(\sum_{z} P(x,z\vert\theta)\right)-\log P(x\vert \theta^{(t)})\\&amp;=\log\left(\sum_z P(x, z\vert\theta)\frac{P(z\vert x,\theta^{(t)})}{P(z\vert x,\theta^{(t)})}\right)-\log P(x\vert \theta^{(t)})\\&amp;\geq \sum_zP(z\vert x,\theta^{(t)})\log\frac{P(x, z\vert\theta)}{P(z\vert x,\theta^{(t)})}-\log P(x\vert \theta^{(t)})&amp;&amp;\text{Jensen&#39;s inequality}\\&amp;=\sum_z P(z\vert x,\theta^{(t)})\log\frac{P(x,z\vert\theta)}{P(z\vert x,\theta^{(t)})P(x\vert \theta^{(t)})}\\&amp;=\sum_z P(z\vert x,\theta^{(t)})\log\frac{P(x,z\vert\theta)}{P(x,z\vert\theta^{(t)})}\\\end{align}\tag{2}\label{jensen}\]</span> 为书写方便，记： <span class="math display">\[l(\theta\vert\theta^{(t)}) = L(\theta^{(t)})+\sum_z P(z\vert x,\theta^{(t)})\log \frac{P(x,z\vert\theta)}{P(x,z\vert\theta^{(t)})}\]</span> 则： <span class="math display">\[L(\theta)\geq l(\theta\vert \theta^{(t)})\]</span> 即 <span class="math inline">\(l(\theta\vert\theta^{(t)})\)</span> 是 <span class="math inline">\(L(\theta)\)</span> 的下界。又由于 <span class="math display">\[l(\theta^{(t)}\vert\theta^{(t)})=L(\theta^{(t)})+\sum_z P(z\vert x,\theta^{(t)})\log 1=L(\theta^{(t)})\]</span> 所以只要我们优化下界 <span class="math inline">\(l(\theta\vert\theta^{(t)})\)</span>，那么一定也优化了 <span class="math inline">\(L(\theta)\)</span>： <span class="math display">\[L(\theta^{(t+1)})\geq l(\theta^{(t+1)}\vert \theta^{(t)})\geq l(\theta^{(t)}\vert\theta^{(t)})=L(\theta^{(t)})\tag{3}\label{mono}\]</span> <img src="graph.png" width=70% /></p><p><span class="math inline">\(l(\theta\vert \theta^{(t)})\)</span> 包含一些与 <span class="math inline">\(\theta\)</span> 无关的常数项，丢掉它们即得到最终的优化目标： <span class="math display">\[Q(\theta,\theta^{(t)})=\sum_zP(z\vert x,\theta^{(t)})\log P(x,z\vert \theta) =\mathbb E_{z\vert x,\theta^{(t)}}\big[\log P(x,z\vert \theta)\big]\tag{4}\label{target}\]</span> 总而言之，经过一通操作我们得出结论：迭代地优化 <span class="math inline">\(\eqref{target}\)</span> 式可以达到优化 <span class="math inline">\(\eqref{original}\)</span> 式的目的。而所谓 EM，E-step 就是求出优化目标 <span class="math inline">\(Q(\theta,\theta^{(t)})\)</span>，M-step 就是优化它。</p><h3 id="算法步骤">1.2 算法步骤</h3><p>综上所述，EM 算法的过程如下：</p><ol type="1"><li><p>随机初始化 <span class="math inline">\(\theta^{(0)}\)</span>；</p></li><li><p><strong>E-step</strong>：给定 <span class="math inline">\(\theta^{(t)}\)</span>，求隐变量的概率分布： <span class="math display">\[P(z\vert x,\theta^{(t)})\]</span> 然后计算优化目标： <span class="math display">\[Q(\theta,\theta^{(t)})=\sum_zP(z\vert x,\theta^{(t)})\log P(x,z\vert \theta)\]</span></p></li><li><p><strong>M-step</strong>：优化上式： <span class="math display">\[\theta^{(t+1)}=\arg\max_\theta Q(\theta,\theta^{(t)})\]</span></p></li><li><p>迭代执行 2、3 步直至收敛。</p></li></ol><h3 id="收敛性">1.3 收敛性</h3><p>EM 算法一定收敛吗？这里的收敛其实包含两层意思：<span class="math inline">\(L(\theta)\)</span> 收敛和 <span class="math inline">\(\theta\)</span> 收敛，<strong>前者并不能蕴含后者</strong>。</p><ol type="1"><li><span class="math inline">\(L(\theta)\)</span> 收敛：上文中 <span class="math inline">\(\eqref{mono}\)</span> 式已经说明了 <span class="math inline">\(L(\theta)\)</span> 在优化过程中单调不减，又它显然有上界 <span class="math inline">\(0\)</span>，根据单调有界定理，<span class="math inline">\(L(\theta)\)</span> 必然收敛。</li><li><span class="math inline">\(\theta\)</span> 收敛：参考文献<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Wu, CF Jeff. On the convergence properties of the EM algorithm. *The Annals of statistics* (1983): 95-103.">[6]</span></a></sup>。</li></ol><p>注意，虽然 EM 算法一定收敛，<strong>但是可能收敛到局部最优解</strong>。实践中往往选取不同的初值多跑几次，选择最佳的结果。</p><h3 id="关于独立重复试验的注解">1.4 关于独立重复试验的注解</h3><p>在上面的推导中，我们把所有的观测结果直接记作 <span class="math inline">\(x\)</span>，这使得结论具有普适性。但实践中我们往往做的是独立重复试验，有 <span class="math inline">\(n\)</span> 个相互独立的样本：<span class="math inline">\(x=(x_1,\ldots,x_n)\)</span>，各样本对应各自的隐变量 <span class="math inline">\(z=(z_1,\ldots,z_n)\)</span>.</p><p>基于独立性，有： <span class="math display">\[P(z\vert x,\theta)=\prod_{i=1}^nP(z_i\vert x_i,\theta)\quad\quad P(x,z\vert\theta)=\prod_{i=1}^nP(x_i,z_i\vert \theta)\]</span> 如果直接把它们代入 <span class="math inline">\(\eqref{target}\)</span> 式的优化目标之中，会得到非常丑陋的结果： <span class="math display">\[\begin{align}Q(\theta,\theta^{(t)})&amp;=\sum_zP(z\vert x,\theta^{(t)})\log P(x,z\vert \theta)\\&amp;=\sum_{z_1,\ldots,z_n}\left[\left(\prod_{i=1}^nP(z_i\vert x_i,\theta)\right)\log\left(\prod_{j=1}^nP(x_j,z_j\vert \theta)\right)\right]\\&amp;=\sum_{z_1,\ldots,z_n}\left[\left(\prod_{i=1}^nP(z_i\vert x_i,\theta)\right)\sum_{j=1}^n\log\left(P(x_j,z_j\vert \theta)\right)\right]\\\end{align}\]</span> 这是因为我们代入得太晚——在 Jensen's inequality 把对数和求和的顺序交换后，再代入就很难化简式子了（当然如果你头够铁，也不是不能化简，见参考资料<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="高斯混合模型（GMM）推导及实现 - 永远在你身后的文章 - 知乎 https://zhuanlan.zhihu.com/p/85338773">[8]</span></a></sup><sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><span class="hint--top hint--rounded" aria-label="【机器学习-白板推导系列(十一)-高斯混合模型GMM（Gaussian Mixture Model）】 https://www.bilibili.com/video/BV13b411w7Xj?p=3&amp;share_source=copy_web&amp;vd_source=a43b4442e295a96065c7ae919b4866d3">[9]</span></a></sup>）。如果我们在 <span class="math inline">\(\eqref{original}\)</span> 式就代入独立性： <span class="math display">\[\begin{align}L(\theta)=\log P(x\vert\theta)&amp;=\log\prod_{i=1}^nP(x_i\vert\theta)\\&amp;=\sum_{i=1}^n\log P(x_i\vert\theta)\\&amp;=\sum_{i=1}^n\log\left(\sum_{z_i}P(x_i,z_i\vert\theta)\right)\end{align}\tag{1&#39;}\]</span> 继续推导，会发现只需要在相应地方加上求和 <span class="math inline">\(\sum_{i=1}^n\)</span>、给 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(z\)</span> 加上下标 <span class="math inline">\(i\)</span> 即可，整个推导过程没有什么大的变化。最终优化目标写作： <span class="math display">\[Q(\theta,\theta^{(t)})=\sum_{i=1}^n\sum_{z_i}P(z_i\vert x_i,\theta^{(t)})\log P(x_i,z_i\vert \theta)\tag{4&#39;}\label{target2}\]</span> <strong><span class="math inline">\(\eqref{target2}\)</span> 式应该是实践中最常用的优化目标形式</strong>。</p><h3 id="其他推导路线-1">1.5 其他推导路线 1</h3><p>参考资料<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="人人都懂EM算法 - August的文章 - 知乎 https://zhuanlan.zhihu.com/p/36331115">[3]</span></a></sup><sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="【机器学习】EM——期望最大（非常详细） - 阿泽的文章 - 知乎 https://zhuanlan.zhihu.com/p/78311644">[4]</span></a></sup><sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Tengyu Ma and Andrew Ng. CS229 Lecture notes: The EM algorithm. https://cs229.stanford.edu/notes2021fall/cs229-notes8.pdf">[5]</span></a></sup>做了看起来和第一小节不太一样，但本质上差不多的推导，且<strong>个人认为这个推导思路更具有启发意义</strong>。对隐变量 <span class="math inline">\(z\)</span>，引入其概率分布 <span class="math inline">\(q(z)\)</span>，则： <span class="math display">\[\begin{align}L(\theta)=\log P(x\vert \theta)&amp;=\log\left(\sum_{z} P(x,z\vert\theta)\right)\\&amp;=\log\left(\sum_z q(z)\frac{P(x,z\vert \theta)}{q(z)}\right)\\&amp;\geq\sum_zq(z)\log\frac{P(x,z\vert \theta)}{q(z)}&amp;&amp;\text{Jensen&#39;s inequality}\\&amp;=J(\theta,q)\end{align}\]</span> 这说明我们找到了“一族” <span class="math inline">\(L(\theta)\)</span> 的下界 <span class="math inline">\(J(\theta,q)\)</span>，称为<strong>证据下界 ELBO (Evidence Lower BOund)</strong>。“一族”的意思是通过改变 <span class="math inline">\(q\)</span>，我们能够得到不同的下界函数。根据 Jensen's inequality 的取等条件，容易推出当 <span class="math inline">\(q(z)=P(z\vert x,\theta)\)</span> 时等号成立。事实上，如果把 <span class="math inline">\(q^\ast(z)=P(z\vert x,\theta)\)</span> 代入上式，我们确实得到了一个等式： <span class="math display">\[J(\theta,q^\ast)=\sum_z P(z\vert x,\theta)\log \frac{P(x,z\vert \theta)}{P(z\vert x,\theta)}=\sum_zP(z\vert x,\theta)\log P(x\vert \theta)=\log P(x\vert \theta)=L(\theta)\]</span> 现在我们把迭代过程加进来。假设当前模型参数为 <span class="math inline">\(\theta^{(t)}\)</span>，那么我们可以取 <span class="math inline">\(q^\ast(z)=P(z\vert x,\theta^{(t)})\)</span>，使得 <span class="math inline">\(J(\theta^{(t)},q^\ast)=L(\theta^{(t)})\)</span>，即取一个特定的下界函数让它在 <span class="math inline">\(\theta^{(t)}\)</span> 处与 <span class="math inline">\(L(\theta^{(t)})\)</span> 相等，这其实就是 E-step；然后我们固定 <span class="math inline">\(q=q^\ast\)</span> 不变，优化 <span class="math inline">\(J(\theta,q^\ast)\)</span>，将模型参数更新为 <span class="math inline">\(\theta^{(t+1)}\)</span>，那么 <span class="math display">\[L(\theta^{(t+1)})\geq J(\theta^{(t+1)},q^\ast)\geq J(\theta^{(t)},q^\ast)=L(\theta^{(t)})\]</span> 即 <span class="math inline">\(L(\theta)\)</span> 也必然得到优化，这就是 M-step. 简而言之，EM 算法通过先优化 <span class="math inline">\(q\)</span>，再优化 <span class="math inline">\(\theta\)</span> 的迭代过程优化对数似然 <span class="math inline">\(L(\theta)\)</span>.</p><p><img src="img.jpg" width=50% /></p><h3 id="其他推导路线-2">1.6 其他推导路线 2</h3><p>在 1.5 节中，我们知道 <span class="math inline">\(L(\theta)\geq J(\theta,q)\)</span>，那他俩之间究竟差了个什么呢？ <span class="math display">\[\begin{align}L(\theta)-J(\theta,q)&amp;=\log P(x\vert \theta)-\sum_zq(z)\log\frac{P(x,z\vert\theta)}{q(z)}\\&amp;=\sum_zq(z)\left[\log P(x\vert \theta)-\log\frac{P(x,z\vert \theta)}{q(z)}\right]\\&amp;=\sum_z q(z)\log \frac{q(z)}{P(z\vert x,\theta)}\\&amp;=\mathrm{KL}(q(z)\|P(z\vert x,\theta))\end{align}\]</span> 啊！原来是引入的分布 <span class="math inline">\(q(z)\)</span> 与 <span class="math inline">\(P(z\vert x,\theta)\)</span> 之间的 KL 散度。之前推导过程中的 Jensen's inequality，其实对应着这里的 KL 散度非负。</p><p>事实上，存在其他的推导路线能直接写出 <span class="math inline">\(L(\theta)=J(\theta,q)+\mathrm{KL}(q(z)\|P(z\vert x,\theta))\)</span> 这个等式而避开 Jensen's inequality 的，如下所示： <span class="math display">\[\begin{align}L(\theta)&amp;=\log P(x\vert \theta)\\&amp;=\sum_z q(z)\log P(x\vert \theta)\\&amp;=\sum_z q(z)\log\left(\frac{P(x,z\vert \theta)}{P(z\vert x,\theta)}\cdot\frac{q(z)}{q(z)} \right)\\&amp;=\sum_z q(z)\left[\log\frac{P(x,z\vert \theta)}{q(z)}+\log\frac{q(z)}{P(z\vert x,\theta)} \right]\\&amp;=J(\theta,q)+\mathrm{KL}(q(z)\|P(z\vert x,\theta))\end{align}\]</span> 从等式的角度看，<span class="math inline">\(L(\theta)\)</span> <strong>被划分成了两部分——ELBO 和 KL 散度</strong>. 在 E-step 中，我们让 <span class="math inline">\(\mathrm{KL}=0\)</span>，也就是让 <span class="math inline">\(L(\theta)=J(\theta,q)\)</span>；在 M-step 中，固定 <span class="math inline">\(q\)</span> 优化 <span class="math inline">\(J(\theta,q)\)</span>，进而优化了 <span class="math inline">\(L(\theta)\)</span>；这时由于 <span class="math inline">\(\theta\)</span> 变了，KL 散度不再是零，我们便继续下一轮迭代。</p><p><img src="img2.jpg" width=80% /></p><h2 id="例子双硬币问题">2 例子：双硬币问题</h2><blockquote><p>本节主要参考资料：<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="Do, Chuong B., and Serafim Batzoglou. What is the expectation maximization algorithm?. *Nature biotechnology* 26, no. 8 (2008): 897-899.">[7]</span></a></sup><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="人人都懂EM算法 - August的文章 - 知乎 https://zhuanlan.zhihu.com/p/36331115">[3]</span></a></sup>。</p></blockquote><p>假设我们有两枚硬币 A,B，它们正面朝上的概率分别是 <span class="math inline">\(\theta_A,\theta_B\)</span>. 我们希望估计参数 <span class="math inline">\(\theta=(\theta_A,\theta_B)\)</span>，并做了 5 次如下试验：首先<strong>等概率</strong>地随机选取一枚硬币，然后掷选出的硬币 10 次。因此我们一共获得了 50 个结果。</p><p>如果第 <span class="math inline">\(i\)</span> 次试验中我们记录了以下变量：</p><ul><li><span class="math inline">\(z_i\in\{A,B\}\)</span>，表示选择的硬币；</li><li><span class="math inline">\(x_i\in\{0,1,\ldots,10\}\)</span>，表示正面朝上的次数</li></ul><p>那么估计 <span class="math inline">\(\theta_A,\theta_B\)</span> 将非常简单： <span class="math display">\[\begin{align}&amp;\theta_A=\frac{\text{\# of heads using coin A}}{\text{total \# of flips using coin A}}\\&amp;\theta_B=\frac{\text{\# of heads using coin B}}{\text{total \# of flips using coin B}}\end{align}\]</span> 注意，虽然这个结果在直观上小学生都能理解，但本质上它是极大似然估计。</p><p><img src="coina.png" width=60% /></p><p>现在，考虑一个更有挑战性的问题——我们并不知道每一次试验选的是哪枚硬币，只知道投掷结果，即 <span class="math inline">\(z_i\in\{A,B\}\)</span> 变成了隐变量。应用 EM 算法：</p><ol type="1"><li><p>随机初始化 <span class="math inline">\(\theta^{(0)}=(\theta_A^{(0)},\theta_B^{(0)})=(0.60,0.50)\)</span>；</p></li><li><p>【<strong>E-step</strong>】求隐变量的概率分布 <span class="math inline">\(P(z_i\vert x_i,\theta^{(0)})\)</span>，以第一次试验为例： <span class="math display">\[\begin{align}&amp;P(z_1=A\vert x_1,\theta^{(0)})=\frac{P(z_1=A,x_1\vert\theta^{(0)})}{P(x_1\vert \theta^{(0)})}=\frac{(0.6)^5\times (0.4)^5}{(0.6)^5\times (0.4)^5+(0.5)^{10}}=0.45\\&amp;P(z_1=B\vert x_1,\theta^{(0)})=1-P(z_1=A\vert x_1,\theta^{(0)})=0.55\end{align}\]</span> &gt; 为了书写方便，以下将 <span class="math inline">\(P(z_i=A\vert x_i,\theta^{(0)})\)</span> 和 <span class="math inline">\(P(z_i=B\vert x_i,\theta^{(0)})\)</span> 分别简记为 <span class="math inline">\(p_i\)</span> 和 <span class="math inline">\((1-p_i)\)</span>.</p><p>计算优化目标 <span class="math inline">\(\eqref{target2}\)</span>： <span class="math display">\[\begin{align}Q(\theta,\theta^{(0)})&amp;=\sum_{i=1}^5\sum_{z_i\in\{A,B\}}P(z_i\vert x_i,\theta^{(0)})\log P(x_i,z_i\vert \theta)\\&amp;=\sum_{i=1}^5P(z_i=A\vert x_i,\theta^{(0)})\log P(x_i,z_i=A\vert \theta)+P(z_i=B\vert x_i,\theta^{(0)})\log P(x_i,z_i=B\vert \theta)\\&amp;=\sum_{i=1}^5 p_{i}\log\left[\theta_A^{x_i}(1-\theta_A)^{10-x_i}\right]+(1-p_i)\log\left[\theta_B^{x_i}(1-\theta_B)^{10-x_i}\right]\end{align}\]</span></p></li><li><p>【<strong>M-step</strong>】优化上述目标，求偏导并令为零： <span class="math display">\[\frac{\partial Q}{\partial \theta_A}=\sum_{i=1}^5 p_i\left(\frac{x_i}{\theta_A}-\frac{10-x_i}{1-\theta_A}\right)=\sum_{i=1}^5 p_i\frac{x_i-10\theta_A}{\theta_A(1-\theta_A)}=0\]</span> 解得： <span class="math display">\[\theta_A=\frac{\sum_{i=1}^5 p_ix_i}{10\sum_{i=1}^5 p_i}=\frac{2.25+7.2+5.84+1.4+4.55}{10\times(0.45+0.80+0.73+0.35+0.65)}\approx0.71\]</span> <span class="math inline">\(\theta_B\)</span> 同理求解。</p><blockquote><p>这是标准的解法，对于这个问题有更为简单的解法，见下图。</p></blockquote></li><li><p>迭代执行 2、3 步直至收敛。</p></li></ol><p><img src="coinb.png" width=60% /></p><h2 id="应用高斯混合模型gmm">3 应用：高斯混合模型（GMM）</h2><blockquote><p>本节参考资料：<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="高斯混合模型（GMM）推导及实现 - 永远在你身后的文章 - 知乎 https://zhuanlan.zhihu.com/p/85338773">[8]</span></a></sup>。</p></blockquote><p>设有 <span class="math inline">\(K\)</span> 个 <span class="math inline">\(d\)</span> 维高斯分布： <span class="math display">\[\phi(x\vert \mu_k,\Sigma_k)=\frac{1}{ {(\sqrt{2\pi})}^d|\Sigma_k|^{1/2} }\exp\left(-\frac{(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)}{2}\right),\quad k=1,\ldots,K\]</span> 样本 <span class="math inline">\(\{x_1,\ldots,x_n:x_i\in\mathbb R^d\}\)</span> 是由如下过程产生的：首先随机选取一个高斯分布（第 <span class="math inline">\(k\)</span> 个高斯分布被选中的概率是 <span class="math inline">\(\alpha_k\)</span>，其中 <span class="math inline">\(\sum_{k=1}^K\alpha_k=1\)</span>），再从被选中的高斯分布中采样。试估计这 <span class="math inline">\(K\)</span> 个高斯分布的参数 <span class="math inline">\(\theta=\{(\alpha_k,\mu_k,\Sigma_k)\vert k=1,\ldots,K\}\)</span>.</p><p>和双硬币问题非常相似，这个问题中我们不知道每次抽样究竟选取的是哪个高斯分布，所以可以定义隐变量 <span class="math inline">\(z_i\in\{1,\ldots,K\}\)</span> 表示第 <span class="math inline">\(i\)</span> 个样本来自哪个高斯分布。使用 EM 算法求解：</p><ul><li><p>【<strong>E-step</strong>】隐变量的概率分布为： <span class="math display">\[P(z_i=k\vert x_i,\theta^{(t)})=\frac{P(x_i,z_i=k\vert \theta^{(t)})}{P(x_i\vert\theta^{(t)})}=\frac{\alpha_k^{(t)}\phi(x_i\vert \mu_k^{(t)},\Sigma_k^{(t)})}{\sum_{j=1}^K\alpha_j^{(t)}\phi(x_i\vert \mu_j^{(t)},\Sigma_j^{(t)})}\]</span></p><p>为书写方便，以下将其简记为 <span class="math inline">\(p_{ik}\)</span>，注意这是一个常数。计算优化目标 <span class="math inline">\(\eqref{target2}\)</span>： <span class="math display">\[\begin{align}Q(\theta,\theta^{(t)})&amp;=\sum_{i=1}^n\sum_{k=1}^KP(z_i=k\vert x_i,\theta^{(t)})\log P(x_i,z_i=k\vert \theta)\\&amp;=\sum_{i=1}^n\sum_{k=1}^Kp_{ik}\log\left(\alpha_k\phi(x_i\vert \mu_k,\Sigma_k)\right)\\&amp;=\sum_{i=1}^n\sum_{k=1}^Kp_{ik}\left[\log\alpha_k+\log\phi(x_i\vert \mu_k,\Sigma_k)\right]\end{align}\]</span></p></li><li><p>【<strong>M-step</strong>】（大计算量警告，不想看可略过）</p><ul><li><p>首先解 <span class="math inline">\(\alpha\)</span>：结合条件 <span class="math inline">\(\sum_{k=1}^K\alpha_k=1\)</span>，去掉与 <span class="math inline">\(\alpha\)</span> 无关的部分，优化问题为： <span class="math display">\[\begin{align}&amp;\max_\theta \sum_{i=1}^n\sum_{k=1}^Kp_{ik}\log\alpha_k\\&amp;\text{ s.t. }\sum_{k=1}^K\alpha_k=1\end{align}\]</span> 引入 Lagrange 乘子： <span class="math display">\[L(\theta)=\sum_{i=1}^n\sum_{k=1}^Kp_{ik}\log\alpha_k+\lambda\left(\sum_{k=1}^K\alpha_k-1\right)\]</span> 求偏导并令为零： <span class="math display">\[\begin{align}&amp;\frac{\partial L}{\partial \alpha_k}=\sum_{i=1}^n\frac{p_{ik}}{\alpha_k}+\lambda=0&amp;&amp;k=1,\ldots,K\\&amp;\frac{\partial L}{\partial \lambda}=\sum_{k=1}^K\alpha_k-1=0\end{align}\]</span> 解得： <span class="math display">\[\alpha_k=\frac{\sum_{i=1}^np_{ik}}{\sum_{i=1}^n\sum_{k=1}^Kp_{ik}}=\frac{\sum_{i=1}^np_{ik}}{n}\quad\quad k=1,\ldots,K\]</span></p></li><li><p>然后解 <span class="math inline">\(\mu\)</span>：去掉与 <span class="math inline">\(\mu\)</span> 无关的部分，优化目标为： <span class="math display">\[\begin{align}Q(\theta,\theta^{(t)})&amp;\to\sum_{i=1}^n\sum_{k=1}^Kp_{ik}\log\phi(x_i\vert\mu_i,\Sigma_i)\\&amp;=\sum_{i=1}^n\sum_{k=1}^K p_{ik}\left(\log\frac{1}{ {(\sqrt{2\pi})}^d|\Sigma_k|^{1/2} }-\frac{(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)}{2}\right)\\&amp;\to\sum_{i=1}^n\sum_{k=1}^K p_{ik}\left(\frac{(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)}{2}\right)=Q&#39;(\theta,\theta^{(t)})\end{align}\]</span> 求导令为零： <span class="math display">\[\begin{align}\frac{\partial Q&#39;}{\partial \mu_k}&amp;=\sum_{i=1}^n\frac{1}{2}p_{ik}\frac{\partial (x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)}{\partial \mu_k}\\&amp;=\sum_{i=1}^np_{ik}\Sigma_k^{-1}(x_i-\mu_k)\\&amp;=\Sigma_k^{-1}\sum_{i=1}^np_{ik}(x_i-\mu_k)=\vec 0\end{align}\]</span></p><blockquote><p>注：第二个等号应用了矩阵求导结论 <span class="math inline">\(\mathrm d(x^TAx)/\mathrm dx=(A+A^T)x\)</span> 和协方差矩阵对称的特性。</p></blockquote><p>解得： <span class="math display">\[\mu_k=\frac{\sum_{i=1}^np_{ik}x_i}{\sum_{i=1}^n p_{ik}}\quad\quad k=1,\ldots,K\]</span></p></li><li><p>最后解 <span class="math inline">\(\Sigma\)</span>：去掉与 <span class="math inline">\(\Sigma\)</span> 无关的部分，优化目标为： <span class="math display">\[\begin{align}Q(\theta,\theta^{(t)})&amp;\to\sum_{i=1}^n\sum_{k=1}^Kp_{ik}\log\phi(x_i\vert\mu_i,\Sigma_i)\\&amp;=\sum_{i=1}^n\sum_{k=1}^K p_{ik}\left(\log\frac{1}{ {(\sqrt{2\pi})}^d}-\frac{1}{2}\log|\Sigma_k|-\frac{(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)}{2}\right)\\&amp;\to\sum_{i=1}^n\sum_{k=1}^K p_{ik}\left(\log|\Sigma_k|+(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right)=Q&#39;&#39;(\theta,\theta^{(t)})\end{align}\]</span> 求导令为零： <span class="math display">\[\begin{align}\frac{\partial Q&#39;&#39;}{\partial \Sigma_k}&amp;=\sum_{i=1}^np_{ik}\left(\Sigma_k^{-1}-(x_i-\mu_k)(x_i-\mu_k)^T\Sigma_{k}^{-2} \right)=\mathbf 0\end{align}\]</span> 解得： <span class="math display">\[\Sigma_k=\frac{\sum_{i=1}^np_{ik}(x_i-\mu_k)(x_i-\mu_k)^T}{\sum_{i=1}^np_{ik}}\quad\quad k=1,\ldots,K\]</span></p></li></ul></li></ul><p><br/></p><p>总结一下，用 EM 算法解 GMM 模型的步骤如下：</p><ol type="1"><li><p>随机初始化模型参数 <span class="math inline">\(\alpha_k,\mu_k,\Sigma_k,\,k=1,\ldots,K\)</span></p></li><li><p><strong>E-step</strong>：计算隐变量的概率分布： <span class="math display">\[p_{ik}=\frac{\alpha_k^{(t)}\phi(x_i\vert \mu_k^{(t)},\Sigma_k^{(t)})}{\sum_{j=1}^K\alpha_j^{(t)}\phi(x_i\vert \mu_j^{(t)},\Sigma_j^{(t)})}\]</span></p></li><li><p><strong>M-step</strong>：计算新参数： <span class="math display">\[\begin{align}&amp;\alpha_k^{(t+1)}=\frac{\sum_{i=1}^np_{ik}}{n}&amp;&amp; k=1,\ldots,K\\&amp;\mu_k^{(t+1)}=\frac{\sum_{i=1}^np_{ik}x_i}{\sum_{i=1}^n p_{ik}}&amp;&amp; k=1,\ldots,K\\&amp;\Sigma_k^{(t+1)}=\frac{\sum_{i=1}^np_{ik}(x_i-\mu_k)(x_i-\mu_k)^T}{\sum_{i=1}^np_{ik}}&amp;&amp; k=1,\ldots,K\end{align}\]</span></p></li><li><p>迭代执行第 2、3 步</p></li></ol><h2 id="一点注记">4 一点注记</h2><p>EM 算法在 E-step 要求我们能表达出后验概率 <span class="math inline">\(P(z\vert x,\theta)\)</span>，从上面两个例子（双硬币问题、GMM）可以看出，它往往是通过贝叶斯公式计算的： <span class="math display">\[P(z\vert x,\theta)=\frac{P(x,z\vert \theta)}{P(x\vert \theta)}=\frac{P(x,z\vert \theta)}{\sum_\hat z P(x,\hat z\vert \theta)}\]</span> 所以说我们终究没有避开计算似然 <span class="math inline">\(P(x\vert \theta)\)</span>，<strong>因此要明晰的一点是，阻碍我们用极大似然法优化 <span class="math inline">\(\eqref{original}\)</span> 式的不是计算似然本身，而是解令对数似然偏导为零后得到的那个方程组</strong>。</p><p>这自然带来了一个问题，如果似然本身就是 intractable 的，那么我们也无法计算后验概率 <span class="math inline">\(P(z\vert x,\theta)\)</span>，那么 EM 算法就做不下去了，典型的例子就是 VAE。至于 VAE 怎么解决这个问题的，留给后文吧。</p><h2 id="参考资料">参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Borman, Sean. The expectation maximization algorithm-a short tutorial. https://www.lri.fr/~sebag/COURS/EM_algorithm.pdf <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>李航.统计学习方法 <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>人人都懂EM算法 - August的文章 - 知乎 https://zhuanlan.zhihu.com/p/36331115 <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>【机器学习】EM——期望最大（非常详细） - 阿泽的文章 - 知乎 https://zhuanlan.zhihu.com/p/78311644 <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Tengyu Ma and Andrew Ng. CS229 Lecture notes: The EM algorithm. https://cs229.stanford.edu/notes2021fall/cs229-notes8.pdf <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Wu, CF Jeff. On the convergence properties of the EM algorithm. <em>The Annals of statistics</em> (1983): 95-103. <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Do, Chuong B., and Serafim Batzoglou. What is the expectation maximization algorithm?. <em>Nature biotechnology</em> 26, no. 8 (2008): 897-899. <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>高斯混合模型（GMM）推导及实现 - 永远在你身后的文章 - 知乎 https://zhuanlan.zhihu.com/p/85338773 <a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>【机器学习-白板推导系列(十一)-高斯混合模型GMM（Gaussian Mixture Model）】 https://www.bilibili.com/video/BV13b411w7Xj?p=3&amp;share_source=copy_web&amp;vd_source=a43b4442e295a96065c7ae919b4866d3 <a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch单机多卡从入门到入土（坑点记录）</title>
    <link href="/blog-main/2022/08/18/PyTorch%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F%EF%BC%88%E5%9D%91%E7%82%B9%E8%AE%B0%E5%BD%95%EF%BC%89/"/>
    <url>/blog-main/2022/08/18/PyTorch%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F%EF%BC%88%E5%9D%91%E7%82%B9%E8%AE%B0%E5%BD%95%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>在 Vision 里用上 Transformer 之后，单卡训练连两位数的 batchsize 都开不了，必须得学学单机多卡的使用了。</p><p>PyTorch 中，多卡训练有两种方案：</p><ul><li><a href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html"><code>DataParallel</code></a>：只支持单机多卡，代码很方便，只需要添加一行，但是效率比较低，不推荐使用</li><li><a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html"><code>DistributedDataParallel</code></a>：支持多机多卡，效率高，但是要折腾一下代码</li></ul><p>基于性能考虑，一般我们会选择第二种 DDP. 其基本概念与流程请参阅网上的相关资料（见<a href="#参考资料">参考资料</a>），此不赘述，这里只记录一些要点和坑点。</p><h2 id="坑点记录">坑点记录</h2><ol type="1"><li><p>保存模型 checkpoint、记录 <code>tensorboard</code>、输出准确率、甚至是 <code>tqdm</code> 都只让主进程（rank0）执行，其他进程不执行。</p></li><li><p>在用 <code>DDP</code> 封装模型后，模型的本体（我们定义的那个类）是 <code>model.module</code>；所以保存模型时，最好保存 <code>model.module.state_dict()</code>，否则存下来的参数的 key 前面会多一个 <code>module.</code>，不便再次 load 模型。</p></li><li><p>我们经常会在训练的每个 epoch 后进行 evaluate / inference，为避免有些进程测试完之后开始了下一轮训练，但其他进程还在测试，最好在每个 epoch 开始训练前（或者每个 epoch 完成训练后）用 <code>dist.barrier()</code> 同步一下。</p></li><li><p>单卡 evaluate / inference 用的模型最好是本地模型 <code>model.module</code> 而非 <code>DDP</code> 包装的模型，否则非主进程会在第 3 条的 <code>dist.barrier()</code> 处卡死，推测原因是 <code>DDP</code> 包装的模型会在一些地方同步 bucket<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://github.com/pytorch/pytorch/issues/54059">[8]</span></a></sup><sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://discuss.pytorch.org/t/torch-distributed-barrier-hangs-in-ddp/114522">[9]</span></a></sup>。</p></li><li><p>多卡训练时报错：</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">RuntimeError: <span class="hljs-literal">one</span> <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> variables needed <span class="hljs-keyword">for</span> gradient computation has been modified <span class="hljs-keyword">by</span> <span class="hljs-keyword">an</span> inplace operation: [torch.cuda.FloatTensor [<span class="hljs-number">1</span>]] is <span class="hljs-keyword">at</span> <span class="hljs-built_in">version</span> <span class="hljs-number">3</span>; expected <span class="hljs-built_in">version</span> <span class="hljs-number">2</span> instead. Hint: enable anomaly detection <span class="hljs-built_in">to</span> find <span class="hljs-keyword">the</span> operation that failed <span class="hljs-built_in">to</span> compute its gradient, <span class="hljs-keyword">with</span> torch.autograd.set_detect_anomaly(True).<br></code></pre></td></tr></table></figure><p>但单卡训练没有问题。这是因为对同一个模型连续 forward 了多次而没有 backward<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="【PyTorch踩坑】一个排查了一下午的坑 - Tramac的文章 - 知乎 https://zhuanlan.zhihu.com/p/409117481">[6]</span></a></sup>（比如在 GAN 的训练中，要分别将真实图像和生成图像通过判别器），DDP 会在每次 forward 中广播主进程模型的 buffers，但这个广播操作是 inplace 的<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://github.com/pytorch/pytorch/issues/22095">[7]</span></a></sup>，从而引起上述报错。</p><p>解决办法：在 <code>DDP</code> 定义时指定参数 <code>broadcast_buffers=False</code>，取消广播。</p></li><li><p>【尚未遇到过】对于依赖于模型初始 parameter 的一些 optimizer，要将其定义在 <code>model=DDP(model)</code> 之后，来保证各进程 optimizer 的初始状态是一致的。这是因为 optimizer 本身是和 DDP 无关的，DDP 不会同步 optimizer 的状态<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="\[原创\]\[深度\]\[PyTorch\] DDP系列第一篇：入门教程 - 996黄金一代的文章 - 知乎 https://zhuanlan.zhihu.com/p/178402798">[1]</span></a></sup>。当然，一般我们用的 optimizer 不会依赖于模型的 parameter，可以不管这一点。</p></li><li><p>多卡训练时在 <code>backward()</code> 处卡死，没有报错，单卡训练正常：</p><p>训练有逻辑分支，其中一条分支中部分模型参数不会被更新；如果没有设置 <code>find_unused_parameters=True</code>，那么进入这条分支后反向传播卡住。</p><p>解决办法：在 <code>DDP</code> 定义时指定参数 <code>find_unused_parameters=True</code>.</p></li></ol><h2 id="何时会同步">何时会同步？</h2><ol type="1"><li>在前向传播前，各进程同步模型的 parameter 和 buffer；</li><li>反向传播时，各进程同步模型的梯度；</li><li>调用 <code>all_reduce()</code> 时；</li><li>调用 <code>dist.barrier()</code> 时；</li><li>调用 <code>torch.cuda.synchronize()</code> 时【尚不清楚 <code>torch.cuda.synchronize()</code> 和 <code>dist.barrier()</code> 的区别】。</li></ol><h2 id="参考资料">参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>[原创][深度][PyTorch] DDP系列第一篇：入门教程 - 996黄金一代的文章 - 知乎 https://zhuanlan.zhihu.com/p/178402798 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>[原创][深度][PyTorch] DDP系列第二篇：实现原理与源代码解析 - 996黄金一代的文章 - 知乎 https://zhuanlan.zhihu.com/p/187610959 <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>[原创][深度][PyTorch] DDP系列第三篇：实战与技巧 - 996黄金一代的文章 - 知乎 https://zhuanlan.zhihu.com/p/250471767 <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>【分布式训练】单机多卡的正确打开方式（三）：PyTorch - Nicolas的文章 - 知乎 https://zhuanlan.zhihu.com/p/74792767 <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Pytorch 分布式训练 - 会飞的闲鱼的文章 - 知乎 https://zhuanlan.zhihu.com/p/76638962 <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>【PyTorch踩坑】一个排查了一下午的坑 - Tramac的文章 - 知乎 https://zhuanlan.zhihu.com/p/409117481 <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>https://github.com/pytorch/pytorch/issues/22095 <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>https://github.com/pytorch/pytorch/issues/54059 <a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>https://discuss.pytorch.org/t/torch-distributed-barrier-hangs-in-ddp/114522 <a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>k-means探究</title>
    <link href="/blog-main/2022/08/12/k-means%E6%8E%A2%E7%A9%B6/"/>
    <url>/blog-main/2022/08/12/k-means%E6%8E%A2%E7%A9%B6/</url>
    
    <content type="html"><![CDATA[<p>谈到聚类（clustering），k-means 无疑是最先想到的算法之一了。其思想异常的简单有效，以至于我之前没有深究过其中的奥秘与坑点。今天就来更深入地探究一下 k-means。</p><h2 id="算法描述">1 算法描述</h2><blockquote><p>本节主要参考资料<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="李航.统计学习方法">[1]</span></a></sup>.</p></blockquote><p>设我们有 <span class="math inline">\(n\)</span> 个样本 <span class="math inline">\(X=\{x_1,\ldots,x_n\}\)</span>，每个样本有 <span class="math inline">\(d\)</span> 维，即 <span class="math inline">\(x_i\in \mathbb R^d\)</span>。k-means 欲将样本分到 <span class="math inline">\(k\)</span> 个类 <span class="math inline">\(G_1,\ldots,G_k\)</span> 中，且这些类构成对样本集 <span class="math inline">\(X\)</span> 的一个划分，一个划分就是一个聚类结果。用 <span class="math inline">\(C\)</span> 表示划分，那么划分其实是一个多对一的函数，记 <span class="math inline">\(l=C(i)\)</span> 表示将样本 <span class="math inline">\(x_i\)</span> 映射到类 <span class="math inline">\(l\)</span> 中.</p><p>具体而言，k-means 采用欧式距离的平方作为度量： <span class="math display">\[\text{dist}(x,y)=\|x-y\|^2=\sum_{j=1}^d (x_j-y_j)^2\]</span> 定义损失函数为样本与所属类中心的距离之和： <span class="math display">\[\mathcal L(C)=\sum_{l=1}^k\sum_{C(i)=l}\|x_i-\bar x_l\|^2\]</span> 可惜上式的优化是一个组合优化问题，直接求解是 NP-hard 的，因此我们采用<strong>迭代</strong>的方式求解：</p><ol type="1"><li><p>随机选择 <span class="math inline">\(k\)</span> 个样本作为中心 <span class="math inline">\((m_1,\ldots,m_k)\)</span>.</p></li><li><p>对给定的中心，求最优划分，即： <span class="math display">\[\min_C \sum_{l=1}^k\sum_{C(i)=l}\|x_i-m_l\|^2\]</span> 显然，最优划分是将每个样本划分给距离它最近的那个中心。</p></li><li><p>对于给定划分，求各类最优中心，即： <span class="math display">\[\min_{m_1,\ldots,m_k} \sum_{l=1}^k\sum_{C(i)=l}\|x_i-m_l\|^2\]</span> 求偏导并令为零，容易得到上式的最优解是： <span class="math display">\[m_l=\frac{\sum_{C(i)=l}x_i}{n_l}\quad l=1,\ldots,k\]</span> 其中 <span class="math inline">\(n_l\)</span> 是属于第 <span class="math inline">\(l\)</span> 类的样本数量，最优解即是对各类分别计算样本的均值（也称质心）。</p></li><li><p>迭代执行 2、3 步直至收敛。</p></li></ol><p><img src="show.gif" width=50% /></p><h2 id="收敛性">2 收敛性</h2><blockquote><p>本节主要参考资料<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="wlad (https://stats.stackexchange.com/users/86522/wlad), Proof of convergence of k-means, URL (version: 2016-10-31): https://stats.stackexchange.com/q/188352">[2]</span></a></sup>,<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="https://www.cse.iitb.ac.in/~shivaram/teaching/old/cs344+386-s2017/resources/classnote-2.pdf">[3]</span></a></sup></p></blockquote><p>k-means 一定收敛吗？答案是肯定的。对每一轮迭代：</p><ol type="1"><li>第 2 步中，如果新的划分与上一轮的划分相同，那么下一次划分也将相同，迭代终止；</li><li>第 2 步中，如果新的划分与上一轮的划分不同，必然是因为有样本发现了更近的类中心，所以损失函数值必然减小；</li><li>第 3 步中，损失函数值不增。</li></ol><p>故每一轮迭代后，如果没有终止，损失函数必然减小。再注意到将 <span class="math inline">\(n\)</span> 个样本划分成 <span class="math inline">\(k\)</span> 类的方案数是<strong>有限</strong>的，因此只可能出现有限次迭代，即 k-means 必然在有限步内收敛。</p><p><img src="iter.png" widht=60% /></p><p>虽然 k-means 一定收敛，但<strong>可能收敛到局部最优解</strong>，这完全取决于初始化的情况。因此在实际应用中，常常用不同随机种子跑多次。</p><h2 id="其他距离度量">3 其他距离度量</h2><p>许多资料在介绍 k-means 算法时，直接称「k-means 是基于欧式距离的」。这自然引发了我们思考——能不能采用其他的距离呢？</p><p>答案是——k-means 的核心思想依旧适用，只不过这时候不该叫做 k-means 了——"means" 这个名称来自于第 3 步中求均值，而之所以求均值，是因为均值是以平方误差（欧式距离）为损失函数的最优解。改变了距离度量，就改变了损失函数，就改变了最优解的形式。</p><p>换个角度说，如果使用其他距离确定划分（第 2 步），却依然用均值来更新类中心（第 3 步），就意味着第 2、3 步的损失函数不同，这很可能导致算法不收敛——毕竟第二节的收敛性证明依赖于第 2、3 步在优化同一个损失函数<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Has QUIT--Anony-Mousse (https://stats.stackexchange.com/users/7828/has-quit-anony-mousse), Why does k-means clustering algorithm use only Euclidean distance metric?, URL (version: 2014-01-07): https://stats.stackexchange.com/q/81496">[4]</span></a></sup>。</p><p>总而言之，以下框架没有任何问题：</p><ol type="1"><li><p>随机初始化 <span class="math inline">\(k\)</span> 个中心 <span class="math inline">\((m_1,\ldots,m_k)\)</span>.</p></li><li><p>对给定的中心，求最优划分，即： <span class="math display">\[\min_C \sum_{l=1}^k\sum_{C(i)=l}\text{dist}(x_i,m_l)\]</span> 显然，最优划分是将每个样本划分给距离它最近的那个中心。</p></li><li><p>对于给定划分，求各类最优中心，即： <span class="math display">\[\min_{m_1,\ldots,m_k} \sum_{l=1}^k\sum_{C(i)=l}\text{dist}(x_i,m_l)\]</span> <strong>对于不同距离的度量，最优解的形式有所不同</strong>。</p></li><li><p>迭代执行 2、3 步直至收敛。</p></li></ol><h3 id="曼哈顿距离k-medians">3.1 曼哈顿距离——k-medians</h3><p>曼哈顿距离，也即 L1 距离，定义为： <span class="math display">\[\text{dist}(x,y)=\|x-y\|_1=\sum_{j=1}^d |x_j-y_j|\]</span> 在曼哈顿距离下，第 3 步的最优解是中位数（median），可以参见<a href="/blog-main/2022/02/24/%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD-%E7%AC%AC%E4%BA%8C%E7%AB%A0%C2%B7%E5%8F%98%E6%8D%A2%E5%92%8C%E6%9C%9F%E6%9C%9B/" title="[统计推断]第二章·变换和期望">链接</a>第2.2节。正因如此，该算法被称作 <strong>k-medians</strong>.</p><p>中位数相比于均值的优势在于不易受到噪声点的干扰——如果有一个数据点特别离谱，它对均值的影响将是巨大的，但中位数可能根本不变。</p><h3 id="汉明距离k-modes">3.2 汉明距离——k-modes</h3><p>如果样本各维度都取离散值，汉明距离也是常用的一种度量： <span class="math display">\[\text{dist}(x,y)=\sum_{j=1}^d[x_j\neq y_j]\]</span> 即比较两个向量有多少维取值不同。</p><p>在汉明距离下，第 3 步的最优解是众数（mode）。正因如此，该算法被称作 <strong>k-modes</strong>.</p><h3 id="任意距离k-medoids">3.3 任意距离——k-medoids</h3><p>对于任意的距离度量，第 3 步很可能没有一个像均值/中位数/众数那么好看的解，而最为暴力的求解方法就是——枚举！我们当然不可能在实值空间里枚举，但可以只在样本点中枚举——这就是 <strong>k-medoids</strong> 算法。从求解过程可以看出，k-medoids 得到的中心点一定是某些样本点，这也是它与 k-means、k-medians 和 k-modes 的一个不同之处。</p><p>k-medoids 有一个特殊的应用场景——<strong>我们只知道样本点两两之间的距离，但不知道样本点具体是多少</strong>。无论是 k-means, k-medians 还是 k-modes，计算均值/中位数/众数必然需要样本点具体的值，所以它们无法应用在这个特殊的场景下。但是仔细看一看 k-medoids 的计算过程会发现，k-medoids 只需要样本点两两之间距离足矣<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Has QUIT--Anony-Mousse (https://stats.stackexchange.com/users/7828/has-quit-anony-mousse), Perform K-means (or its close kin) clustering with only a distance matrix, not points-by-features data, URL (version: 2013-09-19): https://stats.stackexchange.com/q/32942">[5]</span></a></sup>。</p><p>k-medoids 的缺点也很显著——枚举耗时巨大。因此诸如 PAM(Partitioning Around Medoids) 等算法被提出以减小复杂度，此处按下不表。</p><p><br/></p><p>综上所述，我们发现<strong>想要用一个新的距离度量，整体算法框架就是本节开头所述那样，其中只需要想办法求解第 3 步</strong>——如果没有解析解，那就枚举（k-medoids）；如果有，恭喜你，你可以把这个算法叫做 「k-some_strange_word_starting_with_the_letter_m」 了！（大雾）下面我们用余弦相似度举个例子。</p><h3 id="余弦相似度spherical-k-means">3.4 余弦相似度——spherical k-means</h3><p>其实寻找基于余弦相似度的 k-means 算法正是本文的写作动机。最无脑的解决方案无非是用 k-medoids 算法，但为了效率考虑，我们不妨尝试一下第 3 步能否求出解析解<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="k-means 聚类中使用余弦距离 cos distance - kuizhiqing的文章 - 知乎 https://zhuanlan.zhihu.com/p/380389927">[6]</span></a></sup><sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="已计算出个文本间的余弦相似度值，怎么用kmeans聚类？ - 花开如火的回答 - 知乎 https://www.zhihu.com/question/29873270/answer/2411868694">[7]</span></a></sup>： <span class="math display">\[\max_{m_1,\ldots,m_k} \sum_{l=1}^k\sum_{C(i)=l}\cos(x_i,m_l)\iff \max_{m_1,\ldots,m_k} \sum_{l=1}^k\sum_{C(i)=l}\frac{x_i\cdot m_l}{\|x_i\|\|m_l\|}\]</span> 由于类与类互相独立，所以只需考虑： <span class="math display">\[\max_{m_l}\sum_i\frac{x_i\cdot m_l}{\|x_i\|\|m_l\|}\]</span> 求和是对所有 <span class="math inline">\(\{i:C(i)=l\}\)</span> 求和，书写简便起见省略了条件。由于 <span class="math inline">\(m_l\)</span> 模长与优化目标无关，不妨假定为 <span class="math inline">\(1\)</span>，优化问题变为： <span class="math display">\[\begin{align}\max_{m_l}&amp;\sum_i\frac{x_i\cdot m_l}{\|x_i\|}\\\text{s.t.}&amp; \|m_l\|^2=1\end{align}\]</span> 引入拉格朗日乘子： <span class="math display">\[L(m_l, \lambda)=\sum_i\frac{x_i\cdot m_l}{\|x_i\|}-\lambda (\|m_l\|^2-1)\]</span> 求偏导： <span class="math display">\[\begin{align}&amp;\frac{\partial L}{\partial m_l}=\left(\sum_i\frac{x_i}{\|x_i\|}\right)-2\lambda {m_l}&amp;&amp;\text{note that this is a vector}\\&amp;\frac{\partial L}{\partial \lambda}=1-\|m_l\|^2\end{align}\]</span> 令为零，解得： <span class="math display">\[m_l=\frac{1}{2\lambda}\left(\sum_i\frac{x_i}{\|x_i\|}\right)\]</span> 其中 <span class="math inline">\(1/2\lambda\)</span> 是归一化系数，以使得 <span class="math inline">\(m_l\)</span> 是单位向量。</p><p>综上所述，对于类 <span class="math inline">\(l\)</span>，其类中心 <span class="math inline">\(m_l\)</span> 的更新方式为：<strong>首先将属于类 <span class="math inline">\(l\)</span> 的样本归一化，然后求和，最后归一化成单位向量</strong>；当然求和可以改成求平均，反正最后有个归一化，不影响结果。事实上，这个算法被称作 <strong>spherical k-means</strong><sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="Dhillon, Inderjit S., and Dharmendra S. Modha. Concept decompositions for large sparse text data using clustering. *Machine learning* 42, no. 1 (2001): 143-175.">[8]</span></a></sup>，因为归一化使得这些向量分布在球面（sphere）上。</p><p><br/></p><blockquote><p>考虑一个特殊情况：所有样本 <span class="math inline">\(x_i\)</span> 都是归一化后的。</p><p>基于等式：<strong>如果 <span class="math inline">\(x,y\)</span> 模长都为 <span class="math inline">\(1\)</span>，那么 <span class="math inline">\(x,y\)</span> 的余弦相似度和欧式距离的平方具有简单的线性关系</strong><sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><span class="hint--top hint--rounded" aria-label="ttnphns (https://stats.stackexchange.com/users/3277/ttnphns), Why does k-means clustering algorithm use only Euclidean distance metric?, URL (version: 2020-07-21): https://stats.stackexchange.com/q/81494">[9]</span></a></sup>： <span class="math display">\[\|x-y\|_2^2=x^T x+y^T y-2x^Ty=2(1-x^Ty)=2(1-\cos(x,y))\]</span> 容易知道，在这个特殊情况下，根据余弦相似度做 k-means，和根据欧式距离做 k-means 的<strong>唯一区别就是第 3 步算样本均值时，余弦相似度需要把结果归一化成单位向量</strong>，其他地方完全相同。网上许多人直接说用余弦相似度做 k-means 和用欧式距离完全等价，多少有点误人子弟了。</p></blockquote><h2 id="小结">小结</h2><p>本文首先回顾了 k-means 算法的过程，然后证明了其必定在有限步内收敛。本文进一步将 k-means 的欧式距离发散到其他距离度量，得以从一个统一的视角看待 k-means、k-medians、k-modes、k-medoids、spherical k-means 算法，特别是针对余弦相似度给出了详细推导。</p><p>但是，对 k-means 的学习远不止于此，例如文献<sup id="fnref:10" class="footnote-ref"><a href="#fn:10" rel="footnote"><span class="hint--top hint--rounded" aria-label="Bottou, Leon, and Yoshua Bengio. Convergence properties of the k-means algorithms. *Advances in neural information processing systems* 7 (1994).">[10]</span></a></sup>从梯度下降、EM 算法、Newton 优化三个角度对 k-means 算法做了解释并辅以之实验。暂且搁置，以后有空拜读。</p><h2 id="reference">Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>李航.统计学习方法 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>wlad (https://stats.stackexchange.com/users/86522/wlad), Proof of convergence of k-means, URL (version: 2016-10-31): https://stats.stackexchange.com/q/188352 <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>https://www.cse.iitb.ac.in/~shivaram/teaching/old/cs344+386-s2017/resources/classnote-2.pdf <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Has QUIT--Anony-Mousse (https://stats.stackexchange.com/users/7828/has-quit-anony-mousse), Why does k-means clustering algorithm use only Euclidean distance metric?, URL (version: 2014-01-07): https://stats.stackexchange.com/q/81496 <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Has QUIT--Anony-Mousse (https://stats.stackexchange.com/users/7828/has-quit-anony-mousse), Perform K-means (or its close kin) clustering with only a distance matrix, not points-by-features data, URL (version: 2013-09-19): https://stats.stackexchange.com/q/32942 <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>k-means 聚类中使用余弦距离 cos distance - kuizhiqing的文章 - 知乎 https://zhuanlan.zhihu.com/p/380389927 <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>已计算出个文本间的余弦相似度值，怎么用kmeans聚类？ - 花开如火的回答 - 知乎 https://www.zhihu.com/question/29873270/answer/2411868694 <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Dhillon, Inderjit S., and Dharmendra S. Modha. Concept decompositions for large sparse text data using clustering. <em>Machine learning</em> 42, no. 1 (2001): 143-175. <a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>ttnphns (https://stats.stackexchange.com/users/3277/ttnphns), Why does k-means clustering algorithm use only Euclidean distance metric?, URL (version: 2020-07-21): https://stats.stackexchange.com/q/81494 <a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:10" class="footnote-text"><span>Bottou, Leon, and Yoshua Bengio. Convergence properties of the k-means algorithms. <em>Advances in neural information processing systems</em> 7 (1994). <a href="#fnref:10" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从二分类到多分类，从单标签到多标签</title>
    <link href="/blog-main/2022/08/03/%E4%BB%8E%E4%BA%8C%E5%88%86%E7%B1%BB%E5%88%B0%E5%A4%9A%E5%88%86%E7%B1%BB%EF%BC%8C%E4%BB%8E%E5%8D%95%E6%A0%87%E7%AD%BE%E5%88%B0%E5%A4%9A%E6%A0%87%E7%AD%BE/"/>
    <url>/blog-main/2022/08/03/%E4%BB%8E%E4%BA%8C%E5%88%86%E7%B1%BB%E5%88%B0%E5%A4%9A%E5%88%86%E7%B1%BB%EF%BC%8C%E4%BB%8E%E5%8D%95%E6%A0%87%E7%AD%BE%E5%88%B0%E5%A4%9A%E6%A0%87%E7%AD%BE/</url>
    
    <content type="html"><![CDATA[<h2 id="二分类-to-多分类">二分类 <span class="math inline">\(\to\)</span> 多分类</h2><h3 id="sigmoidbce">sigmoid+BCE</h3><p>众所周知，在二分类任务中，我们会让模型输出一个得分 <span class="math inline">\(s\in\mathbb R\)</span>（也称 logit，见参考资料<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="如何理解深度学习源码里经常出现的logits？ - 王峰的回答 - 知乎 https://www.zhihu.com/question/60751553/answer/1986650670">[1]</span></a></sup>），然后经由 sigmoid 函数得到一个实数值： <span class="math display">\[\sigma(s)=\frac{1}{1+e^{-s}}\in(0,1)\]</span> 由于这个数在 <span class="math inline">\((0,1)\)</span> 之间，所以我们可以将它视为属于正例的概率 <span class="math inline">\(p\)</span>，进而用 binary cross-entropy loss 进行优化： <span class="math display">\[\begin{align}\text{BCE}(p,y)&amp;=-y\log p-(1-y)\log (1-p)\end{align}\]</span> 其中 <span class="math inline">\(p\in(0,1),\,y\in\{0,1\}\)</span>.</p><h3 id="softmaxce">softmax+CE</h3><p>而在多分类任务中，模型输出的得分是一个 <span class="math inline">\(C\)</span> 维向量 <span class="math inline">\(\mathbf s\in\mathbb R^C\)</span>（<span class="math inline">\(C\)</span> 是类别数），然后经由 softmax 函数得到一个向量： <span class="math display">\[\text{softmax}(\mathbf s)_i=\frac{e^{s_i}}{\sum_{j=0}^{C-1} e^{s_j}}\]</span> 由于该向量每个分量都在 <span class="math inline">\((0,1)\)</span> 之间，且各分量和为 <span class="math inline">\(1\)</span>，所以我们可以将它视为一个概率向量 <span class="math inline">\(\mathbf p\)</span>，每个分量代表属于对应类别的概率，进而用 cross-entropy loss 进行优化： <span class="math display">\[\begin{align}\text{CE}(\mathbf p, y)=-\log p_y\end{align}\]</span> 其中 <span class="math inline">\(y\in\{0,\ldots,C-1\}\)</span>，<span class="math inline">\(p_y\)</span> 表示取向量 <span class="math inline">\(\mathbf p\)</span> 中的第 <span class="math inline">\(y\)</span> 维分量。</p><h3 id="二者关系">二者关系</h3><p>事实上，binary cross-entropy loss 只是 cross-entropy loss 在二分类情形下的一种更简洁的写法： <span class="math display">\[\begin{align}\text{CE}\left(\begin{bmatrix}p_0\\p_1\end{bmatrix},y\right)&amp;=\begin{cases}-\log p_0,&amp;y=0\\-\log p_1,&amp;y=1\end{cases}\\&amp;=-y\log p_1-(1-y)\log p_0\\&amp;=-y\log p_1-(1-y)\log (1-p_1)\\&amp;=\text{BCE}(p_1,y)\end{align}\]</span> 简洁性源自于 <span class="math inline">\(p_0+p_1=1\)</span>，所以我们只需要 <span class="math inline">\(p_0,p_1\)</span> 其中一个就行了。</p><p>同理，sigmoid 也可以看作是 softmax 在二分类下的特殊情况，或者换句话说，softmax 是 sigmoid 的多分类推广： <span class="math display">\[\text{softmax}\left(\begin{bmatrix}s_0\\s_1\end{bmatrix}\right)=\begin{bmatrix}\frac{\exp s_0}{\exp s_0+\exp s_1}\\\frac{\exp s_1}{\exp s_0+\exp s_1}\end{bmatrix}=\begin{bmatrix}\frac{1}{1+\exp( s_1-s_0)}\\\frac{1}{1+\exp (s_0-s_1)}\end{bmatrix}=\begin{bmatrix}1-\sigma(s_1-s_0)\\\sigma(s_1-s_0)\end{bmatrix}\]</span></p><blockquote><p>注意对于 sigmoid 函数，有 <span class="math inline">\(\sigma(x)=1-\sigma(-x)\)</span>.</p></blockquote><p><img src="softmax-sigmoid.png" width=90% /></p><h2 id="单标签-to-多标签">单标签 <span class="math inline">\(\to\)</span> 多标签</h2><p>其实这一节才是本文的重点。我们说 softmax 解决多分类问题，其实默认说的是<strong>单标签</strong>多分类问题，即每一个样本的正类（目标类、真实标签）只有一个——狗就是狗，猫就是猫。但事实上，现实中很多分类问题是<strong>多标签</strong>的——比如哈士奇可以同时分在动物、狗、中型犬、哈士奇几个类别下，又比如一张图片中有多个物体。</p><p>解决多标签多分类问题，直观来看有两个途径——一是扩展 sigmoid+BCE，把每个标签都看成一个二分类问题，用多个二分类器解决；二是扩展 softmax+CE，直接把单标签多分类问题推广到多标签。下面我们分别来探究一下这两种途径。</p><h3 id="途径1扩展-sigmoidbce">途径1：扩展 sigmoid+BCE</h3><p>最朴素的想法无疑是训练多个二分类器，即在模型输出向量 <span class="math inline">\(f(x)\in\mathbb R^C\)</span> 后接 sigmoid，然后用 <span class="math inline">\(C\)</span> 个 binary cross-entropy loss 分别训练，如下： <span class="math display">\[\begin{align}\mathcal L&amp;=\sum_{j\in\mathcal N}\text{BCE}(\sigma(s_j), 0)+\sum_{i\in\mathcal P}\text{BCE}(\sigma(s_i), 1)\\&amp;=-\sum_{j\in\mathcal N}\log\left(1-\frac{1}{1+e^{-s_j}}\right)-\sum_{i\in\mathcal P}\log\left(\frac{1}{1+e^{-s_i}}\right)\\&amp;=\sum_{j\in\mathcal N}\log(1+e^{s_j})+\sum_{i\in\mathcal P}\log(1+e^{-s_i})\end{align}\tag{1}\label{1}\]</span> 其中 <span class="math inline">\(\mathcal N\)</span> 是所有负类（非目标类）的集合，<span class="math inline">\(\mathcal P\)</span> 是所有正类（目标类）的集合。</p><p><img src="extendsigmoid.png" width=80% /></p><p>这样虽然很简单，但是每个二分类器是独立的，缺少了 softmax 中不同类别之间“交互”的感觉。另外，每个二分类器的数据将极度不平衡，因为正类只有一个类别的数据，而负类包括其他所有 <span class="math inline">\(C-1\)</span> 个类别的数据。</p><h3 id="途径2扩展-softmaxce">途径2：扩展 softmax+CE</h3><h4 id="circle-loss">Circle Loss</h4><p>我们首先改写一下 softmax+CE： <span class="math display">\[\text{CE}(\text{softmax}(\mathbf s), y)=-\log\frac{e^{s_y}}{\sum_{j=0}^{C-1} e^{s_j}}=\log\left[1+{\sum_{j\neq y}e^{s_j-s_y}}\right]\]</span> 可以看出，softmax+CE 是在最小化 <span class="math inline">\((s_j-s_y)\)</span>——即最小化负类的得分 <span class="math inline">\(s_j\)</span> 和正类的得分 <span class="math inline">\(s_y\)</span> 之差。如果沿用这个思路，在不止一个正类的情况下，就让负类和正类<strong>两两配对</strong>，最小化每一对的得分差值，即： <span class="math display">\[\mathcal L=\log\left[1+\sum_{j\in\mathcal N}\sum_{i\in\mathcal P}e^{s_j-s_i}\right]=\log\left[1+\sum_{j\in\mathcal N}e^{s_j}\sum_{i\in\mathcal P}e^{-s_i}\right]\tag{2}\label{2}\]</span> 事实上，这个式子是 CVPR2020 Circle Loss 论文<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Sun, Yifan, et al. Circle loss: A unified perspective of pair similarity optimization. *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2020.">[3]</span></a></sup>中，作者提出的统一形式 loss 的特例： <span class="math display">\[\mathcal L_\text{uni}=\log\left[1+\sum_{i\in\mathcal P}\sum_{j\in\mathcal N} e^{\gamma(s_j-s_i+m)}\right]=\log\left[1+\sum_{j\in\mathcal N}e^{\gamma(s_j+m)}\sum_{i\in\mathcal P}e^{\gamma(-s_i)}\right]\]</span> 其中 scale factor <span class="math inline">\(\gamma\)</span> 和 margin <span class="math inline">\(m\)</span> 是两个超参数，取 <span class="math inline">\(\gamma=1,\,m=0\)</span> 就是 <span class="math inline">\((2)\)</span> 式。</p><p><img src="uniloss.png" width=60% /></p><h4 id="从-hard-form-角度理解">从 hard form 角度理解</h4><p>在之前的<a href="/blog-main/2022/07/25/%E5%90%84%E7%A7%8D%E5%87%BD%E6%95%B0%E7%9A%84hard%E4%B8%8Esoft%E5%BD%A2%E5%BC%8F/" title="各种函数的hard与soft形式">文章</a>中，我们知道了 <span class="math inline">\(\text{logsumexp}\)</span> 是 <span class="math inline">\(\max\)</span> 的平滑近似。如果我们把 softmax+CE 写回 hard 的形式： <span class="math display">\[\text{CE}(\mathbf p, y)=-s_y+\log\sum_{j=0}^{C-1}e^{s_j}\approx-s_y+\max(\mathbf s)=\max\begin{pmatrix}s_0-s_y\\\vdots\\0\\\vdots\\s_{C-1}-s_y\end{pmatrix}=\left[\max_{j\neq y}(s_j)-s_y\right]_+\]</span> 其中 <span class="math inline">\([\bullet]_+\)</span> 是 <span class="math inline">\(\max(0,\bullet)\)</span> 的简写。可以看出，正如上文所言，softmax+CE 的目标是让所有负类的得分小于正类的得分。</p><p><img src="opt1.png" width=100% /></p><p>同样的，我们可以将 <span class="math inline">\(\eqref{2}\)</span> 式写回 hard 形式<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="如何理解与看待在cvpr2020中提出的circle loss？ - 王峰的回答 - 知乎 https://www.zhihu.com/question/382802283/answer/1114719159">[4]</span></a></sup>： <span class="math display">\[\begin{align}\mathcal L&amp;=\log\left[1+\sum_{j\in\mathcal N}e^{s_j}\sum_{i\in\mathcal P}e^{-s_i}\right]\\&amp;\approx\left[\log\left(\sum_{j\in\mathcal N}e^{s_j}\sum_{i\in\mathcal P}e^{-s_i}\right)\right]_+\\&amp;=\left[\log \sum_{j\in\mathcal N}e^{s_j}+\log\sum_{i\in\mathcal P}e^{-s_i}\right]_+\\&amp;\approx\left[\max_{j\in\mathcal N}(s_j)-\min_{i\in\mathcal P}(s_i)\right]_+\end{align}\]</span> 其中第一个约等于利用了 <span class="math inline">\(\text{softplus}(x)=\log(1+e^x)\)</span> 是 <span class="math inline">\(\max(0,x)\)</span> 的平滑近似。从上式看出，<span class="math inline">\((2)\)</span> 式的目标是让所有负类的得分小于正类的得分。</p><p><img src="opt2.png" width=100% /></p><h4 id="引入阈值">引入阈值</h4><p>在大多数多标签场景下，我们并不知道测试图像究竟对应多少个标签，那么面对模型给我们吐出来的 <span class="math inline">\(C\)</span> 个得分，我们应该怎么选取阈值来决定接收哪些得分呢？苏神在他的博客<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="苏剑林. (Apr. 25, 2020). 《将“softmax+交叉熵”推广到多标签分类问题 》[Blog post]. Retrieved from https://kexue.fm/archives/7359">[2]</span></a></sup>中给出了一个方案。</p><p>我们回顾一下上一小节中 <span class="math inline">\(\eqref{2}\)</span> 式的 hard 形式以及画的数轴图，可以看出 <span class="math inline">\(\eqref{2}\)</span> 式只希望让 <span class="math inline">\(\max_{j\in\mathcal N}(s_j)\)</span> 移动到 <span class="math inline">\(\min_{i\in\mathcal P}(s_i)\)</span> 左边就行了，但没说这个分界点到底在哪儿。因此，我们可以人为的添加一个分界点 <span class="math inline">\(\tilde s\)</span>，让 <span class="math inline">\(\max_{j\in \mathcal N}(s_j)&lt;\tilde s\)</span>、同时 <span class="math inline">\(\min_{i\in\mathcal P}(s_i)&gt;\tilde s\)</span>，如图所示：</p><p><img src="opt3.png" width=100% /></p><p>要做到这一点，仿照之前的思路，向 <span class="math inline">\(\eqref{1}\)</span> 式添加最小化 <span class="math inline">\(s_j-\tilde s\)</span> 和 <span class="math inline">\(\tilde s-s_i\)</span> 的两项： <span class="math display">\[\begin{align}\mathcal L&amp;=\log\left[1+\sum_{j\in\mathcal N}\sum_{i\in\mathcal P}e^{s_j-s_i}+\sum_{j\in\mathcal N}e^{s_j-\tilde s}+\sum_{i\in\mathcal P}e^{\tilde s-s_i}\right]\\&amp;=\log\left[\left(e^{\tilde s}+\sum_{j\in\mathcal N}e^{s_j}\right)\left(e^{-\tilde s}+\sum_{i\in\mathcal P}e^{-s_i}\right)\right]\\&amp;=\log\left(e^{\tilde s}+\sum_{j\in\mathcal N}e^{s_j}\right)+\log\left(e^{-\tilde s}+\sum_{i\in\mathcal P}e^{-s_i}\right)\end{align}\tag{3}\label{3}\]</span> 简单起见，取阈值 <span class="math inline">\(\tilde s=0\)</span>，那么 <span class="math inline">\(\eqref{3}\)</span> 式简化为： <span class="math display">\[\mathcal L=\log\left(1+\sum_{j\in\mathcal N}e^{s_j}\right)+\log\left(1+\sum_{i\in\mathcal P}e^{-s_i}\right)\tag{4}\label{4}\]</span> 理论上，使用 <span class="math inline">\(\eqref{4}\)</span> 式训练模型，测试时选取得分为正的类别输出即可。但在苏神博客的评论区中，有网友反映这个阈值可能还是当作超参数在验证集上调一调比较好。</p><blockquote><p><span class="math inline">\(\eqref{2}\)</span> 式可以看作是只关心正类和负类得分的<strong>相对</strong>大小，而 <span class="math inline">\(\eqref{3},\eqref{4}\)</span> 式则是引入了一个人为指定的值使得得分的<strong>绝对</strong>大小有了意义。</p></blockquote><h3 id="殊途同归">殊途同归？</h3><p>最有趣的部分来了，仔细看看 <span class="math inline">\(\eqref{4}\)</span> 式，是不是感觉似曾相识？没错，<span class="math inline">\(\eqref{1}\)</span> 式和它非常相似啊！事实上，<span class="math inline">\(\eqref{4}\)</span> 式是 <span class="math inline">\(\eqref{1}\)</span> 式的低阶<strong>截断</strong>： <span class="math display">\[\begin{align}\mathcal L_{(1)}&amp;=\sum_{j\in\mathcal N}\log(1+e^{s_j})+\sum_{i\in\mathcal P}\log(1+e^{-s_i})\\&amp;=\log\left(\prod_{j\in\mathcal N}(1+e^{s_j})\right)+\log\left(\prod_{i\in\mathcal P}(1+e^{-s_i})\right)\\&amp;=\log\left(1+\sum_{j\in\mathcal N}e^{s_j}+\cdots\right)+\log\left(1+\sum_{i\in\mathcal P}e^{-s_i}+\cdots\right)\\&amp;\xrightarrow{\text{cutoff}}\log\left(1+\sum_{j\in\mathcal N}e^{s_j}\right)+\log\left(1+\sum_{i\in\mathcal P}e^{-s_i}\right)=\mathcal L_{(4)}\end{align}\]</span> 两种推导途径最后竟然殊途同归了！呃……好吧，其实也没有归到一起，因为丢弃掉的高阶项并不是可以忽略的接近 <span class="math inline">\(0\)</span> 的数。苏神评论区中网友们讨论说，这些丢掉的高阶项会加剧数据不平衡的影响，导致 sigmoid+BCE 性能下降，从这个角度说，丢掉它们的 <span class="math inline">\(\eqref{4}\)</span> 式可以看作是 sigmoid+BCE 的一种修正。</p><p>然而，<span class="math inline">\(\eqref{4}\)</span> 式将正类和负类完全分开的形式，少了 softmax+CE 中正负类“交互”的感觉。从梯度的角度说，softmax+CE 产生的梯度<strong>正负类会同时出现</strong>，且有一个优秀的性质是<strong>它对正类产生的梯度的绝对值等于对所有负类产生的梯度之和的绝对值</strong>（符号相反）<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Softmax理解之二分类与多分类 - 王峰的文章 - 知乎 https://zhuanlan.zhihu.com/p/45368976">[6]</span></a></sup>： <span class="math display">\[\begin{align}&amp;\text{CE}(\text{softmax}(\mathbf s),y)=-s_y+\log\sum_{j=0}^{C-1}e^{s_j}\\\implies&amp;\nabla_{s_k}\text{CE}(\text{softmax}(\mathbf s),y)=\begin{cases}-1+(e^{s_y})/\left(\sum_{j=0}^{C-1}e^{s_j}\right)&amp;k=y\\(e^{s_k})/\left(\sum_{j=0}^{C-1}e^{s_j}\right)&amp;k\neq y\end{cases}\\\implies&amp;\nabla_{s_y}\text{CE}(\text{softmax}(\mathbf s),y)+\sum_{k\neq y}\nabla_{s_k}\text{CE}(\text{softmax}(\mathbf s),y)=0\end{align}\]</span> 这个性质也被继承到了 <span class="math inline">\(\eqref{2}\)</span> 式中： <span class="math display">\[\begin{align}&amp;\mathcal L_{(2)}=\log\left[1+\sum_{j\in\mathcal N}e^{s_j}\sum_{i\in\mathcal P}e^{-s_i}\right]\\\implies&amp;\nabla_{s_k}\mathcal L_{(2)}=\begin{cases}\left(e^{s_k}\cdot\sum_{i\in \mathcal P}e^{-s_i}\right)/\left(1+\sum_{j\in\mathcal N}e^{s_j}\sum_{i\in\mathcal P}e^{-s_i}\right)&amp;k\in\mathcal N\\\left(-e^{-s_k}\cdot\sum_{j\in \mathcal N}e^{s_j}\right)/\left(1+\sum_{j\in\mathcal N}e^{s_j}\sum_{i\in\mathcal P}e^{-s_i}\right)&amp;k\in\mathcal P\\\end{cases}\\\implies&amp;\sum_{k\in\mathcal N}\nabla_{s_k}\mathcal L_{(2)}+\sum_{k\in\mathcal P}\nabla_{s_k}\mathcal L_{(2)}=0\end{align}\]</span> 但 <span class="math inline">\(\eqref{4}\)</span> 式并没有这个性质： <span class="math display">\[\begin{align}&amp;\mathcal L_{(4)}=\log\left(1+\sum_{j\in\mathcal N}e^{s_j}\right)+\log\left(1+\sum_{i\in\mathcal P}e^{-s_i}\right)\\\implies&amp;\nabla_{s_k}\mathcal L_{(4)}=\begin{cases}(e^{s_k})/\left(1+\sum_{j\in\mathcal N}e^{s_j}\right)&amp;k\in\mathcal N\\(-e^{-s_k})/\left(1+\sum_{i\in\mathcal P}e^{-s_i}\right)&amp;k\in\mathcal P\end{cases}\end{align}\]</span> 由此看来，<span class="math inline">\(\eqref{4}\)</span> 式也具有一定的局限性。</p><p>话说回来，<span class="math inline">\(\eqref{4}\)</span> 式是由 <span class="math inline">\(\eqref{2}\)</span> 式在引入阈值之后推得的，如果我们的应用场景不需要阈值，就可以直接使用 <span class="math inline">\(\eqref{2}\)</span> 式而不必担心什么了。我目前发现的一个应用场景是：经典的对比学习任务中，一个 anchor 只有一个 positive sample（有多个 negative samples），一般调用 cross-entropy loss 来实现 InfoNCE；而若一个 anchor 具有多个 positive samples，将 cross-entropy loss 修改为 <span class="math inline">\(\eqref{2}\)</span> 式即可。</p><h3 id="pytorch-实现">PyTorch 实现</h3><p>具体到代码实现上，我们要注意几个问题：</p><ol type="1"><li>尽可能调用封装好的 <code>logsumexp</code> 操作来避免数值计算问题</li><li>数学公式里可以把 positive 和 negative 分开写，但是代码里为了保持 tensor 维度的统一，只能通过加上大负数等技巧实现</li></ol><p><span class="math inline">\(\eqref{2}\)</span> 式的 PyTorch 实现： <span class="math display">\[\mathcal L=\log\left[1+\sum_{j\in\mathcal N}e^{s_j}\sum_{i\in\mathcal P}e^{-s_i}\right]=\log\left[1+\exp\left(\log\sum_{j\in\mathcal N}e^{s_j}+\log\sum_{i\in\mathcal P}e^{-s_i}\right)\right]\tag{2}\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimplifiedCircleLoss</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, reduction=<span class="hljs-string">&#x27;mean&#x27;</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.reduction = reduction<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, pred: torch.Tensor, target: torch.Tensor</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            pred (Tensor): [..., C]</span><br><span class="hljs-string">            target (Tensor): [..., C], 1 pos, -1 neg, 0 not care</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">assert</span> ((target == <span class="hljs-number">0</span>) | (target == <span class="hljs-number">1</span>) | (target == -<span class="hljs-number">1</span>)).<span class="hljs-built_in">all</span>()<br>        <span class="hljs-keyword">assert</span> pred.shape == target.shape<br>        logits = -target * pred<br>        BigNegNum = torch.tensor(-<span class="hljs-number">1e12</span>, device=pred.device)<br>        logits_neg = torch.where(target != -<span class="hljs-number">1</span>, BigNegNum, logits)<br>        logits_pos = torch.where(target != <span class="hljs-number">1</span>, BigNegNum, logits)<br>        lse_neg = torch.logsumexp(logits_neg, dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        lse_pos = torch.logsumexp(logits_pos, dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        lse = lse_neg + lse_pos<br>        loss = torch.logsumexp(torch.cat([lse, torch.zeros_like(lse)], dim=-<span class="hljs-number">1</span>), dim=-<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> self.reduction == <span class="hljs-string">&#x27;mean&#x27;</span>:<br>            <span class="hljs-keyword">return</span> torch.mean(loss)<br>        <span class="hljs-keyword">elif</span> self.reduction == <span class="hljs-string">&#x27;sum&#x27;</span>:<br>            <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">sum</span>(loss)<br>        <span class="hljs-keyword">elif</span> self.reduction == <span class="hljs-string">&#x27;none&#x27;</span>:<br>            <span class="hljs-keyword">return</span> loss<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError<br></code></pre></td></tr></table></figure><p>调用时，传入网络的输出 pred 和真实标签 target，二者最后一维大小是类别数。target 中 1 表示正类，-1 表示负类，0 表示不关心的类。</p><p><span class="math inline">\(\eqref{4}\)</span> 式的 PyTorch 实现： <span class="math display">\[\mathcal L=\log\left(1+\sum_{j\in\mathcal N}e^{s_j}\right)+\log\left(1+\sum_{i\in\mathcal P}e^{-s_i}\right)\tag{4}\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiLabelCrossEntropyLoss</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, reduction=<span class="hljs-string">&#x27;mean&#x27;</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.reduction = reduction<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, pred: torch.Tensor, target: torch.Tensor</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            pred (Tensor): [..., C]</span><br><span class="hljs-string">            target (Tensor): [..., C], 1 pos, -1 neg, 0 not care</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">assert</span> ((target == <span class="hljs-number">0</span>) | (target == <span class="hljs-number">1</span>) | (target == -<span class="hljs-number">1</span>)).<span class="hljs-built_in">all</span>()<br>        <span class="hljs-keyword">assert</span> pred.shape == target.shape<br>        logits = -target * pred<br>        BigNegNum = torch.tensor(-<span class="hljs-number">1e12</span>, device=pred.device)<br>        logits_neg = torch.where(target != -<span class="hljs-number">1</span>, BigNegNum, logits)<br>        logits_pos = torch.where(target != <span class="hljs-number">1</span>, BigNegNum, logits)<br>        neg = torch.cat((logits_neg, torch.zeros_like(logits_neg[..., :<span class="hljs-number">1</span>])), dim=-<span class="hljs-number">1</span>)<br>        pos = torch.cat((logits_pos, torch.zeros_like(logits_pos[..., :<span class="hljs-number">1</span>])), dim=-<span class="hljs-number">1</span>)<br>        loss = torch.logsumexp(neg, dim=-<span class="hljs-number">1</span>) + torch.logsumexp(pos, dim=-<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> self.reduction == <span class="hljs-string">&#x27;mean&#x27;</span>:<br>            <span class="hljs-keyword">return</span> torch.mean(loss)<br>        <span class="hljs-keyword">elif</span> self.reduction == <span class="hljs-string">&#x27;sum&#x27;</span>:<br>            <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">sum</span>(loss)<br>        <span class="hljs-keyword">elif</span> self.reduction == <span class="hljs-string">&#x27;none&#x27;</span>:<br>            <span class="hljs-keyword">return</span> loss<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError<br></code></pre></td></tr></table></figure><p>调用时，传入网络的输出 pred 和真实标签 target，二者最后一维大小是类别数。target 中 1 表示正类，-1 表示负类，0 表示不关心的类。</p><hr /><h2 id="参考资料">参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>如何理解深度学习源码里经常出现的logits？ - 王峰的回答 - 知乎 https://www.zhihu.com/question/60751553/answer/1986650670 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>苏剑林. (Apr. 25, 2020). 《将“softmax+交叉熵”推广到多标签分类问题 》[Blog post]. Retrieved from https://kexue.fm/archives/7359 <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Sun, Yifan, et al. Circle loss: A unified perspective of pair similarity optimization. <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2020. <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>如何理解与看待在cvpr2020中提出的circle loss？ - 王峰的回答 - 知乎 https://www.zhihu.com/question/382802283/answer/1114719159 <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>如何理解与看待在cvpr2020中提出的circle loss？ - 孙奕帆的回答 - 知乎 https://www.zhihu.com/question/382802283/answer/1116269890 <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Softmax理解之二分类与多分类 - 王峰的文章 - 知乎 https://zhuanlan.zhihu.com/p/45368976 <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[统计推断]第四章·多维随机变量和不等式</title>
    <link href="/blog-main/2022/07/30/%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD-%E7%AC%AC%E5%9B%9B%E7%AB%A0%C2%B7%E5%A4%9A%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%92%8C%E4%B8%8D%E7%AD%89%E5%BC%8F/"/>
    <url>/blog-main/2022/07/30/%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD-%E7%AC%AC%E5%9B%9B%E7%AB%A0%C2%B7%E5%A4%9A%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E5%92%8C%E4%B8%8D%E7%AD%89%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\E}{\mathbb E}\newcommand{\R}{\mathbb R}\newcommand{\d}{\mathrm d}\newcommand{\Var}{\mathrm{Var}}\newcommand{\Cov}{\mathrm{Cov}}\]</span></p><blockquote><p>本篇是《统计推断》第四章多维随机变量的后 2 节内容，主要关注于多维随机变量和一些不等式。</p></blockquote><h2 id="多维分布">1 多维分布</h2><blockquote><p>关于记号：用黑体表示向量，如 <span class="math inline">\(\mathbf X=(X_1,\ldots,X_n)\)</span>，<span class="math inline">\(\mathbf x=(x_1,\ldots,x_n)\)</span>.</p></blockquote><h3 id="相关概念定义">1.1 相关概念定义</h3><p>设随机向量 <span class="math inline">\(\mathbf X\)</span> 的样本空间是 <span class="math inline">\(\mathbb R^n\)</span> 的子集。若 <span class="math inline">\(\mathbf X\)</span> 为离散随机向量，则<strong>联合概率质量函数</strong>为： <span class="math display">\[f(x_1,\ldots,x_n)=P(X_1=x_1,\ldots,X_n=x_n)\]</span> 且对任意 <span class="math inline">\(A\subset \mathbb R^n\)</span>，有 <span class="math display">\[P(\mathbf X\in A)=\sum_{\mathbf x\in A}f(\mathbf x)\]</span> 若 <span class="math inline">\(\mathbf X\)</span> 为连续随机向量，则<strong>联合概率密度函数</strong>满足 <span class="math display">\[P(\mathbf X\in A)=\int\cdots\int_A f(\mathbf x)\mathrm d\mathbf x=\int\cdots\int_A f(x_1,\ldots,x_n)\mathrm d x_1\cdots \mathrm dx_n\]</span> 设 <span class="math inline">\(g(\mathbf x)\)</span> 是定义在 <span class="math inline">\(\mathbf X\)</span> 样本空间上的实值函数，则 <span class="math inline">\(g(\mathbf X)\)</span> 是随机变量，<strong>期望</strong>为： <span class="math display">\[\E[g(\mathbf X)]=\begin{cases}\int_{-\infty}^{+\infty}\cdots\int_{-\infty}^{+\infty}g(\mathbf x)f(\mathbf x)\mathrm dx&amp;\text{continuous}\\\sum_{\mathbf x\in\mathbb R^n}g(\mathbf x)f(\mathbf x)\end{cases}\]</span> <strong>边缘 pmf/pdf</strong> 由联合 pmf/pdf 关于其余分量求积分/求和得到。例如 <span class="math inline">\((X_1,\ldots,X_n)\)</span> 的前 <span class="math inline">\(k\)</span> 个分量 <span class="math inline">\((X_1,\ldots,X_k)\)</span> 的边缘 pdf 为： <span class="math display">\[f(x_1,\ldots,x_k)=\int_{-\infty}^{+\infty}\cdots\int_{-\infty}^{+\infty}f(x_1,\cdots,x_n)\mathrm dx_{k+1}\cdots\mathrm dx_n\]</span> <strong>条件 pmf/pdf</strong> 由联合 pmf/pdf 除以其余分量的边缘 pmf/pdf 得到。例如，若 <span class="math inline">\(f(x_1,\ldots,x_k)&gt;0\)</span>，则 <span class="math inline">\((X_{k+1},\ldots,X_n)\)</span> 在条件 <span class="math inline">\(X_1=x_1,\ldots,X_n=x_n\)</span> 下的 pdf/pmf 为： <span class="math display">\[f(x_{k+1},\ldots,x_n\mid x_1,\ldots,x_k)=\frac{f(x_1,\ldots,x_n)}{f(x_1,\ldots,x_k)}\]</span></p><h3 id="多项分布二项分布的推广">1.2 多项分布——二项分布的推广</h3><blockquote><p>回顾二项分布： <span class="math display">\[P(X=x\mid n,p)=\binom{n}{x}p^x(1-p)^{n-x},\quad x=0,1,2,\ldots,n\]</span> 表示 <span class="math inline">\(n\)</span> 次独立伯努利试验中成功 <span class="math inline">\(x\)</span> 次的概率。</p></blockquote><p>设 <span class="math inline">\(m,n\)</span> 为正整数，数 <span class="math inline">\(p_1,\ldots,p_n\)</span> 满足 <span class="math inline">\(0\leq p_i\leq 1\)</span>，且 <span class="math inline">\(\sum_{i=1}^N p_i=1\)</span>，若随机向量 <span class="math inline">\((X_1,\ldots,X_n)\)</span> 的联合 pmf 为： <span class="math display">\[f(x_1,\ldots,x_n)=\frac{m!}{x_1!\cdots x_n!}p_1^{x_1}\cdots p_n^{x_n}=m!\prod_{i=1}^n\frac{p_i^{x_i}}{x_i!}\]</span> 其中 <span class="math inline">\(x_i\)</span> 均为非负整数且 <span class="math inline">\(\sum_{i=1}^nx_i=m\)</span>，则称 <span class="math inline">\((X_1,\ldots,X_n)\)</span> 服从 <strong><span class="math inline">\(m\)</span> 次试验、元概率为 <span class="math inline">\(p_1,\ldots,p_n\)</span> 的多项分布（multinomial distribution）</strong>。</p><p>与二项分布类似， 多项分布的意义是：做 <span class="math inline">\(m\)</span> 次独立试验，每次试验有 <span class="math inline">\(n\)</span> 中可能的结果，发生概率分别为 <span class="math inline">\(p_1,\ldots,p_n\)</span>，随机变量 <span class="math inline">\(X_i\)</span> 表示第 <span class="math inline">\(i\)</span> 种结果出现的次数。</p><p><br/></p><p>二项分布中的系数称为二项式系数，类似的，多项分布中的系数是<strong>多项式系数</strong>： <span class="math display">\[\binom{m}{x_1,\ldots,x_n}=\frac{m!}{x_1!\cdots x_n!}\]</span> 表示将 <span class="math inline">\(m\)</span> 个物品分为 <span class="math inline">\(n\)</span> 类，第 <span class="math inline">\(i\)</span> 类有 <span class="math inline">\(x_i\)</span> 个物品的方案数。</p><p>二项式定理可以推广为<strong>多项式定理</strong>：设 <span class="math inline">\(m,n\)</span> 为正整数，<span class="math inline">\(A\)</span> 是满足每个 <span class="math inline">\(x_i\)</span> 都是非负整数且 <span class="math inline">\(\sum_{i=1}^nx_i=m\)</span> 的全体向量 <span class="math inline">\(\mathbf x=(x_1,\ldots,x_n)\)</span> 的集合，则对<u>任意实数</u> <span class="math inline">\(p_1,\ldots,p_n\)</span>，有： <span class="math display">\[(p_1+\cdots+p_n)^m=\sum_{\mathbf x\in A}\frac{m!}{x_1!\cdots x_n!}p_1^{x_1}\cdots p_n^{x_n}\]</span> 根据多项式定理，容易知道多项分布的 pmf 之和确实为 1，而集合 <span class="math inline">\(A\)</span> 恰是其支撑集.</p><p><br/></p><p>根据多项分布的意义，不难想到其第 <span class="math inline">\(i\)</span> 个分量的边缘分布是 <span class="math inline">\(\text{binomial}(m,p_i)\)</span>. 事实上，以第 <span class="math inline">\(n\)</span> 个分量为例： <span class="math display">\[\begin{align}f(x_n)&amp;=\sum_{(x_1,\ldots,x_{n-1})}\frac{m!}{x_1!\cdots x_{n}!}p_1^{x_1}\cdots p_n^{x_n}\\&amp;=\sum_{(x_1,\ldots,x_{n-1})}\frac{m!}{x_1!\cdots x_{n}!}p_1^{x_1}\cdots p_n^{x_n}\frac{(m-x_n)!(1-p_n)^{m-x_n}}{(m-x_n)!(1-p_n)^{m-x_n}}\\&amp;=\left(\frac{m!}{x_n!(m-x_n)!}(1-p_n)^{m-x_n}p_n^{x_n}\right)\left(\sum_{(x_1,\ldots,x_{n-1})}\frac{(m-x_n)!}{x_1!\cdots x_{n-1}!}\prod_{i=1}^{n-1}\left(\frac{p_i}{1-p_n}\right)^{x_i}\right)\\&amp;=\frac{m!}{x_n!(m-x_n)!}(1-p_n)^{m-x_n}p_n^{x_n}\end{align}\]</span> 同样根据多项分布的意义，不难想到在第 <span class="math inline">\(i\)</span> 个分量的条件下，其余分量服从 <span class="math inline">\(m-x_i\)</span> 次试验、元概率为 <span class="math inline">\(p_1/(1-p_i),\ldots\)</span> 的多项分布。事实上，以第 <span class="math inline">\(n\)</span> 个分量作为条件为例： <span class="math display">\[\begin{align}f(x_1,\ldots,x_{n-1}\mid x_n)&amp;=\frac{f(x_1,\ldots,x_n)}{f(x_n)}\\&amp;=\frac{\dfrac{m!}{x_1!\cdots x_n!}p_1^{x_1}\cdots p_n^{x_n}}{\dfrac{m!}{x_n!(m-x_n)!}(1-p_n)^{m-x_n}p_n^{x_n}}\\&amp;=\frac{(m-x_n)!}{x_1!\cdots x_{n-1}!}\prod_{i=1}^{n-1}\left(\frac{p_i}{1-p_n}\right)^{x_i}\end{align}\]</span> <br/></p><p>多项分布的任意两个分量都是负相关的，且： <span class="math display">\[\Cov(X_i,X_j)=-mp_ip_j\]</span> <em>Proof</em>.</p><p>首先，由于多项分布的边缘分布是 <span class="math inline">\(\text{binomial}(m,p_i)\)</span>，所以 <span class="math inline">\(\E X_i=mp_i\)</span>，<span class="math inline">\(\Var X_i=mp_i(1-p_i)\)</span>，<span class="math inline">\(\E X_i^2=\Var X_i+(\E X_i)^2=mp_i(1-p_i+mp_i)\)</span>.</p><p>其次，在 <span class="math inline">\(X_j=x_j\)</span> 的条件下，其余分量是一个多项分布，因此 <span class="math inline">\(X_i\)</span> 服从这个多项分布的边缘分布 <span class="math inline">\(\text{binomial}\left(m-x_j,\frac{p_i}{1-p_j}\right)\)</span>，故 <span class="math inline">\(\E[X_i\mid X_j=x_j]=(m-x_j)p_i/(1-p_j)\)</span>.</p><p>于是： <span class="math display">\[\begin{align}\E[X_iX_j]&amp;=\E[\E[X_iX_j\mid X_j]]&amp;&amp;\text{重期望公式}\\&amp;=\E[X_j\E[X_i\mid X_j]]\\&amp;=\E\left[\frac{X_j(m-X_j)p_i}{(1-p_j)}\right]\\&amp;=\frac{p_i}{(1-p_j)}(m \E X_j-\E X_j^2)\\&amp;=\frac{p_i}{(1-p_j)}(m^2p_j-mp_j(1-p_j+mp_j))\\&amp;=m(m-1)p_ip_j\end{align}\]</span> 进而： <span class="math display">\[\begin{align}\Cov(X_i,X_j)&amp;=\E[X_iX_j]-\E[X_i]\E[X_j]\\&amp;=m(m-1)p_ip_j-mp_imp_j\\&amp;=-mp_ip_j\end{align}\]</span> Q.E.D.</p><h3 id="独立性">1.3 独立性</h3><p>前一篇讲了两个随机变量的独立性，我们将其进一步扩展：设 <span class="math inline">\(\mathbf X_1,\ldots,\mathbf X_n\)</span> 是一列随机向量，其联合 pdf/pmf 为 <span class="math inline">\(f(\mathbf x_1,\ldots,\mathbf x_n)\)</span>，<span class="math inline">\(\mathbf X_i\)</span> 的边缘 pdf/pmf 维 <span class="math inline">\(f_{\mathbf X_i}(\mathbf x_i)\)</span>，若对任意 <span class="math inline">\(x_1,\ldots,x_n\)</span>，都有</p><p><span class="math display">\[f(\mathbf x_1,\ldots,\mathbf x_n)=f_{\mathbf X_1}(x_1)\cdots f_{\mathbf X_n}(x_n)=\prod_{i=1}^nf_{\mathbf X_i}(\mathbf x_i)\]</span> 则称 <span class="math inline">\(\mathbf X_1,\ldots,\mathbf X_n\)</span> 是<strong>相互独立的随机向量</strong>；若每个 <span class="math inline">\(X_i\)</span> 都是一维的，则称 <span class="math inline">\(X_1,\ldots,X_n\)</span> 是<strong>相互独立的随机变量</strong>。</p><blockquote><p>注意：相互独立比两两独立更强，可以构造出两两独立的一组随机向量，但它们并不相互独立。</p></blockquote><p><br/></p><p>二维情形下的许多定理可以直接推广到多维情形：</p><p><strong>定理</strong>：设 <span class="math inline">\(X_1,\ldots,X_n\)</span> 是相互独立的随机变量，<span class="math inline">\(g_1,\ldots,g_n\)</span> 是实值一元函数，则： <span class="math display">\[\E[g_1(X_1)\cdots g_n(X_n)]=\E[g_1(X_1)]\cdots\E[g_n(X_n)]\]</span> <strong>定理</strong>：设 <span class="math inline">\(X_1,\ldots,X_n\)</span> 是相互独立的随机变量，矩母函数分别是 <span class="math inline">\(M_{X_1}(t),\ldots,M_{X_n}(t)\)</span>，令 <span class="math inline">\(Z=X_1+\cdots+X_n\)</span>，则 <span class="math inline">\(Z\)</span> 的矩母函数为： <span class="math display">\[M_Z(t)=M_{X_1}(t)\cdots M_{X_N}(t)\]</span></p><div class="note note-success">            <p>例【伽玛变量和】伽玛分布的矩母函数为 <span class="math inline">\(M(t)=(1-\beta t)^{-\alpha}\)</span>，若 <span class="math inline">\(X_i\sim \text{Gamma}(\alpha_i,\beta)\)</span>，则 <span class="math inline">\(Z=X_1+\cdots+X_n\)</span> 的矩母函数为： <span class="math display">\[M_Z(t)=(1-\beta t)^{-\alpha_1}\cdots(1-\beta t)^{-\alpha_n}=(1-\beta t)^{-(\alpha_1+\cdots+\alpha_n)}\]</span> 故 <span class="math inline">\(Z\sim\text{Gamma}(\alpha_1+\cdots+\alpha_n,\beta)\)</span>.</p>          </div><p><strong>定理</strong>：设 <span class="math inline">\(X_1,\ldots,X_n\)</span> 是相互独立的随机变量，矩母函数分别是 <span class="math inline">\(M_{X_1}(t),\ldots,M_{X_n}(t)\)</span>，令 <span class="math inline">\(Z=(a_1X_1+b_1)+\cdots(a_nX_n+b_n)\)</span>，则 <span class="math inline">\(Z\)</span> 的矩母函数为： <span class="math display">\[M_Z(t)=\left(e^{t\sum b_i}\right)M_{X_1}(a_1t)\cdots M_{X_n}(a_nt)\]</span> <em>Proof</em>. <span class="math display">\[\begin{align}M_Z(t)&amp;=\E e^{tZ}\\&amp;=\E\left[e^{t\sum (a_iX_i+b_i)}\right]\\&amp;=\left(e^{t\sum b_i}\right)\E\left[e^{ta_1X_1}\cdots e^{ta_nX_n}\right]\\&amp;=\left(e^{t\sum b_i}\right)M_{X_1}(a_1t)\cdots M_{X_n}(a_nt)\end{align}\]</span> Q.E.D.</p><div class="note note-success">            <p>例【独立正态随机变量值和仍然服从正态分布】：设 <span class="math inline">\(X_1,\ldots,X_n\)</span> 是相互独立的随机变量，且 <span class="math inline">\(X_i\sim N(\mu_i,\sigma^2_i)\)</span>，则： <span class="math display">\[Z=\sum_{i=1}^n(a_iX_i+b_i)\sim N\left(\sum_{i=1}^n(a_i\mu_i+b_i),\sum_{i=1}^na_i^2\sigma_i^2\right)\]</span> <em>Proof</em>. 回忆 <span class="math inline">\(N(\mu,\sigma^2)\)</span> 随机变量的矩母函数为 <span class="math inline">\(M(t)=\exp(\mu t+\sigma^2t^2/2)\)</span>，于是 <span class="math display">\[\begin{align}M_Z(t)&amp;=\left(e^{t\sum b_i}\right) \exp({\mu_1a_1t+\sigma_1^2a_1^2t^2/2})\cdots \exp({\mu_na_nt+\sigma_n^2a_n^2t^2/2})\\&amp;=\left(e^{t\sum b_i}\right) \exp\left({\sum\mu_ia_it+\sigma_i^2a_i^2t^2/2}\right)\\&amp;=\exp\left[\left(\sum(a_i\mu_i+b_i)\right)t+\left(\sum a_i^2\sigma_i^2\right)t^2/2\right]\\\end{align}\]</span> 故 <span class="math inline">\(Z\sim N\left(\sum(a_i\mu_i+b_i),\sum a_i^2\sigma_i^2\right)\)</span>.</p><p>Q.E.D.</p>          </div><p><strong>定理（独立的充要条件）</strong>：设 <span class="math inline">\(\mathbf X_1,\ldots,\mathbf X_n\)</span> 是一列随机向量，则 <span class="math inline">\(\mathbf X_1,\ldots,\mathbf X_n\)</span> 相互独立当且仅当存在函数 <span class="math inline">\(g_i(\mathbf x_i)\)</span> 使得 <span class="math inline">\((\mathbf X_1,\ldots,\mathbf X_n)\)</span> 的联合 pdf/pmf 可以写作： <span class="math display">\[f(\mathbf x_1,\ldots,\mathbf x_n)=g_1(\mathbf x_1)\cdots g_n(\mathbf x_n)\]</span> <strong>定理</strong>：设 <span class="math inline">\(\mathbf X_1,\ldots,\mathbf X_n\)</span> 是一列独立的随机向量，<span class="math inline">\(g_i(\mathbf x_i)\)</span> 是一元函数，则随机变量 <span class="math inline">\(U_i=g_i(\mathbf X_i)\)</span> 相互独立。</p><h3 id="随机向量变换的分布">1.4 随机向量变换的分布</h3><p>设随机向量 <span class="math inline">\((X_1,\ldots,X_n)\)</span> 的 pdf 为 <span class="math inline">\(f_\mathbf X(x_1,\ldots,x_n)\)</span>，<span class="math inline">\(\mathbf A=\{x:f_\mathbf X(x)&gt;0\}\)</span>. 考察新的随机向量 <span class="math inline">\((U_1,\ldots,U_n)\)</span>，其中 <span class="math inline">\(U_i=g_i(X_1,\ldots,X_n)\)</span>. 设 <span class="math inline">\(A_0,A_1,\ldots,A_k\)</span> 是 <span class="math inline">\(\mathbf A\)</span> 的一个划分且 <span class="math inline">\(P((X_1,\ldots,X_n)\in A_0)=0\)</span>. 对所有 <span class="math inline">\(i=1,\ldots,k\)</span>，变换 <span class="math inline">\((U_1,\ldots,U_n)=(g_1(\mathbf X),\ldots,g_n(\mathbf X))\)</span> 都是从 <span class="math inline">\(A_i\)</span> 到 <span class="math inline">\(\mathbf B\)</span> 的一对一变换，因此对每个 <span class="math inline">\(i\)</span> 都存在从 <span class="math inline">\(\mathbf B\)</span> 到 <span class="math inline">\(A_i\)</span> 的逆变换。记第 <span class="math inline">\(i\)</span> 个逆变换为 <span class="math inline">\(x_1=h_{1i}(u_1,\ldots,u_n),\ldots,x_n=h_{ni}(u_1,\ldots,u_n)\)</span>，则对任意 <span class="math inline">\((u_1,\ldots,u_n)\in\mathbf B\)</span>，它确定了唯一的 <span class="math inline">\((x_1,\ldots,x_n)\in A_i\)</span> 使得 <span class="math inline">\((u_1,\ldots,u_n)=(g_1(x_1,\ldots,x_n),\ldots,g_n(x_1,\ldots,x_n))\)</span>. 记 <span class="math inline">\(J_i\)</span> 为第 <span class="math inline">\(i\)</span> 个逆变换的 Jacobi 行列式： <span class="math display">\[J_i=\begin{vmatrix}\frac{\partial x_1}{\partial u_1}&amp;\frac{\partial x_1}{\partial u_2}&amp;\cdots&amp;\frac{\partial x_1}{\partial u_n}\\\frac{\partial x_2}{\partial u_1}&amp;\frac{\partial x_2}{\partial u_2}&amp;\cdots&amp;\frac{\partial x_2}{\partial u_n}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\frac{\partial x_n}{\partial u_1}&amp;\frac{\partial x_n}{\partial u_2}&amp;\cdots&amp;\frac{\partial x_n}{\partial u_n}\\\end{vmatrix}=\begin{vmatrix}\frac{\partial h_{1i}(\mathbf u)}{\partial u_1}&amp;\frac{\partial h_{1i}(\mathbf u)}{\partial u_2}&amp;\cdots&amp;\frac{\partial h_{1i}(\mathbf u)}{\partial u_n}\\\frac{\partial h_{2i}(\mathbf u)}{\partial u_1}&amp;\frac{\partial h_{2i}(\mathbf u)}{\partial u_2}&amp;\cdots&amp;\frac{\partial h_{2i}(\mathbf u)}{\partial u_n}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\frac{\partial h_{ni}(\mathbf u)}{\partial u_1}&amp;\frac{\partial h_{ni}(\mathbf u)}{\partial u_2}&amp;\cdots&amp;\frac{\partial h_{ni}(\mathbf u)}{\partial u_n}\\\end{vmatrix}\]</span> 假定 <span class="math inline">\(J_i\)</span> 在 <span class="math inline">\(\mathbf B\)</span> 上不恒为 <span class="math inline">\(0\)</span>，则对任意 <span class="math inline">\(\mathbf u\in\mathbf B\)</span>，联合 pdf 可以表示为： <span class="math display">\[f_\mathbf U(u_1,\ldots,u_n)=\sum_{i=1}^k f_\mathbf X(h_{1i}(u_1,\ldots,u_n),\ldots,h_{ni}(u_1,\ldots,u_n))|J_i|\]</span></p><blockquote><p>前面一堆集合划分啥的，只是为了在每个划出来的集合上有逆变换罢了。定理的重点在于，随机向量变换后，新的 pdf 不仅要把变换代入，<strong>还要乘上一个 Jacobi 行列式的绝对值</strong>。</p></blockquote><h2 id="不等式">2 不等式</h2><h3 id="数值不等式">2.1 数值不等式</h3><p><strong>引理</strong>：设 <span class="math inline">\(a,b,p,q\)</span> 为任意正数，且 <span class="math inline">\(p,q\)</span> 满足（显然 <span class="math inline">\(p,q&gt;1\)</span>）： <span class="math display">\[\frac{1}{p}+\frac{1}{q}=1\]</span> 则 <span class="math display">\[\frac{1}{p}a^p+\frac{1}{q}b^q\geq ab\]</span> 当且仅当 <span class="math inline">\(a^p=b^q\)</span> 时等式成立。</p><p><em>Proof</em>. 考察函数 <span class="math display">\[g(a)=\frac{1}{p}a^p+\frac{1}{q}b^q-ab\]</span> 求导令为零： <span class="math display">\[g&#39;(a)=a^{p-1}-b=0\implies b=a^{p-1}\]</span> 又 <span class="math inline">\(g&#39;&#39;(a)=(p-1)a^{p-2}&gt;0\)</span>，故极小值为： <span class="math display">\[\frac{1}{p}a^p+\frac{1}{q}a^{q(p-1)}-a^p=0\quad \text{利用 }1/p+1/q=1\text{ 的条件}\]</span> 又极小值唯一且当且仅当 <span class="math inline">\(b=a^{p-1}\)</span>，也即 <span class="math inline">\(a^p=b^q\)</span> 时达到。</p><p>Q.E.D.</p><p><br/></p><p><strong>Holder 不等式</strong>：设 <span class="math inline">\(X,Y\)</span> 为任意随机变量，<span class="math inline">\(p,q\)</span> 满足 <span class="math inline">\(\frac{1}{p}+\frac{1}{q}=1\)</span>，则： <span class="math display">\[|\E XY|\leq \E |XY|\leq (\E|X|^p)^{1/p}(\E|Y|^q)^{1/q}\]</span> <em>Proof</em>.</p><p>左边：由于 <span class="math inline">\(-|XY|\leq XY\leq |XY|\)</span>，得 <span class="math inline">\(-\E|XY|\leq \E XY\leq \E |XY|\)</span>，即 <span class="math inline">\(|\E XY|\leq \E |XY|\)</span>.</p><p>右边：令 <span class="math display">\[a=\frac{|X|}{(\E |X|^p)^{1/p}}\quad b=\frac{|Y|}{(\E |Y|^q)^{1/q}}\]</span> 则根据引理，有： <span class="math display">\[\frac{1}{p}\frac{|X|^p}{\E |X|^p}+\frac{1}{q}\frac{|Y|^p}{\E |Y|^q}\geq \frac{|XY|}{(\E|X|^p)^{1/p}(\E|Y|^q)^{1/q}}\]</span> 对两边取期望，左边期望为 <span class="math inline">\(1\)</span>，因此： <span class="math display">\[\E|XY|\leq (\E|X|^p)^{1/p}(\E|Y|^q)^{1/q}\]</span> Q.E.D.</p><p><br/></p><p><strong>Cauchy-Schwarz 不等式</strong>：在 Holder 不等式中，取 <span class="math inline">\(p=q=2\)</span>，得： <span class="math display">\[|\E XY|\leq \E |XY|\leq (\E|X|^2)^{1/2}(\E|Y|^2)^{1/2}\]</span> <br/></p><p><strong>协方差不等式</strong>：根据 Cauchy-Schwarz 不等式，有： <span class="math display">\[|\E (X-\E X)(Y-\E Y)|\leq (\E (X-\E X)^2)^{1/2}(\E (Y-\E Y)^2)^{1/2}\]</span> 两边平方，即得到： <span class="math display">\[\Cov(X,Y)^2\leq \Var X\Var Y\]</span> <br/></p><p><strong>Liapounov 不等式</strong>：在 Holder 不等式中，令 <span class="math inline">\(Y\equiv 1\)</span>，则： <span class="math display">\[\E |X|\leq (\E |X|^p)^{1/p}\quad p&gt;1\]</span> 对任意 <span class="math inline">\(1&lt;r&lt;p\)</span>，用 <span class="math inline">\(|X|^r\)</span> 代替上式中的 <span class="math inline">\(|X|\)</span>，则： <span class="math display">\[\E |X|^r\leq (\E |X|^{pr})^{1/p}\]</span> 做变量替换 <span class="math inline">\(s=pr&gt;r\)</span>，得到： <span class="math display">\[(\E |X|^{r})^{1/r}\leq (\E|X|^s)^{1/s}\quad 1&lt;r&lt;s\]</span> <br/></p><p><strong>Minkowski 不等式</strong>：设 <span class="math inline">\(X,Y\)</span> 为任意随机变量，则对任意 <span class="math inline">\(p&gt;1\)</span>，有： <span class="math display">\[[\E|X+Y|^p]^{1/p}\leq [\E |X|^p]^{1/p}+[\E |Y|^p]^{1/p}\]</span> <em>Proof</em>. 由三角不等式 <span class="math inline">\(|X+Y|\leq |X|+|Y|\)</span>，有： <span class="math display">\[\E|X+Y|^p=\E\left[|X+Y||X+Y|^{p-1}\right]\leq \E\left[|X||X+Y|^{p-1}\right]+\E\left[|Y||X+Y|^{p-1}\right]\]</span> 对上式右端两个期望分别使用 Holder 不等式，得到： <span class="math display">\[\E|X+Y|^p\leq (\E |X|^p)^{1/p}\left(\E |X+Y|^{q(p-1)}\right)^{1/q}+(\E |Y|^p)^{1/p}\left(\E |X+Y|^{q(p-1)}\right)^{1/q}\]</span> 其中 <span class="math inline">\(q\)</span> 满足 <span class="math inline">\(\frac{1}{p}+\frac{1}{q}=1\)</span>，即 <span class="math inline">\(q=\frac{p}{p-1}\)</span>. 两边除以 <span class="math inline">\(\left(\E |X+Y|^{q(p-1)}\right)^{1/q}\)</span>，则右边即为所求，左边为： <span class="math display">\[\frac{\E |X+Y|^p}{\left(\E |X+Y|^{q(p-1)}\right)^{1/q}}=\frac{\E |X+Y|^p}{\left(\E |X+Y|^{p}\right)^{1-1/p}}=\left(\E |X+Y|^p\right)^{1/p}\]</span> 亦为所求。Q.E.D.</p><h3 id="函数不等式">2.2 函数不等式</h3><p><strong>凸函数</strong>：如果对任意 <span class="math inline">\(x,y\)</span> 以及 <span class="math inline">\(0&lt;\lambda&lt;1\)</span>，函数 <span class="math inline">\(g(x)\)</span> 都满足 <span class="math inline">\(g(\lambda x+(1-\lambda)y)\leq \lambda g(x)+(1-\lambda)g(y)\)</span>，则称 <span class="math inline">\(g(x)\)</span> 为凸函数；如果 <span class="math inline">\(-g(x)\)</span> 是凸函数，则称 <span class="math inline">\(g(x)\)</span> 为凹函数。</p><p><br/></p><p><strong>Jensen 不等式</strong>：设 <span class="math inline">\(X\)</span> 是任意随机变量，如果 <span class="math inline">\(g(x)\)</span> 是凸函数，则： <span class="math display">\[\E g(X)\geq g(\E X)\]</span> 等号成立当且仅当对于 <span class="math inline">\(g(x)\)</span> 在 <span class="math inline">\(x=\E X\)</span> 处的切线 <span class="math inline">\(l(x)=ax+b\)</span>，有 <span class="math inline">\(P(g(X)=aX+b)=1\)</span>.</p><p><em>Proof</em>. 设 <span class="math inline">\(g(x)\)</span> 在 <span class="math inline">\(x=\E X\)</span> 处的切线为 <span class="math inline">\(l(x)=ax+b\)</span>，由 <span class="math inline">\(g(x)\)</span> 的凸性可知 <span class="math inline">\(g(x)\geq ax+b\)</span>，于是： <span class="math display">\[\E g(X)\geq \E [aX+b]=a \E X+b=l(\E X)=g(\E X)\]</span> Q.E.D.</p><p>若对 <span class="math inline">\(g(x)=x^2\)</span> 使用 Jensen 不等式，得到： <span class="math display">\[\E X^2\geq (\E X)^2\]</span> 若对 <span class="math inline">\(g(x)=1/x\)</span> 使用 Jensen 不等式，得到： <span class="math display">\[\E \frac{1}{X}\geq \frac{1}{\E X}\]</span> <br/></p><p><strong>均值不等式</strong>：设 <span class="math inline">\(a_1,\ldots,a_n\)</span> 均为正数，令： <span class="math display">\[\begin{align}&amp;a_A=\frac{1}{n}(a_1+\cdots+a_n)\\&amp;a_G=(a_1\cdots a_n)^{1/n}\\&amp;a_H=\frac{n}{\frac{1}{a_1}+\cdots+\frac{1}{a_n}}\end{align}\]</span> 则： <span class="math display">\[a_H\leq a_G\leq a_A\]</span> <em>Proof</em>. 我们可以利用 Jensen 不等式完成证明。设随机变量 <span class="math inline">\(X\)</span> 取值范围为 <span class="math inline">\(\{a_1,\ldots,a_n\}\)</span> 且各取值概率相等，由于 <span class="math inline">\(\log x\)</span> 是凹函数，所以： <span class="math display">\[\begin{align}&amp;\log a_G=\frac{1}{n}\sum_{i=1}^n\log a_i=\E[\log X]\leq\log \E X=\log\left(\frac{1}{n}\sum_{i=1}^n a_i\right)=\log a_A\\&amp;\log\frac{1}{a_H}=\log\left(\frac{1}{n}\sum_{i=1}^n\frac{1}{a_i}\right)=\log \E\frac{1}{X}\geq \E \left[\log\frac{1}{X}\right]=-\frac{1}{n}\sum_{i=1}^n\log a_i=-\log a_G=\log\frac{1}{a_G}\end{align}\]</span> Q.E.D.</p><p><br/></p><p><strong>协方差不等式 - Ⅱ</strong>：设 <span class="math inline">\(X\)</span> 是任意随机变量，<span class="math inline">\(g(x),h(x)\)</span> 是任意函数且 <span class="math inline">\(\E g(X),\E h(X)\)</span> 与 <span class="math inline">\(\E g(X)h(X)\)</span> 均存在，</p><ol type="1"><li><p>若 <span class="math inline">\(g(x)\)</span> 是递增函数，<span class="math inline">\(h(x)\)</span> 是递减函数，则 <span class="math display">\[\E[g(X)h(X)]\leq (\E g(X))(\E h(X))\]</span></p></li><li><p>若 <span class="math inline">\(g(x), h(x)\)</span> 同为递增或递减函数，则 <span class="math display">\[\E [g(X)h(X)]\geq (\E g(X))(\E h(X))\]</span></p></li></ol><p>协方差不等式有明显的直观解释：上面两种情形恰好反映了 <span class="math inline">\(g,h\)</span> 之间的负相关和正相关，借助该不等式我们可以直接估计期望，而无需计算高阶矩。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>统计推断</category>
      
    </categories>
    
    
    <tags>
      
      <tag>statistics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>各种函数的hard与soft形式</title>
    <link href="/blog-main/2022/07/25/%E5%90%84%E7%A7%8D%E5%87%BD%E6%95%B0%E7%9A%84hard%E4%B8%8Esoft%E5%BD%A2%E5%BC%8F/"/>
    <url>/blog-main/2022/07/25/%E5%90%84%E7%A7%8D%E5%87%BD%E6%95%B0%E7%9A%84hard%E4%B8%8Esoft%E5%BD%A2%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h2 id="max-与-logsumexp">max 与 logsumexp</h2><p>一个常见的误解是：<span class="math inline">\(\text{softmax}\)</span> 是 <span class="math inline">\(\max\)</span> 的 soft 版本，但其实稍微想一下就知道这是不对的——<span class="math inline">\(\max\)</span> 函数的输出是一个实数，而 <span class="math inline">\(\text{softmax}\)</span> 的输出是一个向量，一个向量怎么可能去近似一个实数呢？</p><p>事实上，<strong><span class="math inline">\(\max\)</span> 函数的 soft 版本是 <span class="math inline">\(\text{logsumexp}\)</span> 函数</strong>： <span class="math display">\[\text{logsumexp}(\vec{x};\tau)=\tau\log\sum_{i=1}^n\exp(x_i/\tau)\]</span> 其中温度系数 <span class="math inline">\(\tau\)</span> 越小，<span class="math inline">\(\text{logsumexp}\)</span> 越接近 <span class="math inline">\(\max\)</span>.</p><div class="note note-secondary">            <p>证明：由于 <span class="math display">\[\begin{align}\tau\log\sum_{i=1}^n\exp(x_i/\tau)&amp;\leq\tau\log\sum_{i=1}^n\exp(\vec{x}_\max/\tau)\\&amp;=\tau\log(n\exp(\vec{x}_\max /\tau))\\&amp;=\tau\log n+\vec{x}_\max\\&amp;\to \vec{x}_\max\quad(\tau \to 0)\end{align}\]</span> 又 <span class="math display">\[\tau\log\sum_{i=1}^n\exp(x_i/\tau)\geq\tau\log[\exp(\vec{x}_\max/\tau)]=\vec{x}_\max\]</span> 所以根据夹逼定理， <span class="math display">\[\lim_{\tau \to 0} \text{logsumexp}(\vec{x};\tau)=\lim_{\tau\to 0}\tau\log\sum_{i=1}^n\exp(x_i/\tau)=\vec{x}_\max\]</span></p>          </div><p>类似的，添加一个负号，<span class="math inline">\(\text{logsumexp}\)</span> 成为 <span class="math inline">\(\min\)</span> 的平滑近似： <span class="math display">\[\text{logsumexp}(\vec{x};-\tau)=-\tau\log\sum_{i=1}^n\exp(-x_i/\tau)\]</span></p><blockquote><p>说句题外话，在许多科学计算包中，<span class="math inline">\(\text{logsumexp}\)</span> 已经被封装为了一个函数，为了避免数值计算问题应尽可能调用它而不是自己从头写一遍。</p></blockquote><h2 id="onehot-与-softmax">onehot 与 softmax</h2><p>考虑到 <span class="math inline">\(\text{softmax}\)</span> 是一个概率向量，即所有维度相加为一，因此它的 hard 版本自然是 <span class="math inline">\([0,\ldots,1,\ldots,0]\)</span> 的形式，也即 <span class="math inline">\(\text{onehot}\)</span> 向量。因此，<strong><span class="math inline">\(\text{softmax}\)</span> 是 <span class="math inline">\(\text{onehot}(\text{argmax})\)</span> 的平滑近似</strong>。 <span class="math display">\[\text{softmax}(\vec{x};\tau)_i=\frac{\exp(x_i/\tau)}{\sum_{j=1}^n\exp (x_j/\tau)}\]</span> 其中温度系数 <span class="math inline">\(\tau\)</span> 越小，<span class="math inline">\(\text{softmax}\)</span> 越接近 <span class="math inline">\(\text{onehot}\)</span>.</p><p><br/></p><p>另外，<span class="math inline">\(\text{softmax}\)</span> 与 <span class="math inline">\(\text{logsumexp}\)</span> 有如下关系： <span class="math display">\[\log\text{softmax}(\vec{x};\tau)_i=x_i/\tau-\log\sum_{j=1}^n\exp(x_j/\tau)=x_i/\tau-\text{logsumexp}(\vec{x};\tau)\]</span></p><h2 id="argmax">argmax</h2><p>鉴于 <span class="math inline">\(\text{argmax}\)</span> 是一个臭名昭著的不可导操作，我们非常希望找到它的可导 soft 形式。由于 <span class="math inline">\(\text{argmax}\)</span> 可以写作： <span class="math display">\[\text{argmax}(\vec{x})=\sum_{i=1}^n i\times \text{onehot}(\text{argmax}(\vec{x}))_i\]</span> 利用上一小节的结论，将 <span class="math inline">\(\text{onehot}(\text{argmax})\)</span> 替换为 <span class="math inline">\(\text{softmax}\)</span> 得到： <span class="math display">\[\text{argmax}(\vec{x})\approx \sum_{i=1}^ni\times \text{softmax}(\vec{x};\tau)_i=\frac{1}{\sum_{j=1}^n e^{x_j/\tau}}\sum_{i=1}^ni\times e^{x_i/\tau}\]</span> 即用 <span class="math inline">\(\text{softmax}\)</span> 向量对下标做加权平均。</p><h2 id="relu-与-softplus">relu 与 softplus</h2><p>由于 <span class="math inline">\(\text{relu}\)</span> 可以用 <span class="math inline">\(\max\)</span> 写出来，因此利用 <span class="math inline">\(\max\)</span> 的平滑近似 <span class="math inline">\(\text{logsumexp}\)</span>，我们可以推导出 <span class="math inline">\(\text{relu}\)</span> 的平滑近似，称为 <span class="math inline">\(\text{softplus}\)</span>： <span class="math display">\[\begin{align}&amp;\text{relu}(x)=\max(0, x)\\&amp;\text{softplus}=\tau\log(1+e^{x/\tau})\end{align}\]</span> <img src="relu.png" width=50% /></p><h2 id="references">References</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>苏剑林. (May. 02, 2015). 《寻求一个光滑的最大值函数 》[Blog post]. Retrieved from https://spaces.ac.cn/archives/3290 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>如何理解与看待在cvpr2020中提出的circle loss？ - 王峰的回答 - 知乎 https://www.zhihu.com/question/382802283/answer/1114719159 <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>苏剑林. (May. 20, 2019). 《函数光滑化杂谈：不可导函数的可导逼近 》[Blog post]. Retrieved from https://spaces.ac.cn/archives/6620 <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[论文阅读]Inductive Biases for Deep Learning of Higher-Level Cognition</title>
    <link href="/blog-main/2022/06/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Inductive-Biases-for-Deep-Learning-of-Higher-Level-Cognition/"/>
    <url>/blog-main/2022/06/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Inductive-Biases-for-Deep-Learning-of-Higher-Level-Cognition/</url>
    
    <content type="html"><![CDATA[<p>论文 link：<a href="https://arxiv.org/abs/2011.15091" class="uri">https://arxiv.org/abs/2011.15091</a></p><p>相关演讲：<a href="https://www.bilibili.com/video/BV18J41167cY?spm_id_from=333.337.search-card.all.click&amp;vd_source=40724707bbd72181096cd78bab835646">Yoshua Bengio: From System 1 to System 2 Deep Learning (NeurIPS 2019)</a></p><p>首先放上一个我做的论文报告的 slide 吧：</p><object data="./my-slide.pdf" type="application/pdf" width="100%" height="1000px"></object><p><br/></p><p>现在正文开始！</p><blockquote><p>注：本文是笔记而非原论文的翻译（但几乎也快是了），掺杂了个人叙述，如有不妥欢迎讨论。</p></blockquote><h2 id="深度学习收敛了吗">1 深度学习收敛了吗？</h2><blockquote><p>Has Deep Learning Converged?</p></blockquote><p>任何一个机器学习入门课程/博客/视频/教程，必然会不厌其烦地强调一点：我们必须将数据集划分为训练集 (training set) 和测试集 (test set)，在需要精细调超参数的场景下还应划分验证集 (validation set)，原因是避免报告了<strong>过拟合</strong>的结果——模型直接记住了训练数据从而在训练集上表现极为优异，但是面对没有见过的数据（测试集）时性能一落千丈。这往往被称为模型的<strong>泛化性</strong> (generalization ability)。</p><p>但是，上述泛化性真的足够「泛」吗？</p><p>众所周知，机器学习研究者的一个共识或假设是<strong>「训练集和测试集应是同分布的」</strong>，但是真实世界往往不这么理想——在实际应用中，模型面对的数据分布可能会与训练数据不同、可能会有很多干扰、可能会随着时间推移而变化……因此，如果我们希望让现有的机器智能向人类智能迈步前进，就必须解决一个问题——如何让模型具有对<strong>分布外</strong>数据的泛化能力？</p><p>Anirudh Goyal 和 Yoshua Bengio 的这篇论文的主要假设 (hypothesis) 是——<strong>为了从分布内泛化走向分布外泛化，我们需要向深度学习算法中加入更多的归纳偏置 (inductive biases)</strong>。论文希望通过研究人类在认知活动中所利用的归纳偏置，帮助深度学习进一步的发展。</p><p>那么当下众多表现优异的机器学习/深度学习模型和人类智能的差距在哪里呢？这些模型往往只在某一个特定的任务上表现很好，但人类智能恰恰相反——人们能够用一种统一 (unified) 的方式理解周围的环境，并在面对新任务时利用已有知识快速地适应。有相关研究使用多个数据集训练，每个数据集从某个角度揭示事物，但这没有从本质层面解决问题。作者认为，要真正解决问题，我们必须抛弃「训练集和测试集同分布」的假设，并转而寻求新的假设——不同任务和不同数据分布之间是怎样联系的。</p><p>这也许也意味着，我们需要找到一种方法跳出统计学习的框架，不是把数据集看成一组独立同分布的数据，而是在一个动态的变化中找寻数据背后本质的东西。</p><p>当然，这不是否定大量、多样的数据的作用——它们无疑是使得现在的深度学习取得巨大的进展的重要推动力。只是说，要让机器像人类一样具有快速适应新环境的能力，我们需要诉求于数据以外的东西，比如一种更高层次的知识表示方式。本论文指出一种可行的思路是：<strong>将知识分解为小的片段，当使用时根据需要动态地将这些片段组合起来</strong>。这种特点在人类自然语言中表现得非常明显。</p><h2 id="关于归纳偏置">2 关于归纳偏置</h2><blockquote><p>About Inductive Biases</p></blockquote><p>一个非常有意思的问题：我们的训练目标既然是让模型尽可能拟合训练集，那它的泛化能力究竟是从哪里来的呢？答案是一些先验的偏好 (preferences) 或称归纳偏置 (inductive biases)。目前已经在各种神经网络中使用的偏置如下表所示：</p><p><img src="table1.png" width=80% /></p><p><strong>归纳偏置与算法</strong></p><p>很多模型的架构、优化目标、算法的设计等等本身就隐含了归纳偏置。典型的例子像是卷积隐含了平移不变性、正则化项隐含了更简单的模型……但把某些归纳偏置设计到机器学习方法之中不是一件容易的事，这往往是机器学习论文的核心贡献。</p><p><strong>归纳偏置与数据</strong></p><p>如果缺少足够强大的归纳偏置，可以用更多的数据来弥补。当数据集非常大的时候，归纳偏置的作用也许会变得较小，这时一些迁移学习的结果更能体现出归纳偏置的优势。</p><p><strong>归纳偏置让学习更简单</strong></p><p>诸如 attention, residual connections 等方法让训练变得更简单、收敛更快。</p><p><strong>智能体，序列决策，非稳定数据流</strong></p><p>在强化学习的第一课，我们知道了强化学习与其他有监督、无监督学习最不同的一点就是：其数据来源不是稳定的独立同分布数据，而是时序的、不稳定的数据流，而智能体的任务就是在这样的环境下做出决策。也许人们会认为在其他任务中，比如一个简简单单的目标检测，我们不需要考虑这些东西。但如果我们想建立能够适应各种数据分布的模型，在分布变化的序列数据上训练是必要的。</p><p><strong>迁移学习 (Transfer Learning) 与持续学习 (Continual Learning)</strong></p><p>要谈论迁移学习，我们必须明确的两点是：1. 迁移的任务和源任务之间有什么共同点，有什么是稳定不变的？2. 它们有什么区别，这样的变化是怎样产生的？作者进而联想到了元学习 (meta-learning)：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs pseudocode">Outer loop (meta-learning) [slow learning, stable and stationary]<br>Inner loop (regular-learning) [fast learning, task-specific]<br></code></pre></td></tr></table></figure><p><strong>系统性泛化 (systematic generalization) 和分布外泛化 (OOD generalization)</strong></p><p>Systematic generalization 指人们可以通过组合一个新事物具有的特征来了解新事物。一个有趣的点是，这种认知方式甚至能够泛化到根本不存在的事物上——即便你能收集到无限多的数据，也无法看到这种不存在的组合。人类智能用这种组合式的归纳偏置，从有限的数据学习并泛化到更大的概念去。</p><h2 id="基于高层次认知的归纳偏置是通向泛化-ood-系统的一条路径">3 基于高层次认知的归纳偏置是通向泛化 OOD 系统的一条路径</h2><blockquote><p>Inductive biases based on higher-level cognition as a path towards systems that generalize OOD</p></blockquote><p>我们要从人类智能中学习，必然离不开一些认知神经科学的研究。本节将从一些神经科学的发现出发，启发我们为深度学习找到归纳偏置。</p><h3 id="大脑中有意识处理-vs-无意识处理">3.1 大脑中有意识处理 vs 无意识处理</h3><blockquote><p>Conscious vs Unconscious Processing in Brains</p></blockquote><p>在人类的大脑中，信息处理有两种模式：</p><ol type="1"><li>无意识 (unconscious) 的处理：也称习惯性 (habitual) 的处理、system 1 cognition，是默认的行为</li><li>有意识 (conscious) 的处理：也称控制性 (controlled) 的处理、system 2 cognition，是在出现新的情况时的行为，需要额外的注意力</li></ol><p>对应的，知识也分为两种：</p><ol type="1"><li>system 1 knowledge：直觉上的、隐式的知识</li><li>system 2 knowledge：能够显式地表达、交流和推理的知识</li></ol><p>对于目前的深度学习方法，它们擅长于 system 1 task，即能在短时间中（GPU 上）在训练它们的数据上得到好的结果；另一方面，人类擅长于 system 2 task，具有快速学习和系统性泛化的能力。因此，我们希望下一代的深度学习技术能结合这两方面：system 1 / implicit / unconscious + system 2 / explicit / conscious. 接下来的部分将提出一些有助于提升 system 2 能力的 inductive bias.</p><h3 id="高层次表征将可用语言表达的概念描述为语义变量">3.2 高层次表征将可用语言表达的概念描述为语义变量</h3><blockquote><p>High-level Representations Describe Verbalizable Concepts as Semantic Variables</p></blockquote><p><strong>本论文提出的最重要的归纳偏置：「有意操控的高层次变量基本都是可用语言表达的」</strong>。这里的高层次语义变量可以想象为一个单词（或短语，反正就是描述一个事物）。从这个归纳偏置中，我们可以衍生出其他的归纳偏置。当然，这些归纳偏置并不能涵盖整个世界的方方面面，只需要涵盖能够用语言表达的方面即可。其余的部分可以由 system 1 来做（例如 encoder-decoder 将低层次信息映射到高层次变量）。</p><p>这个归纳偏置有什么用呢？我们可以做出这样的假设：<strong>存在一个简单的有损的映射，从高层次语义表征映射到自然语言的表示</strong>。实操中，我们可以用 attention 机制来产生这个映射。这样，底层的自然语言表示可以看作是对高层语义表征的一个弱监督，只要二者有简单的关系即可。</p><h3 id="语义变量具有因果作用关于它们的知识是模块化的">3.3 语义变量具有因果作用，关于它们的知识是模块化的</h3><blockquote><p>Semantic Variables Play a Causal Role and Knowledge about them is Modular</p></blockquote><p>生物学的研究表明，大脑由模块化的方式组成，有一组专家模块之间进行稀疏的交流。受此启发，我们可以将知识分解为可重组的片段，所谓稀疏，是指任意时刻只会有少量的片段被调入，然后选择哪些模块将与哪些知识发生作用。至于如何将知识进行分解，作者认为可以从理解世界如何运作的因果观点中获取灵感。</p><p>在结构化因果模型中，一个常见的假设是知识应该被分解成独立的 mechanisms，其中每个 mechanism 和一个或多个影响变量有关，而一个 mechanism 不能提供关于 mechanism 的任何信息。这样如果一个 mechanism 发生变化，其他的 mechanism 不会发生变化。</p><h3 id="分布在语义空间的局部变化">3.4 分布在语义空间的局部变化</h3><blockquote><p>Local Changes in Distribution in Semantic Space</p></blockquote><p>考虑一个智能体，假定环境在任何时刻都处于某种状态，那么其 observation 的分布的不稳定性来源于哪里？</p><ol type="1"><li>环境、智能体的策略没有收敛，比如玩家开始玩一个新游戏</li><li>该智能体或其他智能体的因果干预，比如在一个迷宫中锁住一些门（可能导致最优策略急剧变化）</li></ol><p>人类用自然语言描述这些变化时，往往用几个词就足以解释清楚。这意味着以下假设可以视为一个归纳偏置：<strong>「大多数分布的变化可以在适当的语义空间中被定位在局部」</strong>。这里的重点是「局部」，即只有少数的变量或机制需要因变化而改变。</p><h3 id="世界的稳定属性">3.5 世界的稳定属性</h3><blockquote><p>Stable Properties of the World</p></blockquote><p>我们前面讨论了不稳定因素导致分布的变化，但世界具有很多稳定的方面的。我们希望尽可能多的知识是稳定的，智能体可以在整个生命周期中学习它们并最终收敛，同时希望智能体快速学习那些非稳定的部分。从这个角度，我们也看到了两种学习速度——和上文中 meta-learning 涉及到的类似。根据这些讨论，作者提出归纳偏置：<strong>「学习的速度应该有几种，稳定的方面学得更慢，不稳定的方面学得更快，并在快速变化之中发现稳定的方面」</strong>。</p><h3 id="语义变量空间中的稀疏因子图">3.6 语义变量空间中的稀疏因子图</h3><blockquote><p>Sparse Factor Graph in the Space of Semantic Variables</p></blockquote><p>这一节提出的归纳偏置是：<strong>「高层次概念之间的联合分布可以被稀疏因子图 (sparse factor graph) 所表示」</strong>。这里的重点是「稀疏」，因为任何联合分布都可以被因子图表示。</p><p>用语言表达知识就满足稀疏性：一句话中仅包含非常少的变量，但它却能表达出很多知识。这是因为这些变量具有很强的语义信息。相反，像素空间就不满足这样的稀疏性：很难从三个像素推导出某个像素的值。</p><p>有些研究者的工作将高层次的变量假设为边缘独立的，即联合分布可以被分解为独立边缘分布。但作者认为这背离了深度学习的初衷。这些高层次变量捕捉的语义特征可以用自然语言表达，因此应该具有稀疏的依赖关系。</p><h3 id="变量实例和可重用的知识片段">3.7 变量，实例和可重用的知识片段</h3><blockquote><p>Variables, Instances and Reusable Knowledge Pieces</p></blockquote><p>作者认为，不同于在因子图中分别定义特定的因子，我们应该定义一种「广义因子」(generic factors)，或称因子模板 (factor templates)、模式 (schemas) 等，是一种具有量词的类概率逻辑规则，类似于传统 AI 中的产生式规则。当我们提到“小明饿了会去吃饭”时，应该将其推广到“人饿了回去吃饭（一定概率）”。基于此，作者提出归纳偏置：<strong>「指定变量之间以来关系的独立 mechanisms 应是通用的，即可以以多种形式被实例化」</strong>。因此，因子图中我们不是存储某一个具体的实体，而是通用的模式。</p><h3 id="学习或推理的因果链往往非常短">3.8 学习或推理的因果链往往非常短</h3><blockquote><p>Relevant causal chains (for learning or inference) tend to be very short</p></blockquote><p>大脑将感知到的输入划分为若干事件，并在需要时根据当前情况有选择性从遥远的过去的拉出相应的信息。基于此，作者提出归纳偏置：<strong>「学习或推理的因果链被分解成短的事件因果链，这些事件在时间上可能很远，但通过语义变量的高层次因子图联系起来」</strong>。</p><h3 id="依赖于场景的处理包括目标自顶而下的影响和自底而上的竞争">3.9 依赖于场景的处理包括目标、自顶而下的影响和自底而上的竞争</h3><blockquote><p>Context-dependent processing involving goals, top-down influence, and bottom-up competition</p></blockquote><p>人类感知包含自顶而下和自底而上两种信号，前者包括相关场景和先验认知，后者包括真正感知到的事物。人类智能能够动态地结合二者，从而对扰动和噪声有鲁棒性。另外，我应该在处理的每一个层级、计算的每一个阶段都将自顶而下和自底而上的信号结合起来。综上，作者提出关于架构的归纳偏置：<strong>「自顶而下的环境信息和自底而上的感知信号应该在于低层次和高层次表征相关的计算的每一个层级动态地结合起来」</strong>。</p><h3 id="从计算机编程获取的灵感">3.10 从计算机编程获取的灵感</h3><blockquote><p>Inspiration from Computer Programming</p></blockquote><p>直到现在我们都在从人类认知学中寻找归纳偏置，但编程语言也可以是一个启发点。</p><p>编程语言与人类自然语言相近，且具有语义信息，并总能够规约到机器指令。同时，它们为使用少量可重用的代码完成复杂功能提供了有效的抽象。</p><ul><li><p><strong>类和对象</strong>：已有相关的研究在探索，让循环神经网络中不同状态的模块动态地学习何时以及怎么共享参数。</p></li><li><p><strong>递归</strong>：目前的许多网络并没有一个简单的方法学习处理递归方程。例如，前向传播网络（MLPs, convnets, transformers）在层与层之间没有共享参数，使得递归非常困难。</p></li><li><p><strong>函数具有命名和特定类型的参数</strong>：</p><ul><li>深度学习中的 key-value 注意力机制可以看做编程语言中用对命名变量做 hard 选择的 soft 版本。</li><li>在支持变量类型的语言中，参数类型需要和预期相匹配，query 和 key 可以看做预期类型和实例的实际类型。</li></ul><p>如果 system 2 神经网络中的各个模块可以用具有命名和特定类型参数的函数表示，那么就可以将它们与具有预期形式的输入绑定，从而实现系统性泛化 (systematic generalization)。</p></li></ul><h2 id="因果关系的知识">4 因果关系的知识</h2><blockquote><p>Declarative Knowledge of Causal Dependencies</p></blockquote><p>一个概率统计模型能捕获单一的联合分布，而一个因果模型能捕获一族联合分布，每个对应于不同的干扰，它修改其他未受干扰的分布。举个例子，概率论告诉我们 <span class="math inline">\(P(A,B)=P(A)P(B\mid A)=P(B)P(A\mid B)\)</span>，但只有其中一个对应正确的因果结构，比如 <span class="math inline">\(A\)</span> 表示海拔而 <span class="math inline">\(B\)</span> 表示温度。</p><p><strong>前置知识</strong>：贝叶斯网络中的每个节点表示一个随机变量，节点之间构成有向无环图 (DAG)，且满足给定某节点所有邻居的条件下，该节点与其他节点独立： <span class="math display">\[p(X_1,X_2,\ldots,X_n)=\prod_{i=1}^mp(X_i\mid \textbf{PA}_i)\]</span> <strong>结构化因果模型 (Structural causal models, SCMs)</strong>： <span class="math display">\[X_i:=f_i(X_{pa(i,C)},N_i)\quad\forall i\in\{1,\ldots,M\}\]</span> 其中，<span class="math inline">\(f_i\)</span> 是一个确定的函数，<span class="math inline">\(N_1,\ldots,N_M\)</span> 是联合独立的一组噪声，<span class="math inline">\(pa(i,C)\)</span> 是变量 <span class="math inline">\(i\)</span> 在设置 <span class="math inline">\(C\)</span> 下的父节点（直接原因），<span class="math inline">\(C\in\{0,1\}^{M\times M}\)</span>，<span class="math inline">\(c_{ij}=1\)</span> 表示节点 <span class="math inline">\(j\)</span> 是节点 <span class="math inline">\(i\)</span> 的父节点（即 <span class="math inline">\(X_j\in X_{pa(i,C)}\)</span>）。因果结构学习指从观察和干预研究中恢复真实的 <span class="math inline">\(C\)</span>.</p><p><strong>干预</strong>：没有实验，或干预，在一个纯观察的设置下，因果图只能在马尔可夫等价类的程度上被区分开来。为了得到真正的因果图，需要进行干预性实验。</p><h3 id="独立因果机制">4.1 独立因果机制</h3><blockquote><p>Independent Causal Mechanisms (ICM)</p></blockquote><p><em>ICM Principle</em>：一个复杂的模型，可以被视为由若干互相独立的机制构成的，即任一机制不应给其他机制带来任何信息或影响。</p><p><strong>因果因子图 (Causal Factor Graph)</strong>：作者指出，有向图模型、甚至是结构化因果模型 (SCMs)，也许都与 ICM 的思想不一致，而因子图的特定形式（有向边代表因果方向）更加合适。</p><h3 id="探寻因果干预导致的分布改变">4.2 探寻因果干预导致的分布改变</h3><blockquote><p>Exploit changes in distribution due to causal interventions</p></blockquote><p><strong>Nature doesn't shuffle examples.</strong> 我们常常做的事情是将获取的数据打乱，从而得到 iid 的数据，但我们从自然界获取的数据不是 iid 的，我们应该利用其中的非稳定性而不是摧毁它。</p><h3 id="深度学习的挑战">4.3 深度学习的挑战</h3><blockquote><p>Challenges for Deep Learning</p></blockquote><p>机器学习中的许多因果关系的工作假设因果变量本身具有已知的语义或是被观察到的。但是在实际中，一个试图了解其环境的 AI 智能体只能获取低层次变量（例如图片的像素和马达动作），因此它需要共同发现抽象的高层次表征和高层次因果关系。</p><h3 id="元学习因果关系ood-泛化和快速迁移学习之间的关系">4.4 元学习、因果关系、OOD 泛化和快速迁移学习之间的关系</h3><blockquote><p>Relation between meta-learning, causality, OOD generalization and fast transfer learning</p></blockquote><p>Bengio 举过一个例子。假设 <span class="math inline">\(A,B\)</span> 是两个随机变量，分别具有 <span class="math inline">\(N\)</span> 个取值。我们假设 <span class="math inline">\(A,B\)</span> 是相关的，目标是确定因果图是 <span class="math inline">\(A\to B\)</span> 还是 <span class="math inline">\(B\to A\)</span>. 注意该因果图不能从单一的分布 <span class="math inline">\(p\)</span> 观察到的数据中确定，因为它们同属一个马尔可夫等价类。为了区分这两种情形，除了利用分布 <span class="math inline">\(p\)</span> 的样本，还要利用转移分布 <span class="math inline">\(\tilde p\)</span> 的样本。</p><p>相关研究显示，在正确的因果图上模型能更快的适应到转移分布 <span class="math inline">\(\tilde p\)</span>.</p><h3 id="动作和承受力是因果模型的一部分">4.5 动作和承受力是因果模型的一部分</h3><blockquote><p>Actions and affordances as part of the causal model</p></blockquote><p>理解因果关系是人类认知中的重要部分。现实生活中，智能体除了需要从低层次观察中发现因果变量和关系以外，还需要了解高层次意图和动作如何与低层次观察以及对高层次因果变量的干预联系起来的。</p><p>这个观点的一个版本在心理学中称为承受力 (affordance)。学习承受力（作为智能体可以怎样影响环境和其他智能体的表征）比学习数据分布更强大。</p><p>目前，对于深度强化学习的许多表征学习方法来说，感知子系统首先收集感知信息建立起一个对环境的内部表示，然后将其和过去经验的表示一起用来决定行动方案。但与环境的持续行互动往往不允许智能体停下来思考、建立对周围环境的完整知识；相反，智能体应该准备好在短时间内进行调整，执行仅部分准备好的行动。</p><h2 id="高层次认知的生物学启发">5 高层次认知的生物学启发</h2><blockquote><p>Biological Inspiration and Characterization of High-Level Cognition</p></blockquote><h3 id="人工智能研究和认知神经科学之间的协同作用">5.1 人工智能研究和认知神经科学之间的协同作用</h3><blockquote><p>Synergy between AI research and cognitive neuroscience</p></blockquote><p>我们可以从人类的认知神经科学中寻求灵感，促进人工智能的发展；同时，人工智能模型可以推动对神经机制的新认识，形成一个良性循环。</p><h3 id="注意力机制">5.2 注意力机制</h3><blockquote><p>Attention</p></blockquote><p>注意力机制是指依次选择对什么做什么计算。</p><ul><li><p><strong>基于内容的软注意力 (Content Based Soft Attention)</strong>：软注意力进行软的选择——我们对前一级的计算结果进行凸的组合，组合的权重来自于 softmax，体现出每个元素的 key vector 和某 query vector 的匹配程度。Attention 向网络引入了 non-local 的归纳偏置，使之能够推断长距离 (long-range) 依赖关系。Attention 也使得网络在集合而非向量（向量的各维是有序的）上操作。典型的 Transformers 中的注意力为： <span class="math display">\[\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V\]</span></p></li><li><p><strong>作为动态连接的注意力 (Attention as dynamic connections)</strong>：注意力可以被想象为在若干计算块之间的动态连接，而在传统神经网络中，连接是固定的。</p></li><li><p><strong>注意力实现了变量绑定 (Attention implements variable binding)</strong>：当每个因子的输入和输出都是一个对象或实体集合（每个对象或实体与一个 key 和一个 value vector 相关联），我们有一种类似于操纵编程语言变量的广义对象处理机制——将其看作可交换的函数参数。在传统神经网络中，一个神经元对固定的输入变量做计算，而有了 key-value 注意力后，一个神经元可以动态地选择哪些变量（具有 key/value vector）将被用来作为什么参数（每个参数有不同的 query embedding）。就好似编程传参时进行名字或类型匹配一样。</p></li></ul><h3 id="大脑和神经网络中的模块化">5.3 大脑和神经网络中的模块化</h3><blockquote><p>Modularity in the brain and in nueral networks</p></blockquote><p>机器学习的目标是学习以一种灵活的方式去理解世界并与之互动。在每一时刻，不是所有可观察环境中的元素都会发生变化，因此对世界的方方面面建模是浪费的。因此我们需要模块化。</p><p>动态选择模块来解决一个任务对获取系统性泛化看起来是合适的。每个模块都能捕捉到一个知识片段，而有意识的处理过程只需即时选择那些目前相关的、需要连贯组合的模块。</p><h3 id="全局工作空间理论">5.4 全局工作空间理论</h3><blockquote><p>Global workspace theory, GWT</p><p>以下内容摘录自 <a href="https://zh.wikipedia.org/wiki/%E5%85%A8%E5%B1%80%E5%B7%A5%E4%BD%9C%E7%A9%BA%E9%97%B4%E7%90%86%E8%AE%BA">Wikipedia</a>。</p><p>全局工作空间理论（英语：Global workspace theory，GWT）是美国心理学家伯纳德·巴尔斯提出的意识模型。该理论假设意识与一个全局的“广播系统”相关联，这个系统会在整个大脑中广播资讯。大脑中专属的智能处理器会按照惯常的方式自动处理资讯，这个时候不会形成意识。当人面对新的或者是与习惯性刺激不同的事物时，各种专属智能处理器会透过合作或竞争的方式，在全局工作空间中对新事物进行分析以获得最佳结果，而意识正是在这个过程中得以产生。</p><p>GWT可以用“剧院比喻”来解释。在“意识剧场”中，“选择性注意的聚光灯”会在舞台上照出一个照明圈。这个照明圈揭示了意识的内容，演员们进进出出，发表演讲或是相互交流。未被照亮的观众则在黑暗中（即无意识）观赏戏剧演出。在幕后，还有导演（执行程序）、舞台师、编剧、场景设计师等等。它们塑造了待在照明圈的可见性活动，但自身却不露面。</p></blockquote><p>GWT 启发了本论文中的许多归纳偏置。</p><ul><li><p><strong>通过一个共享的工作空间实现一致性 (Coherence through a shared workspace)</strong></p><p>GWT 主张一个共享的表征（黑板/工作空间），可以由任一智能处理器修改并广播给其他处理器。GWT 表明了一种转瞬即逝的记忆能力，在任何特定的时刻，只有一个一致的内容可以占主导地位。作者假设这是因为高层次语义变量之间联合分布因子图是稀疏的，因此推理的每一步只有少数变量需要同步。</p></li><li><p><strong>串行和并行计算 (Serial and Parallel Computations)</strong></p><p>不同模块的计算是并行的，但当它们需要与其他模块沟通时，信息会经过一个路由瓶颈。瓶颈的作用是选择少量的元素（知识片段）形成一个连贯的解释，广播给其他模块。由于每一步只选择少量元素，所以推理过程通常需要多步完成，这导致 system 2 计算的串行性（相比 system 1 计算的高度并行性）。</p></li></ul><h3 id="口头报告和基础语言学习">5.5 口头报告和基础语言学习</h3><blockquote><p>Verbal Reporting and Grounded Language Learning</p></blockquote><p>有意识的内容是通过报告来揭示的，这表明有意识操纵的高层次变量与语言密切相关。然而，我们大脑中知道的很多东西不容易用语言描述，它们构成了 system 1 的内容。这意味着 system 2 （可用语言表述）的知识是不完整的——词语大多只是 system 1 中语义变量的指针。这意味着仅用大量语料训练模型，不足以建立起能理解句子含义的系统。自然语言理解系统有必要以一种将自然语言与它所指的内容相结合的方式进行训练，这是基础语言学习的想法。但是本论文的讨论表明，仅仅是被动地观察事物和它们的口头描述可能是不够的：为了捕捉人类所理解的因果结构，智能体有必要与环境主动交互，进而发现因果结构。</p><h3 id="慢处理和解决分布外问题">5.6 慢处理和解决分布外问题</h3><blockquote><p>Slow processing and out-of-distribution problem solving</p></blockquote><p>人类经常面对自身或其他智能体的动作而导致的环境频繁变化。大多数时候，人类遵循其习惯性策略，但在处理不熟悉的环境时，往往会采用 system 2 认知。它使人类能够以非常强大的能力泛化到分布外的情况。这意味着 system 2 对面对变化时的灵活性和稳健型至关重要。</p><h3 id="模块间的语言和通信拓扑结构">5.7 模块间的语言和通信拓扑结构</h3><blockquote><p>Between-Modules Interlingua and Communication Topology</p></blockquote><p>如果大脑是由不同的模块组成的，那么我们需要思考它们之间是用怎样的语言交流的。GWT 的瓶颈可能会迫使这种语言的出现——例如模块 A 收到“有火”信息可能来自于其他任何模块（例如模块 B 通过嗅觉探测到火，模块 C 通过视觉探测到火），则模块 B 和模块 C 需要一种兼容的表示通过 GWT 瓶颈广播给 A.</p><p>然而，GWT 瓶颈并不是模块之间交流的唯一途径。大脑既使用固定的局部连接，也使用全局广播系统。这种分层现象在视觉中也存在（从感知到像素到物体识别），并且在卷积网络中已经得到了非常成功的使用。因此，我们需要考虑在深度网络中结合不同类型的通信方式：1. 靠近的模块可能直接通信而不使用全局通道；2. 远离的模块可以通过全局通道交换信息。</p><p>另外，GWT 中的工作存储区不仅仅是一个缓冲区，它还代表了不同模块都应该一致相信的变量。</p><h3 id="推理-vs-陈述性的知识">5.8 推理 vs 陈述性的知识</h3><blockquote><p>Inference versus declarative knowledge</p></blockquote><p>我们讨论过的知识表示有两种形式：陈述性知识（特别是因果关系和环境动态，具有明确的因果图结构），以及推理机制（特别是与模块相互交流以解决问题、回答问题、想象解决方案）。标准的图模型只代表陈述性知识，通常需要代价大的迭代计算（如蒙特卡洛马尔科夫链）来进行近似推理。然而，大脑需要快速的推理机制。只用陈述性知识（图模型）进行推理是非常灵活的（任何形式的“给定其他变量或想象的干预，预测一些变量”的问题都可以被回答），但也非常缓慢。我们还知道，在 system 2 被反复要求处理新情况后，大脑倾向于将这些反应模式烙在习惯性的 system 1 电路中，这些电路可以更快、更准确地完成工作，但已经失去了一些灵活性。这可能是 system 1（快速、并行、近似、不灵活的推理）和 system 2（较慢、串行、但更灵活的推理）之间的重要差异。</p><h3 id="根据相关事件序列进行推理">5.9 根据相关事件序列进行推理</h3><blockquote><p>Reasoning through Sequences of Relevant Events</p></blockquote><p>时间信息处理任务通常要求将连续的时间流分割成不同的时间间隔或选定的事件序列。在基于时钟的分割中，边界在时间上的间隔是相等的，从而形成固定的时间间隔；在基于事件的分割中，边界取决于环境的状态，导致可变的时间间隔。认知科学和人工智能中的时间信息处理模型通常是前者（基于时钟）。然而，基于事件的分割可以大大简化时间信息处理的任务。一个与世界交互的智能体无法实时地处理所有信息，某种定向机制可以在相关刺激出现时提醒智能体（它不需要知道应该怎么处理，只需要探测到相关刺激的出现），使智能体能够处理和回应刺激。这个定向机制就实现了基于事件的分割。基于事件的分割可以解释诸如熟悉的路线感觉比陌生的路线更短的现象。</p><h2 id="最近和正在进行的工作">6 最近和正在进行的工作</h2><blockquote><p>Recent and Ongoing Work</p></blockquote><h3 id="recurrent-independent-mechanisms-goyal-et-al.-2019">6.1 Recurrent Independent Mechanisms (Goyal et al., 2019)</h3><p>RIMs 的灵感来自于稀疏因子图假设（第 3.6 节）和因果知识模块化归纳偏置（第 3.3 节）所表明的「知识可分解为小的可交换片段」。</p><h3 id="learning-to-combine-top-down-and-bottom-up-information-mittal-et-al.-2020">6.2 Learning to combine top-down and bottom-up information (Mittal et al., 2020)</h3><p>这项工作基于与 RIMs 相同的归纳偏置，但增加了第 3.9 节中讨论的关于需要结合自上而下和自下而上信息流的归纳偏置。</p><h3 id="object-files-and-schemata-goyal-et-al.-2020">6.3 Object Files and Schemata (Goyal et al., 2020)</h3><p>出了RIMs 中的归纳偏置，这个架构包括了 3.7 节中对通用知识（规则、模式）的归纳偏置，这些知识可以被实例化到不同的对象上。</p><h3 id="sparse-attentive-backtracking-ke-et-al.-2018">6.4 Sparse Attentive Backtracking (Ke et al., 2018)</h3><p>理想情况下，注意力机制应该只选择可能相关的记忆中的几个。这样做的一个方法是，根据一些适当学习的注意力分数，只关注 top-k 相关记忆。</p><p>这项工作利用了 3.8 节相关因果链非常短的归纳偏置，尽管它没有明确地处理因果关系，且使用反向传播的形式进行分配 credit。</p><h3 id="a-meta-transfer-objective-for-learning-to-disentangle-causal-mechanisms-bengio-et-al.-2019">6.5 A meta-transfer objective for learning to disentangle causal mechanisms (Bengio et al., 2019)</h3><p>本文作者研究了 3.4 节介绍的从因果关系研究中得出的关于分布变化的归纳偏置。工作表明，无论是理论上还是通过模拟，这样的假设确实有助于在干预之后更快地适应。</p><h3 id="learning-neural-causal-models-from-unknown-interventions-ke-et-al.-2019">6.6 Learning neural causal models from unknown interventions (Ke et al., 2019)</h3><p>本文扩展了前一篇论文，将同样的归纳偏置从具有两个因果变量的生成模型扩展到具有任意数量的因果变量的生成模型。为了避免可能的因果图数量的指数级增长，本文提出将图上的置信度分布因子化（逐渐适应），每条有向边有一个自由参数。此外，论文发现，推断哪个变量受到了干预有助于训练的收敛。它还提出了一种方法，将关于一些边的先验知识与对缺失的边的学习相结合。</p><h2 id="展望未来的项目">7 展望未来的项目</h2><blockquote><p>Projects Looking Forward</p></blockquote><p>本文提出的观点仍处于早期阶段，依旧存在许多开放的问题，作者在此强调几个问题：</p><ul><li>剩下的一个很大的挑战是，共同学习一个大规模 encoder（低层次像素映射到高层次变量）和这些高层次变量的因果模型。一个理想的做法是基于模型的强化学习，其中因果模型将学习系统的随机性动态。Bengio 等人在小规模上（两个因果变量）做了研究，并使用了一个保证 Jacobian 矩阵没有奇异值 1 的 encoder 来避免崩溃。为了避免崩溃，一种可能的做法是在高层次使用对比损失 (contrastive loss)。</li><li>另一个主要的挑战是将陈述性知识表示（如结构化因果模型）和推理机制（也许用注意力和模块化实现，如 RIMs 及其变体）统一在一个架构中。现有的关于变分自编码器的工作可以作为灵感（在这种情况下，编码器是推理机，解码器是因果模型）。</li><li>目前大多数深度学习模型使用固定的参数共享和固定的有规律的内存访问模式，这很适合现代计算硬件（如 GPU 和 TPU）依靠 SIMD 并行。然而，本文所描述的注意力驱动的计算形式可能需要动态的、不规则的和稀疏的内存访问和参数共享，这并不适合 GPU，并且 minibatch 计算难以并行化。解决这个问题可能需要在神经架构、底层编程和硬件设计方面进行创新。</li><li>人类计划的方式与目前在基于模型的 RL（或基于 MCTS 和价值函数的 AlphaZero 等）中使用的方法非常不同。人类似乎利用了关于因果因子图的稀疏性的归纳偏置，以及抽象空间的推理序列可能非常短的事实。这意味着，当人类计划时，他们不建立完整状态的轨迹，而是建立部分状态的轨迹，其中只考虑状态的某些方面（变量）。此外，他们不会为每一个离散的时间步骤展开未来的轨迹，而是直接学习如何将时间上遥远的事件联系起来。当我们计划时，我们可以考虑新情况的可能性，如果一个模型错过了因果结构的重要方面，它可能不能很好地概括这些新的变化，计划可能从根本上高估或低估了一些新的可能性。</li><li>我们真的希望在模块和数据点的计算中具有稀疏性。这一点又很难用小批处理，而小批是充分利用GPU的必要条件。有效地优化这些计算是具有挑战性的，但可以大大有助于推动研究的发展。</li><li>扩展到大量的模块：大脑可能是由非常多的独立模块组成的，而目前模块化深度学习的大部分工作涉及的模块数量要少得多，比如20个。考虑新的算法和架构，以帮助扩展到非常多的模块，这将是很有趣的。</li><li>宏观和微观模块：GWT通常考虑的模块种类是相当高级的，例如，人脸识别、步态识别、物体识别、视觉常规、听觉语音感知、听觉物体识别、触觉物体识别。这些都是宏观模块，而不是把视觉输入分割成单一对象的模块，即微观模块。我们所做的大部分工作都集中在微观模块上。模块化的层次结构应该如何构建，以说明这些大规模和小规模的模块化方式？</li></ul><h2 id="回顾过去与符号人工智能的联系">8 回顾过去：与符号人工智能的联系</h2><blockquote><p>Looking Backward: Relation to Good Old-Fashioned Symbolic AI</p></blockquote><p>system 2 和传统的符号人工智能有什么区别呢？我们首先回顾一些传统符号人工智能的问题，这些问题促使人们在深度学习的基础上建立解决方案。</p><ol type="1"><li>我们想要高效的大规模学习，例如，SGD 的变种和现代深度学习的端到端学习。由于操作的离散性，在大规模的学习中进行纯粹的符号操作是具有挑战性的。</li><li>我们希望高层次的概念在低层次的观察和低层次的行动方面有语义基础（这是由大脑中的 system 1 计算完成的）。这一点很重要，因为对世界的一些理解（也许是很大一部分）并没有在有意识的 system 2 层面上体现出来，而这一点在纯粹用符号表示知识中完全没有。</li><li>我们希望高层次概念具有分布式表征：纯粹的符号表示使得每个符号与其他每个符号保持相同的距离，而分布式表示则通过一个矢量来表示符号，相关的符号会具有重叠的表示。</li><li>我们想要高效的搜索和推理。符号人工智能的一个计算瓶颈是搜索，一般来说是难以解决的，需要进行近似处理。变分自编码器显示了这种计算成本如何通过训练推理机制来摊销。这是我们目前所知道的唯一的通用的方法，它与认知神经科学中的习惯性技能从 system 2 转移到 system 1 是一致的。</li><li>我们想处理不确定性，大多数机器学习方法都是为了处理这个问题。</li></ol><p>目前的深度学习已经具备了这些能力。现在缺少的是把系统性泛化和将知识分解成可交换的小片段整合进来，而这通常与符号人工智能有关。我们认为，由于上述原因，特别是第 1、3 和 4 点，仅仅在神经网络产生的表征之上使用符号方法是不够的。</p><h2 id="结论">9 结论</h2><blockquote><p>Conclusions</p></blockquote><p>为了能够处理动态的、不断变化的环境条件，我们希望能够从 system 1 的深度统计模型转变为能够利用 system 1 的计算主干来执行 system 2 任务的深度结构模型。当今的深度网络可能可以受益于额外的结构和归纳偏置，从而在 system 2 任务、自然语言理解、分布外泛化和高效转移学习方面做得更好。作者试图澄清这些归纳偏置的一些内容，但还需要做很多后续的工作提高对此的理解，并找到适当的方法将这些先验因素纳入神经架构和训练框架之中。</p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>重参数化技巧 The Reparameterization Trick</title>
    <link href="/blog-main/2022/06/22/%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96%E6%8A%80%E5%B7%A7-The-Reparameterization-Trick/"/>
    <url>/blog-main/2022/06/22/%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96%E6%8A%80%E5%B7%A7-The-Reparameterization-Trick/</url>
    
    <content type="html"><![CDATA[<h2 id="重参数化技巧">重参数化技巧</h2><p>很早就听说过「重参数化技巧」，但一直没有去了解，近来这个词又反复出现在我眼中，遂搜索资料学了一下，记录于此。</p><p>一般而言，我们在机器学习/深度学习的过程中遇到的优化目标长这样： <span class="math display">\[\mathbb E_{z\sim \mathcal P}[f_\theta(z)]\]</span> 使用梯度下降优化之： <span class="math display">\[\begin{align}\nabla_\theta\mathbb E_{z\sim \mathcal P}[f_\theta(z)]&amp;=\nabla_\theta\left[\int p(z)f_\theta(z)\mathrm dz\right]\\&amp;=\int p(z)\nabla_\theta f_\theta(z) \mathrm dz\\&amp;=\mathbb E_{z\sim \mathcal P}[\nabla_\theta f_\theta(z)]\end{align}\]</span> 即期望和求梯度是可交换的，这使得我们可以通过「采样 <span class="math inline">\(z\sim \mathcal P\)</span> <span class="math inline">\(\to\)</span> 对每一个样本计算梯度 <span class="math inline">\(\nabla_\theta f_\theta(z)\)</span> <span class="math inline">\(\to\)</span> 求平均」的训练过程来近似上式。然而，在有些情形下（VAE、强化学习等），优化目标中的概率分布也由参数 <span class="math inline">\(\theta\)</span> 决定： <span class="math display">\[\mathbb E_{z\sim \mathcal P_\theta}[f_\theta(z)]\]</span> 如果我们尝试求它的梯度： <span class="math display">\[\begin{align}\nabla_\theta \mathbb E_{z\sim \mathcal P_\theta}[f_\theta(z)]&amp;=\nabla_\theta\left[\int p_\theta(z)f_\theta(z)\mathrm dz\right]\\&amp;=\int f_\theta(z)\nabla_\theta p_\theta(z)\mathrm dz+\int p_\theta(z)\nabla_\theta f_\theta(z)\mathrm dz\\&amp;={\color{purple}{\int f_\theta(z)\nabla_\theta p_\theta(z)\mathrm dz}}+\mathbb E_{z\sim \mathcal P_\theta}[\nabla f_\theta(z)]\end{align}\]</span> 会发现紫色那一坨没法通过采样近似——假若我们依旧从 <span class="math inline">\(\mathcal P_\theta\)</span> 中采样，采出来的样本并不能告诉我们怎么去更新 <span class="math inline">\(\theta\)</span>，换句话说，<strong>采样是一个不可导的操作</strong>。这时就需要用到重参数化技巧了！</p><p>既然不能直接从 <span class="math inline">\(\mathcal P_\theta\)</span> 中采样，那就曲线救国——先从无参数分布 <span class="math inline">\(\mathcal Q\)</span> 中采样一个 <span class="math inline">\(\epsilon\)</span>，再通过变换 <span class="math inline">\(z=g_\theta(\epsilon)\)</span> 得到 <span class="math inline">\(z\)</span>. 这样，梯度就能够不经过采样操作传递给 <span class="math inline">\(\theta\)</span>：</p><p><span class="math display">\[\nabla_\theta \mathbb E_{z\sim \mathcal P_\theta}[f_\theta(z)]=\nabla_\theta \mathbb E_{\epsilon\sim \mathcal Q}[f_\theta(g_\theta(\epsilon))]=\mathbb E_{\epsilon\sim \mathcal Q}[\nabla_\theta f_\theta(g_\theta(\epsilon))]\]</span> 因此训练过程就是「采样 <span class="math inline">\(\epsilon\sim \mathcal Q\)</span> <span class="math inline">\(\to\)</span> 对每一个样本计算梯度 <span class="math inline">\(\nabla_\theta f_\theta(g_\theta(\epsilon))\)</span> <span class="math inline">\(\to\)</span> 求平均」，和一般情形并无不同。</p><p><img src="img.png" width=60% alt="采样操作在计算图之外"/></p><p>现在的问题就是，怎样确定分布 <span class="math inline">\(\mathcal Q\)</span> 和变换 <span class="math inline">\(z=g_\theta(\epsilon)\)</span>，使得变换后的结果满足 <span class="math inline">\(z\sim \mathcal P_\theta\)</span> 呢？这就得具体问题具体分析了。</p><h3 id="高斯分布情形">高斯分布情形</h3><p>在 VAE 中，<span class="math inline">\(\mathcal P_\theta\)</span> 要求是一个高斯分布，即：<span class="math inline">\(z\sim \mathcal P_\theta=\mathcal N(\mu_\theta, \sigma^2_\theta)\)</span>，其中 <span class="math inline">\(\mu_\theta, \sigma^2_\theta\)</span> 由一个 encoder 网络输出而来，<span class="math inline">\(\theta\)</span> 是这个 encoder 网络的参数。</p><p>这是一种较为简单的情形，我们很容易想到取 <span class="math inline">\(\epsilon\sim \mathcal Q=\mathcal N(0, 1)\)</span>，并作变换 <span class="math inline">\(z=g_\theta(\epsilon)=\sigma_\theta \epsilon+\mu_\theta\)</span> 即可。</p><h3 id="离散分布情形">离散分布情形</h3><p>假若 <span class="math inline">\(z\)</span> 是离散随机变量，不妨设 <span class="math inline">\(z\sim \mathcal P_\theta=[p_1,p_2,\ldots,p_k]^T\)</span>，其中 <span class="math inline">\(\sum_{i=1}^k p_i=1\)</span>，那么 <strong>Gumbel Max</strong> 提供了一种将采样过程重参数化的方式： <span class="math display">\[\mathop{\text{argmax}}_{i=1}^k\left[\log p_i-\log(-\log \epsilon_i)\right]\quad\quad \epsilon_i\sim U[0,1]\]</span> 可以证明，依据上式采样即相当于依据概率分布 <span class="math inline">\([p_1,p_2,\ldots,p_k]\)</span> 采样。</p><blockquote><p>证明：不妨设 <span class="math inline">\(\text{argmax}\)</span> 输出为 <span class="math inline">\(1\)</span>，这意味着： <span class="math display">\[\log p_1-\log(-\log \epsilon_1)&gt;\log p_j-\log(-\log \epsilon_j)\quad\forall j\neq 1\]</span> 略作化简： <span class="math display">\[\epsilon_j&lt;\epsilon_1^{p_j/p_1}\quad\forall j\neq 1\]</span> 因为 <span class="math inline">\(\epsilon_i\)</span> 都是 <span class="math inline">\([0,1]\)</span> 上的均匀分布，所以在给定 <span class="math inline">\(\epsilon_1\)</span> 的条件下，上式成立的条件概率就是： <span class="math display">\[\prod_{j\neq 1}\epsilon_1^{p_j/p_1}=\epsilon_1^{1/p_1-1}\]</span> 因此采样结果为 <span class="math inline">\(1\)</span> 的概率是： <span class="math display">\[\int_0^1 \epsilon_1^{1/p_1-1}\mathrm d \epsilon_1=p_1\cdot\left.\epsilon_1^{1/p_1}\right|_0^1=p_1\]</span> 所以说，依据 Gumbel Max 采样和依据 <span class="math inline">\([p_1,p_2,\ldots,p_k]\)</span> 采样效果相同。</p></blockquote><p>但是这里有个问题，虽然 Gumbel Max 使得采样操作避开了求导，却又引入了 <span class="math inline">\(\text{argmax}\)</span> 这个不可导操作！因此，我们需要进一步地用可导的 <span class="math inline">\(\text{softmax}\)</span> 对 <span class="math inline">\(\text{argmax}\)</span> 做近似（或者更准确地说，是对 <span class="math inline">\(\text{argmax}\)</span> 对应的那个 <span class="math inline">\(\text{onehot}\)</span> 向量做近似），我们将下式称为 <strong>Gumbel Softmax</strong>： <span class="math display">\[\text{softmax}\left(\frac{\log p_i-\log (-\log \epsilon_i)}{\tau}\right)\quad\quad \epsilon_i\sim U[0,1]\]</span> 其中 <span class="math inline">\(\tau&gt;0\)</span> 是温度参数，<span class="math inline">\(\tau\to 0\)</span> 时 <span class="math inline">\(\text{softmax}\to\text{onehot}\)</span>.</p><p><img src="paper.png" width=80% alt="source:[5]" /></p><p>说了这么多，我们现在总结一下，欲计算 <span class="math inline">\(\mathbb E_{z\sim \mathcal P_\theta}[f_\theta(z)]\)</span>，首先采样 <span class="math inline">\(k\)</span> 个服从 <span class="math inline">\(U[0,1]\)</span> 的样本 <span class="math inline">\(\epsilon_i\)</span>，然后计算 Gumbel Softmax，得到一个 <span class="math inline">\(k\)</span> 维向量 <span class="math inline">\(\tilde z\)</span>，那么 <span class="math inline">\(f_\theta(\tilde z)\approx \mathbb E_{z\sim P_\theta}[f_\theta(z)]\)</span>.</p><h3 id="小结">小结</h3><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">高斯分布情形</th><th style="text-align: center;">离散分布情形</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><span class="math inline">\(\mathcal P_\theta\)</span></td><td style="text-align: center;"><span class="math inline">\(\mathcal N(\mu_\theta, \sigma^2_\theta)\)</span></td><td style="text-align: center;"><span class="math inline">\([p_1,p_2,\ldots,p_k]\)</span></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(\mathcal Q\)</span></td><td style="text-align: center;"><span class="math inline">\(\mathcal N(0,1)\)</span></td><td style="text-align: center;"><span class="math inline">\(U[0,1]\)</span> (<span class="math inline">\(k\)</span> 个)</td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(g_\theta(\epsilon)\)</span></td><td style="text-align: center;"><span class="math inline">\(z=\sigma_\theta \epsilon+\mu_\theta\)</span></td><td style="text-align: center;"><span class="math inline">\(z\)</span> 通过 Gumbel Softmax 计算</td></tr></tbody></table><h2 id="参考资料">参考资料</h2><p>[1] 苏剑林. (Jun. 10, 2019). 《漫谈重参数：从正态分布到Gumbel Softmax 》[Blog post]. Retrieved from https://spaces.ac.cn/archives/6705</p><p>[2] The ReparameteriTrick. https://gregorygundersen.com/blog/2018/04/29/reparameteri/</p><p>[3] PyTorch 32.Gumbel-Softmax Trick - 科技猛兽的文章 - 知乎 https://..com/p/166632315</p><p>[4] 盘点深度学习中的不可导操作(次梯度和重参数化) - Houye的文章 - 知乎 https://..com/p/97465608</p><p>[5] Jang, Eric, Shixiang Gu, and Ben Poole. Categorical reparameteriwith gumbel-softmax. <em>arXiv preprint arXiv:1611.01144</em> (2016). https://arxiv.org/pdf/1611.01144.pdf</p><p>[6] 【Learning Notes】Gumbel 分布及应用浅析. https://blog.csdn.net/jackytintin/article/details/79364490]</p><p>[7] [知识点] Reparametritricks重参数技巧讲解及应用 - 救命稻草人来了的文章 - 知乎 https://..com/p/35218887</p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[David Silver强化学习]7·Policy Gradient</title>
    <link href="/blog-main/2022/05/25/David-Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-7%C2%B7Policy-Gradient/"/>
    <url>/blog-main/2022/05/25/David-Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-7%C2%B7Policy-Gradient/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\calA}{\mathcal A}\newcommand{\calP}{\mathcal P}\newcommand{\calR}{\mathcal R}\newcommand{\calS}{\mathcal S}\newcommand{\bbP}{\mathbb P}\newcommand{\E}{\mathbb E}\newcommand{\bfw}{\mathbf w}\newcommand{\bfx}{\mathbf x}\]</span></p><h2 id="introduction">1 Introduction</h2><p>在以往的课程中，我们用线性函数或神经网络对 state-value function 或 action-value function 做估计： <span class="math display">\[V_\theta(s)\approx V^\pi(s)\quad Q_\theta(s,a)\approx Q^\pi(s,a)\]</span> 然后根据贪心或 <span class="math inline">\(\epsilon\text{-greedy}\)</span> 从 value function 中生成策略。</p><p>这节课我们直接参数化策略： <span class="math display">\[\pi_\theta(s,a)=\bbP[a\mid s,\theta]\]</span> 对比 value-based 和 policy-based RL：</p><p><img src="value policy.png" width=40% /></p><ul><li>Value-based<ul><li>学习 value function</li><li>Policy 是隐式的</li></ul></li><li>Policy-based<ul><li>没有 value function</li><li>学习 policy</li></ul></li><li>Actor-Critic<ul><li>学习 value function</li><li>学习 policy</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>David Silver</category>
      
    </categories>
    
    
    <tags>
      
      <tag>reinforcement learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[David Silver强化学习]6·Value Function Approximation</title>
    <link href="/blog-main/2022/05/13/David-Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-6%C2%B7Value-Function-Approximation/"/>
    <url>/blog-main/2022/05/13/David-Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-6%C2%B7Value-Function-Approximation/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\calA}{\mathcal A}\newcommand{\calP}{\mathcal P}\newcommand{\calR}{\mathcal R}\newcommand{\calS}{\mathcal S}\newcommand{\E}{\mathbb E}\newcommand{\bfw}{\mathbf w}\newcommand{\bfx}{\mathbf x}\]</span></p><h2 id="introduction">1 Introduction</h2><p>之前我们学习的所有算法都要求存储下 <span class="math inline">\(V(s)\)</span> 或者 <span class="math inline">\(Q(s,a)\)</span>，称之为 Table Lookup 的方法。但是实际问题的状态和动作数量可能非常多，甚至于是连续的而非离散的，表格就无法存储并处理。此时，我们可以用一个模型（函数）去近似 value function，即： <span class="math display">\[\hat v(s,\bfw)\approx v_\pi(s)\quad \hat q(s,a,\bfw)\approx q_\pi(s,a)\]</span> 其中 <span class="math inline">\(\bfw\)</span> 是模型的参数，譬如神经网络的参数。这样做还有一个好处，假若模型具有良好的泛化性，即能从见过的状态泛化到没有见过的状态，那么我们还可以得知没见过的状态的 value function.</p><p><img src="approx.png" width=60% /></p><p>用来近似的模型（函数）有许多选择：</p><ul><li>线性模型（特征的线性组合）</li><li>神经网络</li><li>决策树</li><li>最近邻</li><li>傅立叶/小波基</li><li>……</li></ul><p>本课程关注可微分的模型，即线性模型和神经网络。</p><h2 id="incremental-methods">2 Incremental Methods</h2><h3 id="gradient-descent">2.1 Gradient Descent</h3><p>因为我们只考虑可微分的模型，所以优化模型的方法可以用梯度下降。更具体地说，我们使用 MSE (Mean-Squared Error) 作为估计值与真实值的损失函数： <span class="math display">\[J(\bfw)=\frac{1}{2}\E_\pi\left[(v_\pi(S)-\hat v(S, \bfw))^2\right]\]</span> 则更新的每一步为： <span class="math display">\[\Delta \bfw=-\alpha\nabla_\bfw J(\bfw)=\alpha\E_\pi[(v_\pi(S)-\hat v(S,\bfw))\nabla_\bfw \hat v(S,\bfw)]\]</span> 由于本节我们考虑 incremental methods，即每走一步就更新，所以： <span class="math display">\[\Delta \bfw =\alpha(v_\pi(S)-\hat v(S,\bfw))\nabla_\bfw \hat v(S,\bfw)\]</span></p><h3 id="linear-function-approximation">2.2 Linear Function Approximation</h3><p>最简单的模型即线性模型，我们使用特征向量来表示一个状态： <span class="math display">\[\bfx(S)=\begin{bmatrix}\bfx_1(S)\\\vdots\\\bfx_n(S)\end{bmatrix}\]</span> 则模型为特征向量的线性组合： <span class="math display">\[\hat v(S,\bfw)=\bfx(S)^T\bfw=\sum_{j=1}^n\bfx_j(S)\bfw_j\]</span> 使用 MSE 损失函数和梯度下降优化方法： <span class="math display">\[\begin{align}&amp;\nabla_\bfw \hat v(S,\bfw)=\bfx(S)\\&amp;\Delta\bfw=\alpha(v_\pi(S)-\hat v(S,\bfw))\bfx(S)\end{align}\]</span></p><blockquote><p><span class="math inline">\(\text{更新量}=\text{步长（学习率）}\times\text{预测误差}\times\text{特征向量}\)</span></p></blockquote><p>值得注意的是，Table Lookup 和线性模型近似并不是完全不同的两种方式，如果我们设置特征向量为： <span class="math display">\[\bfx(S)=\begin{bmatrix}\mathbf 1(S=s_1)\\\vdots\\\mathbf 1(S=s_n)\end{bmatrix}\]</span> 则线性模型本质就是选取当前所在状态的对应权重，并使其逼近 <span class="math inline">\(v_\pi(S)\)</span>，因此权重 <span class="math inline">\(\bfw\)</span> 正是我们存储的表格 <span class="math inline">\(V(S)\)</span>.</p><h3 id="incremental-prediction-algorithms">2.3 Incremental Prediction Algorithms</h3><p>上文依赖于真实的 value function <span class="math inline">\(v_\pi(S)\)</span> 作为监督信号，但是在 RL 中这显然不成立。因此在实践中，我们用 target 代替 <span class="math inline">\(v_\pi(s)\)</span>：</p><ul><li><p>MC <span class="math display">\[\Delta\bfw=\alpha({\color{purple}{G_t}}-\hat v(S_t,\bfw))\nabla_\bfw\hat v(S_t, \bfw)\]</span></p></li><li><p>TD(0) <span class="math display">\[\Delta\bfw=\alpha({\color{purple}{R_{t+1}+\gamma\hat v(S_{t+1},\bfw)}}-\hat v(S_t,\bfw))\nabla_\bfw\hat v(S_t, \bfw)\]</span></p></li><li><p><span class="math inline">\(\text{TD}(\lambda)\)</span> <span class="math display">\[\Delta\bfw=\alpha({\color{purple}{G_t^\lambda}}-\hat v(S_t,\bfw))\nabla_\bfw\hat v(S_t, \bfw)\]</span> Backward view： <span class="math display">\[\begin{align}&amp;\delta_t=R_{t+1}+\gamma\hat v(S_{t+1},\bfw)-\hat v(S_t,\bfw)\\&amp;E_t=\gamma\lambda E_{t-1}+\bfx(S_{t})\\&amp;\Delta\bfw=\alpha\delta_tE_t\end{align}\]</span></p></li></ul><h3 id="incremental-control-algorithms">2.4 Incremental Control Algorithms</h3><p>做 control 的基本思路不变，即在评价策略和更新策略之间反复迭代。同样的，我们需要将 <span class="math inline">\(V\)</span> 替换为 <span class="math inline">\(Q\)</span>.</p><p><img src="iteration.png" width=50% /></p><p>在 MSE 损失函数下，每一步的更新为： <span class="math display">\[\Delta \bfw=\alpha(q_\pi(S,A)-\hat q(S,A,\bfw))\nabla_\bfw\hat q(S,A,\bfw)\]</span> 若设特征向量： <span class="math display">\[\bfx(S,A)=\begin{bmatrix}\bfx_1(S,A)\\\vdots\\\bfx_n(S,A)\end{bmatrix}\]</span> 并使用<strong>线性模型</strong>： <span class="math display">\[\hat q(S,A,\bfw)=\bfx(S,A)^T\bfw=\sum_{j=1}^n\bfx_j(S,A)\bfw_j\]</span> 则更新为： <span class="math display">\[\Delta \bfw=\alpha(q_\pi(S,A)-\hat q(S,A,\bfw))\bfx(S,A)\]</span> 类似 prediction，我们实操时用 target 代替 <span class="math inline">\(q_\pi(s,a)\)</span>：</p><ul><li><p>MC <span class="math display">\[\Delta\bfw=\alpha({\color{purple}{G_t}}-\hat q(S_t,A_t,\bfw))\nabla_\bfw\hat q(S_t,A_t,\bfw)\]</span></p></li><li><p>TD(0) <span class="math display">\[\Delta\bfw=\alpha({\color{purple}{R_{t+1}+\gamma\hat q(S_{t+1}, A_t)}}-\hat q(S_t,A_t,\bfw))\nabla_\bfw\hat q(S_t,A_t,\bfw)\]</span></p></li><li><p><span class="math inline">\(\text{TD}(\lambda)\)</span> <span class="math display">\[\Delta\bfw=\alpha({\color{purple}{q_t^\lambda}}-\hat q(S_t,A_t,\bfw))\nabla_\bfw\hat q(S_t,A_t,\bfw)\]</span> Backward view： <span class="math display">\[\begin{align}&amp;\delta_t=R_{t+1}+\gamma\hat q(S_{t+1},A_{t+1},\bfw)-\hat q(S_t,A_t,\bfw)\\&amp;E_t=\gamma\lambda E_{t-1}+\nabla_\bfw \hat q(S_t,A_t,\bfw)\\&amp;\Delta \bfw=\alpha\delta_tE_t\end{align}\]</span></p></li></ul><h3 id="convergence">2.5 Convergence</h3><p><img src="convergence1.png" width=50% /></p><p>TD 在 off-policy 和 non-linear function approximation 时不收敛是因为它并没有沿着任何目标函数的梯度进行更新，有人提出了改进的 Gradient TD：</p><p><img src="convergence2.png" width=50% /></p><p><img src="convergence3.png" width=50% /></p><h2 id="batch-methods">3 Batch Methods</h2><h3 id="least-squares-prediction">3.1 Least Squares Prediction</h3><p>虽然 gradient descent 很简单，但是它并不 sample efficient，因为我们浪费了许多过往的经验。不妨把这些过往的经验存下来，视为一个<strong>训练集</strong>，每次更新取出一个 batch，就像我们在监督学习里做的那样。</p><p>形式化地说，经验（experience）是一系列 <span class="math inline">\(\langle \text{state}, \text{value}\rangle\)</span> pairs： <span class="math display">\[\mathcal D=\{\langle s_1, v_1^\pi\rangle,\ldots,\langle s_T, v_T^\pi\rangle\}\]</span> 我们将优化目标设置为最小二乘（least squares）： <span class="math display">\[LS(\bfw)=\sum_{t=1}^T(v_t^\pi-\hat v(s_t,\bfw))^2=\E_\mathcal D[(v^\pi-\hat v(s,\bfw))^2]\]</span></p><p><br/></p><p><strong>SGD with Experience Replay</strong></p><p>一般地，我们可以使用 SGD 迭代求解上述最小二乘问题，步骤如下：</p><ol type="1"><li><p>从 experience 中采样 <span class="math display">\[\langle s,v^\pi\rangle\sim \mathcal D\]</span></p></li><li><p>使用梯度下降更新参数 <span class="math display">\[\Delta \bfw=\alpha(v^\pi-\hat v(s,\bfw))\nabla_\bfw \hat v(s,\bfw)\]</span></p></li><li><p>反复上述步骤，最终收敛到最小二乘的解： <span class="math display">\[\bfw^\pi=\arg\min_{\bfw}LS(\bfw)\]</span></p></li></ol><p><br/></p><p><strong>Experience Replay in Deep Q-Networks (DQN)</strong></p><p>特别地，我们考虑使用神经网络作为近似函数的情形。DQN 使用 <strong>experience replay</strong> 和 <strong>fixed Q-targets</strong> 这两个 trick 使得训练过程更加稳定，其步骤如下：</p><ol type="1"><li><p>根据 <span class="math inline">\(\epsilon\text{-greedy}\)</span> 策略采取行动 <span class="math inline">\(a_t\)</span></p></li><li><p>将转移 <span class="math inline">\((s_t,a_t,r_{t+1},s_{t+1})\)</span> 存储在 replay memory <span class="math inline">\(\mathcal D\)</span> 中</p></li><li><p>从 <span class="math inline">\(\mathcal D\)</span> 中随机采样一个 mini-batch <span class="math inline">\((s,a,r,s&#39;)\)</span></p></li><li><p><strong>用老的、固定的参数 <span class="math inline">\(\bfw^-\)</span></strong> 计算 Q-learning targets</p></li><li><p>使用 SGD 优化 MSE 损失函数： <span class="math display">\[\mathcal L_i(\bfw_i)=\E_{s,a,r,s&#39;\sim\mathcal D_i}\left[\left(r+\gamma \max_{a&#39;}Q(s&#39;,a&#39;;\bfw_i^-)-Q(s,a;\bfw_i)\right)^2\right]\]</span></p></li></ol><p><br/></p><p><strong>Linear Least Squares Prediction</strong></p><p>上面我们用数值解法（SGD）求解了最小二乘，但众所周知，最小二乘在线性模型下是可以直接写出解析解的。如果我们用线性模型去近似 <span class="math inline">\(q(s,a)\)</span>，那么可以直接解出： <span class="math display">\[\bfw=\left(\sum_{t=1}^T\bfx(s_t)\bfx(s_t)^\top\right)^{-1}\sum_{t=1}^T\bfx(s_t)v_t^\pi\]</span> 对于长度为 <span class="math inline">\(N\)</span> 的特征向量，直接求解的复杂度是 <span class="math inline">\(O(N^3)\)</span> 的，使用 Shermann-Morrison 算法是 <span class="math inline">\(O(N^2)\)</span> 的。</p><p><br/></p><p><strong>Algorithms</strong></p><p>上文内容只是方法论，实际中我们不知道 <span class="math inline">\(v_t^\pi\)</span>，需要将 <span class="math inline">\(v_t^\pi\)</span> 换做 MC target, TD target 或 <span class="math inline">\(\text{TD}(\lambda)\)</span> target，得到具体的算法。</p><ul><li>LSMC：<span class="math inline">\(v_t^\pi\approx G_t\)</span></li><li>LSTD：<span class="math inline">\(v_t^\pi\approx R_{t+1}+\gamma \hat v(S_{t+1},w)\)</span></li><li><span class="math inline">\(\text{LSTD}(\lambda)\)</span>：<span class="math inline">\(v_t^\pi\approx G_t^\lambda\)</span></li></ul><p><img src="convergence4.png" width=50% /></p><h3 id="least-squres-control">3.2 Least Squres Control</h3><p>Control 只需将 <span class="math inline">\(v\)</span> 替换为 <span class="math inline">\(q\)</span>，然后遵循 policy iteration 的方式——使用最小二乘的方式进行策略评估，贪心地更新策略，如此迭代。</p><p><img src="convergence5.png" width=50% /></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>David Silver</category>
      
    </categories>
    
    
    <tags>
      
      <tag>reinforcement learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[David Silver强化学习]5·Model-Free Control</title>
    <link href="/blog-main/2022/05/07/David-Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-5%C2%B7Model-Free-Control/"/>
    <url>/blog-main/2022/05/07/David-Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-5%C2%B7Model-Free-Control/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\calA}{\mathcal A}\newcommand{\calP}{\mathcal P}\newcommand{\calR}{\mathcal R}\newcommand{\calS}{\mathcal S}\newcommand{\E}{\mathbb E}\]</span></p><h2 id="introduction">1 Introduction</h2><p>上一节课我们介绍了 model-free prediction，即如何在一个未知的 MDP（不知道转移矩阵 <span class="math inline">\(\calP\)</span> 和 reward function 向量 <span class="math inline">\(\calR\)</span>）中评价一个策略。我们学习了 Monte-Carlo Learning, Temporal-Difference Learning 和 <span class="math inline">\(\text{TD}(\lambda)\)</span>. 它们都基于采样，区别在于更新的步长——MC 要求采出的 episode 终止，<span class="math inline">\(\text{TD}(0)\)</span> 每走一步就更新一次，而 <span class="math inline">\(\text{TD}(\lambda)\)</span> 是它们的折中，为不同步长的结果做几何级数的系数加权。</p><p>这节课我们来学习 model-free control，即如何在未知的 MDP 中更新策略、最终找到最优策略。我们讲的方法可以归为三类：</p><ul><li>On-Policy Monte-Carlo Control</li><li>On-Policy Temporal-Difference Learning</li><li>Off-Policy Learning</li></ul><p>那什么是 on-policy 和 off-policy 呢？前者指 agent 从自己的经验中学习，没有别人的参考；而后者参考其他 agent，甚至人类的行为，从他人的策略中采样，进而学习自己的策略。</p><h2 id="on-policy-monte-carlo-control">2 On-Policy Monte-Carlo Control</h2><h3 id="generalized-policy-iteration">2.1 Generalized Policy Iteration</h3><p>回顾上上节课学的 policy iteration，我们使用 iterative policy evaluation 算法评估当前策略，然后根据新的 value function 贪心地更新策略，并不断迭代这个过程。</p><p><img src="policy iteration.png" width=70% /></p><p>我们也提到过，使用任何一个策略评价方式和任何一个能够得到更优策略的更新方式，这个过程都是可行的。因此我们自然地想到，能不能直接把这个过程用在 model-free control 中呢？遗憾的是，有两个新的问题需要解决。</p><p>第一个问题出现在更新策略中，我们现有的贪心算法如下： <span class="math display">\[\pi&#39;(s)=\arg\max_{a\in\calA} q(s,a)=\arg\max_{a\in\calA}\calR^a_s+\gamma\sum_{s&#39;\in\calS}\calP^a_{ss&#39;}V(s&#39;)\]</span> 这里用到了 <span class="math inline">\(\calR\)</span> 和 <span class="math inline">\(\calP\)</span>，但是我们现在是 model-free 的，并不知道 <span class="math inline">\(\calR\)</span> 和 <span class="math inline">\(\calP\)</span> 啊！解决方法是使用 action-value function 的估计值 <span class="math inline">\(Q\)</span> 而不是 state-value function 的估计值 <span class="math inline">\(V\)</span>： <span class="math display">\[\pi&#39;(s)=\arg\max_{a\in\calA}Q(s,a)\]</span> 没错，使用 <span class="math inline">\(q\)</span> 值是 DP 所不喜欢的（因为复杂度太高），但在这里我们不得不用它。</p><p>第二个问题来自于采样。和 DP 能遍历所有状态不同，贪心地采样很可能导致自己困在局部最优解出不去。换句话说，我们基本没有 exploration. 解决方法非常简单，称为 <span class="math inline">\(\epsilon\text{-greedy}\)</span> exploration：假设一共有 <span class="math inline">\(m\)</span> 个动作，我们有 <span class="math inline">\(1-\epsilon\)</span> 的概率选择贪心策略，剩下 <span class="math inline">\(\epsilon\)</span> 的概率随机选择策略，即： <span class="math display">\[\pi(a\mid s)=\begin{cases}\epsilon/m+1-\epsilon&amp;\text{if }a=a^\ast=\arg\max_{a&#39;\in\calA}Q(s,a&#39;)\\\epsilon/m&amp;\text{otherwise}\end{cases}\]</span> 为了说明 <span class="math inline">\(\epsilon\text{-greedy}\)</span> 是确实能优化策略，我们需要证明 <span class="math inline">\(v_{\pi&#39;}(s)\geq v_\pi(s)\)</span>，同第三节课讲过的一样，只需证明 <span class="math inline">\(q_\pi(s,\pi&#39;(s))\geq v_\pi(s)\)</span>. <span class="math display">\[\begin{align}q_{\pi}(s,\pi&#39;(s))-v_\pi(s)&amp;=\sum_{a\in\calA}\pi&#39;(a\mid s)q_\pi(s,a)-\sum_{a\in\calA}\pi(a\mid s)q_\pi(s,a)\\&amp;=\epsilon/m\sum_{a\in\calA}q_\pi(s,a)+(1-\epsilon)\max_{a\in\calA}q_\pi(s,a)-\sum_{a\in\calA}\pi(a\mid s)q_\pi(s,a)\\&amp;=(1-\epsilon)\left[\max_{a\in\calA}q_\pi(s,a)-{\color{purple}{\sum_{a\in\calA}\frac{\pi(a\mid s)-\epsilon/m}{1-\epsilon}q_\pi(s,a)}}\right]\end{align}\]</span> 由于 <span class="math display">\[\sum_{a\in\calA}\frac{\pi(a\mid s)-\epsilon/m}{1-\epsilon}=\frac{1}{1-\epsilon}\sum_{a\in\calA}\pi(a\mid s)-\frac{\epsilon}{1-\epsilon}=\frac{1}{1-\epsilon}-\frac{\epsilon}{1-\epsilon}=1\]</span> 所以紫色的一坨是对 <span class="math inline">\(q_\pi(s,a)\)</span> 的加权求和，它一定不大于 <span class="math inline">\(\max_{a\in\calA}q_\pi(s,a)\)</span>，因此 <span class="math inline">\(q_\pi(s,\pi&#39;(s))\geq v_\pi(s)\)</span>.</p><p>解决了这两个问题，我们就得到了 generalized policy iteration（下左图）：</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="generalized policy iteration.png" width=100% /></div><div class="group-image-wrap"><img src="generalized policy iteration2.png" width=100% /></div></div></div><p>Monte-Carlo 方法理论上要采样很多 episodes，然后算平均、更新 value function；实操中一个效率更高的方式是采出一条 episode 之后就根据这条 episode 更新 value function（上右图）。</p><blockquote><p>类似于梯度下降与随机梯度下降的区别。</p></blockquote><h3 id="glie">2.2 GLIE</h3><p><span class="math inline">\(\epsilon\text{-greedy}\)</span> 其实还带来了一个问题：我们的最终目标是找到最优策略 <span class="math inline">\(\pi^\ast\)</span>，它应该是一个确定性策略，可是 <span class="math inline">\(\epsilon\text{-greedy}\)</span> 给出的策略是随机性策略。这种随机性在学习初期是必要的，它保证我们能够去 explore；但是在我们已经找到最优策略之后，我们不希望还存在这种随机性。形式化地说，我们希望：</p><ul><li><p>所有 state-action pairs 都能被无限次访问到： <span class="math display">\[\lim_{k\to\infty}N_k(s,a)=\infty\]</span></p></li><li><p>策略最终会收敛到贪心策略： <span class="math display">\[\lim_{k\to\infty}\pi_k(a\mid s)=\mathbf 1(a=\arg\max_{a&#39;\in\calA}Q_k(s,a&#39;))\]</span></p></li></ul><p>这称为 GLIE (Greedy in the Limit with Infinite Exploration).</p><p>一个自然简单的想法是，随着学习的进行，逐步减小 <span class="math inline">\(\epsilon\)</span>，譬如 <span class="math inline">\(\epsilon_k=1/k\)</span>. 步骤如下：</p><ol type="1"><li><p>根据策略 <span class="math inline">\(\pi\)</span> 采样出一条 episode（假设是第 <span class="math inline">\(k\)</span> 条）：<span class="math inline">\(\{S_1,A_1,R_2,\ldots,S_T\}\sim \pi\)</span></p></li><li><p>对 episode 中的每一个 <span class="math inline">\(S_t\)</span> 和 <span class="math inline">\(A_t\)</span>， <span class="math display">\[\begin{align}&amp;N(S_t,A_t)\gets N(S_t,A_t)+1\\&amp;Q(S_t,A_t)\gets Q(S_t,A_t)+\frac{1}{N(S_t,A_t)}(G_t-Q(S_t,A_t))\end{align}\]</span> 注意更新 <span class="math inline">\(Q\)</span> 的方式是上一节课提到的增量（incremental）更新。</p></li><li><p>根据新的 <span class="math inline">\(Q\)</span> 更新策略： <span class="math display">\[\begin{align}&amp;\epsilon\gets 1/k\\&amp;\pi\gets\epsilon\text{-greedy}(Q)\end{align}\]</span></p></li></ol><h2 id="on-policy-temporal-difference-learning">3 On-Policy Temporal-Difference Learning</h2><p>正如我们上一节课的思路，鉴于 TD 对 MC 的优势，我们想从 MC Control 发展到 TD Control. 一个自然的做法是在迭代循环中用 TD 替代 MC，即：</p><ol type="1"><li>使用 TD 评估 <span class="math inline">\(Q(S,A)\)</span></li><li>使用 <span class="math inline">\(\epsilon\text{-greedy}\)</span> 更新策略</li><li>每走一步更新一次</li></ol><h3 id="sarsa">3.1 Sarsa</h3><p><img src="sarsa.png" width=10% /></p><p>从一个 state-action pair 开始，环境给出 reward <span class="math inline">\(R\)</span> 和下一个 state <span class="math inline">\(S&#39;\)</span>，我们根据当前策略（对当前 <span class="math inline">\(Q\)</span> 进行 <span class="math inline">\(\epsilon\text{-greedy}\)</span> 得到）选择下一个 <span class="math inline">\(A&#39;\)</span>，随即更新 <span class="math inline">\(Q\)</span>： <span class="math display">\[Q(S,A)\gets Q(S,A)+\alpha(R+\gamma Q(S&#39;,A&#39;)-Q(S,A))\]</span> Sarsa 名称的来源？再仔细看看上图 (doge)。</p><p>Algorithm:</p><p><img src="sarsa_alg.png" width=70% /></p><p>Sarsa 的收敛性由以下定理保证：Sarsa 能收敛到最优的 action-value function，即 <span class="math inline">\(Q(s,a)\to q_\ast(s,a)\)</span>，需要满足以下条件：</p><ol type="1"><li><p>策略序列 <span class="math inline">\(\pi_t(a\mid s)\)</span> 满足 GLIE</p></li><li><p><span class="math inline">\(\alpha_t\)</span> 序列满足 Robbins-Monro： <span class="math display">\[\sum_{t=1}^\infty a_t=\infty\quad\sum_{t=1}^\infty a_t^2&lt;\infty\]</span></p></li></ol><p>但是，David Silver 在课上说，实践中我们一般不会考虑第 2 个条件，甚至有时都不考虑第 1 个条件，Sarsa 依然能 work.</p><h3 id="n-step-sarsa">3.2 <span class="math inline">\(n\)</span>-step Sarsa</h3><p>可以看出，上述 Sarsa 过程对应着 <span class="math inline">\(\text{TD}(0)\)</span>，那么相应的，我们可以得到对应 <span class="math inline">\(n\)</span>-step TD 的 <span class="math inline">\(n\)</span>-step Sarsa 和对应 <span class="math inline">\(\text{TD}(\lambda)\)</span> 的 <span class="math inline">\(\text{Sarsa}(\lambda)\)</span>.</p><p>首先将 Sarsa 扩展到 <span class="math inline">\(n\)</span>-step Sarsa：</p><p><img src="n-step sarsa.png" width=50% /></p><p>定义 <span class="math inline">\(n\)</span>-step Q-return： <span class="math display">\[q^{(n)}_t=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma ^n Q(S_{t+n})\]</span> 那么 <span class="math inline">\(n\)</span>-step Sarsa 的更新就是： <span class="math display">\[Q(S,A)\gets Q(S,A)+\alpha(q_t^{(n)}-Q(S,A))\]</span></p><h3 id="textsarsalambda">3.3 <span class="math inline">\(\text{Sarsa}(\lambda)\)</span></h3><p>对 <span class="math inline">\(n\)</span>-step Sarsa 做几何级数的加权求和，即是 <span class="math inline">\(\text{Sarsa}(\lambda)\)</span>. 同样的，我们有 forward view 和 backward view.</p><p>Forward view 要先把各 <span class="math inline">\(q_t^{(n)}\)</span> 求出来，再做加权和，因此有着与 MC 一样的缺点：</p><p><img src="sarsa forward.png" width=40% /> <span class="math display">\[\begin{align}&amp;q_t^\lambda=(1-\lambda)\sum_{n=1}^\infty\lambda^{n-1}q_t^{(n)}\\&amp;Q(S_t,A_t)\gets Q(S_t,A_t)+\alpha(q_t^\lambda-Q(S_t,A_t))\end{align}\]</span> Backward view 使用 eligibility traces 避免了 forward view 的缺点，不过此时我们对每一个 state-action pair 都要存储一个 eligibility trace <span class="math inline">\(E_t(s,a)\)</span>： <span class="math display">\[\begin{align}&amp;E_0(s,a)=0\\&amp;E_t(s,a)=\gamma\lambda E_{t-1}(s,a)+\mathbf 1(S_t=s,A_t=a)\end{align}\]</span> 更新方式为： <span class="math display">\[\begin{align}&amp;\delta_t=R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\\&amp;Q(s,a)\gets Q(s,a)+\alpha\delta_t E_t(s,a)\end{align}\]</span> Algorithm:</p><p><img src="sarsa_lambda_alg.png" width=70% /></p><h2 id="off-policy-learning">4 Off-Policy Learning</h2><p>Off-policy learning 的基本思想是，根据一个策略 <span class="math inline">\(\mu(a\mid s)\)</span> 进行采样，转而评估另一个策略 <span class="math inline">\(\pi(a\mid s)\)</span>. 因此我们称 <span class="math inline">\(\mu\)</span> 为<strong>行为策略</strong>，而 <span class="math inline">\(\pi\)</span> 为<strong>目标策略</strong>。这样做有几点好处：</p><ul><li>从人类或其他 agent 处学习</li><li>重复利用以前的策略得到的经验</li><li>跟随一个具有探索性（exploratory）的策略，并随之学习到最优策略</li><li>跟随一个策略并随之学习到多个策略</li></ul><h3 id="importance-sampling">4.1 Importance Sampling</h3><p>一个简单的恒等变换，可以将对分布 <span class="math inline">\(P\)</span> 求期望变换为对分布 <span class="math inline">\(Q\)</span> 求期望，称作 importance sampling： <span class="math display">\[\begin{align}\E_{X\sim P}[f(X)]&amp;=\sum_x P(x)f(x)\\&amp;=\sum_x Q(x)\frac{P(x)}{Q(x)}f(x)\\&amp;=\E_{X\sim Q}\left[\frac{P(X)}{Q(X)}f(X)\right]\end{align}\]</span> 运用这种思想，在 Off-Policy Monte-Carlo 中，我们想从策略 <span class="math inline">\(\mu\)</span> 采样来评估策略 <span class="math inline">\(\pi\)</span>，那么可以如下计算 return： <span class="math display">\[G_t^{\pi/\mu}=\frac{\pi(A_t\mid S_t)}{\mu(A_t\mid S_t)}\frac{\pi(A_{t+1}\mid S_{t+1})}{\mu(A_{t+1}\mid S_{t+1})}\cdots\frac{\pi(A_T\mid S_T)}{\mu(A_T\mid S_T)}G_t\]</span> 于是更新方式为： <span class="math display">\[V(S_t)\gets V(S_t)+\alpha\left({\color{purple}{G_t^{\pi/\mu}}}-V(S_t)\right)\]</span> 然而，这个方法并不实用，因为它要求 <span class="math inline">\(\mu\neq0\)</span>，并且多项连乘将导致极大的方差，极其不稳定。</p><p>更实用的是 Off-Policy TD，对 TD target 使用 importance sampling： <span class="math display">\[V(S_t)\gets V(S_t)+\alpha\left({\color{purple}{\frac{\pi(A_t\mid S_t)}{\mu(A_t\mid S_t)}\left(R_{t+1}+\gamma V(S_{t+1})\right)}}-V(S_t)\right)\]</span></p><h3 id="q-learning">4.2 Q-Learning</h3><p><strong>基本思想</strong>：上一节我们考虑了 state-value function <span class="math inline">\(V\)</span> 的 off-policy learning，现在我们考虑 action-value function <span class="math inline">\(Q\)</span> 的 off-policy learning. 实际上，其形式对应着 <span class="math inline">\(\text{Sarsa}(0)\)</span>，只不过采样的策略换成了 <span class="math inline">\(\mu\)</span> 而非 <span class="math inline">\(\pi\)</span>：</p><ol type="1"><li><p>下一个动作是从行为策略 <span class="math inline">\(\mu\)</span> 中采样得来 <span class="math inline">\(A_{t+1}\sim\mu(\bullet\mid S_t)\)</span></p></li><li><p>但是我们考虑的是策略 <span class="math inline">\(\pi\)</span> 中采样的动作 <span class="math inline">\(A&#39;\sim\pi(\bullet\mid S_t)\)</span></p></li><li><p>更新 <span class="math inline">\(Q(S_t,A_t)\)</span>： <span class="math display">\[Q(S_t,A_t)\gets Q(S_t,A_t)+\alpha({\color{purple}{R_{t+1}+\gamma Q(S_{t+1},A&#39;)}}-Q(S_t,A_t))\]</span></p></li></ol><blockquote><p>注意，这里我们不必使用 importance sampling. 为什么呢？不知道！</p></blockquote><p><br/></p><p>上述思想带给我们一个好处：如果策略 <span class="math inline">\(\mu\)</span> 和 <span class="math inline">\(\pi\)</span> 都是可更新的，那么我们得以跟随一个随机的、具有探索性的行为策略 <span class="math inline">\(\mu\)</span>，去优化一个确定性的、<strong>贪心的</strong>目标策略 <span class="math inline">\(\pi\)</span>，从而解决我们在 on-policy learning 中遇到的一个棘手的问题——最优策略是确定性的，但是优化过程需要探索性。</p><p>具体而言，目标策略 <span class="math inline">\(\pi\)</span> 是关于 <span class="math inline">\(Q(s,a)\)</span> 的贪心策略： <span class="math display">\[\pi(s)=\arg\max_{a&#39;\in\calA}Q(s,a&#39;)\]</span> 而行为策略 <span class="math inline">\(\mu\)</span> 是关于 <span class="math inline">\(Q(s,a)\)</span> 的 <span class="math inline">\(\epsilon\text{-greedy}\)</span> 策略。那么 Q-learning target 将简化为： <span class="math display">\[\begin{align}&amp;R_{t+1}+\gamma Q(S_{t+1},A&#39;)\\=&amp;R_{t+1}+\gamma Q(S_{t+1},\arg\max_{a&#39;}Q(S_{t+1},a&#39;))\\=&amp;R_{t+1}+\gamma\max_{a&#39;\in\calA}Q(S_{t+1},a&#39;)\end{align}\]</span> 于是更新方式为： <span class="math display">\[Q(S,A)\gets Q(S,A)+\alpha(R+\gamma\max_{a&#39;\in\calA} Q(S&#39;,a&#39;)-Q(S,A))\]</span> 这就是 <strong>Q-Learning</strong>.</p><p><br/></p><p>对比 Sarsa：<span class="math inline">\(Q(S,A)\gets Q(S,A)+\alpha(R+\gamma Q(S&#39;,A&#39;)-Q(S,A))\)</span>，可见 Q-Learning 和 Sarsa 的唯一区别只在于将 TD target 中的 <span class="math inline">\(Q(S&#39;,A&#39;)\)</span> 替换为了 <span class="math inline">\(\arg\max_{a&#39;\in\calA}Q(S&#39;,a&#39;)\)</span>，也就是说，在更新时不再是随便从当前策略中采样一个动作，而是选出具有最大 <span class="math inline">\(Q\)</span> 值的那个动作，如下图所示：</p><p><img src="qlearning.png" width=20% /></p><p>Algorithm：</p><p><img src="qlearning_alg.png" width=70% /></p><h2 id="小结dp-与-td">5 小结·DP 与 TD</h2><p>通过这几节课的学习，我们已经发现许多算法之间具有对应和发展的关系，这里，我们将 DP 和 TD 的算法总结如下表：</p><p><img src="sum.png" width=70% /></p><p><img src="sum2.png" width=70% /></p><p>其中，<span class="math inline">\(x\overset{\alpha}{\gets}y\equiv x\gets x+\alpha(y-x)\)</span>，表示用 <span class="math inline">\(y\)</span> 来更新 <span class="math inline">\(x\)</span>.</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>David Silver</category>
      
    </categories>
    
    
    <tags>
      
      <tag>reinforcement learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[David Silver强化学习]4·Model-Free Prediction</title>
    <link href="/blog-main/2022/04/30/David-Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-4%C2%B7Model-Free-Prediction/"/>
    <url>/blog-main/2022/04/30/David-Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-4%C2%B7Model-Free-Prediction/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\calA}{\mathcal A}\newcommand{\calP}{\mathcal P}\newcommand{\calR}{\mathcal R}\newcommand{\calS}{\mathcal S}\newcommand{\E}{\mathbb E}\]</span></p><h2 id="introduction">1 Introduction</h2><p>上节课我们学习了使用动态规划求解一个已知的 MDP. 我们学习了 iterative policy evaluation 来评价某个给定策略（prediction 问题），以及 policy iteration 和 value iteration 来寻找最优策略（control 问题）。</p><p>这两节课我们将探讨 model-free 方法。我们不再已知整个 <span class="math inline">\(\calP\)</span> 矩阵和 <span class="math inline">\(\calR\)</span> 向量，也即是说我们面对的是一个未知的 MDP，通过与环境直接交互来解决 prediction / control 问题。这节课讲 model-free prediction，下节课讲 model-free control.</p><p>我们将学习 3 种 model-free prediction 方法：Monte-Carlo Learning，Temporal-Difference Learning 和 <span class="math inline">\(\text{TD}(\lambda)\)</span>.</p><h2 id="monte-carlo-learning">2 Monte-Carlo Learning</h2><p>MC 直接从采样出的 episodes 进行学习，这里要求 episode 最后到达终止状态。</p><blockquote><p>回顾：</p><ul><li><p>Return 是未来的总奖励加权和： <span class="math display">\[G_t=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{T-1-t}R_T\]</span></p></li><li><p>Value function 是 return 的条件期望： <span class="math display">\[v_\pi(s)=\E[G_t\mid S_t=s]\]</span></p></li></ul></blockquote><p>给定策略 <span class="math inline">\(\pi\)</span>，我们可以说采样的 episode 服从 <span class="math inline">\(\pi\)</span>，记作： <span class="math display">\[S_1,A_1,R_2,\ldots,S_k\sim \pi\]</span> MC 方法的目标是学习 <span class="math inline">\(v_\pi\)</span>，思想很简单：用经验均值来近似期望。也就是说，对于某个状态 <span class="math inline">\(s\)</span>，我们想从它开始采样，通过计算样本 episodes 的 return 来近似 <span class="math inline">\(v_\pi(s)\)</span>.</p><h3 id="first-visit-monte-carlo-policy-evaluation">2.1 First-Visit Monte-Carlo Policy Evaluation</h3><p>一个 episode 可能会反复回到同一个状态，First-Visit MC Policy Evaluation 只考虑<strong>第一次</strong>访问那个状态的时候，它未来的 return 是怎样的。具体而言，为了估计状态 <span class="math inline">\(s\)</span> 的 value function，我们</p><ol type="1"><li>采样一条 episode，找到第一次访问状态 <span class="math inline">\(s\)</span> 的时刻 <span class="math inline">\(t\)</span></li><li><span class="math inline">\(N(s)\gets N(s)+1,\,S(s)\gets S(s)+G_t\)</span></li><li>重复上述过程若干次</li><li>计算 <span class="math inline">\(V(s)=S(s)/N(s)\)</span>，根据大数定律，当 <span class="math inline">\(N(s)\to\infty\)</span> 时，<span class="math inline">\(V(s)\to v_\pi(s)\)</span></li></ol><blockquote><p>由于第 2 步涉及到了 <span class="math inline">\(G_t\)</span>，是未来整个过程的加权奖励，因此我们必须要求 episode 最后终止，因为我们在终止之后才能计算出 <span class="math inline">\(G_t\)</span>.</p></blockquote><h3 id="every-visit-monte-carlo-policy-evaluation">2.2 Every-Visit Monte-Carlo Policy Evaluation</h3><p>顾名思义，Every-Visit 把某个状态的<strong>每一次</strong>访问都纳入考量，具体来说，</p><ol type="1"><li>采样一条 episode</li><li>对于该 episode 中每次访问状态 <span class="math inline">\(s\)</span> 的时刻 <span class="math inline">\(t\)</span>，<span class="math inline">\(N(s)\gets N(s)+1,\,S(s)\gets S(s)+G_t\)</span></li><li>重复上述过程若干次</li><li>计算 <span class="math inline">\(V(s)=S(s)/N(s)\)</span>，当 <span class="math inline">\(N(s)\to\infty\)</span> 时，<span class="math inline">\(V(S)\to v_\pi(s)\)</span></li></ol><h3 id="incremental-monte-carlo">2.3 Incremental Monte-Carlo</h3><p>在计算平均值的时候，不必每次都把所有数加起来，再除以个数，用一个简单的递推即可做到 <span class="math inline">\(O(1)\)</span> 的<strong>在线</strong>更新： <span class="math display">\[\begin{align}\mu_k&amp;=\frac{1}{k}\sum_{j=1}^kx_j\\&amp;=\frac{1}{k}\left(x_k+\sum_{j=1}^{k-1}x_j\right)\\&amp;=\frac{1}{k}(x_k+(k-1)\mu_{k-1})\\&amp;=\mu_{k-1}+\frac{1}{k}(x_k-\mu_{k-1})\end{align}\]</span> 这个递推式可以解释为：新的均值是原来的均值加上一个误差项 <span class="math inline">\(\frac{1}{k}(x_k-\mu_{k-1})\)</span>.</p><p>将其用在 MC 方法中，我们称作 Incremental Monte-Carlo Updates：</p><ol type="1"><li>采样一条 episode <span class="math inline">\(S_1,A_1,R_2,\ldots,S_T\)</span></li><li>对于每一个 <span class="math inline">\(S_t\)</span> 及其 return <span class="math inline">\(G_t\)</span>，计算 <span class="math inline">\(N(S_t)\gets N(S_t)+1,\,V(S_t)\gets V(S_t)+\dfrac{1}{N(S_t)}(G_t-V(S_t))\)</span></li><li>重复上述过程</li></ol><p>如果我们把系数 <span class="math inline">\(1/N(S_t)\)</span> 替换为某个固定常数 <span class="math inline">\(\alpha\)</span>，那就得到了<strong>指数移动平均</strong>的形式： <span class="math display">\[V(S_t)\gets V(S_t)+\alpha(G_t-V(S_t))\]</span> 这意味着我们会“遗忘”很早以前的结果。指数移动平均在非平稳（波动很大）的情形下很有用，我们不希望过早的历史信息对现在仍有相同比重的影响。</p><h2 id="temporal-difference-learning">3 Temporal-Difference Learning</h2><h3 id="texttd0">3.1 <span class="math inline">\(\text{TD}(0)\)</span></h3><p>TD 也通过采样、与环境直接交互学习，但与 MC 不同的是，TD 不需要采样完整的、最后终止的 episode，它使用 <strong>bootstrapping</strong> 来估计。</p><p>一个最简单的 TD 算法 <span class="math inline">\(\text{TD}(0)\)</span> 如下：</p><ol type="1"><li><p>采样一条 episode</p></li><li><p>使用 estimated return 来<strong>在线</strong>更新 value function： <span class="math display">\[V(S_t)\gets V(S_t)+\alpha({\color{purple}{R_{t+1}+\gamma V(S_{t+1})}}-V(S_t))\]</span> 其中 <span class="math inline">\(R_{t+1}+\gamma V(S_{t+1})\)</span> 称为 <strong>TD target</strong>，<span class="math inline">\(\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)\)</span> 称作 <strong>TD error</strong>.</p></li><li><p>重复上述过程</p></li></ol><p>可以看见，与 Incremental MC（的指数移动平均形式）相对比，<span class="math inline">\(\text{TD}(0)\)</span> 用一个带有估计性质的 <span class="math inline">\(R_{t+1}+\gamma V(S_{t+1})\)</span> 代替了真实的 <span class="math inline">\(G_t\)</span>，这就是 bootstrapping 的含义——用自己手上的估计值而非真实值。</p><p>David 在课堂上举了一个有趣的例子说明 TD 相比 MC 的好处。考虑一个开车的场景，在某一个 episode 中，我们与对面驶来的车擦肩而过——差点就车祸但是没有车祸。如果使用 MC 方法，我们不会得到任何负面的反馈，因为车祸毕竟没有发生，但使用 TD 方法，我们将期望车祸很有可能发生，因而会立刻更新 value function，而不是一定要等到挂掉之后才能更新。</p><p>到这里，我们可以看到——TD 可以在得到最终结果之前学习，即可以在线学习；MC 必须等到一个 episode 结束后、return 被计算出来后才能更新；TD 可以在无法到达终止状态的环境中学习，MC 不可以。</p><h3 id="bias-variance-trade-off">3.2 Bias / Variance Trade-off</h3><p>Return <span class="math inline">\(G_t\)</span> 是 <span class="math inline">\(v_\pi(S_t)\)</span> 的无偏（unbiased）估计（因为后者本身就是前者的期望），真实的 TD target <span class="math inline">\(R_{t+1}+\gamma v_\pi(S_{t+1})\)</span> 也是 <span class="math inline">\(v_\pi(S_t)\)</span> 的无偏估计，但是 TD target <span class="math inline">\(R_{t+1}+\gamma V(S_{t+1})\)</span> 是有偏（biased）的。</p><blockquote><p>类似于方差和样本方差的区别。</p></blockquote><p>另一方面，由于 <span class="math inline">\(G_t\)</span> 依赖于后续发生的动作、转移、奖励等等，有很多种可能，而 TD target 只考虑下一步的可能情形，因此前者将比后者具有更大的方差。</p><ul><li>MC 高方差，零偏置<ul><li>好的收敛性</li><li>对初始值不敏感</li><li>简单，易于理解</li></ul></li><li>TD 低方差，有偏置<ul><li>通常效率更高</li><li><span class="math inline">\(\text{TD}(0)\)</span> 收敛到 <span class="math inline">\(v_\pi(s)\)</span></li><li>对初始值更敏感</li></ul></li></ul><p><br/></p><p><strong>Random Walk Example</strong></p><p>为了直观对比 MC 和 TD 的收敛速度，我们考虑下面这个随机游走的例子。</p><p><img src="RW.png" width=60% /></p><p>从 C 点开始随机游走，如果终止在右边获得 1 的奖励，终止在左边获得 0 的奖励。</p><p>假设我们把所有状态的 value function 都初始化为 0.5，随着采样数量的增加，value function 确实逐渐逼近真实值：</p><p><img src="RW2.png" width=50% /></p><p>不同的方法（MC v.s. TD）、不同的步长 <span class="math inline">\(\alpha\)</span>，有着不同的收敛速度：</p><p><img src="RW3.png" width=50% /></p><h3 id="batch-mc-and-td">3.3 Batch MC and TD</h3><p>如果我们无法不断地采样，手上只有一批有限数量的样本，那么根据这批样本做 MC 或 TD，能收敛到正确结果吗？</p><p>举一个简单的例子，假设 MDP 只有两个状态：A 和 B，我们手上有 8 个 episodes：</p><ol type="1"><li>A, 0, B, 0</li><li>B, 1</li><li>B, 1</li><li>B, 1</li><li>B, 1</li><li>B, 1</li><li>B, 1</li><li>B, 0</li></ol><p>使用 MC 方法，我们将得到 <span class="math inline">\(V(A)=0,\,V(B)=0.75\)</span>；而使用 TD 方法，我们将得到 <span class="math inline">\(V(A)=0.75,\,V(B)=0.75\)</span>.</p><p>可以看出，本质上来说，</p><ul><li><p>MC 缩小 value function 和观察到的 return 之间的均方误差</p></li><li><p><span class="math inline">\(\text{TD}(0)\)</span> 相当于先根据 episodes 建立起最符合这些样本的 MDP，然后解这个 MDP</p><p><img src="BatchTD.png" width=30% /></p></li></ul><p>因为 TD 首先建立 MDP 模型，它更能够去利用 Markov property，在 Markov environments 下效率更高；而 MC 忽视了 Markov property，在 non-Markov environments 下效率更高。</p><h3 id="unified-view">3.4 Unified View</h3><p>Okay，现在我们将 MC, TD 和 DP 解 MDP 总结一下：</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="MC.png" /></div><div class="group-image-wrap"><img src="TD.png" /></div><div class="group-image-wrap"><img src="DP.png" /></div></div></div><p>MC 会采出一条到达终止状态的 episode，然后更新沿途经过的状态；TD 每次更新只需向前走一步，用下一步的 value function 更新当前状态；而 DP 建立在我们已知整个 MDP 的基础上，我们也只向前走一步，但是严格按照概率对所有的可能进行递推计算。</p><p><br/></p><p>按照是否 sample，MC 和 TD 被划分为一类，DP 被划分为一类；按照是否 bootstrapping（用自己手上的值更新自己，而非真实值），TD 和 DP 被划分为一类，MC 被划分为一类。因此我们可以画一个 2D 的分类图：</p><p><img src="unified.png" width=50% /></p><p>我们很容易注意到，有一个角落是没有讲过的——既不 sample，也不 bootstrapping。这其实对应着最暴力的穷尽搜索。但是我们还应注意到，在是/否 bootstrapping 之间，其实存在灰色地带——我们往前多走几步，但是又不走到头，称之为 <span class="math inline">\(n\)</span>-step TD.</p><h2 id="texttdlambda">4 <span class="math inline">\(\text{TD}(\lambda)\)</span></h2><h3 id="n-step-td">4.1 <span class="math inline">\(n\)</span>-step TD</h3><p>上一节的最后已经解释了 <span class="math inline">\(n\)</span>-step TD 的基本思想：</p><p><img src="TDlambda.png" width=50% /></p><p>反映在公式中，即是：</p><p><img src="TDlambda2.png" width=50% /> <span class="math display">\[G_t^{(n)}=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma ^{n-1}R_{t+n}+\gamma^nV(S_{t+n})\]</span> 更新方式变为： <span class="math display">\[V(S_t)\gets V(S_t)+\alpha(G_t^{(n)}-V(S_t))\]</span> 一个自然的问题就是，向前走的步数 <span class="math inline">\(n\)</span> 取多大最好呢？面对不同的问题，最好的 <span class="math inline">\(n\)</span> 值往往不同，我们需要一个算法帮助我们确定最好的 <span class="math inline">\(n\)</span> 值。</p><p>一个简单的想法是，既然我们不知道哪个 <span class="math inline">\(n\)</span> 最好，那就干脆取多个 <span class="math inline">\(n\)</span> 值算平均，这样至少保证结果不会很差，比较稳定。当然，实践中我们需要一个更高效的算法，这就引入了 <span class="math inline">\(\text{TD}(\lambda)\)</span>.</p><h3 id="forward-view-of-texttdlambda">4.2 Forward View of <span class="math inline">\(\text{TD}(\lambda)\)</span></h3><p>我们对所有 <span class="math inline">\(n\)</span> 值<strong>按照几何级数</strong>做平均，称之为 <span class="math inline">\(\lambda\)</span>-return：</p><p><img src="lambda-return.png" width=30% /></p><p>当然，最后终止的那一步不再是几何级数，而是 <span class="math inline">\(1\)</span> 减去之前所有的系数之和。这里为了简便起见，没有把这一点体现在公式中： <span class="math display">\[G_t^\lambda=(1-\lambda)\sum_{n=1}^\infty\lambda^{n-1}G_t^{(n)}\]</span> 更新方式变为： <span class="math display">\[V(S_t)\gets V(S_t)+\alpha (G_t^\lambda-V(S_t))\]</span> 当 <span class="math inline">\(\lambda=0\)</span> 时，就是上一节的 <span class="math inline">\(\text{TD}(0)\)</span>；当 <span class="math inline">\(\lambda=1\)</span> 时，就是 Monte-Carlo.</p><p><br/></p><p>为了计算 <span class="math inline">\(G_t^\lambda\)</span>，我们需要像 MC 一样采样完整的 episode，将每一步的 <span class="math inline">\(G_t^{(n)}\)</span> 计算出来，然后加权求和。这种方法称作 forward view：</p><p><img src="forward.png" width=60% /></p><h3 id="backward-view-of-texttdlambda">4.3 Backward View of <span class="math inline">\(\text{TD}(\lambda)\)</span></h3><p>Forward view 更多是理论上的贡献，鉴于它具有 MC 一样的缺点，我们更常采用 backward view.</p><p>首先我们引入一个概念——<strong>eligibility traces</strong>. 它类似于神经元受刺激后的激活状态——受刺激的瞬间激活值拉高，然后随时间推移逐渐降低，直到下一次刺激。具体而言，我们对每一个状态 <span class="math inline">\(s\)</span> 维护一个值 <span class="math inline">\(E_t(s)\)</span>，满足： <span class="math display">\[\begin{align}&amp;E_0(s)=0\\&amp;E_t(s)=\gamma\lambda E_{t-1}(s)+\mathbf 1(S_t=s)\end{align}\]</span> 在时刻 <span class="math inline">\(t\)</span>，如果没有进入状态 <span class="math inline">\(s\)</span>，则 <span class="math inline">\(E_t(s)\)</span> 减少到 <span class="math inline">\(\gamma\lambda\)</span> 倍的上一时刻值；否则，<span class="math inline">\(E_t(s)\)</span> 有一个瞬时的 <span class="math inline">\(1\)</span> 的增加：</p><p><img src="eligibility.png" width=60% /></p><p>利用 eligibility traces，我们在每个时刻对所有状态依 TD-error <span class="math inline">\(\delta_t\)</span> 和 <span class="math inline">\(E_t(s)\)</span> 按比例对 <span class="math inline">\(V(s)\)</span> 做更新： <span class="math display">\[\begin{align}&amp;\delta_t = R_{t+1}+\gamma V(S_{t+1})-V(S_t)\\&amp;V(s)\gets V(s)+\alpha\delta_tE_t(s)\end{align}\]</span> <img src="backward.png" width=60% /></p><p>当 <span class="math inline">\(\lambda=0\)</span> 时，<span class="math inline">\(E_t(s)=\mathbf 1(S_t=s)\)</span>，因此我们只对当前进入的这个状态做更新 <span class="math inline">\(V(s)\gets V(s)+\alpha\delta_t\)</span>，这等价于 <span class="math inline">\(\text{TD}(0)\)</span>.</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>David Silver</category>
      
    </categories>
    
    
    <tags>
      
      <tag>reinforcement learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[David Silver强化学习]3·Planning by Dynamic Programming</title>
    <link href="/blog-main/2022/04/17/David-Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-3%C2%B7Planning-by-Dynamic-Programming/"/>
    <url>/blog-main/2022/04/17/David-Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-3%C2%B7Planning-by-Dynamic-Programming/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\calA}{\mathcal A}\newcommand{\calP}{\mathcal P}\newcommand{\calR}{\mathcal R}\newcommand{\calS}{\mathcal S}\]</span></p><h2 id="introduction">1 Introduction</h2><p>算法课上我们都学过，动态规划解决的问题具有两个性质：</p><ul><li>Optimal substructure 最优子结构：问题可以划分为若干子问题，且原问题得到最优解就意味着子问题也得到了最优解</li><li>Overlapping subproblems 重叠子问题：子问题反复出现，因而可以将解存储下来复用</li></ul><p>MDP 满足这两个性质，因而可以用动态规划解决。事实上，用 dp 的话术来说，value function 就是 dp 状态，Bellman 方程就是 dp 转移方程。</p><blockquote><p>回顾一下 Bellman 方程：</p><p><strong>Bellman equation</strong>： <span class="math display">\[v(s)=\calR_s+\gamma\sum_{s&#39;\in\calS}\calP_{ss&#39;}v(s&#39;)\]</span> <strong>Bellman expectation equation</strong>（只涉及 state-value function 的部分）： <span class="math display">\[v_\pi(s)=\sum_{a\in\calA}\pi(a\mid s)\left[\calR_s^a+\gamma\sum_{s&#39;\in\mathcal S}\calP^a_{ss&#39;}v_\pi(s&#39;)\right]\]</span> <strong>Bellman optimality equation</strong>（只涉及 state-value function 的部分）： <span class="math display">\[v_\ast(s)=\max_a\calR^a_s+\gamma\sum_{s&#39;\in\calS}\calP^a_{ss&#39;}v_\ast(s&#39;)\]</span></p></blockquote><p>从 Bellman 方程可以看出，欲使用动态规划解 MDP，我们必须提前知道 <span class="math inline">\(\calP\)</span> 矩阵和 <span class="math inline">\(\calR\)</span> 向量，这意味着我们知道 MDP 的 full knowledge. 换句话说，我们已经知道环境的运行机制，因此我们解决的是 planning 问题（而不是 learning 问题，见第一章最后一节）。</p><p>我们之前也辨析过 prediction 和 control 的概念：</p><ul><li>Prediction：评价已知策略<ul><li>输入 ：MDP <span class="math inline">\(\langle\calS,\calA,\calP,\calR,\gamma\rangle\)</span> 和策略 <span class="math inline">\(\pi\)</span>，或者其对应 MRP <span class="math inline">\(\langle\calS,\calP^\pi,\calR^\pi,\gamma\rangle\)</span></li><li>输出：value function <span class="math inline">\(v_\pi\)</span>.</li></ul></li><li>Control：寻找最优策略<ul><li>输入：MDP <span class="math inline">\(\langle\calS,\calA,\calP,\calR,\gamma\rangle\)</span></li><li>输出：最佳策略 <span class="math inline">\(\pi_\ast\)</span> 和最佳 value function <span class="math inline">\(v_\ast\)</span>.</li></ul></li></ul><p>接下来的三节我们将涉及 iterative policy evaluation, policy iteration 和 value iteration。Iterative policy evaluation 对应的是 prediction 问题，而后两者对应 control 问题。</p><h2 id="policy-evaluation">2 Policy Evaluation</h2><h3 id="iterative-policy-evaluation">2.1 Iterative Policy Evaluation</h3><p>在数值计算的课程中，我们知道线性方程可以通过迭代求解。我翻了翻当时的笔记：</p><blockquote><p>迭代法解线性方程组</p><p>基本思想：将 <span class="math inline">\(Ax=b\)</span> 改写为 <span class="math inline">\(x= Bx+g\)</span>，并建立迭代格式： <span class="math display">\[x^{(k+1)}=Bx^{(k)}+g,\quad k=0,1,2,\ldots\]</span> <span class="math inline">\(x^{(0)}\)</span> 为初始解向量，<span class="math inline">\(B\)</span> 为迭代矩阵，迭代法的收敛性依赖于 <span class="math inline">\(B\)</span>. 我们有以下定理。</p><p>定理：迭代法收敛当且仅当 <span class="math inline">\(\rho(B)&lt;1\)</span>，且 <span class="math inline">\(\rho(B)\)</span> 越小，收敛越快。其中 <span class="math inline">\(\rho(B)\)</span> 是矩阵 <span class="math inline">\(B\)</span> 的谱半径。</p></blockquote><p>而 Bellman expectation equation 正好就是写作了迭代形式的线性方程： <span class="math display">\[\mathbf v=\calR^\pi+\gamma \calP^\pi\mathbf v\]</span> 因此自然而然地可以建立起迭代格式： <span class="math display">\[\mathbf v^{k+1}=\calR^\pi+\gamma \calP^\pi\mathbf v^k\]</span> 反复迭代直至收敛，这就是 iterative policy evaluation.</p><h3 id="example-small-gridworld">2.2 Example: Small Gridworld</h3><p><img src="gridworld.png" width=60% /></p><ul><li>本例中使用无衰减的 MDP（<span class="math inline">\(\gamma=1\)</span>）</li><li>标号的 14 个格子是非终止状态，阴影格子即终止状态</li><li>往边界外走定义为保持当前状态不变（相当于被墙弹回来了）</li><li>每走一步的奖励是 <span class="math inline">\(-1\)</span></li><li>agent 遵循随机策略（四个方向概率 <span class="math inline">\(1/4\)</span>）</li></ul><p>Iterative policy evaluation 的过程如下图所示：左列是迭代的 <span class="math inline">\(\mathbf v^{k}\)</span>，右列是根据当前 <span class="math inline">\(\mathbf v^{k}\)</span> 基于贪心（走下一步使得 value 最大）得到的策略。（注意格子里的小数没有显示完整）</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="gridworld1.png" width=90% /></div><div class="group-image-wrap"><img src="gridworld2.png" width=90% /></div></div></div><p>需要注意的是，尽管我们在右边列出了贪心策略，但是迭代计算时始终使用初始的随机策略——因为 iterative policy evaluation 的目的是评价一个给定策略。</p><p>从这个例子中可以看见，当 value function 收敛时，贪心策略也收敛到了最优策略，但是后者比前者收敛得更快——在 <span class="math inline">\(k=3\)</span> 时，贪心策略已经达到最优了，<span class="math inline">\(k\)</span> 继续增大它也不会改变。</p><h2 id="policy-iteration">3 Policy Iteration</h2><h3 id="policy-evaluation-policy-improvement">3.1 Policy Evaluation &amp; Policy Improvement</h3><p>在 Iterative policy evaluation 中，如果我们更新 value function 后根据贪心更新 policy，然后用新的 policy 更新 value function，那么就得到了 policy iteration 算法：</p><ul><li><p>给定 <span class="math inline">\(\pi\)</span></p><ul><li><p>Policy evaluation： <span class="math display">\[v_\pi(s)=\mathbb E[R_{t+1}+\gamma R_{t+2}+\cdots\mid S_t=s]\]</span></p></li><li><p>Policy improvement： <span class="math display">\[\pi&#39;=\text{greedy}(v_\pi)\]</span></p></li></ul></li></ul><p>上述过程最终会收敛到最优策略 <span class="math inline">\(\pi_\ast\)</span>.</p><p><img src="policy iter.png" width=70% /></p><p>下面以<u>确定性策略</u>为例，说明为什么上述过程能收敛到最优策略。假设我们现在有一个策略 <span class="math inline">\(a=\pi(s)\)</span>，那么根据贪心思想，它将被更新为：<span class="math inline">\(\pi&#39;(s)=\arg\max_{a\in\calA}q_\pi(s,a)\)</span>，这意味着 <span class="math display">\[q_\pi(s,\pi&#39;(s))=\max_{a\in\calA}q_\pi(s,a)\geq q_\pi(s,\pi(s))=v_\pi(s)\]</span> 这个式子的含义是，如果我们第一步依照策略 <span class="math inline">\(\pi&#39;\)</span> 行动，这之后再依照策略 <span class="math inline">\(\pi\)</span> 行动，那么得到的奖励将大于等于一直按照策略 <span class="math inline">\(\pi\)</span> 行动。反复使用该式，最终我们就会知道：一直依照策略 <span class="math inline">\(\pi&#39;\)</span> 行动，得到的奖励将大于等于一直依照策略 <span class="math inline">\(\pi\)</span> 行动。换句话说，贪心思想确实让我们找到了一个更优的策略。</p><p>最终，当我们的策略无法被进一步更新时， <span class="math display">\[q_\pi(s,\pi&#39;(s))=\max_{a\in\calA}q_\pi(s,a)=q_\pi(s,\pi(s))=v_\pi(s)\]</span> 于是 Bellman optimality equation 得到了满足：<span class="math inline">\(v_\pi(s)=\max_{a\in\calA}q_\pi(s,a)\)</span>. 这代表对任意 <span class="math inline">\(s\)</span>，<span class="math inline">\(v_\pi(s)=v_\ast(s)\)</span>，即 <span class="math inline">\(\pi\)</span> 是最优策略。</p><h3 id="extensions-to-policy-iteration">3.2 Extensions to Policy Iteration</h3><ul><li><p>在 2.2 节中，我们看到策略的收敛速度快于 value function 的收敛，因此我们可以提前终止迭代过程。</p><p>例如引入阈值 <span class="math inline">\(\epsilon\)</span>，或者简单地在 <span class="math inline">\(k\)</span> 步之后停止迭代。</p></li><li><p>上文我们用 Iterative policy evaluation 来评价一个策略，用贪心思想来更新策略。事实上，我们不必局限于这两个算法，使用任何一个策略评价方式和任何一个能够得到更优策略的更新方式都行。</p></li></ul><h2 id="value-iteration">4 Value Iteration</h2><h3 id="value-iteration-in-mdps">4.1 Value Iteration in MDPs</h3><p><strong>Principle of Optimality</strong>：一个策略 <span class="math inline">\(\pi(a\mid s)\)</span> 能从状态 <span class="math inline">\(s\)</span> 开始获取最优值，即 <span class="math inline">\(v_\pi(s)=v_\ast(s)\)</span>，当且仅当：对任意从 <span class="math inline">\(s\)</span> 可达的状态 <span class="math inline">\(s&#39;\)</span>，策略 <span class="math inline">\(\pi\)</span> 能从 <span class="math inline">\(s&#39;\)</span> 开始获取最优值。</p><p>这本质就是 DP 的最优子结构性质。根据动态规划思想，假设我们知道了 <span class="math inline">\(v_\ast(s&#39;)\)</span>，那么利用 Bellman optimality equation 做状态转移即可： <span class="math display">\[v_\ast(s)\gets\max_a\calR^a_s+\gamma\sum_{s&#39;\in\calS}\calP^a_{ss&#39;}v_\ast(s&#39;)\]</span> 依此建立迭代格式，就是 value iteration 算法： <span class="math display">\[\mathbf v_{k+1}=\max_{a\in\calA}\calR^a+\gamma\calP^a\mathbf v_k\]</span></p><h3 id="example-shortest-path">4.2 Example: Shortest Path</h3><p><img src="sp.png" width=80% /></p><p>从这个例子我们可以看出，value iteration 的过程没有一个显式的策略，甚至迭代过程中的 value function 可能根本无法对应到某种策略上。但最终，它会收敛到最优 value function.</p><p><br/></p><p>学习到这里，我们做一个小结：</p><p><img src="overview.png" width=70% /></p><p>我们还看到无论是 iterative policy evaluation，还是 policy iteration，还是 value iteration，它们都是对 state-value function <span class="math inline">\(v_\pi(s)\)</span> 做迭代。如果我们有 <span class="math inline">\(n\)</span> 个状态，<span class="math inline">\(m\)</span> 个动作，那么每一次迭代要考虑当前轮的 <span class="math inline">\(n\)</span> 个状态、下一轮的 <span class="math inline">\(n\)</span> 个状态、<span class="math inline">\(m\)</span> 个动作，因此迭代复杂度就是 <span class="math inline">\(O(mn^2)\)</span>.</p><p>为什么不对 action-value function <span class="math inline">\(q_\pi(s,a)\)</span> 迭代呢？理论上，我们有 action-value function 的 Bellman equation，也能仿照 state-value function 的方式进行迭代呀？因为复杂度太高了：每一轮要考虑当前轮的 <span class="math inline">\(n\)</span> 个状态和 <span class="math inline">\(m\)</span> 个动作的组合，以及下一轮的 <span class="math inline">\(n\)</span> 个状态和 <span class="math inline">\(m\)</span> 个动作的组合，于是迭代复杂度高达 <span class="math inline">\(O(m^2n^2)\)</span>.</p><h2 id="extensions-to-dynamic-programming">5 Extensions to Dynamic Programming</h2><h3 id="asynchronous-dynamic-programming">5.1 Asynchronous Dynamic Programming</h3><p>前面提及的所有迭代算法都是<strong>同步更新</strong>的，或者说，每一次迭代中，<strong>所有</strong>状态的 value function 都会根据上一轮迭代的结果进行更新，这无疑是一种浪费。譬如，在 4.2 节 Shortest Path 例子中，前 5 次对最后一行最后一列的迭代更新其实没有什么用，我只需要在计算到 <span class="math inline">\(\mathbf v_7\)</span> 的时候更新它就够了。</p><p>如果每一轮只挑选一部分状态进行更新，我们就称之为<strong>异步</strong>的。神奇的是，只要我们保证每个状态都会时不时地被选中来更新，那么结果就能收敛。</p><p>异步 DP 包含以下几种：In-place dp, prioritised sweeping, real-time dp.</p><ul><li><p><strong>In-place dynamic programming</strong></p><p>在同步更新的 dp 中，我们至少需要存储 2 份 value function，<span class="math inline">\(\mathbf v_\text{new}\)</span> 和 <span class="math inline">\(\mathbf v_\text{old}\)</span>： <span class="math display">\[v_\text{new}(s)\gets\max_{a\in\calA}\left(\calR_s^a+\gamma\sum_{s&#39;\in\calS}\calP^a_{ss&#39;}v_\text{old}(s&#39;)\right)\]</span> In-place 指我们只存 1 份 value function，这样我们用的永远都是最新的值： <span class="math display">\[v(s)\gets\max_{a\in\calA}\left(\calR_s^a+\gamma\sum_{s&#39;\in\calS}\calP^a_{ss&#39;}v(s&#39;)\right)\]</span></p><blockquote><p>Jacobi 迭代与 Gauss-Seidel 迭代的即视感。</p></blockquote></li><li><p><strong>Prioritised sweeping</strong></p><p>In-place DP 并没有说明我们究竟用什么顺序来异步更新。Prioritised sweeping 认为，我们提出一个优先级指标，先更新优先级高的状态。这个优先级可以用 Bellman error： <span class="math display">\[\left|\max_{a\in\calA}\left(\calR_s^a+\gamma\sum_{s&#39;\in\calS}\calP^a_{ss&#39;}v(s&#39;)\right)-v(s)\right|\]</span> 其思想是：如果我更新这个状态之后，它的 value function 变化很大，这说明它将对后续的计算产生非常大的影响，我应该赶紧更新它。</p><p>具体实现时，我们可以维护一个优先队列，取出优先级最高的状态更新，然后计算受影响状态的新的 Bellman error，插入优先队列。</p></li><li><p><strong>Real-time dynamic programming</strong></p><p>基本思想是我们只更新 agent 确实访问过的状态。所谓 real-time，想象我们在真实环境中运行一个 agent，我们收集到它实时反馈的状态等信息，那我们只需要更新这些状态，并不关心还没有访问过的状态。 <span class="math display">\[v({\color{red}{S_t}})\gets\max_{a\in\calA}\left(\calR^a_{ {\color{red}{S_t} } }+\gamma\sum_{s&#39;\in\calS}\calP^a_{ {\color{red}{S_t} }s&#39;}v(s&#39;)\right)\]</span></p></li></ul><h3 id="full-width-and-sample-backups">5.2 Full-Width and Sample Backups</h3><p>所谓 full-width，是指 DP 过程中每一步我们会考虑所有可能的动作和所有可能的后继状态，这将带来巨大的计算开销，因为随着我们一步一步往下走，状态数量成指数增长。</p><p>相反，在接下来的课程中我们将用采样（sample）的方式选取动作与下一个状态。于是我们不必知道真实 reward function <span class="math inline">\(\calR\)</span> 和真实的转移概率矩阵 <span class="math inline">\(\calP\)</span>，而是采样 <span class="math inline">\(\langle S,A,R,S&#39;\rangle\)</span>.</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>David Silver</category>
      
    </categories>
    
    
    <tags>
      
      <tag>reinforcement learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[David Silver强化学习]2·Markov Decision Processes</title>
    <link href="/blog-main/2022/04/15/David-Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-2%C2%B7Markov-Decision-Processes/"/>
    <url>/blog-main/2022/04/15/David-Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-2%C2%B7Markov-Decision-Processes/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\calS}{\mathcal S}\newcommand{\calP}{\mathcal P}\newcommand{\calR}{\mathcal R}\newcommand{\calA}{\mathcal A}\newcommand{\bbP}{\mathbb P}\newcommand{\E}{\mathbb E}\]</span></p><p><img src="overview.png" width=80% /></p><h2 id="markov-processes">1 Markov Processes</h2><h3 id="markov-property">1.1 Markov Property</h3><p>一言以蔽之："The <strong>future</strong> is independent of the <strong>past</strong> given the <strong>present</strong>."</p><p>定义：一个状态 <span class="math inline">\(S_t\)</span> 是 Markov 当且仅当 <span class="math display">\[\mathbb P(S_{t+1}\mid S_t)=\mathbb P(S_{t+1}\mid S_1,\ldots,S_t)\]</span></p><h3 id="state-transition-matrix">1.2 State Transition Matrix</h3><p>对于 Markov state <span class="math inline">\(s\)</span> 和它的后继状态 <span class="math inline">\(s&#39;\)</span>，定义： <span class="math display">\[\calP_{ss&#39;}=\bbP(S_{t+1}=s&#39;\mid S_t=s)\]</span> 称矩阵 <span class="math inline">\(\calP\)</span> 为状态转移矩阵。</p><h3 id="markov-process-markov-chain">1.3 Markov Process (Markov Chain)</h3><p>定义：Markov Process 是一个元组 <span class="math inline">\(\langle \calS,\calP\rangle\)</span>，其中：</p><ul><li><span class="math inline">\(\calS\)</span> 是一个有限的状态集合</li><li><span class="math inline">\(\calP\)</span> 是状态转移矩阵</li></ul><p><br/></p><p>例子：Student Markov Chain</p><p><img src="mp.png" width=100% /></p><p>我们可以从这个 Markov Process 中采样，得到一些 <strong>episodes</strong>，例如：</p><ul><li>C1, C2, C3, Pass, Sleep</li><li>C1, FB, FB, C1, C2, Sleep</li><li>C1, C2, C3, Pub, C2, C3, Pass, Sleep</li><li>C1, FB, FB, C1, C2, C3, Pub, C1, FB, FB, FB, C1, C2, C3, Pub, C2, Sleep</li><li>……</li></ul><h2 id="markov-reward-processes">2 Markov Reward Processes</h2><p>在 Markov Process 中加入 reward，就得到了 Markov Reward Process.</p><h3 id="markov-reward-process">2.1 Markov Reward Process</h3><p>回顾：奖励 reward <span class="math inline">\(R_t\)</span> 是一个随机变量，表示 <span class="math inline">\(t\)</span> 时刻的奖励。为什么说它是一个随机变量呢，因为对于从同一个 Markov process 中采样出的不同 episode，<span class="math inline">\(t\)</span> 时刻的奖励是不同的——对于某特定采样出的 episode，其 <span class="math inline">\(t\)</span> 时刻的奖励是随机变量 <span class="math inline">\(R_t\)</span> 的某个特定取值。</p><p>定义：Markov Reward Process 是一个元组 <span class="math inline">\(\langle\calS, \calP, \calR, \gamma\rangle\)</span>​，其中：</p><ul><li><span class="math inline">\(\calS\)</span> 是一个有限的状态集合</li><li><span class="math inline">\(\calP\)</span> 是状态转移矩阵</li><li><span class="math inline">\(\calR\)</span> 是 reward function，<span class="math inline">\(\calR_s=\E[R_{t+1}\mid S_t=s]\)</span></li><li><span class="math inline">\(\gamma\in[0,1]\)</span> 是衰减系数（discount factor）</li></ul><p><br/></p><p>例子：Student MRP</p><p><img src="mrp.png" width=60% /></p><h3 id="return">2.2 Return</h3><p>定义 <strong>return</strong> <span class="math inline">\(G_t\)</span> 是从时间戳 <span class="math inline">\(t\)</span> 开始的总 reward： <span class="math display">\[G_t=R_{t+1}+\gamma R_{t+2}+\cdots=\sum_{k=0}^\infty\gamma^kR_{t+1+k}\]</span> 由于 <span class="math inline">\(R_t\)</span> 是随机变量，<span class="math inline">\(G_t\)</span> 自然也是随机变量，其具体取值随采样出的 episode 的不同而不同。</p><p><strong>为什么需要衰减系数？</strong></p><ul><li>数学上处理较为方便</li><li>避免在 Markov process 的环上出现无穷大的 return</li><li>我们的建模往往不太精准，我们对未来的把握不是很确定</li><li>对动物/人类的行为研究表明，我们更喜欢短期（立即的）奖励</li><li>如果你真的觉得衰减系数不好，那设置 <span class="math inline">\(\gamma=1\)</span> 即可</li></ul><h3 id="value-function">2.3 Value Function</h3><p>Return <span class="math inline">\(G_t\)</span> 是一个随机变量，对于不同的采样结果具有不同的值，为了衡量平均的未来总 reward，定义价值函数 <span class="math inline">\(v(s)\)</span> 为 <span class="math inline">\(G_t\)</span> 的期望： <span class="math display">\[v(s)=\E[G_t\mid S_t=s]\]</span> 仍以 Student MRP 为例：</p><p><img src="mrp_g.png" width=70% /></p><p>【个人认为这一页 slide 上的 <span class="math inline">\(v_1\)</span> 应该写作 <span class="math inline">\(g_1\)</span>（随机变量 <span class="math inline">\(G_1\)</span> 的各种可能取值），<span class="math inline">\(v_1\)</span> 应该是这些 <span class="math inline">\(g_1\)</span> 的平均，即 <span class="math inline">\(G_1\)</span> 的期望】</p><p><br/></p><p><strong>对上述定义的小结</strong>：</p><ul><li>Reward <span class="math inline">\(R_t\)</span> 是时间 <span class="math inline">\(t\)</span> 的函数，是一个<strong>随机变量</strong>，表示 <span class="math inline">\(t\)</span> 时刻获得的奖励。</li><li>Reward function <span class="math inline">\(\calR_s\)</span> 是状态 <span class="math inline">\(s\)</span> 的函数，表示在状态 <span class="math inline">\(s\)</span> 处下一步获得奖励的<strong>条件期望</strong>：<span class="math inline">\(\calR_s=\E[R_{t+1}\mid S_t=s]\)</span></li><li>Return <span class="math inline">\(G_t\)</span> 是时间 <span class="math inline">\(t\)</span> 的函数，是一个<strong>随机变量</strong>，表示从 <span class="math inline">\(t\)</span> 时刻开始未来的（加权）总奖励。</li><li>Value function <span class="math inline">\(v(s)\)</span> 是状态 <span class="math inline">\(s\)</span> 的函数，表示在状态 <span class="math inline">\(s\)</span> 处未来的（加权）总奖励的<strong>条件期望</strong>：<span class="math inline">\(v(s)=\E[G_t\mid S_t=s]\)</span></li></ul><blockquote><p>以时间戳 <span class="math inline">\(t\)</span> 为下标的是随机变量，以状态 <span class="math inline">\(s\)</span> 为下标的是对应随机变量的条件期望。</p></blockquote><p><img src="sum.png" width=50% /></p><h3 id="bellman-equation">2.4 Bellman Equation</h3><p>根据 <span class="math inline">\(v(s)\)</span> 的定义式，有： <span class="math display">\[\begin{align}v(s)&amp;=\E[G_t\mid S_t=s]\\&amp;=\E[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots\mid S_t=s]\\&amp;=\E[R_{t+1}+\gamma G_{t+1}\mid S_t=s]\\\end{align}\]</span> 这意味着 <span class="math inline">\(v(s)\)</span> 由立即 reward <span class="math inline">\(R_{t+1}\)</span> 和衰减后的未来 reward <span class="math inline">\(G_{t+1}\)</span> 构成，我们继续推导： <span class="math display">\[\begin{align}v(s)&amp;=\E[R_{t+1}+\gamma G_{t+1}\mid S_t=s]\\&amp;=\E[R_{t+1}\mid S_t=s]+\gamma\E[G_{t+1}\mid S_{t}=s]\\&amp;=\E[R_{t+1}\mid S_t=s]+\gamma\sum_{s&#39;\in \mathcal S}\bbP(S_{t+1}=s&#39;\mid S_t=s)\E[G_{t+1}\mid S_{t+1}=s&#39;]\\&amp;=\calR_s+\gamma\sum_{s&#39;\in\mathcal S}\calP_{ss&#39;}v(s&#39;)\end{align}\]</span> 倒数第二行利用了<strong>全期望公式</strong>，通过全概率公式容易推得。上式就是 <strong>Bellman 方程</strong>，我们可以将其写作矩阵形式： <span class="math display">\[v=\calR+\gamma\calP v\]</span> 也即 <span class="math display">\[\begin{bmatrix}v_1\\\vdots\\v_n\end{bmatrix}=\begin{bmatrix}\calR_1\\\vdots\\\calR_n\end{bmatrix}+\gamma\begin{bmatrix}\calP_{11}&amp;\cdots&amp;\calP_{1n}\\\vdots&amp;\ddots&amp;\vdots\\\calP_{n1}&amp;\cdots&amp;\calP_{nn}\end{bmatrix}\begin{bmatrix}v_1\\\vdots\\v_n\end{bmatrix}\]</span> 可以看出，Bellman 方程是一个线性方程，可以直接求解： <span class="math display">\[v=(I-\gamma\calP)^{-1}\calR\]</span> 然而，求解的复杂度是 <span class="math inline">\(O(n^3)\)</span>，其中 <span class="math inline">\(n\)</span> 是状态数量，因此只适用于小型 MRP。在后续课程中我们会学到一些求解大型 MRP 的迭代算法，包括：</p><ul><li>Dynamic programming</li><li>Monte-Carlo evaluation</li><li>Temporal-Difference learning</li></ul><h2 id="markov-decision-process">3 Markov Decision Process</h2><p>在 Markov Reward Process 中加入 action，就得到了 Markov Decision Process.</p><h3 id="markov-decision-process-1">3.1 Markov Decision Process</h3><p>定义：Markov Decision Process 是一个元组 <span class="math inline">\(\langle\calS, \calA, \calP, \calR, \gamma\rangle\)</span>，其中：</p><ul><li><span class="math inline">\(\calS\)</span> 是一个有限的状态集合</li><li><span class="math inline">\(\calA\)</span> 是一个有限的动作集合</li><li><span class="math inline">\(\calP\)</span> 是状态转移矩阵，<span class="math inline">\(\calP^a_{ss&#39;}=\bbP(S_{t+1}=s&#39;\mid S_t=s)\)</span></li><li><span class="math inline">\(\calR\)</span> 是 reward function，<span class="math inline">\(\calR_s^a=\E[R_{t+1}\mid S_t=s,A_t=a]\)</span></li><li><span class="math inline">\(\gamma\in[0,1]\)</span> 是衰减系数（discount factor）</li></ul><blockquote><p>注意 <span class="math inline">\(\calP,\calR\)</span> 的定义都加上了动作 <span class="math inline">\(a\)</span> 作为条件。</p></blockquote><p><br/></p><p>依旧用我们熟悉的例子，Student MDP 如下：</p><p><img src="mdp.png" width=60% /></p><h3 id="policy">3.2 Policy</h3><p>回顾策略 policy 的定义： <span class="math display">\[\pi(a\mid s)=\bbP(A_t=a\mid S_t=s)\]</span> 一个策略定义了 agent 的行为，它仅依赖于当前状态而与历史无关。虽然上述定义式中写了下标 <span class="math inline">\(t\)</span>，但是仅是为了书写方便，仔细想想不难知道，策略是静态的，和时间无关。</p><p><br/></p><p>MDP 与 MP 和 MRP 的联系：给出一个 MDP <span class="math inline">\(\mathcal M=\langle\calS,\calA,\calP,\calR,\gamma\rangle\)</span> 和一个策略 <span class="math inline">\(\pi\)</span>，则：</p><ul><li><span class="math inline">\(\langle\calS,\calP^\pi\rangle\)</span> 是一个 Markov Process</li><li><span class="math inline">\(\langle \calS,\calP^\pi,\calR^\pi,\gamma\rangle\)</span> 是一个 Markov Reward Process</li></ul><p>其中 <span class="math display">\[\begin{align}&amp;\calP^\pi_{ss&#39;}=\sum_{a\in\calA}\pi(a\mid s)\calP^a_{ss&#39;}\\&amp;\calR^\pi_s=\sum_{a\in\calA}\pi(a\mid s)\calR^a_s\end{align}\]</span></p><blockquote><p>这两个式子本质上是条件概率（以 <span class="math inline">\(S_t=s\)</span> 为条件）下的全概率公式——在状态 <span class="math inline">\(s\)</span> 下，依照策略 <span class="math inline">\(\pi\)</span>，有 <span class="math inline">\(\pi(a\mid s)\)</span> 的概率做出动作 <span class="math inline">\(a\)</span>，做出动作 <span class="math inline">\(a\)</span> 后有 <span class="math inline">\(\calP^a_{ss&#39;}\)</span> 的概率转移到状态 <span class="math inline">\(s&#39;\)</span>，能获得期望 reward 为 <span class="math inline">\(\calR_s^a\)</span>.</p></blockquote><h3 id="value-function-1">3.3 Value Function</h3><p>仿照 MRP 中 value function 的定义，加入策略 <span class="math inline">\(\pi\)</span> 的因素，定义 <strong>state-value function</strong> <span class="math inline">\(v_\pi(s)\)</span>： <span class="math display">\[v_\pi(s)=\E_\pi[G_{t}\mid S_t=s]\]</span> 换句话说，当我们在对 <span class="math inline">\(G_t\)</span> 采样时，需要依照策略 <span class="math inline">\(\pi\)</span> 来采样。</p><p>假若在策略 <span class="math inline">\(\pi\)</span> 下，我们第一步做出了动作 <span class="math inline">\(a\)</span>，那么在此条件下，定义 <strong>action-value function</strong> <span class="math inline">\(q_\pi(s,a)\)</span>： <span class="math display">\[q_\pi(s,a)=\E_\pi[G_t\mid S_t=s,A_t=a]\]</span> 根据条件概率定义和全概率公式，容易知道： <span class="math display">\[\begin{align}&amp;\color{purple}{v_\pi(s)=\sum_{a\in\calA}\pi(a\mid s)q_\pi(s,a)}\\&amp;q_\pi(s,a)=\pi(a\mid s)v_\pi(s)\end{align}\]</span></p><h3 id="bellman-expectation-equation">3.4 Bellman Expectation Equation</h3><p>仿照 MRP 中关于 Bellman Equation 的推导，在 MDP 中进行类似推导： <span class="math display">\[\begin{align}v_\pi(s)&amp;=\E_\pi[G_t\mid S_t=s]\\&amp;=\E_\pi[R_{t+1}+\gamma G_{t+1}\mid S_t=s]\\&amp;=\E_\pi[R_{t+1}\mid S_t=s]+\gamma\E_\pi[G_{t+1}\mid S_t=s]\\&amp;=\calR_s^\pi+\gamma\sum_{s&#39;\in\mathcal S}\calP^\pi_{ss&#39;}v_\pi(s&#39;)\\&amp;=\color{purple}{\sum_{a\in\calA}\pi(a\mid s)\left[\calR_s^a+\gamma\sum_{s&#39;\in\mathcal S}\calP^a_{ss&#39;}v_\pi(s&#39;)\right]}\end{align}\]</span> 对 <span class="math inline">\(q_\pi(s,a)\)</span> 也可以进行类似的推导： <span class="math display">\[\begin{align}q_\pi(s,a)&amp;=\E_\pi[G_t\mid S_t=s,A_t=a]\\&amp;=\E_\pi[R_{t+1}+\gamma G_{t+1}\mid S_t=s,A_t=a]\\&amp;=\E_\pi[R_{t+1}\mid S_t=s,A_t=a]+\gamma\E_\pi[G_{t+1}\mid S_t=s,A_t=a]\\&amp;=\calR_s^a+\gamma\sum_{s&#39;\in\calS}\E_\pi[G_{t+1}\mid S_{t+1}=s&#39;]\bbP(S_t=s+1\mid S_t=s,A_t=a))\\&amp;=\color{purple}{\calR_s^a+\gamma\sum_{s&#39;\in\calS}\calP^a_{ss&#39;}}v_\pi(s&#39;)\\&amp;=\color{purple}{\calR_s^a+\gamma\sum_{s&#39;\in\calS}\calP^a_{ss&#39;}}\sum_{a&#39;\in\calA}\pi(a&#39;\mid s&#39;)q_\pi(s&#39;,a&#39;)\\\end{align}\]</span> 紫色的四个式子分别建立起了 <span class="math inline">\(v_\pi(s)\)</span> 与 <span class="math inline">\(v_\pi(s&#39;)\)</span>、<span class="math inline">\(v_\pi(s)\)</span> 与 <span class="math inline">\(q_\pi(s,a)\)</span>、<span class="math inline">\(q_\pi(s,a)\)</span> 与 <span class="math inline">\(q_\pi(s&#39;,a&#39;)\)</span>、<span class="math inline">\(q_\pi(s,a)\)</span> 与 <span class="math inline">\(v_\pi(s&#39;)\)</span> 的关系，分别对应下面的四张图：</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="bellman_vq.png" width=90%/></div><div class="group-image-wrap"><img src="bellman_qv.png" width=90%/></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="bellman_vv.png" width=90%/></div><div class="group-image-wrap"><img src="bellman_qq.png" width=90%/></div></div></div><h3 id="optimal-value-function">3.5 Optimal Value Function</h3><p>我们做强化学习的最终目的是找到最佳的策略，因此定义最佳 <strong>state-value function</strong> 和最佳 <strong>action-value function</strong> 为：</p><p><span class="math display">\[\begin{align}&amp;v_\ast(s)=\max_\pi v_\pi(s)\\&amp;q_\ast(s,a)=\max_\pi q_\pi(s,a)\end{align}\]</span> 定理：对于任何 MDP，存在最优策略 <span class="math inline">\(\pi_\ast\)</span>（不一定唯一），并且 <span class="math inline">\(v_{\pi_\ast}(s)=v_\ast(s)\)</span>，<span class="math inline">\(q_{\pi_\ast}(s,a)=q_\ast(s,a)\)</span>.</p><p>反之，沿着最大化 <span class="math inline">\(q(s,a)\)</span> 的方向做决策，我们就能得到最优策略，即： <span class="math display">\[\pi_\ast(a\mid s)=\begin{cases}1&amp;\text{if }a=\arg\max\limits_{a\in\calA} q_\ast(s,a)\\0&amp;\text{otherwise}\end{cases}\]</span> 这意味着：对任何 MDP，我们的最优策略都是确定性的（而不是概率性的）；只要我们求得 <span class="math inline">\(q_\ast(s,a)\)</span>，那么就能立刻知道最优策略是什么。</p><h3 id="bellman-optimality-equation">3.6 Bellman Optimality Equation</h3><p>最优策略也是策略的一种，所以四个 Bellman 方程自然在最优情况下也成立，略作化简可得： <span class="math display">\[\begin{align}&amp;v_\ast(s)=\max_aq_\ast(s,a)\\&amp;q_\ast(s,a)=\calR^a_s+\gamma\sum_{s&#39;\in\calS}\calP^a_{ss&#39;}v_\ast(s&#39;)\\&amp;v_\ast(s)=\max_a\calR^a_s+\gamma\sum_{s&#39;\in\calS}\calP^a_{ss&#39;}v_\ast(s&#39;)\\&amp;q_\ast(s,a)=\calR_s^a+\gamma\sum_{s&#39;\in\calS}\calP^a_{ss&#39;}\max_{a&#39;}q_\ast(s&#39;,a&#39;)\end{align}\]</span> 与 Bellman Equation 和 Bellman Expectation Equation 不同，由于 <span class="math inline">\(\max\)</span> 操作的存在，Bellman Optimality Equation 是非线性的，一般没有封闭解，但存在许多迭代解法，例如：</p><ul><li>Value Iteration</li><li>Policy Iteration</li><li>Q-learning</li><li>Sarsa</li></ul><h2 id="extensions-to-mdps了解">4 Extensions to MDPs（了解）</h2><p>我们之前考虑的 MDP 都是有限的、离散的、fully observable 的，如果没有这些限制，我们可以对 MDP 进行拓展。</p><h3 id="infinite-mdps">4.1 Infinite MDPs</h3><p>无限也分好几种：</p><ul><li>可数无限的状态/动作空间：这种拓展是比较直接的</li><li>连续的状态/动作空间：Closed form for linear quadratic model</li><li>时间上连续：需要偏微分方程，Hamilton-Jacobi-Bellman equation，是 Bellman equation 在 <span class="math inline">\(t\to0\)</span> 的极限情形</li></ul><h3 id="partially-observable-mdps">4.2 Partially Observable MDPs</h3><p>POMDP 是一个具有隐状态的 MDP，是具有 actions 的隐马尔可夫模型。</p><p>定义：POMDP 是一个元组 <span class="math inline">\(\langle\calS,\calA,\mathcal O,\calP,\calR,\mathcal Z,\gamma\rangle\)</span>，其中 <span class="math inline">\(\calS,\calA,\calP,\calR,\gamma\)</span> 的定义不变，新增加了：</p><ul><li><span class="math inline">\(\mathcal O\)</span>：observations 的有限集合</li><li><span class="math inline">\(\mathcal Z\)</span>：observation function，<span class="math inline">\(\mathcal Z_{s&#39;o}^a=\bbP(O_{t+1}=o\mid S_{t+1}=s&#39;,A_t=a)\)</span></li></ul><p>回顾：<strong>历史 history</strong> 是动作、观察和奖励的序列： <span class="math display">\[H_t=A_0,O_1,R_1,\ldots,A_{t-1},O_t,R_t\]</span> 定义 <strong>belief state</strong> <span class="math inline">\(b(n)\)</span> 是在给定历史的条件下，状态的概率分布： <span class="math display">\[b(h)=(\bbP(S_t=s^1\mid H_t=h),\ldots,\bbP(S_t=s^n\mid H_t=h))\]</span> 类似于 MDP 中我们画的两种树，POMDP 也可以规约成两种树：</p><p><img src="pomdp.png" width=80% /></p><h3 id="ergodic-markov-process">4.3 Ergodic Markov Process</h3><p>Ergodic Markov Process 指：</p><ul><li>Recurrent：每一个状态会被无限次访问</li><li>Aperiodic：每一个状态被访问的时间不具有周期性</li></ul><p>定理：一个 Ergodic Markov Process 具有极限的稳态分布 <span class="math inline">\(d^\pi(s)\)</span> 满足： <span class="math display">\[d^\pi(s)=\sum_{s&#39;\in\calS}d^\pi(s&#39;)\calP_{ss&#39;}\]</span> 对于任意策略 <span class="math inline">\(\pi\)</span>，一个 ergodic MDP 有一个与起始状态无关的<strong>每时刻平均奖励</strong> <span class="math inline">\(\rho^\pi\)</span>： <span class="math display">\[\rho^\pi=\lim_{T\to\infty}\frac{1}{T}\E\left[\sum_{t=1}^TR_t\right]\]</span> 利用 <span class="math inline">\(\rho^\pi\)</span> 的定义，我们可以给出 undiscounted, ergodic MDP 的描述。设 <span class="math inline">\(\tilde v_\pi(s)\)</span> 表示由于从状态 <span class="math inline">\(s\)</span> 起始而带来的额外奖励，则 <span class="math display">\[\tilde v_\pi(s)=\E_\pi\left[\sum_{k=1}^\infty(R_{t+k}-\rho^\pi)\mid S_t=s\right]\]</span> 我们可以相应地推导 average reward Bellman equation.</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>David Silver</category>
      
    </categories>
    
    
    <tags>
      
      <tag>reinforcement learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[统计推断]第四章·二维随机变量</title>
    <link href="/blog-main/2022/03/14/%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD-%E7%AC%AC%E5%9B%9B%E7%AB%A0%C2%B7%E4%BA%8C%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F/"/>
    <url>/blog-main/2022/03/14/%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD-%E7%AC%AC%E5%9B%9B%E7%AB%A0%C2%B7%E4%BA%8C%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\E}{\mathbb E}\newcommand{\R}{\mathbb R}\newcommand{\d}{\mathrm d}\newcommand{\Var}{\mathrm{Var}}\newcommand{\Cov}{\mathrm{Cov}}\]</span></p><blockquote><p>本篇是《统计推断》第四章多维随机变量的前 5 节内容，主要关注于二维随机变量。</p></blockquote><h2 id="联合分布与边缘分布">1 联合分布与边缘分布</h2><p><span class="math inline">\(n\)</span> 维随机<strong>向量</strong>：样本空间 <span class="math inline">\(S\)</span> 到欧氏空间 <span class="math inline">\(\R^n\)</span> 的函数。</p><p><strong>联合 pmf / pdf</strong>： <span class="math display">\[P((X,Y)\in A)=\begin{cases}\sum_{(x,y)\in A}f(x,y)&amp;&amp;\text{discrete}\\\iint_A f(x,y)\d x\d y&amp;&amp;\text{continuous}\end{cases}\]</span> <strong>期望</strong>： <span class="math display">\[\E[g(X,Y)]=\begin{cases}\sum_{(x,y)\in\R^2}g(x,y)f(x,y)&amp;&amp;\text{discrete}\\\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}g(x,y)f(x,y)\d x\d y&amp;&amp;\text{continuous}\end{cases}\]</span> <strong>边缘 pmf / pdf</strong>： <span class="math display">\[f_X(x)=\begin{cases}\sum_{y\in\R}f_{X,Y}(x,y)&amp;&amp;\text{discrete}\\\int_{-\infty}^{+\infty}f_{X,Y}(x,y)\d y&amp;&amp;\text{continuous}\end{cases}\]</span> <strong>联合 cdf</strong>： <span class="math display">\[F(x,y)=P(X\leq x,Y\leq y)\]</span> 对连续二维随机向量： <span class="math display">\[\frac{\partial^2F(x,y)}{\partial x\partial y}=f(x,y)\]</span></p><h2 id="条件分布与独立性">2 条件分布与独立性</h2><h3 id="条件分布条件期望条件方差">2.1 条件分布、条件期望、条件方差</h3><p><strong>conditional pmf / pdf</strong>：设 <span class="math inline">\(f_X(x)&gt;0\)</span>， <span class="math display">\[f(y\mid x)=\frac{f(x,y)}{f_X(x)}\]</span> <strong>条件期望</strong>： <span class="math display">\[\E[g(Y)\mid X=x]=\begin{cases}\sum_yg(y)f(y\mid x)&amp;&amp;\text{discrete}\\\int_{-\infty}^{+\infty}g(y)f(y\mid x)\mathrm dy&amp;&amp;\text{continuous}\end{cases}\]</span> <strong>条件方差</strong>： <span class="math display">\[\Var(Y\mid X=x)=\E[Y^2\mid X=x]-(\E[Y\mid X=x])^2\]</span> <span class="math inline">\(Y\)</span> 在条件 <span class="math inline">\(X=x\)</span> 下的条件分布通常因 <span class="math inline">\(x\)</span> 的取值而异，所以我们实际上得到了 <span class="math inline">\(y\)</span> 的一族概率分布，每一个分布对应着一个 <span class="math inline">\(x\)</span>。类似的，<span class="math inline">\(\E[g(Y)\mid X=x]\)</span> 是 <span class="math inline">\(x\)</span> 的函数，因此 <span class="math inline">\(E[g(Y)\mid X]\)</span> 是一个取值依赖于 <span class="math inline">\(X\)</span> 的随机变量。</p><h3 id="独立的定义和一个充要条件">2.2 独立的定义和一个充要条件</h3><p><strong>独立</strong>： <span class="math display">\[f(x,y)=f_X(x)f_Y(y)\]</span> 易知：<span class="math inline">\(f(y\mid x)=f_Y(y)\)</span>，即条件 <span class="math inline">\(X=x\)</span> 并没有提供关于 <span class="math inline">\(Y\)</span> 的额外信息。</p><p><br/></p><p><strong>引理</strong>：<span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 独立当且仅当存在函数 <span class="math inline">\(g(x)\)</span> 和 <span class="math inline">\(h(y)\)</span>，使得对于任意 <span class="math inline">\(x,y\in\R\)</span> 都有： <span class="math display">\[f(x,y)=g(x)h(y)\]</span> <em>Proof</em>. 取 <span class="math inline">\(g(x)=f_X(x),\,h(y)=f_Y(y)\)</span> 即可证明必要性。为证明充分性，不妨设 <span class="math inline">\(X,Y\)</span> 都是连续随机变量，并令： <span class="math display">\[c=\int_{-\infty}^{+\infty}g(x)\mathrm dx\quad d=\int_{-\infty}^{+\infty}h(y)\mathrm dy\]</span> 则： <span class="math display">\[\begin{align}cd&amp;=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}g(x)h(y)\mathrm dx\mathrm dy\\&amp;=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x,y)\mathrm dx\mathrm dy\\&amp;=1\end{align}\]</span> 且边缘 pdf 为： <span class="math display">\[\begin{align}f_X(x)&amp;=\int_{-\infty}^{+\infty}f(x,y)\mathrm dy=\int_{-\infty}^{+\infty}g(x)h(y)\mathrm dy=dg(x)\\f_Y(y)&amp;=\int_{-\infty}^{+\infty}f(x,y)\mathrm dx=\int_{-\infty}^{+\infty}g(x)h(y)\mathrm dx=ch(y)\end{align}\]</span> 于是： <span class="math display">\[f(x,y)=g(x)h(y)=cdg(x)h(y)=f_X(x)f_Y(y)\]</span> Q.E.D.</p><blockquote><p>注记：虽然 <span class="math inline">\(f(x,y)=g(x)h(y)\)</span> 并不意味着 <span class="math inline">\(g(x)\)</span> 和 <span class="math inline">\(h(y)\)</span> 就是边缘分布，但是他们和边缘分布呈倍数关系，且这两个倍数的系数乘积为 <span class="math inline">\(1\)</span>.</p></blockquote><h3 id="独立随机变量的期望和矩母函数">2.3 独立随机变量的期望和矩母函数</h3><p>独立随机变量下，某些计算将变得十分简单：</p><p><strong>定理</strong>：设 <span class="math inline">\(X,Y\)</span> 是独立随机变量，则： <span class="math display">\[\E[g(X)h(Y)]=\E[g(X)]\E[h(Y)]\]</span> <em>Proof</em>. 不妨设 <span class="math inline">\(X,Y\)</span> 是连续随机变量，则： <span class="math display">\[\begin{align}\E[g(X)h(Y)]&amp;=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}g(x)h(y)f(x,y)\mathrm dx\mathrm dy\\&amp;=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}g(x)h(y)f_X(x)f_Y(y)\mathrm dx\mathrm dy\\&amp;=\left(\int_{-\infty}^{+\infty}g(x)f_X(x)\mathrm dx\right)\left(\int_{-\infty}^{+\infty}h(y)f_Y(y)\mathrm dy\right)\\&amp;=\E[g(X)]\E[h(Y)]\end{align}\]</span> Q.E.D.</p><p><br/></p><p>应用上述定理，我们可以推导两个独立随机变量的和的矩母函数：</p><p><strong>定理</strong>：设独立随机变量 <span class="math inline">\(X,Y\)</span> 的矩母函数分别为 <span class="math inline">\(M_X(t)\)</span> 和 <span class="math inline">\(M_Y(t)\)</span>，则随机变量 <span class="math inline">\(Z=X+Y\)</span> 的矩母函数为 <span class="math inline">\(M_Z(t)=M_X(t)M_Y(t)\)</span>.</p><p><em>Proof</em>. <span class="math display">\[M_Z(t)=\E[e^{tZ}]=\E[e^{t(X+Y)}]=\E[e^{tX}e^{tY}]=\E[e^{tX}]\E[e^{tY}]=M_X(t)M_Y(t)\]</span> Q.E.D.</p><div class="note note-success">            <p>例【独立正态随机变量的和】设 <span class="math inline">\(X\sim N(\mu,\sigma^2),\,Y\sim N(\gamma,\tau^2)\)</span> 是两个独立的正态随机变量，则随机变量 <span class="math inline">\(Z=X+Y\)</span> 服从 <span class="math inline">\(N(\mu+\gamma,\sigma^2+\tau^2)\)</span>.</p><p><em>Proof</em>. 根据前面章节的计算，我们知道 <span class="math inline">\(X,Y\)</span> 的矩母函数分别是： <span class="math display">\[M_X(t)=\exp(\mu t+\sigma^2t^2/2)\quad M_Y(t)=\exp(\gamma t+\tau^2t^2/2)\]</span> 于是根据上一条定理，有： <span class="math display">\[M_Z(t)=M_X(t)M_Y(t)=\exp((\mu+\gamma)t+(\sigma^2+\tau^2)t^2/2)\]</span> 于是 <span class="math inline">\(Z\sim N(\mu+\gamma,\sigma^2+\tau^2)\)</span>.</p><p>Q.E.D.</p>          </div><h2 id="二维变换">3 二维变换</h2><h3 id="二维随机向量的向量函数">3.1 二维随机向量的向量函数</h3><p>设 <span class="math inline">\((X,Y)\)</span> 是概率分布已知的二维随机向量，考察新的二维随机向量 <span class="math inline">\((U, V)\)</span>，其中 <span class="math inline">\(U=g_1(X,Y),\,V=g_2(X,Y)\)</span>.</p><p>若 <span class="math inline">\((X,Y)\)</span> 是离散的，则存在一个可数集 <span class="math inline">\(A\)</span> 使得 <span class="math inline">\((X,Y)\)</span> 的联合 pmf 在其上取值大于 <span class="math inline">\(0\)</span>. 令 <span class="math inline">\(B=\{(u,v):\exists (x,y)\in A\text{ s.t. }u=g_1(x,y),\,v=g_2(x,y)\}\)</span>，则 <span class="math inline">\(B\)</span> 是离散随机向量 <span class="math inline">\((U,V)\)</span> 全体可能的取值所构成的集合，是可数集。记 <span class="math inline">\(A_{uv}=\{(x,y)\in A:g_1(x,y)=u,\,g_2(x,y)=v\}\)</span>，则： <span class="math display">\[f_{U,V}(u,v)=P(U=u,V=v)=P((X,Y)\in A_{uv})=\sum_{(x,y)\in A_{uv}}f_{X,Y}(x,y)\]</span> <div class="note note-success">            <p>例【独立泊松随机变量的和】设 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 是一对独立的泊松随机变量，参数分别为 <span class="math inline">\(\theta\)</span> 和 <span class="math inline">\(\lambda\)</span>，则 <span class="math inline">\(X+Y\)</span> 服从参数为 <span class="math inline">\(\theta+\lambda\)</span> 的泊松分布。</p>          </div></p><p>若 <span class="math inline">\((X,Y)\)</span> 是连续的，且 <span class="math inline">\(u=g_1(x,y)\)</span> 和 <span class="math inline">\(v=g_2(x,y)\)</span> 都是一对一的，则我们能从中解出逆变换：<span class="math inline">\(x=h_1(u,v),\,y=h_2(u,v)\)</span>. 定义 Jacobi 行列式： <span class="math display">\[J=\begin{vmatrix}\frac{\partial x}{\partial u}&amp;\frac{\partial x}{\partial v}\\\frac{\partial y}{\partial u}&amp;\frac{\partial y}{\partial v}\end{vmatrix}=\frac{\partial x}{\partial u}\frac{\partial y}{\partial v}-\frac{\partial x}{\partial v}\frac{\partial y}{\partial u}\]</span> 则： <span class="math display">\[f_{U,V}(u,v)=f_{X,Y}(h_1(u,v),h_2(u,v))|J|\]</span> &gt; 回忆第二章中，随机变量的单调函数的结论： &gt; <span class="math display">\[&gt; f_Y(y)=f_X(g^{-1}(y))\left|\frac{\mathrm d}{\mathrm dy}g^{-1}(y)\right|&gt; \]</span> &gt; 可以看见他们具有类似的形式，二维情形就是一维的拓展。</p><h3 id="独立随机变量的函数依然独立">3.2 独立随机变量的函数依然独立</h3><p><strong>定理</strong>：设 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 是一对独立的随机变量，<span class="math inline">\(g(x)\)</span> 是 <span class="math inline">\(x\)</span> 的一元函数，<span class="math inline">\(h(y)\)</span> 是 <span class="math inline">\(y\)</span> 的一元函数，则随机变量 <span class="math inline">\(U=g(X)\)</span> 与 <span class="math inline">\(V=h(Y)\)</span> 独立。</p><p><em>Proof</em>. 不妨设 <span class="math inline">\(U,V\)</span> 都是连续随机变量。记：<span class="math inline">\(A_u=\{x:g(x)\leq u\},\,B_v=\{y:h(y)\leq v\}\)</span>，则 <span class="math inline">\(U,V\)</span> 的联合 cdf 为： <span class="math display">\[F_{U, V}(u, v)=P(U\leq u, V\leq v)=P(X\in A_u,Y\in B_v)=P(X\in A_u)P(Y\in B_v)\]</span> 故联合 pdf 为： <span class="math display">\[f_{U, V}(u, v)=\frac{\partial^2}{\partial u\partial v}F_{U, V}(u, v)=\left(\frac{\mathrm d}{\mathrm du}P(X\in A_u)\right)\left(\frac{\mathrm d}{\mathrm dv}P(X\in B_v)\right)\]</span> 该乘积的第一项是 <span class="math inline">\(u\)</span> 的函数，第二项是 <span class="math inline">\(v\)</span> 的函数，由上一节独立的充要条件知 <span class="math inline">\(U, V\)</span> 独立。</p><p>Q.E.D.</p><h2 id="多层模型与混合分布">4 多层模型与混合分布</h2><h3 id="多层模型">4.1 多层模型</h3><p>把事件分层建模往往更加容易。一个经典的例子是，一只昆虫产下大量的卵，已知每颗卵的成活率为 <span class="math inline">\(p\)</span>，问平均有多少颗卵能存活？</p><p>昆虫产卵的数量 <span class="math inline">\(Y\)</span> 是一个服从参数为 <span class="math inline">\(\lambda\)</span> 的泊松分布的随机变量，存活卵的数量 <span class="math inline">\(X\)</span> 是一个服从参数为 <span class="math inline">\((Y,p)\)</span> 的二项分布的随机变量，因此我们可以建立分层模型： <span class="math display">\[\begin{align}&amp;X\mid Y\sim\text{binomial}(Y, p)&amp;&amp;\text{第一层（顶层）}\\&amp;Y\sim \text{poisson}(\lambda)&amp;&amp;\text{第二层}\end{align}\]</span> 那么 <span class="math inline">\(X\)</span> 实际上具有如下分布： <span class="math display">\[\begin{align}P(X=x)&amp;=\sum_{y=0}^{+\infty}P(X=x, Y=y)\\&amp;=\sum_{y=0}^{+\infty}P(X=x\mid Y=y)P(Y=y)\\&amp;=\sum_{y=0}^{+\infty}\binom{y}{x}p^x(1-p)^{y-x}\frac{e^{-\lambda}\lambda^y}{y!}\\&amp;=\frac{e^{-\lambda}(p\lambda)^x}{x!}\sum_{y=x}^{+\infty}\frac{(\lambda(1-p))^{y-x}}{(y-x)!}\\&amp;=\frac{e^{-\lambda}(p\lambda)^x}{x!}\sum_{t=0}^{+\infty}\frac{(\lambda(1-p))^{t}}{t!}&amp;&amp;t=y-x\\&amp;=\frac{e^{-\lambda}(p\lambda)^x}{x!}e^{\lambda(1-p)}\\&amp;=\frac{(p\lambda)^x}{x!}e^{-p\lambda}\end{align}\]</span> 故 <span class="math inline">\(X\sim\text{poisson}(p\lambda)\)</span>，与 <span class="math inline">\(Y\)</span> 没有关系。</p><h3 id="重期望公式">4.2 重期望公式</h3><p><strong>重期望公式</strong>：设 <span class="math inline">\(X，Y\)</span> 是任意随机变量，若下列期望存在，则有： <span class="math display">\[\E X=\E[\E[X\mid Y]]\]</span> <em>Proof</em>. 设 <span class="math inline">\(f(x,y)\)</span> 是联合 pdf，则： <span class="math display">\[\E X=\iint xf(x,y)\mathrm dx\mathrm dy=\int\left[\int xf(x\mid y)\mathrm dx\right]f(y)\mathrm dy=\E[\E[X\mid Y]]\]</span> Q.E.D.</p><h3 id="混合分布">4.3 混合分布</h3><p><strong>混合分布</strong>指的是多层模型导出的分布，可以定义为：若随机变量 <span class="math inline">\(X\)</span> 的分布依赖于服从某分布的另一个量，则称 <span class="math inline">\(X\)</span> 服从混合分布。</p><p><strong>例【Poisson-Gamma 混合分布】</strong>设 <span class="math inline">\(Y\)</span> 有多层模型： <span class="math display">\[\begin{align}&amp;Y\mid \Lambda\sim\text{poisson}(\Lambda)\\&amp;\Lambda\sim \text{Gamma}(\alpha,\beta)\end{align}\]</span> 则 <span class="math inline">\(Y\)</span> 的边缘分布（当 <span class="math inline">\(\alpha\in\mathbb Z\)</span> 时）为负二项分布： <span class="math display">\[\begin{align}P(Y=y)&amp;=\int_0^{+\infty} P(Y=y,\Lambda=\lambda)\mathrm d\lambda\\&amp;=\int_0^{+\infty} P(Y=y\mid\Lambda=\lambda)P(\Lambda=\lambda)\mathrm d\lambda\\&amp;=\int_0^{+\infty}\frac{e^{-\lambda}\lambda^y}{y!}\frac{1}{\Gamma(\alpha)\beta^\alpha}\lambda ^{\alpha-1}e^{-\lambda/\beta}\mathrm d\lambda\\&amp;=\frac{1}{y!(\alpha-1)!\beta^\alpha}\int_0^{+\infty}\lambda ^{y+\alpha-1}e^{-\frac{\beta+1}{\beta}\lambda}\mathrm d\lambda\\&amp;=\frac{\beta^{y}}{y!(\alpha-1)!{(\beta+1)}^{y+\alpha}}\int_0^{+\infty}\left(\frac{\beta+1}{\beta}\lambda\right)^{y+\alpha-1}e^{-\left(\frac{\beta+1}{\beta}\lambda\right)}\mathrm d\left(\frac{\beta+1}{\beta}\lambda\right)\\&amp;=\frac{\beta^{y}}{y!(\alpha-1)!{(\beta+1)}^{y+\alpha}}\Gamma(y+\alpha)\\&amp;=\binom{y+\alpha-1}{y}\left(\frac{\beta}{\beta+1}\right)^{y}\left(\frac{1}{\beta+1}\right)^\alpha\end{align}\]</span></p><h3 id="方差恒等式">4.4 方差恒等式</h3><p>设 <span class="math inline">\(X，Y\)</span> 是任意随机变量，若下列期望存在，则有： <span class="math display">\[\Var X=\E[\Var(X\mid Y)]+\Var(\E[X\mid Y])\]</span> <em>Proof</em>. 根据 <span class="math inline">\(\Var X=\E X^2-(\E X)^2\)</span> 和重期望公式，有： <span class="math display">\[\begin{align}&amp;\E[\Var(X\mid Y)]+\Var(\E[X\mid Y])\\=&amp;\E[\E[X^2\mid Y]-(\E[X\mid Y])^2]+\E[(\E[X\mid Y])^2]-(\E[\E[X\mid Y]])^2\\=&amp;\E X^2-(\E X)^2\\=&amp;\Var X\end{align}\]</span> Q.E.D.</p><h2 id="协方差与相关系数">5 协方差与相关系数</h2><h3 id="定义">5.1 定义</h3><p>随机变量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 的<strong>协方差</strong>为： <span class="math display">\[\Cov(X,Y)=\E[(X-\E X)(Y-\E Y)]\]</span> <strong>相关系数</strong>为： <span class="math display">\[\rho_{XY}=\frac{\Cov(X,Y)}{\sqrt{\Var X\Var Y}}\]</span></p><h3 id="定理">5.2 定理</h3><p><strong>定理</strong>：设 <span class="math inline">\(X,Y\)</span> 是任意随机变量，则： <span class="math display">\[\Cov(X,Y)=\E XY-\E X\E Y\]</span> <em>Proof</em>. <span class="math display">\[\begin{align}\Cov(X,Y)&amp;=\E[(X-\E X)(Y-\E Y)]\\&amp;=\E[XY-X\E Y-Y\E X+\E X\E Y]\\&amp;=\E XY-\E X\E Y-\E X\E Y+\E X\E Y\\&amp;=\E XY-\E X\E Y\\\end{align}\]</span> Q.E.D.</p><p><br/></p><p><strong>定理</strong>：设 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 是一对<strong>独立</strong>的随机变量，则 <span class="math inline">\(\Cov(X,Y)=\rho_{XY}=0\)</span>.</p><p><em>Proof</em>. 由于 <span class="math inline">\(X,Y\)</span> 独立，根据上一篇的定理知 <span class="math inline">\(\E XY=\E X\E Y\)</span>，故 <span class="math inline">\(\Cov(X,Y)=\E XY-\E X\E Y=0\)</span>. Q.E.D.</p><p>但是，<span class="math inline">\(\Cov(X,Y)=0\)</span> 或 <span class="math inline">\(\E XY=\E X\E Y\)</span> 并不代表 <span class="math inline">\(X,Y\)</span> 独立。</p><p><br/></p><p><strong>定理</strong>：设 <span class="math inline">\(X,Y\)</span> 是任意随机变量，<span class="math inline">\(a,b\)</span> 是任意两个常量，则： <span class="math display">\[\Var(aX+bY)=a^2\Var X+b^2\Var Y+2ab\Cov(X,Y)\]</span> <em>Proof</em>. <span class="math display">\[\begin{align}\Var(aX+bY)&amp;=\E[(aX+bY-a\E X-b\E Y)^2]\\&amp;=\E[(a(X-\E X)+b(Y-\E Y))^2]\\&amp;=\E[a^2(X-\E X)^2]+\E[b^2(Y-\E Y)^2]+\E[2ab(X-\E X)(Y-\E Y)]\\&amp;=a^2\E[(X-\E X)^2]+b^2\E[(Y-\E Y)^2]+2ab\E[(X-\E X)(Y-\E Y)]\\&amp;=a^2\Var X+b^2\Var Y+2ab\Cov(X,Y)\end{align}\]</span> Q.E.D.</p><p>特别的，如果 <span class="math inline">\(X,Y\)</span> 独立，那么 <span class="math inline">\(\Var(aX+bY)=a^2\Var X+b^2\Var Y\)</span>.</p><h3 id="协方差与相关本质是度量线性关系">5.3 协方差与相关本质是度量线性关系</h3><p><strong>定理</strong>：设 <span class="math inline">\(X,Y\)</span> 是任意随机变量，则：</p><ol type="1"><li><span class="math inline">\(\rho_{XY}\in[-1,1]\)</span></li><li><span class="math inline">\(|\rho_{XY}|=1\)</span> 当且仅当存在数 <span class="math inline">\(a\neq 0\)</span> 以及 <span class="math inline">\(b\)</span> 使得 <span class="math inline">\(P(Y=aX+b)=1\)</span>. 若 <span class="math inline">\(\rho_{XY}=1\)</span>，则 <span class="math inline">\(a&gt;0\)</span>；若 <span class="math inline">\(\rho_{XY}=-1\)</span>，则 <span class="math inline">\(a&lt;0\)</span>.</li></ol><p><em>Proof</em>. 考察关于 <span class="math inline">\(t\)</span> 的函数 <span class="math inline">\(h(t)\)</span>： <span class="math display">\[h(t)=\E[((X-\E X)t+(Y-\E Y))^2]=t^2\Var X+2t\Cov(X,Y)+\Var Y\]</span> 这是一个二次函数。由于对于任意 <span class="math inline">\(t\)</span>，<span class="math inline">\(h(t)\)</span> 是一个非负随机变量的期望，所以其值非负，故二次函数判别式小于等于 <span class="math inline">\(0\)</span>： <span class="math display">\[\Delta=4\Cov(X,Y)^2-4\Var X\Var Y\leq 0\]</span> 得到： <span class="math display">\[\Cov(X,Y)^2\leq \Var X\Var Y\implies \rho_{XY}^2=\frac{\Cov(X,Y)^2}{\Var X\Var Y}\leq 1\implies-1\leq\rho_{XY}\leq 1\]</span> 这证明了第一个结论。</p><p>另外，当 <span class="math inline">\(\rho_{XY}^2=1\)</span> 时，<span class="math inline">\(\Delta=0\)</span>，说明 <span class="math inline">\(h(t)\)</span> 有一个二重根，设为 <span class="math inline">\(t_0\)</span>，即 <span class="math inline">\(h(t_0)=0\)</span>. 为书写方便，记 <span class="math inline">\(Z=((X-\E X)t_0+(Y-\E Y))^2\geq 0\)</span>，则 <span class="math display">\[h(t_0)=\E Z=\int_0^{+\infty} zP(Z=z)\mathrm dz\]</span> 可以看出，<span class="math inline">\(h(t_0)=0\)</span> 当且仅当 <span class="math inline">\(P(Z=0)=1\)</span>，即： <span class="math display">\[P(((X-\E X)t_0+(Y-\E Y))^2=0)=1\]</span> 也即： <span class="math display">\[P((X-\E X)t_0+(Y-\E Y)=0)=1\]</span> 取 <span class="math inline">\(a=-t_0,\,b=t_0\E X+\E Y\)</span> 即得第二个结论的前半部分。</p><p>又因为从 <span class="math inline">\(h(t_0)=0\)</span> 中可以解出：<span class="math inline">\(t_0=\frac{-2\Cov(X,Y)\pm\sqrt\Delta}{2\Var X}=-\frac{\Cov(X,Y)}{\Var X}\)</span>，可以看出 <span class="math inline">\(a\)</span> 与 <span class="math inline">\(\rho_{XY}\)</span> 同号，这证明了第二个结论的后半部分。</p><p>Q.E.D.</p><div class="note note-success">            <p>例【依赖关系很强但相关系数很小】：设 <span class="math inline">\(X\sim U(-1,1)\)</span>，<span class="math inline">\(Z\sim U(0,1/10)\)</span>，且 <span class="math inline">\(X\)</span> 与 <span class="math inline">\(Z\)</span> 独立。令 <span class="math inline">\(Y=X^2+Z\)</span>，考察随机向量 <span class="math inline">\((X,Y)\)</span>，在给定 <span class="math inline">\(X=x\)</span> 的条件下，<span class="math inline">\(Y=x^2+Z\)</span>，条件分布是 <span class="math inline">\(U(x^2,x^2+1/10)\)</span>，即： <span class="math display">\[f_Y(y\mid X=x)=\begin{cases}10,&amp;x^2\leq y\leq x^2+1/10\\0,&amp;\text{otherwise}\end{cases}\]</span> 于是联合分布： <span class="math display">\[f(x,y)=f_X(x)f_Y(y\mid X=x)=5\quad -1&lt;x&lt;1,\,x^2&lt;y&lt;x^2+1/10\]</span> 下图显示了 <span class="math inline">\((X,Y)\)</span> 的支撑集：</p><p><img src="rho0.jpg" width=50% /></p><p>可以看出，<span class="math inline">\(X,Y\)</span> 有着很强的依赖关系，但这种关系<strong>是非线性的</strong>，我们下面证明，它们的相关系数其实是 <span class="math inline">\(0\)</span>.</p><p>由于 <span class="math inline">\(\E X=\E X^3=0\)</span>，<span class="math inline">\(\E XZ=\E X\E Z=0\)</span>，故 <span class="math display">\[\begin{align}\Cov(X,Y)&amp;=\E [X(X^2+Z)]-\E X\E(X^2+Z)\\&amp;=\E X^3+\E XZ-\E X\E(X^2+Z)\\&amp;=0\end{align}\]</span> 进而 <span class="math inline">\(\rho_{XY}=0\)</span>.</p>          </div><h3 id="二维正态分布">5.4 二维正态分布</h3><p>设 <span class="math inline">\(-\infty&lt;\mu_X&lt;+\infty,\,-\infty&lt;\mu_Y&lt;+\infty,\,0&lt;\sigma_X,\,0&lt;\sigma_Y,\,-1&lt;\rho&lt;1\)</span>，则<strong>期望为 <span class="math inline">\(\mu_X,\mu_Y\)</span>、方差为 <span class="math inline">\(\sigma_X^2,\sigma_Y^2\)</span>、相关系数为 <span class="math inline">\(\rho\)</span> 的二维正态概率密度函数</strong>为： <span class="math display">\[f(x,y)=\frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}}\exp\left(-\frac{1}{2(1-\rho^2)}\left(\left(\frac{x-\mu_X}{\sigma_X}\right)^2-2\rho\left(\frac{x-\mu_X}{\sigma_X}\right)\left(\frac{y-\mu_Y}{\sigma_Y}\right)+\left(\frac{y-\mu_Y}{\sigma_Y}\right)^2\right) \right)\]</span> 二维正态分布有很多很好的性质：</p><ol type="1"><li><span class="math inline">\(X\)</span> 的边缘分布为 <span class="math inline">\(N(\mu_X,\sigma_X)\)</span></li><li><span class="math inline">\(Y\)</span> 的边缘分布为 <span class="math inline">\(N(\mu_Y,\sigma_Y)\)</span></li><li><span class="math inline">\(X,Y\)</span> 的相关系数为 <span class="math inline">\(\rho_{XY}=\rho\)</span></li><li>对任意常量 <span class="math inline">\(a,b\)</span>，<span class="math inline">\(aX+bY\sim N(a\mu_X+b\mu_Y,a^2\sigma_X^2+b^2\sigma_Y^2+2ab\rho\sigma_X\sigma_Y)\)</span></li></ol><p>注意，二维正态分布的所有边缘分布都是正态的，但是边缘分布是正态的并不能说明联合分布是正态的。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>统计推断</category>
      
    </categories>
    
    
    <tags>
      
      <tag>statistics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[统计推断]第三章·常见分布族（三）</title>
    <link href="/blog-main/2022/03/06/%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD-%E7%AC%AC%E4%B8%89%E7%AB%A0%C2%B7%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83%E6%97%8F%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <url>/blog-main/2022/03/06/%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD-%E7%AC%AC%E4%B8%89%E7%AB%A0%C2%B7%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83%E6%97%8F%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\E}{\mathbb E}\newcommand{\Var}{\mathrm{Var}}\newcommand{\Beta}{\mathrm{B}}\newcommand{\btheta}{\boldsymbol{\theta}}\]</span></p><h2 id="指数族">1 指数族</h2><p>若一个概率密度函数族或概率质量函数族可以表示为： <span class="math display">\[f(x\vert\btheta)=h(x)c(\btheta)\exp\left(\sum_{i=1}^kw_i(\btheta)t_i(x)\right)\tag{1}\label{form1}\]</span> 则称之为<strong>指数族（exponential family）</strong>。其中 <span class="math inline">\(h(x)\geq 0\)</span>，<span class="math inline">\(t_1(x),\ldots,t_k(x)\)</span> 是观测值 <span class="math inline">\(x\)</span> 且不依赖于 <span class="math inline">\(\btheta\)</span> 的实值函数，<span class="math inline">\(c(\btheta)\geq 0\)</span>，<span class="math inline">\(w_1(\btheta),\ldots,w_k(\btheta)\)</span> 是参数向量 <span class="math inline">\(\btheta\)</span> 且不依赖于 <span class="math inline">\(x\)</span> 的实值函数。连续的正态分布族、伽玛分布族、贝塔分布族，离散的二项分布族、泊松分布族、负二项分布族都是指数族。</p><p>指数族的特殊形式决定了它具有很好的统计学性质：</p><p><strong>定理</strong>：设随机变量 <span class="math inline">\(X\)</span> 的 pdf 或 pmf 形如上式，则： <span class="math display">\[\begin{align}&amp;\E\left[\sum_{i=1}^k\frac{\partial w_i(\btheta)}{\partial \btheta_j}t_i(X)\right]=-\frac{\partial}{\partial\btheta_j}\ln c(\btheta)\\&amp;\Var\left(\sum_{i=1}^k\frac{\partial w_i(\btheta)}{\partial \btheta_j}t_i(X)\right)=-\frac{\partial^2}{\partial\btheta_j^2}\ln c(\btheta)-\E\left[\sum_{i=1}^k\frac{\partial^2 w_i(\btheta)}{\partial \btheta_j^2}t_i(X)\right]\end{align}\]</span> 上面的公式虽然看似复杂，但应用时却很奏效，它们将积分与求和变成求导运算，而后者计算起来非常容易。</p><p><em>Proof</em>.</p><ol type="1"><li><p>对 <span class="math display">\[\int f(x\vert \btheta)\mathrm dx=\int h(x)c(\btheta)\exp\left(\sum_{i=1}^kw_i(\btheta)t_i(x)\right)\mathrm dx=1\]</span> 求导，得到： <span class="math display">\[\begin{align}&amp;\int\left[h(x)\exp\left(\sum_{i=1}^kw_i(\btheta)t_i(x)\right)\left(\frac{\partial c(\btheta)}{\partial\btheta_j}+c(\btheta)\frac{\partial w_i(\btheta)}{\partial \btheta_j}t_i(x)\right)\right]\mathrm dx\\=&amp;\E\left[\sum_{i=1}^k\frac{\partial w_i(\btheta)}{\partial \btheta_j}t_i(X)\right]+\frac{\partial c(\btheta)}{\partial\btheta_j}\int\left[h(x)\exp\left(\sum_{i=1}^kw_i(\btheta)t_i(x)\right)\right]\mathrm dx\\=&amp;\E\left[\sum_{i=1}^k\frac{\partial w_i(\btheta)}{\partial \btheta_j}t_i(X)\right]+\frac{\partial c(\btheta)}{\partial\btheta_j}\frac{1}{c(\btheta)}\\=&amp;\E\left[\sum_{i=1}^k\frac{\partial w_i(\btheta)}{\partial \btheta_j}t_i(X)\right]+\frac{\partial}{\partial\btheta_j}\ln c(\btheta)\\=&amp;0\end{align}\]</span></p></li><li><p>对上式求两次导，可证得第二式，过程略。</p></li></ol><p>Q.E.D.</p><div class="note note-success">            <p>例子【二项分布】设 <span class="math inline">\(n\)</span> 为正整数，<span class="math inline">\(0&lt;p&lt;1\)</span>： <span class="math display">\[\begin{align}f(x\vert p)&amp;=\binom{n}{x}p^x(1-p)^{n-x}\\&amp;=\binom{n}{x}(1-p)^n\left(\frac{p}{1-p}\right)^x\\&amp;=\binom{n}{x}(1-p)^n\exp\left(x\cdot\ln\frac{p}{1-p}\right)&amp;&amp;x=0,\ldots,n\end{align}\]</span> 取 <span class="math display">\[\begin{align}&amp;h(x)=\binom{n}{x}&amp;&amp;x=0,\ldots,n\\&amp;c(p)=(1-p)^n&amp;&amp;0&lt;p&lt;1\\&amp;w_1(p)=\ln\frac{p}{1-p}&amp;&amp;0&lt;p&lt;1\\&amp;t_1(x)=x&amp;&amp;x=0,\ldots,n\end{align}\]</span> 由于我们限制了 <span class="math inline">\(x=0,\ldots,n\)</span>，<span class="math inline">\(0&lt;p&lt;1\)</span>，保证了 <span class="math inline">\(h(x)\geq 0,\,c(p)\geq0\)</span>，因此在给定 <span class="math inline">\(n\)</span> 的情形下，二项分布族是一个指数族。</p><p>由于 <span class="math display">\[\begin{align}&amp;\frac{\mathrm d}{\mathrm dp}w_1(p)=\frac{\mathrm d}{\mathrm dp}\ln\frac{p}{1-p}=\frac{1}{p(1-p)}\\&amp;\frac{\mathrm d^2}{\mathrm dp^2}w_1(p)=\frac{\mathrm d}{\mathrm dp}\frac{1}{p(1-p)}=\frac{2p-1}{p^2(1-p)^2}\\&amp;\frac{\mathrm d}{\mathrm dp}\ln c(p)=\frac{\mathrm d}{\mathrm dp}n\ln (1-p)=\frac{-n}{1-p}\\&amp;\frac{\mathrm d^2}{\mathrm dp^2}\ln c(p)=\frac{\mathrm d}{\mathrm dp}\frac{-n}{1-p}=\frac{-n}{(1-p)^2}\end{align}\]</span> 故根据前文定理得： <span class="math display">\[\begin{align}&amp;\E\left[\frac{X}{p(1-p)}\right]=\frac{n}{1-p}\\&amp;\Var\left(\frac{X}{p(1-p)}\right)=\frac{n}{(1-p)^2}-\E\left[\frac{2p-1}{p^2(1-p)^2}X\right]\end{align}\]</span> 解得：<span class="math inline">\(\E X=np,\,\Var X=np^2-np(2p-1)=np(1-p)\)</span>.</p>          </div><div class="note note-success">            <p>例子【正态分布】设 <span class="math inline">\(-\infty&lt;\mu&lt;+\infty\)</span>，<span class="math inline">\(\sigma&gt;0\)</span>： <span class="math display">\[f(x\vert \mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\mu^2}{2\sigma^2}\right)\exp\left(-\frac{x^2}{2\sigma^2}+\frac{\mu x}{\sigma^2}\right)\]</span> 取 <span class="math display">\[\begin{align}&amp;h(x)=1&amp;&amp;-\infty&lt;x&lt;+\infty\\&amp;c(\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\mu^2}{2\sigma^2}\right)&amp;&amp;-\infty&lt;\mu&lt;+\infty,\,\sigma&gt;0\\&amp;w_1(\mu,\sigma)=-\frac{1}{\sigma^2}&amp;&amp;-\infty&lt;\mu&lt;+\infty,\,\sigma&gt;0\\&amp;w_2(\mu,\sigma)=\frac{\mu}{\sigma^2}&amp;&amp;-\infty&lt;\mu&lt;+\infty,\,\sigma&gt;0\\&amp;t_1(x)=\frac{x}{2}&amp;&amp;-\infty&lt;x&lt;+\infty\\&amp;t_2(x)=x&amp;&amp;-\infty&lt;x&lt;+\infty\end{align}\]</span> 故正态分布族是指数族。</p>          </div><p>注意，<u>在把概率密度/质量函数族写作 <span class="math inline">\(\eqref{form1}\)</span> 形式时，应保证 <span class="math inline">\(x\)</span> 的定义域范围不变</u>，我们可以借助<strong>示性函数</strong>将 <span class="math inline">\(x\)</span> 的定义范围写入表达式。例如对于正态分布： <span class="math display">\[f(x\vert \mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\mu^2}{2\sigma^2}\right)\exp\left(-\frac{x^2}{2\sigma^2}+\frac{\mu x}{\sigma^2}\right)I_{(-\infty,+\infty)}(x)\]</span> 定义域有时候会比较 tricky。例如概率密度函数 <span class="math inline">\(f(x\vert \theta)=\frac{1}{\theta}\exp\left(1-\frac{x}{\theta}\right)I_{[\theta,+\infty)}(x)\)</span> 虽然看起来很像指数族，但是由于 <span class="math inline">\(x\)</span> 的定义范围 <span class="math inline">\(I_{[\theta,+\infty)}(x)\)</span> 依赖于 <span class="math inline">\(\theta\)</span>，故它并不是指数族。</p><p><br/></p><p>如果对指数族定义式做一个变量替换 <span class="math inline">\(\eta_i=w_i(\btheta)\)</span>，得到指数族的另一个形式： <span class="math display">\[f(x\vert \eta)=h(x)c^\ast(\eta)\exp\left(\sum_{i=1}^k\eta_i t_i(x)\right)\tag{2}\label{form2}\]</span> 称集合 <span class="math display">\[\mathcal H=\left\{\eta=(\eta_1,\ldots,\eta_k):\int_{-\infty}^{+\infty}h(x)\exp\left(\sum_{i=1}^k\eta_it_i(x)\right)\mathrm dx&lt;+\infty\right\}\]</span> 为指数族的<strong>自然参数空间</strong>。对于任意 <span class="math inline">\(\eta\in\mathcal H\)</span>，为保证概率密度函数积分为 <span class="math inline">\(1\)</span>，必有 <span class="math display">\[c^\ast(\eta)=\left[\int_{-\infty}^{+\infty}h(x)\exp\left(\sum_{i=1}^k\eta_it_i(x)\right)\mathrm dx\right]^{-1}\]</span> <span class="math inline">\(\eqref{form2}\)</span> 式比 <span class="math inline">\(\eqref{form1}\)</span> 式更为灵活和广泛，集合 <span class="math inline">\(\{\eta=(w_1(\btheta),\ldots,w_k(\btheta)):c(\btheta)&gt;0\}\)</span> 是自然参数空间 <span class="math inline">\(\mathcal H\)</span> 的子集，而 <span class="math inline">\(\mathcal H\)</span> 中可能还存在其他的 <span class="math inline">\(\eta\)</span> 值。</p><p><br/></p><p>如果指数族 pdf 满足：<span class="math inline">\(\dim\btheta &lt;k\)</span>，则称作<strong>曲指数族</strong>；如果 <span class="math inline">\(\dim\btheta=k\)</span>，则称作<strong>完全指数族</strong>。</p><h2 id="位置和尺度族">2 位置和尺度族</h2><p>本节将构造三种分布族：位置族、尺度族、位置-尺度族，构造方法为：预先给定该分布族的一个标准 pdf <span class="math inline">\(f(x)\)</span>，然后用指定方法对其进行变换以得到该分布族的所有其他 pdf.</p><p><strong>定理</strong>：设 <span class="math inline">\(f(x)\)</span> 是 pdf，<span class="math inline">\(\mu\)</span> 和 <span class="math inline">\(\sigma&gt;0\)</span> 是任意给定参数，则函数 <span class="math display">\[g(x\vert\mu,\sigma)=\frac{1}{\sigma}f\left(\frac{x-\mu}{\sigma}\right)\]</span> 也是 pdf. 证明非常容易，略去。</p><p><br/></p><p><strong>位置族</strong></p><p>设 <span class="math inline">\(f(x)\)</span> 是 pdf，则称 <span class="math inline">\(f(x-\mu),\,-\infty&lt;\mu&lt;+\infty\)</span> 是<strong>标准 pdf 为 <span class="math inline">\(f(x)\)</span> 的位置族</strong>，称参数 <span class="math inline">\(\mu\)</span> 为<strong>位置参数</strong>。</p><p>直观上就是将标准 pdf 向右平移了 <span class="math inline">\(\mu\)</span> 个单位。</p><p><img src="location.png" alt="Exponential location densities" width=50% /></p><p><br/></p><p><strong>尺度族</strong></p><p>设 <span class="math inline">\(f(x)\)</span> 是 pdf，则称 <span class="math inline">\((1/\sigma)f(x/\sigma),\,\sigma&gt;0\)</span> 是<strong>标准 pdf 为 <span class="math inline">\(f(x)\)</span> 的尺度族</strong>，称参数 <span class="math inline">\(\sigma\)</span> 为<strong>尺度参数</strong>。</p><p>直观上就是将标准 pdf 横向拉伸了 <span class="math inline">\(\sigma\)</span> 倍并纵向压缩到原来的 <span class="math inline">\(1/\sigma\)</span>。</p><p><img src="scale.png" alt="Scale family" width=50% /></p><p><br/></p><p><strong>位置-尺度族</strong></p><p>设 <span class="math inline">\(f(x)\)</span> 是 pdf，则称 <span class="math inline">\((1/\sigma)f((x-\mu)/\sigma),\,\sigma&gt;0\)</span> 是<strong>标准 pdf 为 <span class="math inline">\(f(x)\)</span> 的位置-尺度族</strong>。</p><p>正态分布族和双指数分布族都是位置-尺度族的例子。</p><p>位置-尺度族的概率计算常通过标准化变量计算： <span class="math display">\[P(X\leq x)=P\left(\frac{X-\mu}{\sigma}\leq\frac{x-\mu}{\sigma}\right)=P\left(Z\leq\frac{x-\mu}{\sigma}\right)\]</span> 标准化变量的概率 <span class="math inline">\(P(Z\leq z)\)</span> 常常容易计算或可查表得到。</p><h2 id="不等式与恒等式">3 不等式与恒等式</h2><h3 id="切比雪夫不等式">3.1 切比雪夫不等式</h3><p>设 <span class="math inline">\(X\)</span> 是随机变量，<span class="math inline">\(g(x)\)</span> 是非负函数，则对任意 <span class="math inline">\(r&gt;0\)</span> 有 <span class="math display">\[P(g(X)\geq r)\leq \frac{\E g(X)}{r}\]</span> <em>Proof</em>. <span class="math display">\[\begin{align}\E g(X)&amp;=\int_{-\infty}^{+\infty}g(x) f_X(x)\mathrm dx\\&amp;\geq\int_{\{x:g(x)\geq r\}}g(x)f_X(x)\mathrm dx\\&amp;\geq r\int_{\{x:g(x)\geq r\}}f_X(x)\mathrm dx\\&amp;=rP(g(X)\geq r)\end{align}\]</span> Q.E.D.</p><p><br/></p><p>可能切比雪夫不等式更常见的形式不是上面这样，但可以从它推导出来：设 <span class="math inline">\(g(x)=(x-\mu)^2/\sigma^2\)</span>，其中 <span class="math inline">\(\mu=\E X,\,\sigma^2=\Var X\)</span>. 取 <span class="math inline">\(r=t^2\)</span>，代入 Chebychev 不等式得到： <span class="math display">\[P\left(\frac{(x-\mu)^2}{\sigma^2}\geq t^2\right)\leq\frac{1}{t^2}\E\left[\frac{(X-\mu)^2}{\sigma^2}\right]=\frac{1}{t^2}\]</span> 即： <span class="math display">\[P(|X-\mu|\geq t\sigma)\leq \frac{1}{t^2}\]</span> 此不等式告诉我们随机变量偏离其均值的上界，<strong>无论 <span class="math inline">\(X\)</span> 是什么分布</strong>。也正是因为它没有对分布作限制，这个结论往往比较保守。</p><h3 id="马尔可夫不等式">3.2 马尔可夫不等式</h3><p>若 <span class="math inline">\(P(Y\geq 0)=1\)</span> 且 <span class="math inline">\(P(Y=0)&lt;1\)</span>，则对任意 <span class="math inline">\(r&gt;0\)</span>，有 <span class="math display">\[P(Y\geq r)\leq\frac{\E X}{r}\]</span></p><h3 id="stein-引理">3.3 Stein 引理</h3><p>设 <span class="math inline">\(X\sim N(\theta,\sigma^2)\)</span>，<span class="math inline">\(g\)</span> 是满足 <span class="math inline">\(\E |g&#39;(X)|&lt;+\infty\)</span> 的可导函数，则： <span class="math display">\[\E[g(X)(X-\theta)]=\sigma^2\E g&#39;(X)\]</span> <em>Proof</em>. <span class="math display">\[\begin{align}\E[g(X)(X-\theta)]&amp;=\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{+\infty}g(x)(x-\theta)e^{-(x-\theta)^2/2\sigma^2}\mathrm dx\\&amp;=-\frac{\sigma}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}g(x)\mathrm de^{-(x-\theta)^2/2\sigma^2}\\&amp;=-\frac{\sigma}{\sqrt{2\pi}}\left[\left.g(x)e^{-(x-\theta)^2/2\sigma^2}\right|_{-\infty}^{+\infty}-\int_{-\infty}^{+\infty}e^{-(x-\theta)^2/2\sigma^2}g&#39;(x)\mathrm dx\right]\\&amp;=\sigma^2\E g&#39;(x)\end{align}\]</span> Q.E.D.</p><p><br/></p><p><strong>高阶正态矩</strong>：利用 Stein 引理可以简化计算高阶正态矩：设 <span class="math inline">\(X\sim N(\theta,\sigma^2)\)</span>，则： <span class="math display">\[\begin{align}\E X^3&amp;=\E[X^2(X-\theta+\theta)]\\&amp;=\E[X^2(X-\theta)]+\theta\E X^2\\&amp;=2\sigma^2\E X+\theta\E X^2\\&amp;=2\sigma^2\theta+\theta(\sigma^2+\theta^2)\\&amp;=3\theta\sigma^2+\theta^3\end{align}\]</span></p><h3 id="一个关于-chi2-随机变量的恒等式">3.4 一个关于 <span class="math inline">\(\chi^2\)</span> 随机变量的恒等式</h3><p>设 <span class="math inline">\(\chi^2_p\)</span> 是自由度为 <span class="math inline">\(p\)</span> 的 <span class="math inline">\(\chi^2\)</span> 随机变量，则对任意函数 <span class="math inline">\(h(x)\)</span>，有： <span class="math display">\[\E h(\chi^2_p)=p\E\left[\frac{h(\chi^2_{p+2})}{\chi^2_{p+2}}\right]\]</span> 这里假定上述期望存在。</p><p><em>Proof</em>. <span class="math display">\[\begin{align}\E h(\chi^2_p)&amp;=\frac{1}{\Gamma(p/2)2^{p/2}}\int_0^{+\infty}h(x)x^{p/2-1}e^{-x/2}\mathrm dx\\&amp;=\frac{p}{\Gamma({(p+2)}/2)2^{(p+2)/2}}\int_0^{+\infty}\frac{h(x)}{x} x^{(p+2)/2-1}e^{-x/2}\mathrm dx\\&amp;=p\E\left[\frac{h(\chi^2_{p+1})}{\chi^2_{p+1}}\right]\end{align}\]</span> Q.E.D.</p><p><br/></p><p><strong>高阶 <span class="math inline">\(\chi^2\)</span> 矩</strong>：利用上述恒等式可以简化计算高阶 <span class="math inline">\(\chi^2\)</span> 矩： <span class="math display">\[\E[(\chi^2_{p})^2]=p\E[\chi^2_{p+2}]=p(p+2)\]</span></p><h3 id="hwang-给出的两个恒等式">3.5 Hwang 给出的两个恒等式</h3><blockquote><p>文献：Hwang, Jiunn Tzon. "Improving upon standard estimators in discrete exponential families with applications to Poisson and negative binomial cases." <em>The Annals of Statistics</em> (1982): 857-867.</p></blockquote><p>设函数 <span class="math inline">\(g(x)\)</span> 满足 <span class="math inline">\(-\infty&lt;\E g(X)&lt;+\infty\)</span> 且 <span class="math inline">\(-\infty&lt;g(-1)&lt;+\infty\)</span>，那么：</p><ol type="1"><li><p>若 <span class="math inline">\(X\)</span> 服从参数为 <span class="math inline">\(\lambda\)</span> 的 Poisson 分布，则： <span class="math display">\[\E[\lambda g(X)]=\E[Xg(X-1)]\]</span></p></li><li><p>若 <span class="math inline">\(X\)</span> 服从参数为 <span class="math inline">\((r,p)\)</span> 的负二项分布，则： <span class="math display">\[\E[(1-p)g(X)]=\E\left[\frac{X}{r+X-1}g(X-1)\right]\]</span></p></li></ol><p><em>Proof</em>.</p><ol type="1"><li><p><span class="math display">\[\begin{align}\E[\lambda g(X)]&amp;=\sum_{x=0}^{+\infty}\lambda g(x)\frac{e^{-\lambda}\lambda^x}{x!}\\&amp;=\sum_{x=0}^{+\infty}(x+1)g(x)\frac{e^{-\lambda}\lambda^{x+1}}{(x+1)!}\\&amp;=\sum_{y=1}^{+\infty}yg(y-1)\frac{e^{-\lambda}\lambda^y}{y!}&amp;&amp;y=x+1\\&amp;=\sum_{y=0}^{+\infty}yg(y-1)\frac{e^{-\lambda}\lambda^y}{y!}&amp;&amp;y=x+1\\&amp;=\E[Xg(X-1)]\end{align}\]</span></p></li><li><p><span class="math display">\[\begin{align}\E[(1-p)g(X)]&amp;=\sum_{x=0}^{+\infty}(1-p)g(x)\binom{r+x-1}{x}p^r(1-p)^x\\&amp;=\sum_{y=1}^{+\infty}g(y-1)\binom{r+y-2}{y-1}p^r(1-p)^{y}&amp;&amp;y=x+1\\&amp;=\sum_{y=1}^{+\infty}g(y-1)\frac{y}{r+y-1}\binom{r+y-1}{y}p^r(1-p)^{y}\\&amp;=\sum_{y=0}^{+\infty}g(y-1)\frac{y}{r+y-1}\binom{r+y-1}{y}p^r(1-p)^{y}\\&amp;=\E\left[\frac{X}{r+X-1}g(X-1)\right]\end{align}\]</span></p></li></ol><p>Q.E.D.</p><p><br/></p><p><strong>高阶 Poisson 矩</strong>：利用上述恒等式可以简化计算高阶 Poisson 矩：设 <span class="math inline">\(X\sim\mathrm{poisson}(\lambda)\)</span>，则： <span class="math display">\[\E[\lambda X^2]=\E[X(X-1)^2]=\E X^3-2\E X^2+\E X\]</span> 故 <span class="math display">\[\E X^3=(\lambda+2)\E X^2-\E X=\lambda(\lambda+1)(\lambda+2)-\lambda=\lambda(\lambda^2+3\lambda+1)\]</span></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>统计推断</category>
      
    </categories>
    
    
    <tags>
      
      <tag>statistics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[David Silver强化学习]1·Introduction</title>
    <link href="/blog-main/2022/03/05/David-Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-1%C2%B7Introduction/"/>
    <url>/blog-main/2022/03/05/David-Silver%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-1%C2%B7Introduction/</url>
    
    <content type="html"><![CDATA[<h2 id="about-rl">1 About RL</h2><p>强化学习是一个交叉领域，与计算机科学、工程学、数学、神经学、心理学、生态学等众多学科都有联系。</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="subjects.png" width=90%/></div><div class="group-image-wrap"><img src="ml.png" width=95%/></div></div></div><p>一般而言，我们把机器学习分成有监督学习、无监督学习和强化学习三大类。强化学习与另外二者有很大的区别：</p><ul><li>没有监督，只有 reward 信号</li><li>反馈不及时，有延迟</li><li>时序非常重要，没有独立同分布的数据</li><li>Agent 的动作会影响它后续能获得的数据</li></ul><p>强化学习有很多应用，例如：控制直升机的特技飞行、下棋、管理投资、控制类人机器人行走、玩游戏……</p><h2 id="the-rl-problem">2 The RL Problem</h2><h3 id="rewards">2.1 Rewards</h3><p>奖励 reward 是一个标量的反馈信号，记 <span class="math inline">\(R_t\)</span> 表示 <span class="math inline">\(t\)</span> 时刻的奖励。</p><p><strong>Reward Hypothesis</strong>：强化学习的所有目标都可以描述为最大化累积奖励的期望。</p><p>尽管目标很明确，但 agent 的决策过程却要考虑众多复杂的因素：当下的动作可能会造成长期影响、奖励可能会有延迟、牺牲短期奖励也许能带来长期奖励……</p><h3 id="environments">2.2 Environments</h3><p><img src="overview.png" width=40% /></p><p>强化学习概览：</p><ul><li>在时刻 <span class="math inline">\(t\)</span>，agent 会：<ul><li>执行一个动作 action <span class="math inline">\(A_t\)</span></li><li>收到对环境的观察 observation <span class="math inline">\(O_t\)</span></li><li>收到奖励 reward <span class="math inline">\(R_t\)</span></li></ul></li><li>而 environment 会：<ul><li>收到一个动作 action <span class="math inline">\(A_t\)</span></li><li>给出对环境的观察 observation <span class="math inline">\(O_{t+1}\)</span></li><li>给出奖励 reward <span class="math inline">\(R_{t+1}\)</span></li></ul></li></ul><blockquote><p>对下标的一点说明：关于强化学习的符号下标，也即时间戳，有两种写法。一种人认为，执行一个动作 <span class="math inline">\(A_t\)</span> 之后，环境已经发生了改变，因此此时返回的奖励下标应该 +1，即 <span class="math inline">\(R_{t+1}\)</span>；另一种人认为，返回的奖励是对刚执行的这个动作的反馈，因此下标和动作保持一致，即 <span class="math inline">\(R_t\)</span>。两种写法都可行，它们没有本质区别，仅仅是记号上的差异而已。本课程采用第一种写法，特此说明以免在后续公式中产生疑问。</p></blockquote><h3 id="state">2.3 State</h3><p>定义<strong>历史 history</strong> 是一系列 observations、actions 和 rewards： <span class="math display">\[H_t=O_1,R_1,A_1,\ldots,A_{t-1},O_t,R_t\]</span> 而<strong>状态 state</strong> 定义为历史的函数，我理解为从历史中提取的我们想要的信息： <span class="math display">\[S_t=f(H_t)\]</span> <br/></p><p>状态分为环境状态 environment state <span class="math inline">\(S_t^e\)</span> 和 agent state <span class="math inline">\(S_t^a\)</span>。</p><p>前者是环境的内在表述方式，环境根据当前状态和其运行机制给出下一时刻的观察和奖励；它通常对 agent 是不可见的；就算是可见的也可能包含无关信息。</p><p>后者是 agent 对环境的表述，是做决策所需要的信息，比如你看到的游戏画面。</p><p><br/></p><p>定义：称一个状态是 <strong>Markov state</strong> 当且仅当 <span class="math display">\[\mathbb P(S_{t+1}\mid S_t)=\mathbb P(S_{t+1}\mid S_1,\ldots,S_t)\]</span> 就是说<strong>下一时刻的状态仅决定于当前状态而与历史状态无关</strong>。根据定义，易知环境状态 <span class="math inline">\(S_t^e\)</span> 是 Markov 的，历史 <span class="math inline">\(H_t\)</span> 也满足 Markov 性质。</p><p><br/></p><p>状态的定义方式对决策有很大的影响。课程举了下面这个非常形象的例子，如果你以历史三次观察到的事件为状态表示，你将预测会被电击；如果你以过去各事件发生次数为状态表示，你会觉得应该有奶酪；而如果你以整个历史为状态表示，你无法预测会被电击还是有奶酪。</p><p><img src="rat.png" width=50% /></p><p><br/></p><p>当 agent 能够直接观察到 environment state，即 <span class="math inline">\(O_t=S_t^a=S_t^e\)</span> 时，称为 fully observable environments，称该环境下的问题是 <strong>Markov decision process (MDP)</strong>，这是课程的核心。</p><p>相反，一些环境是 partially observable 的，例如只有摄像机的机器人无法知道它在房间的绝对位置，称这样的问题是 <strong>partially observable Markov decision process (POMDP)</strong>. 这时，agent 必须自己构建状态的表述，例如：</p><ul><li>所有历史：<span class="math inline">\(S_t^a=H_t\)</span></li><li>Environment state 的置信概率：<span class="math inline">\(S_t^a=[\mathbb P(S_t^e=s^1),\ldots,\mathbb P(S_t^e=s^n)]\)</span></li><li>用 RNN 建模：<span class="math inline">\(S_t^a=\sigma(S_{t-1}^a W_s+O_t W_o)\)</span></li></ul><h2 id="inside-an-rl-agent">3 Inside An RL Agent</h2><h3 id="major-components">3.1 Major Components</h3><p>一个 agent 由以下一个或多个组成成分构成：策略、价值函数、模型。</p><p><br/></p><p><strong>策略 Policy</strong>：agent 的行为函数</p><p>策略是从状态到行为的映射，可分为确定性和不确定性</p><ul><li>确定性（deterministic）策略：<span class="math inline">\(a=\pi(s)\)</span></li><li>不确定性（stochastic）策略：<span class="math inline">\(\pi(a\mid s)=\mathbb P(A_t=a\mid S_t=s)\)</span></li></ul><p><br/></p><p><strong>价值函数 Value function</strong>：评价一个状态或动作的好坏</p><p>价值函数是对未来奖励的预测，定义为<u>在某个特定策略 <span class="math inline">\(\pi\)</span> 下</u>，未来获得奖励的加权和的期望： <span class="math display">\[v_\pi(s)=\mathbb E_\pi[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots\mid S_t=s]\]</span> 其中 <span class="math inline">\(\gamma\in[0,1]\)</span> 是衰减系数。这个式子的意思是，假设当前状态为 <span class="math inline">\(s\)</span>，即 <span class="math inline">\(S_t=s\)</span>，那么 agent 依据某特定策略 <span class="math inline">\(\pi\)</span> 做动作，会立刻得到奖励 <span class="math inline">\(R_{t+1}\)</span>，进入状态 <span class="math inline">\(S_{t+1}\)</span>，然后继续依据策略 <span class="math inline">\(\pi\)</span> 做动作，得到奖励 <span class="math inline">\(R_{t+2}\)</span>……由于这个过程是随机的（一方面，策略可能是不确定性策略；另一方面，做出动作后环境的变化也可能是随机的），所以要取期望。</p><p>更详尽的内容在下一课探讨。</p><p><br/></p><p><strong>模型 Model</strong>：agent 对环境的建模</p><p>设 <span class="math inline">\(\mathcal P\)</span> 预测下一个状态，<span class="math inline">\(\mathcal R\)</span> 预测下一个时刻的奖励： <span class="math display">\[\begin{align}&amp;\mathcal P_{ss&#39;}^a=\mathbb P(S_{t+1}=s&#39;\mid S_t=s,A_t=a)\\&amp;\mathcal R_s^a=\mathbb E[R_{t+1}\mid S_t=s,A_t=a]\end{align}\]</span> <span class="math inline">\(\mathcal P\)</span> 意味着在特定状态做特定动作，导致的下一个状态是不确定的。这可能与玩游戏不太一致（毕竟我们按下左方向键，游戏角色总不会往右跑吧），但是这种随机性的引入能更好地反映现实。</p><p><br/></p><p>以走迷宫为例，策略、价值函数、模型分别可以形象化的表示为：</p><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="maze_policy.png" /></div><div class="group-image-wrap"><img src="maze_value.png" /></div><div class="group-image-wrap"><img src="maze_model.png" /></div></div></div><p>注意走迷宫的目标是找到最短路径，所以模型中每一步的奖励都是负的，这样最大化累积奖励就是最小化走的步数。</p><h3 id="categories">3.2 Categories</h3><p>根据 RL agents 组成成分的不同，可以分成三类：</p><ul><li>Value based：只有 value function，没有 policy 或者说 policy 隐含于 value function 之中</li><li>Policy based：只有 policy，没有 value function</li><li>Actor critic：既有 policy，又有 value function</li></ul><p>根据 RL agents 是否对环境建立模型，可以分成两类：</p><ul><li>Model free：仅关注于 policy 和/或 value function，不对环境建模。也就是说，我们不关心环境的运行机制究竟是怎样的，仅根据我们与环境之间的交互结果作决策。</li><li>Model based：对环境建模，让模型表述环境的运行机制，这样我们清楚地知道每一步会发生什么，并寻找最优决策方式。</li></ul><p><img src="taxonomy.png" width=40% /></p><h2 id="problems-within-rl">4 Problems within RL</h2><p><strong>Learning and Planning</strong></p><p>在 learning 问题中，agent 不知道环境的运行机制、不知道游戏的规则，仅能通过与环境交互来改善策略；在 planning 问题中，agent 有一个环境的模型，无需与环境交互就可以依靠模型进行运算来改善策略。</p><p>二者是紧密相连的，面对一个 learning 问题，agent 首先通过交互构建模型，学习环境的运行机制，然后用模型进行规划。</p><p><strong>Exploration and Exploitation</strong></p><p>Exploration 和 exploitation 是一种 trade-off 的关系。前者意味着试错，可能导致更差的奖励，但也可能发现更好的策略；后者意味着用已知的信息找寻最好的策略，而不是尝试新事物。</p><p><strong>Prediction and control</strong></p><p>Prediction 指给定一个策略后，计算未来的奖励。可以理解为求解给定策略下的价值函数。</p><p>Control 指找到最佳策略以最大化未来的奖励。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>David Silver</category>
      
    </categories>
    
    
    <tags>
      
      <tag>reinforcement learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[统计推断]第三章·常见分布族（二）</title>
    <link href="/blog-main/2022/03/01/%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD-%E7%AC%AC%E4%B8%89%E7%AB%A0%C2%B7%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83%E6%97%8F%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <url>/blog-main/2022/03/01/%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD-%E7%AC%AC%E4%B8%89%E7%AB%A0%C2%B7%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83%E6%97%8F%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\E}{\mathbb E}\newcommand{\Var}{\mathrm{Var}}\newcommand{\Beta}{\mathrm{B}}\]</span></p><p>本篇列举常见的<strong>连续分布</strong>。</p><h2 id="均匀分布">1 均匀分布</h2><p><span class="math display">\[f(x\vert a,b)=\begin{cases}\frac{1}{b-a},&amp;x\in[a,b]\\0,&amp;x\notin [a,b]\end{cases}\]</span></p><div class="note note-info">            <p><strong>期望</strong>： <span class="math display">\[\E X=\int_a^b\frac{x}{b-a}\mathrm dx=\frac{a+b}{2}\]</span></p>          </div><div class="note note-info">            <p><strong>方差</strong> <span class="math display">\[\Var X=\int_a^b\frac{\left(x-\frac{a+b}{2}\right)^2}{b-a}\mathrm dx=\frac{(a-b)^2}{12}\]</span></p>          </div><h2 id="伽玛分布">2 伽玛分布</h2><p><strong><span class="math inline">\(\Gamma\)</span> 函数</strong>： <span class="math display">\[\Gamma(\alpha)=\int_0^{+\infty}t^{\alpha-1}e^{-t}\mathrm dt\]</span> 利用分部积分法易得递推式： <span class="math display">\[\Gamma(\alpha+1)=\alpha\Gamma(\alpha),\quad \alpha&gt;0\]</span> 又 <span class="math inline">\(\Gamma(1)=1\)</span>，因此对于正整数 <span class="math inline">\(n\)</span>，<span class="math inline">\(\Gamma(n)=(n-1)!\)</span>.</p><p><br/></p><p>基于 <span class="math inline">\(\Gamma\)</span> 函数，可以定义参数为 <span class="math inline">\((\alpha,\beta)\)</span> 的<strong>伽玛分布族</strong>： <span class="math display">\[f(x\vert \alpha,\beta)=\frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha-1}e^{-x/\beta},\quad 0&lt;x&lt;+\infty,\,\alpha&gt;0,\,\beta&gt;0\]</span> 其中 <span class="math inline">\(\alpha\)</span> 主要影响分布的峰起状态，称为形状参数；<span class="math inline">\(\beta\)</span> 主要影响分布的散度情况，称为尺度参数；记该分布为 <span class="math inline">\(\text{Gamma}(\alpha,\beta)\)</span>.</p><p><img src="gamma.png" width=50% /></p><div class="note note-info">            <p><strong>期望</strong> <span class="math display">\[\begin{align}\E X&amp;=\frac{1}{\Gamma(\alpha)\beta^\alpha}\int_0^{+\infty}x^{\alpha}e^{-x/\beta}\mathrm dx\\&amp;=\frac{\beta}{\Gamma(\alpha)}\int_0^{+\infty}t^{\alpha}e^{-t}\mathrm dt&amp;&amp;t=x/\beta\\&amp;=\frac{\beta}{\Gamma(\alpha)}\Gamma(\alpha+1)\\&amp;=\alpha\beta\end{align}\]</span></p>          </div><div class="note note-info">            <p><strong>二阶矩</strong> <span class="math display">\[\begin{align}\E X^2&amp;=\frac{1}{\Gamma(\alpha)\beta^\alpha}\int_0^{+\infty}x^{\alpha+1} e^{-x/\beta}\mathrm dx\\&amp;=\frac{\beta^2}{\Gamma(\alpha)}\int_0^{+\infty}t^{\alpha+1} e^{-t}\mathrm dt&amp;&amp;t=x/\beta\\&amp;=\frac{\beta^2}{\Gamma(\alpha)}\Gamma(\alpha+2)\\&amp;=\alpha(\alpha+1)\beta^2\end{align}\]</span></p>          </div><div class="note note-info">            <p><strong>方差</strong> <span class="math display">\[\Var X=\E X^2-(\E X)^2=\alpha\beta^2\]</span></p>          </div><div class="note note-info">            <p><strong>矩母函数</strong>：第二章已经推导过了 <span class="math display">\[M_X(t)=\left(\frac{1}{1-\beta t}\right)^\alpha,\quad t&lt;\frac{1}{\beta}\]</span></p>          </div><p><strong>伽玛分布与泊松分布</strong>：设 <span class="math inline">\(X\)</span> 是参数为 <span class="math inline">\((\alpha,\beta)\)</span> 的伽玛随机变量，其中 <span class="math inline">\(\alpha\)</span> 为整数，<span class="math inline">\(Y\)</span> 服从参数为 <span class="math inline">\((x/\beta)\)</span> 的泊松分布，则对任意 <span class="math inline">\(x\)</span>，都有： <span class="math display">\[P(X\leq x)=P(Y\geq \alpha)\]</span> <em>Proof</em>. 反复运用分部积分法。</p><h2 id="chi2-分布">3 <span class="math inline">\(\chi^2\)</span> 分布</h2><p>在伽玛分布中，若令 <span class="math inline">\(\alpha=p/2,\,\beta=2\)</span>，其中 <span class="math inline">\(p\)</span> 是整数，则伽玛概率密度函数变为： <span class="math display">\[f(x\vert p)=\frac{1}{\Gamma(p/2)2^{p/2}}x^{p/2-1}e^{-x/2},\quad 0&lt;x&lt;\infty\]</span> 这是自由度为 <span class="math inline">\(p\)</span> 的 <span class="math inline">\(\chi^2\)</span> 概率密度函数。</p><h2 id="指数分布">4 指数分布</h2><p>在伽玛分布中，若令 <span class="math inline">\(\alpha=1\)</span>，则得到指数概率密度函数： <span class="math display">\[f(x\vert \beta)=\frac{1}{\beta}e^{-x/\beta},\quad 0&lt;x&lt;\infty\]</span> <div class="note note-info">            <p><strong>期望</strong>（分部积分易得） <span class="math display">\[\E X=\beta\]</span></p>          </div></p><div class="note note-info">            <p><strong>方差</strong>（分部积分易得） <span class="math display">\[\Var X=\beta^2\]</span></p>          </div><div class="note note-info">            <p><strong>无记忆性</strong>：对任意 <span class="math inline">\(s&gt;t\geq 0\)</span>，有 <span class="math display">\[P(X&gt;s\vert X&gt;t)=P(X&gt;s-t)\]</span></p>          </div><h2 id="weibull-分布">5 Weibull 分布</h2><p>设 <span class="math inline">\(X\)</span> 服从参数为 <span class="math inline">\(\beta\)</span> 的指数分布，则 <span class="math inline">\(Y=X^{1/\gamma}\)</span> 服从参数为 <span class="math inline">\((\gamma,\beta)\)</span> 的 <strong>Weibull 分布</strong>，即其概率密度函数为： <span class="math display">\[f(y\vert \gamma,\beta)=\frac{\gamma}{\beta}y^{\gamma-1}e^{-y^\gamma/\beta},\quad 0&lt;y&lt;\infty,\,\gamma&gt;0,\,\beta&gt;0\]</span></p><blockquote><p>推导：由于 <span class="math inline">\(y=g(x)=x^{1/\gamma}\)</span> 是单调函数，根据第二章的结论可知： <span class="math display">\[f_Y(y)=f_X(g^{-1}(y))\left|\frac{\mathrm d}{\mathrm dy}g^{-1}(y)\right|=\frac{\gamma}{\beta}y^{\gamma-1}e^{-y^\gamma/\beta}\]</span></p></blockquote><p>Weibull 分布广泛引用于寿命分析和危险率函数的建模。</p><h2 id="正态分布">6 正态分布</h2><p><span class="math inline">\(X\)</span> 服从参数为 <span class="math inline">\((\mu,\sigma^2)\)</span> 的正态分布： <span class="math display">\[f(x\vert \mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}\exp\frac{-(x-\mu)^2}{2\sigma^2},\quad -\infty&lt;x&lt;+\infty\]</span> 记作 <span class="math inline">\(N(\mu,\sigma^2)\)</span>.</p><p><br/></p><p>验证上述概率密度函数积分为 <span class="math inline">\(1\)</span> 需要一些技巧，一个常见的证法是转换为二重积分。首先，作变量代换 <span class="math inline">\(z=(x-\mu)/\sigma\)</span>，则易知我们只需要证明： <span class="math display">\[\int_0^{+\infty}e^{-z^2/2}\mathrm dz=\sqrt{\frac{\pi}{2}}\]</span> 我们计算其平方并使用极坐标换元： <span class="math display">\[\begin{align}\left(\int_0^{+\infty}e^{-z^2/2}\mathrm dz\right)^2&amp;=\left(\int_0^{+\infty}e^{-t^2/2}\mathrm dt\right)\left(\int_0^{+\infty}e^{-u^2/2}\mathrm du\right)\\&amp;=\int_0^{+\infty}\int_0^{+\infty}e^{-(t^2+u^2)/2}\mathrm dt\mathrm du\\&amp;=\int_0^{+\infty}\int_0^{\pi/2}re^{-r^2/2}\mathrm d\theta\mathrm dr\\&amp;=\frac{\pi}{2}\left.e^{-r^2/2}\right|_{+\infty}^0=\frac{\pi}{2}\end{align}\]</span> 事实上，若令 <span class="math inline">\(w=z^2/2\)</span>，则上述积分本质是 <span class="math inline">\(\Gamma(1/2)\)</span>，即： <span class="math display">\[\Gamma(1/2)=\int_0^{+\infty}w^{-1/2}e^{-w}\mathrm dw=\sqrt\pi\]</span> <div class="note note-info">            <p><strong>期望</strong> <span class="math display">\[\E X=\mu\]</span></p>          </div></p><div class="note note-info">            <p><strong>方差</strong> <span class="math display">\[\Var X=\sigma^2\]</span></p>          </div><div class="note note-info">            <p><strong>矩母函数</strong> <span class="math display">\[\begin{align}M_X(t)&amp;=\E[e^{tX}]\\&amp;=\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{+\infty}e^{tx}e^{-{(x-\mu)^2}/{2\sigma^2}}\mathrm dx\\&amp;=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{t(\sigma s+\mu)}e^{-{s^2}/{2}}\mathrm ds&amp;&amp;s=(x-\mu)/\sigma\\&amp;=\frac{e^{t\mu}}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{t^2\sigma^2/2}e^{-{(s-t\sigma)^2}/{2}}\mathrm ds\\&amp;=e^{\mu t+\sigma^2t^2/2}\end{align}\]</span></p>          </div><p>对正态分布的概率密度函数求导易知，其在 <span class="math inline">\(x=\mu\)</span> 处取得极值，<span class="math inline">\(\mu\pm\sigma\)</span> 是拐点。</p><p><img src="norm.png" width=50% /></p><p><br/></p><p>正态分布常用于近似其他分布，但怎样的近似足够好并无绝对的标准。以二项分布为例，经验上当 <span class="math inline">\(\min(np,n(1-p))\geq 5\)</span> 时，可使用 <span class="math inline">\(N(np, np(1-p))\)</span> 近似 <span class="math inline">\(\text{Binomial}(n,p)\)</span>.</p><p>但由于二项分布是离散分布，在近似时使用<strong>连续性校正</strong>可以使近似更精确：设 <span class="math inline">\(X\sim\text{Binomial}(n,p)\)</span>，<span class="math inline">\(Y\sim N(np,np(1-p))\)</span>，则： <span class="math display">\[P(X\leq x)\approx P(Y\leq x+1/2)\quad P(X\geq x)\approx P(Y\geq x-1/2)\]</span></p><h2 id="贝塔分布">7 贝塔分布</h2><p><strong><span class="math inline">\(\Beta\)</span> 函数</strong>： <span class="math display">\[\Beta(\alpha,\beta)=\int_0^1 x^{\alpha-1}(1-x)^{\beta-1}\mathrm dx\]</span> <span class="math inline">\(\Beta\)</span> 函数和 <span class="math inline">\(\Gamma\)</span> 函数的关系： <span class="math display">\[\Beta(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}\]</span> 一般不直接处理 <span class="math inline">\(\Beta\)</span> 函数，而是用这个关系式转化成 <span class="math inline">\(\Gamma\)</span> 函数。</p><p><br/></p><p><strong>贝塔分布</strong>： <span class="math display">\[f(x\vert\alpha,\beta)=\frac{1}{\Beta(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1},\quad 0&lt;x&lt;1,\,\alpha&gt;0,\,\beta&gt;0\]</span> <div class="note note-info">            <p><strong><span class="math inline">\(n\)</span> 阶矩</strong> <span class="math display">\[\begin{align}\E X^n&amp;=\frac{1}{\Beta(\alpha,\beta)}\int_0^1 x^nx^{\alpha-1}(1-x)^{\beta-1}\mathrm dx\\&amp;=\frac{\Beta(\alpha+n,\beta)}{\Beta(\alpha,\beta)}\\&amp;=\frac{\Gamma(\alpha+n)\Gamma(\alpha+\beta)}{\Gamma(\alpha+\beta+n)\Gamma(\alpha)}\end{align}\]</span></p>          </div></p><div class="note note-info">            <p><strong>期望</strong> <span class="math display">\[\E X=\frac{\Gamma(\alpha+1)\Gamma(\alpha+\beta)}{\Gamma(\alpha+\beta+1)\Gamma(\alpha)}=\frac{\alpha}{\alpha+\beta}\]</span></p>          </div><div class="note note-info">            <p><strong>二阶矩</strong> <span class="math display">\[\E X^2=\frac{\Gamma(\alpha+2)\Gamma(\alpha+\beta)}{\Gamma(\alpha+\beta+2)\Gamma(\alpha)}=\frac{(\alpha+1)\alpha}{(\alpha+\beta+1)(\alpha+\beta)}\]</span></p>          </div><div class="note note-info">            <p><strong>方差</strong> <span class="math display">\[\Var X=\E X^2-(\E X)^2=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\]</span></p>          </div><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="beta1.png" width=100% alt="Beta densities" /></div><div class="group-image-wrap"><img src="beta2.png" width=100% alt="Symmetric beta densities" /></div></div></div><h2 id="柯西分布">8 柯西分布</h2><p>柯西分布是 <span class="math inline">\((-\infty,+\infty)\)</span> 上的一类对称钟形分布，概率密度函数为： <span class="math display">\[f(x\vert \theta)=\frac{1}{\pi}\frac{1}{1+(x-\theta)^2},\quad-\infty&lt;x&lt;+\infty,\,-\infty&lt;\theta&lt;+\infty\]</span> 第二章已经证明过，柯西分布期望不存在，于是任意阶矩也不存在，矩母函数亦不存在。</p><p><img src="cauchy.png" width=50% /></p><p><strong>两个标准正态分布之比就是柯西分布</strong>。</p><h2 id="对数正态分布">9 对数正态分布</h2><p>若 <span class="math inline">\(\log X\sim N(\mu,\sigma^2)\)</span>，则称 <span class="math inline">\(X\)</span> 服从对数正态分布。利用第二章随机变量的单调函数相关结论，易知： <span class="math display">\[f(x\vert\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}\frac{1}{x}\exp\frac{-(\log x-\mu)^2}{2\sigma^2},\quad 0&lt;x&lt;+\infty,\,-\infty&lt;\mu&lt;+\infty,\,\sigma&gt;0\]</span> <div class="note note-info">            <p><strong>期望</strong>：设 <span class="math inline">\(Y=\log X\sim N(\mu,\sigma^2)\)</span>，则： <span class="math display">\[\E X=\E e^{\log X}=\E e^Y=M_Y(1)=e^{\mu+\sigma^2/2}\]</span></p>          </div></p><div class="note note-info">            <p><strong>二阶矩</strong> <span class="math display">\[\E X^2=\E e^{2\log X}=\E e^{2Y}=M_Y(2)=e^{2\mu+2\sigma^2}\]</span></p>          </div><div class="note note-info">            <p><strong>方差</strong> <span class="math display">\[\Var X=\E X^2-(\E X)^2=e^{2\mu+2\sigma^2}-e^{2\mu+\sigma^2}\]</span></p>          </div><p>对数正态分布广泛应用于<strong>右偏变量</strong>的建模，例如工资。</p><p><img src="lognorm.png" width=50% /></p><h2 id="双指数分布">10 双指数分布</h2><p>将指数分布关于原点作对称并平移 <span class="math inline">\(\mu\)</span> 个单位，就得到了<strong>双指数分布</strong>，其概率密度函数为： <span class="math display">\[f(x\vert \mu,\sigma)=\frac{1}{2\sigma}e^{-|x-\mu|/\sigma},\quad -\infty&lt;x&lt;+\infty,\,-\infty&lt;\mu&lt;+\infty,\,\sigma&gt;0\]</span> 双指数分布是<u>尾部很粗的对称分布</u>，任意阶矩都存在。</p><div class="note note-info">            <p><strong>期望</strong> <span class="math display">\[\begin{align}\E X&amp;=\frac{1}{2\sigma}\int_{-\infty}^{+\infty}xe^{-|x-\mu|/\sigma}\mathrm dx\\&amp;=\frac{1}{2\sigma}\left(\int_{\mu}^{+\infty}xe^{-(x-\mu)/\sigma}\mathrm dx+\int_{-\infty}^{\mu}xe^{(x-\mu)/\sigma}\mathrm dx\right)\\&amp;=\mu\end{align}\]</span></p>          </div><div class="note note-info">            <p><strong>方差</strong> <span class="math display">\[\Var X=2\sigma^2\]</span></p>          </div>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>统计推断</category>
      
    </categories>
    
    
    <tags>
      
      <tag>statistics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[统计推断]第三章·常见分布族（一）</title>
    <link href="/blog-main/2022/03/01/%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD-%E7%AC%AC%E4%B8%89%E7%AB%A0%C2%B7%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83%E6%97%8F%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/blog-main/2022/03/01/%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD-%E7%AC%AC%E4%B8%89%E7%AB%A0%C2%B7%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83%E6%97%8F%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p><span class="math display">\[\newcommand{\E}{\mathbb E}\newcommand{\Var}{\mathrm{Var}}\newcommand{\Beta}{\mathrm{B}}\]</span></p><p>本篇列举常见的<strong>离散分布</strong>。</p><p>由于含参分布依赖于参数的取值，我们将参数记于 pmf、pdf、cdf 或期望中并以 <span class="math inline">\(\vert\)</span> 为引导符。在不会引发混淆时可以略去参数以简化记号。</p><h2 id="离散均匀分布">1 离散均匀分布</h2><p>称随机变量 <span class="math inline">\(X\)</span> 服从参数为 <span class="math inline">\((1,N)\)</span> 的<strong>离散均匀分布</strong>，若： <span class="math display">\[P(X=x\vert N)=\frac{1}{N},\quad x=1,2,\ldots,N\]</span></p><div class="note note-info">            <p><strong>期望</strong> <span class="math display">\[\E X=\sum_{x=1}^Nx\frac{1}{N}=\frac{N+1}{2}\]</span></p>          </div><div class="note note-info">            <p><strong>二阶矩</strong> <span class="math display">\[\E X^2=\sum_{x=1}^N x^2\frac{1}{N}=\frac{(N+1)(2N+1)}{6}\]</span></p>          </div><div class="note note-info">            <p><strong>方差</strong> <span class="math display">\[\Var X=\E X^2-(\E X)^2=\frac{(N+1)(N-1)}{12}\]</span></p>          </div><h2 id="超几何分布">2 超几何分布</h2><p>超几何分布可以从经典的摸球游戏中导出：设有 <span class="math inline">\(N\)</span> 个小球，其中 <span class="math inline">\(M\)</span> 个红球，<span class="math inline">\(N-M\)</span> 个绿球，随机取出 <span class="math inline">\(K\)</span> 个球（一次性无放回），恰好摸出 <span class="math inline">\(x\)</span> 个红球的概率是多少？ <span class="math display">\[P(X=x\vert N, M, K)=\frac{\binom{M}{x}\binom{N-M}{K-x}}{\binom{N}{K}},\quad x=0,1,\ldots,K\]</span> 称满足上式的随机变量 <span class="math inline">\(X\)</span> 服从<strong>超几何分布</strong>。</p><div class="note note-info">            <p><strong>期望</strong> <span class="math display">\[\begin{align}\E X&amp;=\sum_{x=0}^Kx\frac{\binom{M}{x}\binom{N-M}{K-x}}{\binom{N}{K}}\\&amp;=\sum_{x=1}^Kx\frac{\binom{M}{x}\binom{N-M}{K-x}}{\binom{N}{K}}\\&amp;=\sum_{x=1}^K\frac{M\binom{M-1}{x-1}\binom{N-M}{K-x}}{\frac{N}{K}\binom{N-1}{K-1}}\\&amp;=\frac{KM}{N}\sum_{y=0}^{K-1}\frac{\binom{M-1}{y}\binom{N-M}{K-1-y}}{\binom{N-1}{K-1}}&amp;&amp;y=x-1\\&amp;=\frac{KM}{N}\end{align}\]</span></p><p>最后一个等式是因为和式是对参数为 <span class="math inline">\(N-1,M-1,K-1\)</span> 的另一个超几何分布求和。另外，推导过程还利用了组合恒等式 <span class="math inline">\(x\binom{M}{x}=M\binom{M-1}{x-1}\)</span> 和 <span class="math inline">\(\binom{N}{K}=\frac{N}{K}\binom{N-1}{K-1}\)</span>.</p>          </div><div class="note note-info">            <p><strong>二阶矩</strong> <span class="math display">\[\begin{align}\E[X^2]&amp;=\sum_{x=0}^Kx^2\frac{\binom{M}{x}\binom{N-M}{K-x}}{\binom{N}{K}}\\&amp;=\frac{KM}{N}\sum_{x=1}^Kx\frac{\binom{M-1}{x-1}\binom{N-M}{K-x}}{\binom{N-1}{K-1}}\\&amp;=\frac{KM}{N}\left[\sum_{y=0}^{K-1}\frac{\binom{M-1}{y}\binom{N-M}{K-1-y}}{\binom{N-1}{K-1}}+\sum_{y=0}^{K-1}y\frac{\binom{M-1}{y}\binom{N-M}{K-1-y}}{\binom{N-1}{K-1}}\right]\\&amp;=\frac{KM}{N}\left[1+\frac{(K-1)(M-1)}{N-1}\right]\end{align}\]</span></p>          </div><div class="note note-info">            <p><strong>方差</strong> <span class="math display">\[\Var X=\E[X^2]-(\E X)^2=\frac{KM}{N}\frac{(N-M)(N-K)}{N(N-1)}\]</span></p>          </div><h2 id="伯努利分布-二项分布">3 伯努利分布 &amp; 二项分布</h2><p>称随机变量 <span class="math inline">\(X\)</span> 服从参数为 <span class="math inline">\(p\)</span> 的<strong>伯努利分布</strong>，若： <span class="math display">\[P(X=0\vert p)=1-p,\quad P(X=1\vert p)=p\]</span> 记作 <span class="math inline">\(\text{Bernoulli}(p)\)</span>.</p><div class="note note-info">            <p><strong>期望</strong> <span class="math display">\[\E X=1\cdot p+0\cdot(1-p)=p\]</span></p>          </div><div class="note note-info">            <p><strong>方差</strong> <span class="math display">\[\Var X=(1-p)^2\cdot p+(0-p)^2\cdot(1-p)=p(1-p)\]</span></p>          </div><p><br/></p><p>进行 <span class="math inline">\(n\)</span> 次相同的伯努利试验，考虑出现 <span class="math inline">\(x\)</span> 次成功试验的概率，可以导出<strong>二项分布</strong>： <span class="math display">\[P(X=x\vert n,p)=\binom{n}{x}p^x(1-p)^{n-x},\quad x=0,1,2,\ldots,n\]</span> 记作 <span class="math inline">\(\text{Binomial}(n,p)\)</span> 或 <span class="math inline">\(B(n,p)\)</span>.</p><div class="note note-info">            <p><strong>期望</strong> <span class="math display">\[\begin{align}\E X&amp;=\sum_{x=0}^nx\binom{n}{x}p^x(1-p)^{n-x}\\&amp;=\sum_{x=1}^nx\binom{n}{x}p^x(1-p)^{n-x}\\&amp;=n\sum_{x=1}^n \binom{n-1}{x-1}p^x(1-p)^{n-x}\\&amp;=np\sum_{y=0}^{n-1} \binom{n-1}{y}p^{y}(1-p)^{n-1-y}&amp;&amp;y=x-1\\&amp;=np\end{align}\]</span></p>          </div><div class="note note-info">            <p><strong>二阶矩</strong> <span class="math display">\[\begin{align}\E X^2&amp;=\sum_{x=0}^n x^2\binom{n}{x}p^x(1-p)^{n-x}\\&amp;=n\sum_{x=1}^n x\binom{n-1}{x-1}p^x(1-p)^{n-x}\\&amp;=n\left[\sum_{x=1}^n (x-1)\binom{n-1}{x-1}p^x(1-p)^{n-x}+\sum_{x=1}^n\binom{n-1}{x-1}p^x(1-p)^{n-x}\right]\\&amp;=n[(n-1)p^2+p]\\&amp;=np(np-p+1)\end{align}\]</span></p>          </div><div class="note note-info">            <p><strong>方差</strong> <span class="math display">\[\Var X=\E X^2-(\E X)^2=n^2p^2-np^2+np-n^2p^2=np(1-p)\]</span></p>          </div><h2 id="泊松分布">4 泊松分布</h2><p>称取值非负整数的随机变量 <span class="math inline">\(X\)</span> 服从参数为 <span class="math inline">\(\lambda\)</span> 的<strong>泊松分布</strong>，若： <span class="math display">\[P(X=x\vert \lambda)=\frac{e^{-\lambda}\lambda^x}{x!},\quad x=0,1,\ldots\]</span> 泊松分布有着广泛的应用，其<strong>基本假设是：在较短的时间段内，事物出现的概率与等待时间成正比</strong>。更正式地说，设 <span class="math inline">\(N_t\)</span> 表示 <span class="math inline">\(0\)</span> 到 <span class="math inline">\(t\)</span> 时间段内对象到达的次数，且：</p><ol type="1"><li><span class="math inline">\(N_0=0\)</span>（0 时刻无对象到达）</li><li><span class="math inline">\(s&lt;t\)</span> <span class="math inline">\(\implies\)</span> <span class="math inline">\(N_s\)</span> 与 <span class="math inline">\(N_t-N_s\)</span> 相互独立（不交时间段的到达行为无关）</li><li><span class="math inline">\(N_s\)</span> 与 <span class="math inline">\(N_{t+s}-N_t\)</span> 的分布相同（对象到达的数量只与时间长度有关）</li><li><span class="math inline">\(\lim_{t\to0}\frac{P(N_t=1)}{t}=\lambda\)</span>（当时段长度很小时，到达概率与时间长度成正比）</li><li><span class="math inline">\(\lim_{t\to 1}\frac{P(N_t&gt;1)}{t}=0\)</span>（不存在同时到达的现象）</li></ol><p>若上述条件均成立，则对任意整数 <span class="math inline">\(n\)</span>，有： <span class="math display">\[P(N_t=n)=e^{-\lambda t}\frac{(\lambda t)^n}{n!}\]</span> 即 <span class="math inline">\(N_t\sim \text{Poisson}(\lambda t)\)</span>.</p><div class="note note-info">            <p><strong>期望</strong> <span class="math display">\[\begin{align}\E X&amp;=\sum_{x=0}^{+\infty} x\frac{e^{-\lambda}\lambda^x}{x!}\\&amp;=\sum_{x=1}^{+\infty} x\frac{e^{-\lambda}\lambda^x}{x!}\\&amp;=\lambda\sum_{y=0}^{+\infty}\frac{e^{-\lambda}\lambda^y}{y!}&amp;&amp;y=x-1\\&amp;=\lambda\end{align}\]</span></p>          </div><div class="note note-info">            <p><strong>二阶矩</strong> <span class="math display">\[\begin{align}\E X^2&amp;=\sum_{x=0}^{+\infty}x^2\frac{e^{-\lambda}\lambda^x}{x!}\\&amp;=\lambda\sum_{y=0}^{+\infty}(y+1)\frac{e^{-\lambda}\lambda^y}{y!}\\&amp;=\lambda\left[\sum_{y=0}^{+\infty}y\frac{e^{-\lambda}\lambda^y}{y!}+\sum_{y=0}^{+\infty}\frac{e^{-\lambda}\lambda^y}{y!}\right]\\&amp;=\lambda(\lambda+1)\end{align}\]</span></p>          </div><div class="note note-info">            <p><strong>方差</strong> <span class="math display">\[\Var X=\E X^2-(\E X)^2=\lambda(\lambda+1)-\lambda^2=\lambda\]</span></p>          </div><div class="note note-info">            <p><strong>矩母函数</strong> <span class="math display">\[\begin{align}M_X(t)&amp;=\E[e^{tX}]\\&amp;=\sum_{x=0}^{+\infty}e^{tx}\frac{e^{-\lambda}\lambda^x}{x!}\\&amp;=e^{-\lambda}\sum_{x=0}^{+\infty}\frac{(e^t\lambda)^x}{x!}\\&amp;=e^{-\lambda}\cdot e^{e^t\lambda}&amp;&amp;\text{Taylor&#39;s Formula}\\&amp;=e^{\lambda(e^t-1)}\end{align}\]</span></p>          </div><div class="note note-info">            <p><strong>递推式</strong> <span class="math display">\[P(X=x\vert\lambda)=\frac{\lambda}{x}P(X=x-1\vert\lambda),\quad x=1,2,\ldots\]</span></p>          </div><div class="note note-success">            <p>例【等待现象】设一个话务员每 3 分钟接 5 个电话，问下一分钟没有电话的概率是多少？至少有两个电话概率是多少？</p><p>设随机变量 <span class="math inline">\(X\)</span> 是一分钟内电话的个数，则 <span class="math inline">\(X\)</span> 服从泊松分布且 <span class="math inline">\(\E X=\lambda=5/3\)</span>。于是， <span class="math display">\[P(\text{一分钟内没有电话})=P(X=0)=\frac{e^{-5/3}(5/3)^0}{0!}=e^{-5/3}=0.189\]</span></p><p><span class="math display">\[P(\text{一分钟内至少两个电话})=1-P(X=0)-P(X=1)=\cdots=0.496\]</span></p>          </div><h2 id="负二项分布">5 负二项分布</h2><p>二项分布讨论的是在指定数量的伯努利试验中成功试验的个数，而负二项分布讨论的是为了得到指定数量的成功试验所需伯努利试验的个数。</p><p>设有一列独立的成功概率为 <span class="math inline">\(p\)</span> 的伯努利试验，记随机变量 <span class="math inline">\(X\)</span> 表示该序列中第 <span class="math inline">\(r\)</span> 个成功试验出现的位置（换句话说，得到第 <span class="math inline">\(r\)</span> 个成功所需的试验个数），其中 <span class="math inline">\(r\)</span> 是预先指定的整数，则： <span class="math display">\[P(X=x\vert r, p)=\binom{x-1}{r-1}p^r(1-p)^{x-r},\quad x=r,r+1,\ldots\]</span> 称 <span class="math inline">\(X\)</span> 服从参数为 <span class="math inline">\((r,p)\)</span> 的<strong>负二项分布</strong>，记作 <span class="math inline">\(\text{NB}(r,p)\)</span>.</p><p>负二项分布还有另一种定义：记 <span class="math inline">\(Y\)</span> 是得到第 <span class="math inline">\(r\)</span> 个成功以前失败试验的个数，那么 <span class="math inline">\(Y=X-r\)</span>，其本质和 <span class="math inline">\(X\)</span> 是一样的，但概率质量函数变成： <span class="math display">\[P(Y=y\vert r,p)=\binom{r+y-1}{y}p^r(1-p)^{y},\quad y=0,1,\ldots\]</span> 本书采用后一种定义。</p><p>根据负整数的二项系数定义：<span class="math inline">\(\binom{r+y-1}{y}=(-1)^y\binom{-r}{y}\)</span>，可以把 pmf 进一步写作： <span class="math display">\[P(Y=y\vert r,p)=(-1)^y\binom{-r}{y}p^r(1-p)^{y}\]</span> <div class="note note-info">            <p><strong>期望</strong> <span class="math display">\[\begin{align}\E Y&amp;=\sum_{y=0}^{+\infty}y\binom{r+y-1}{y}p^r(1-p)^y\\&amp;=\sum_{y=1}^{+\infty}(-1)^yy\binom{-r}{y}p^r(1-p)^y\\&amp;=r\sum_{y=1}^{+\infty}(-1)^{y-1}\binom{-r-1}{y-1}p^r(1-p)^y\\&amp;=r\frac{1-p}{p}\sum_{z=0}^{+\infty}(-1)^{z}\binom{-r-1}{z}p^{r+1}(1-p)^{z}&amp;&amp;z=y-1\\&amp;=r\frac{1-p}{p}\end{align}\]</span></p><p>最后一个等式是因为和式是对参数为 <span class="math inline">\((r+1,p)\)</span> 的负二项分布 pmf 求和。注意组合恒等式 <span class="math inline">\(y\binom{r}{y}=r\binom{r-1}{y-1}\)</span> 对负整数情况依然成立。</p>          </div></p><div class="note note-info">            <p><strong>二阶矩</strong> <span class="math display">\[\begin{align}\E Y^2&amp;=\sum_{y=0}^{+\infty}y^2\binom{r+y-1}{y}p^r(1-p)^y\\&amp;=r\frac{1-p}{p}\left[\sum_{z=0}^{+\infty}(-1)^{z}z\binom{-r-1}{z}p^{r+1}(1-p)^{z}+\sum_{z=0}^{+\infty}(-1)^{z}\binom{-r-1}{z}p^{r+1}(1-p)^{z}\right]\\&amp;=r\frac{1-p}{p}\left[(r+1)\frac{1-p}{p}+1\right]\end{align}\]</span></p>          </div><div class="note note-info">            <p><strong>方差</strong> <span class="math display">\[\Var Y=\E Y^2-(\E Y)^2=r\frac{1-p}{p^2}\]</span> 一个有趣的事实是： <span class="math display">\[\Var Y=\E Y+\frac{1}{r}(\E Y)^2\]</span> 即方差是期望的二次函数。</p>          </div><h2 id="几何分布">6 几何分布</h2><p>几何分布是负二项分布的特例，即 <span class="math inline">\(r=1\)</span> 的情形，可理解为直到第一次成功所需的试验个数。若 <span class="math inline">\(X\)</span> 的概率质量函数为： <span class="math display">\[P(X=x\vert p)=p(1-p)^{x-1},\quad x=1,2,\ldots\]</span> 称 <span class="math inline">\(X\)</span> 服从参数为 <span class="math inline">\(p\)</span> 的<strong>几何分布</strong>，记作 <span class="math inline">\(\text{Geometric}(p)\)</span>.</p><div class="note note-info">            <p><strong>期望</strong> <span class="math display">\[\begin{align}\E X&amp;=\sum_{x=1}^{+\infty}xp(1-p)^{x-1}\\&amp;=(1-q)\sum_{x=1}^{+\infty}xq^{x-1}&amp;&amp;q=1-p\\&amp;=(1-q)\frac{\mathrm d}{\mathrm dq}\sum_{x=1}^{+\infty} q^x\\&amp;=(1-q)\frac{\mathrm d}{\mathrm dq}\left(\frac{q}{1-q}\right)\\&amp;=\frac{1}{1-q}=\frac{1}{p}\end{align}\]</span></p>          </div><div class="note note-info">            <p><strong>二阶矩</strong> <span class="math display">\[\begin{align}\E X^2&amp;=\sum_{x=1}^{+\infty}x^2p(1-p)^{x-1}\\&amp;=(1-q)\frac{\mathrm d}{\mathrm dq}\sum_{x=1}^{+\infty}xq^x&amp;&amp;q=1-p\\&amp;=(1-q)\frac{\mathrm d}{\mathrm dq}\left(\frac{q}{(1-q)^2}\right)\\&amp;=\frac{1+q}{(1-q)^2}=\frac{2-p}{p^2}\end{align}\]</span></p>          </div><div class="note note-info">            <p><strong>方差</strong> <span class="math display">\[\Var X=\E X^2-(\E X)^2=\frac{1-p}{p^2}\]</span></p>          </div><div class="note note-info">            <p><strong>无记忆性</strong>：对任意整数 <span class="math inline">\(s&gt;t\)</span>，有 <span class="math display">\[P(X&gt;s\vert X&gt;t)=P(X&gt;s-t)\]</span></p>          </div>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>统计推断</category>
      
    </categories>
    
    
    <tags>
      
      <tag>statistics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Useful commands</title>
    <link href="/blog-main/2022/02/28/Useful-commands/"/>
    <url>/blog-main/2022/02/28/Useful-commands/</url>
    
    <content type="html"><![CDATA[<blockquote><p>本文收集一些有用的命令。</p></blockquote><h2 id="ssh-scp">ssh &amp; scp</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">ssh [username]@[ip address] -p [port number]<br>scp -P [port] username@host1:file1 username@host2:file2<br></code></pre></td></tr></table></figure><p>包含跳板机：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">ssh -J username@jumper_ip:port username@server_ip -p port<br>scp -P server_port -o &#x27;ProxyJump jumper_username@jumper_ip:port&#x27; username@host1:file1 username@host2:file2<br></code></pre></td></tr></table></figure><h2 id="wake-on-lan">wake on lan</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">wolcmd [mac address] [ip address] [subnet mask] [port number]<br></code></pre></td></tr></table></figure><h2 id="screen">screen</h2><p>创建新的 screen 会话</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">screen -S session_name<br></code></pre></td></tr></table></figure><p>显示当前所有的 screen 会话</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">screen -ls<br></code></pre></td></tr></table></figure><p>手动 attach 指定 screen 会话，断线之后重连就用这个</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">screen -r session_name<br></code></pre></td></tr></table></figure><p>手动 detach 指定 screen 会话，被 detach 后会话仍然在后台运行，可通过 <code>screen -r</code> 重新绑定</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">screen -d session_name<br></code></pre></td></tr></table></figure><p><strong><code>Ctrl+a</code> 系列</strong>：先按 <code>Ctrl+a</code>，松开后按以下按键：</p><ul><li><code>d</code>：detach 当前对话</li><li><code>k</code>：kill 当前对话</li></ul><h2 id="tmux">tmux</h2><p>开启新 session（并命名）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">tmux<br>tmux new -s my_session<br></code></pre></td></tr></table></figure><p>显示所有session</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tmux ls<br></code></pre></td></tr></table></figure><p>使用 session 编号或名称接入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">tmux attach -t 0<br>tmux attach -t &lt;session-name&gt;<br>tmux a -t name # 简写<br></code></pre></td></tr></table></figure><p>使用 session 编号或名称 kill</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">tmux kill-session -t 0<br>tmux kill-session -t &lt;session-name&gt;<br></code></pre></td></tr></table></figure><p>使用 session 编号或名称切换</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">tmux switch -t 0<br>tmux switch -t &lt;session-name&gt;<br></code></pre></td></tr></table></figure><p>重命名 session</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tmux rename-session -t 0 &lt;new-name&gt;<br></code></pre></td></tr></table></figure><p><strong><code>Ctrl+b</code> 系列</strong>：先按 <code>Ctrl+b</code>，松开后按以下按键：</p><ul><li><p><code>s</code>：选择需要跳转的 session 会话</p></li><li><p><code>$</code>：重命名当前会话</p></li><li><p><code>d</code>：断开当前 session</p></li><li><p><code>c</code>：在当前 session 中多加一个 window</p></li><li><p><code>w</code>：在一个 session 中的多个 window 中作出选择</p></li><li><p><code>x</code>：关闭当前 session 中的当前 window</p></li><li><p><code>[</code>：进入 tmux 翻屏模式, 实现上下翻页；进入翻屏模式后 <code>PgUp</code> <code>PgDn</code> 实现上下翻页（mac 可以用 <code>fn</code> + <code>↑ ↓</code> 实现上下翻页）；<code>q</code> 退出翻屏模式</p></li><li><p><code>!</code>：关闭一个 session 中所有窗口</p></li><li><p><code>%</code>：将当前窗口分成左右两分</p></li><li><p><code>"</code>：将当前窗口分成上下两分</p></li><li><p>方向键 ：让光标在不同的窗口中跳转</p></li><li><p>按住 <code>Ctrl+b</code> 不放，同时按住方向键，可以调节光标所在窗口的大小</p></li></ul><h2 id="pigz">pigz</h2><p>gzip 的并行实现。</p><ul><li><p><strong>压缩单个文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pigz -k your_file_name<br></code></pre></td></tr></table></figure><p>加上<code>-k</code>选项保留原始文件，会在当前工作目录获得压缩后的<code>your_file_name.gz</code> 文件。</p></li><li><p><strong>解压单个文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">unpigz -d your_file_name.gz<br></code></pre></td></tr></table></figure><p>同样，如果需要保留.gz文件，记得加上<code>-k</code>选项。</p></li><li><p><strong>压缩文件夹</strong></p><p>pigz 没有压缩文件夹的选项，只能压缩单个文件。若想压缩文件夹，可以结合 tar 使用：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tar -cvf - dir1 dir2 dir3 | pigz &gt; output.tar.gz<br></code></pre></td></tr></table></figure></li><li><p><strong>解压文件夹</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tar -xzvf output.tar.gz<br></code></pre></td></tr></table></figure></li></ul><p><strong>常用参数</strong>：</p><ul><li><code>-0</code> ~ <code>-9</code> 压缩等级，数字越大压缩率越高，速度越慢，默认为6</li><li><code>-k --keep</code> 压缩后不删除原始文件</li><li><code>-l --list</code> 列出压缩输入的内容</li><li><code>-K --zip</code> Compress to PKWare zip (.zip) single entry format</li><li><code>-d --decompress</code> 解压缩输入</li><li><code>-p --processes n</code> 使用 n 核处理，默认为使用所有CPU核心</li></ul>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>技术栈</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[统计推断]第二章·变换和期望</title>
    <link href="/blog-main/2022/02/24/%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD-%E7%AC%AC%E4%BA%8C%E7%AB%A0%C2%B7%E5%8F%98%E6%8D%A2%E5%92%8C%E6%9C%9F%E6%9C%9B/"/>
    <url>/blog-main/2022/02/24/%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD-%E7%AC%AC%E4%BA%8C%E7%AB%A0%C2%B7%E5%8F%98%E6%8D%A2%E5%92%8C%E6%9C%9F%E6%9C%9B/</url>
    
    <content type="html"><![CDATA[<h2 id="随机变量函数的分布">1 随机变量函数的分布</h2><h3 id="随机变量的函数">1.1 随机变量的函数</h3><p>设 <span class="math inline">\(X\)</span> 是一随机变量且累积分布函数为 <span class="math inline">\(F_X(x)\)</span>，则 <span class="math inline">\(Y=g(X)\)</span> 也是随机变量。</p><p>设 <span class="math inline">\(g(x):\mathcal X\to\mathcal Y\)</span>，定义逆映射： <span class="math display">\[g^{-1}(A)=\{x\in\mathcal X:g(x)\in A\}\]</span> <strong>注意这个定义将集合映射到集合</strong>。即便 <span class="math inline">\(g(x)\)</span> 不是单调函数，<span class="math inline">\(g^{-1}\)</span> 也有定义。</p><p>在上述定义下，任给集合 <span class="math inline">\(A\subset \mathcal Y\)</span>，则有：<span class="math inline">\(P(Y\in A)=P(\{x\in\mathcal X:g(x)\in A\})=P(X\in g^{-1}(A))\)</span>.</p><p><br/></p><p>若 <span class="math inline">\(X\)</span> 与 <span class="math inline">\(Y\)</span> 都是连续随机变量，则常用的推导 <span class="math inline">\(Y\)</span> 的 cdf 的方式是： <span class="math display">\[F_Y(y)=P(Y\leq y)=P(g(X)\leq y)=P(\{x\in X:g(x)\leq y\})=\int_{\{x\in X:g(x)\leq y\}}p_X(x)\mathrm dx\]</span></p><div class="note note-success">            <p>例【伽玛概率密度函数和逆伽玛概率密度函数】设 <span class="math inline">\(f_X(x)\)</span> 为伽玛概率密度函数： <span class="math display">\[f_X(x)=\frac{1}{(n-1)!\beta^n}x^{n-1}e^{-x/\beta},\quad 0&lt;x&lt;+\infty\]</span> 其中 <span class="math inline">\(\beta\)</span> 是某大于零的常数，<span class="math inline">\(n\)</span> 是一正整数。现求 <span class="math inline">\(Y=g(X)=\frac{1}{X}\)</span> 的概率密度函数。那么： <span class="math display">\[F_Y(y)=P(Y\leq y)=P\left(X\geq 1/y\right)=1-P\left(X\leq1/y\right)=1-F_X(1/y)\]</span> 故： <span class="math display">\[f_Y(y)=\frac{\mathrm dF_Y(y)}{\mathrm dy}=\frac{f_X(1/y)}{y^2}=\frac{1}{(n-1)!\beta^n}\left(\frac{1}{y}\right)^{n+1}e^{-1/(\beta y)}\]</span> 这就是逆伽玛概率密度函数（的一个特例）。</p>          </div><p>在对随机变量进行变换时，需要明确随机变量的样本空间，常取： <span class="math display">\[\mathcal X=\{x:f_X(x)&gt;0\}\quad \mathcal Y=\{y:\exists x,\,y=g(x)\}\]</span> 上述 <span class="math inline">\(\mathcal X\)</span> 也称作随机变量 <span class="math inline">\(X\)</span> 的分布的<strong>支撑集（support set）</strong>，即 <span class="math inline">\(f_X(x)\)</span> 取值为正的地方。</p><h3 id="随机变量的单调函数">1.2 随机变量的单调函数</h3><p>单调情形非常常见，因此特意拿出来推导一下。设 <span class="math inline">\(f_X(x)\)</span> 是 <span class="math inline">\(\mathcal X\)</span> 上的连续函数，<span class="math inline">\(g^{-1}(y)\)</span> 在 <span class="math inline">\(\mathcal Y\)</span> 上有连续导数，则： <span class="math display">\[F_Y(y)=P(Y\leq y)=\begin{cases}P(X\leq g^{-1}(y))=F_X(g^{-1}(y))&amp;&amp;\text{若 }g\text{ 是增函数}\\P(X\geq g^{-1}(y))=1-F_X(g^{-1}(y))&amp;&amp;\text{若 }g\text{ 是减函数}\end{cases}\]</span> 求导可以得到： <span class="math display">\[f_Y(y)=f_X(g^{-1}(y))\left|\frac{\mathrm d}{\mathrm dy}g^{-1}(y)\right|,\quad y\in\mathcal Y\]</span></p><div class="note note-success">            <p>例【均匀分布与指数分布的联系】设 <span class="math inline">\(X\)</span> 服从 <span class="math inline">\((0,1)\)</span> 上的均匀分布，<span class="math inline">\(Y=g(X)=-\log X\)</span>，则 <span class="math inline">\(g^{-1}(y)=e^{-y}\)</span>，于是 <span class="math display">\[F_Y(y)=1-F_X(g^{-1}(y))=1-F_X(e^{-y})=1-e^{-y}\]</span> 或直接计算 pdf： <span class="math display">\[f_Y(y)=f_X(g^{-1}(y))\left|\frac{\mathrm d}{\mathrm dy}g^{-1}(y)\right|=e^{-y}f_X(e^{-y})=e^{-y}\]</span> 因而 <span class="math inline">\(Y\)</span> 服从参数为 <span class="math inline">\(1\)</span> 的指数分布。</p>          </div><h3 id="概率积分变换">1.3 概率积分变换</h3><p><strong>定理</strong>：设随机变量 <span class="math inline">\(X\)</span> 有连续累积分布函数 <span class="math inline">\(F_X(x)\)</span>。令 <span class="math inline">\(Y=F_X(X)\)</span>，则 <span class="math inline">\(Y\sim U(0, 1)\)</span>，即 <span class="math inline">\(P(Y\leq y)=y,0&lt;y&lt;1\)</span>.</p><p>在证明前，首先定义 <span class="math inline">\(F_X\)</span> 的逆 <span class="math inline">\(F_X^{-1}\)</span>。鉴于 <span class="math inline">\(F_X\)</span> 不一定严格单调递增，我们定义累积分布函数的逆如下： <span class="math display">\[F^{-1}_X(y)=\inf\{x:F_X(x)\geq y\}\]</span> 如此，在下图的情况下，有 <span class="math inline">\(F_X^{-1}(y)=x_1\)</span>。</p><p><img src="F_X.png" width=30% /></p><p>接下来我们完成证明： <span class="math display">\[\begin{align}P(Y\leq y)&amp;=P(F_X(X)\leq y)\\&amp;=P(F_X^{-1}(F_X(X))\leq F_X^{-1}(y))&amp;&amp;\text{因为 }F_X^{-1}\text{ 是增函数}\\&amp;=P(X\leq F_X^{-1}(y))&amp;&amp;\text{理由见下文}\\&amp;=F_X(F_X^{-1}(y))&amp;&amp;F_X\text{ 的定义}\\&amp;=y\end{align}\]</span> 对于第三个等号，如果 <span class="math inline">\(F_X\)</span> 严格递增，那么 <span class="math inline">\(F_X^{-1}(F_X(x))=x\)</span>，等式显然成立；如果不是严格递增，例如上面图中对于 <span class="math inline">\(x\in[x_1,x_2]\)</span> 一段，都有 <span class="math inline">\(F_X^{-1}(F_X(x))=x_1\)</span>，但由于 <span class="math inline">\(P(X\leq x)=P(X\leq x_1)\)</span>，所以等式依旧成立。</p><p><strong>利用概率积分变换定理，我们可以根据给定的概率分布构造相应的随机样本</strong>。如果我们想构造 <span class="math inline">\(X\)</span> 使其具有累积分布函数 <span class="math inline">\(F_X\)</span>，则只需要首先构造 <span class="math inline">\((0,1)\)</span> 上的均匀分布，再从 <span class="math inline">\(F_X(x)=U\)</span> 中解出 <span class="math inline">\(x\)</span>。这是一种能广泛应用的构造方法。</p><div class="note note-info">            <p>注记：这个定理叫做概率积分变换，但上文没看到积分呢？其实“积分”出现在：<span class="math inline">\(Y=F_X(X)=\int_{-\infty}^{X}f_X(t)\mathrm dt\)</span>。直观来说，<span class="math inline">\(Y\)</span> 是 <span class="math inline">\((-\infty,X)\)</span> 上 pdf 曲线下的面积。</p><p>该定理在图像处理领域的<a href="https://en.wikipedia.org/wiki/Histogram_equalization">直方图均衡化/规定化</a>中也有应用，只不过应用的是这个定理的离散形式，详见《数字图像处理》3.3 节。</p>          </div><h2 id="期望">2 期望</h2><h3 id="期望-1">2.1 期望</h3><p><span class="math display">\[\newcommand{\E}{\mathbb E}\newcommand{\Var}{\mathrm{Var}}\E g(X)=\begin{cases}\int_{-\infty}^{+\infty}g(x)f_X(x)\mathrm dx&amp;&amp;X\text{ 连续}\\\sum_{x\in\mathcal X}g(x)f_X(x)\mathrm dx&amp;&amp;X\text{ 离散}\\\end{cases}\]</span></p><p>如果 <span class="math inline">\(\E |g(X)|=+\infty\)</span>，称 <span class="math inline">\(\E g(X)\)</span> 不存在。<strong>注意这有个绝对值</strong>。一个经典的期望不存在的随机变量是柯西(Cauchy)随机变量。</p><p>为什么有个绝对值？因为在期望的定义式中，为了避免由于求和（积分）顺序的改变而导致结果改变，应要求期望是<strong>绝对收敛</strong>而非条件收敛的。</p><div class="note note-success">            <p>指数期望：<span class="math inline">\(f_X(x)=\frac{1}{\lambda}e^{-x/\lambda},\,0\leq x&lt;\infty,\,\lambda&gt;0\implies \E X=\lambda\)</span></p><p>二项期望：<span class="math inline">\(f_X(x)=\binom{n}{x}p^x(1-p)^{n-x}\implies \E X=np\)</span></p><p>柯西期望：<span class="math inline">\(f_X(x)=\frac{1}{\pi}\frac{1}{1+x^2}\implies \E |X|=+\infty\)</span></p>          </div><h3 id="距离最小化">2.2 距离最小化</h3><p>如果我们用 <span class="math inline">\((X-b)^2\)</span> 来度量随机变量 <span class="math inline">\(X\)</span> 与常数 <span class="math inline">\(b\)</span> 之间的距离，则可以通过求使得 <span class="math inline">\(\E(X-b)^2\)</span> 最小的 <span class="math inline">\(b\)</span> 作为 <span class="math inline">\(X\)</span> 的估计。由于： <span class="math display">\[\begin{align}\E(X-b)^2&amp;=\E(X-\E X+\E X-b)^2\\&amp;=\E(X-\E X)^2+2\E[(X-\E X)(\E X-b)]+\E(\E X-b)^2\\&amp;=\E(X-\E X)^2+2(\E X-b)\E(X-\E X)+(\E X-b)^2\\&amp;=\E(X-\E X)^2+(\E X-b)^2\end{align}\]</span> 其中第一项与 <span class="math inline">\(b\)</span> 无关，第二项非负且当 <span class="math inline">\(\E X=b\)</span> 时取到 <span class="math inline">\(0\)</span>，因此： <span class="math display">\[\min_b\E(X-b)^2=\E(X-\E X)^2\]</span> 这意味着我们应取 <span class="math inline">\(b=\E X\)</span>.</p><p><br/></p><p>但是如果用 <span class="math inline">\(|X-b|\)</span> 来作为距离的度量，结论还会一样吗？下面假设 <span class="math inline">\(X\)</span> 是连续随机变量。 <span class="math display">\[\begin{align}\E|X-b|&amp;=\int_{-\infty}^{+\infty}|X-b|f(x)\mathrm dx\\&amp;=\int_{-\infty}^{b}(b-x)f(x)\mathrm dx+\int_{b}^{+\infty}(x-b)f(x)\mathrm dx\\&amp;=\left(\int_{b}^{+\infty}xf(x)\mathrm dx-\int_{-\infty}^{b}xf(x)\mathrm dx\right)+b\left(\int_{-\infty}^{b}f(x)\mathrm dx-\int_{b}^{+\infty}f(x)\mathrm dx\right)\end{align}\]</span> 对 <span class="math inline">\(b\)</span> 求导得： <span class="math display">\[\begin{align}\frac{\mathrm d\E|X-b|}{\mathrm db}&amp;=-bf(b)-bf(b)+\int_{-\infty}^b f(x)\mathrm dx-\int_b^{+\infty}f(x)\mathrm dx+b(f(b)+f(b))\\&amp;=\int_{-\infty}^b f(x)\mathrm dx-\int_b^{+\infty}f(x)\mathrm dx\end{align}\]</span> 令其为零： <span class="math display">\[\int_{-\infty}^b f(x)\mathrm dx=\int_b^{+\infty}f(x)\mathrm dx\]</span> 又因为二者相加为 <span class="math inline">\(1\)</span>，所以最优的 <span class="math inline">\(b\)</span> 是随机变量 <span class="math inline">\(X\)</span> 的<strong>中位数</strong> <span class="math inline">\(m\)</span>： <span class="math display">\[\int_{-\infty}^m f(x)\mathrm dx=\int_m^{+\infty}f(x)\mathrm dx=\frac{1}{2}\]</span></p><h2 id="矩和矩母函数">3 矩和矩母函数</h2><h3 id="矩">3.1 矩</h3><p>概率分布的矩是一类重要的期望。对任意整数 <span class="math inline">\(n\)</span>，<span class="math inline">\(X\)</span> 的 <strong><span class="math inline">\(n\)</span> 阶矩</strong>和<strong><span class="math inline">\(n\)</span> 阶中心矩</strong>分别为： <span class="math display">\[\mu_n&#39;=\E X^n,\quad \mu_n=\E(X-\E X)^n\]</span> 其中二阶中心矩即是方差 <span class="math inline">\(\Var X=\E(X-\E X)^2\)</span>.</p><div class="note note-success">            <p>指数方差：<span class="math inline">\(\Var X=\lambda^2\)</span></p><p>二项方差：<span class="math inline">\(\Var X=np(1-p)\)</span></p>          </div><h3 id="矩母函数">3.2 矩母函数</h3><p>设随机变量 <span class="math inline">\(X\)</span> 的 cdf 为 <span class="math inline">\(F_X\)</span>，则<strong>矩母函数（moment generating function, mgf）</strong>定义为： <span class="math display">\[M_X(t)=\E[e^{tX}]=\begin{cases}\int_{-\infty}^{+\infty}e^{tx}f_X(x)\mathrm dx&amp;&amp;X\text{ 连续}\\\sum_x e^{tx}f_X(x)&amp;&amp;X\text{ 离散}\end{cases}\]</span> 这里假设 <span class="math inline">\(t\)</span> 在 <span class="math inline">\(0\)</span> 的某邻域内时上式中的期望存在。如果在 <span class="math inline">\(0\)</span> 的任意邻域内该期望都不存在，称矩母函数不存在。</p><p><strong>从矩母函数可以计算矩</strong>： <span class="math display">\[\E X^n=\left.\frac{\mathrm d^n}{\mathrm d t^n}M_X(t)\right|_{t=0}\]</span> 以连续随机变量为例证明：假设求导可放入积分（下一节叙述），则： <span class="math display">\[\frac{\mathrm d^n}{\mathrm d t^n}M_X(t)=\int_{-\infty}^{+\infty} \frac{\mathrm d^n e^{tx}}{\mathrm d t^n}f_X(x)\mathrm dx=\int_{-\infty}^{+\infty}x^n e^{tx}f_X(x)\mathrm dx=\E[X^ne^{tX}]\]</span> 于是易得上述结论。</p><div class="note note-success">            <p>例【伽玛矩母函数】一般的伽玛概率密度函数的形式如下： <span class="math display">\[f_X(x)=\frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha-1}e^{-x/\beta},\quad0&lt;x&lt;+\infty,\,\alpha&gt;0,\,\beta&gt;0\]</span> 则矩母函数为： <span class="math display">\[\begin{align}M_X(t)&amp;=\E[e^{tX}]=\frac{1}{\Gamma(\alpha)\beta^\alpha}\int_0^{+\infty}x^{\alpha-1}e^{-x/\frac{\beta}{1-\beta t}}\mathrm dx\\&amp;=\left(\frac{1}{1-\beta t}\right)^\alpha\int_0^{+\infty}\frac{1}{\Gamma(\alpha)(\frac{\beta}{1-\beta t})^\alpha}x^{\alpha-1}e^{-x/\frac{\beta}{1-\beta t}}\mathrm dx\\&amp;=\left(\frac{1}{1-\beta t}\right)^\alpha\end{align}\]</span> 最后一个等式成立是因为积分内是另一个伽玛概率密度函数的积分。</p><p>根据矩母函数，容易知道伽玛分布的期望是： <span class="math display">\[\E X=\left.\frac{\mathrm d M_X(t)}{\mathrm dt}\right|_{t=0}=\alpha\beta\]</span></p>          </div><p><strong>矩母函数的主要作用并不是求矩，而是在大多数情况下它唯一地确定了一个概率分布。与之相对的，仅确定了全部（无数个）矩并不能唯一确定概率分布。</strong>换句话说，存在两个不同的概率分布，它们所有的矩都相等。</p><p>但是当随机变量的支撑集有界时，全部矩就能唯一确定概率分布了。总结来说：</p><ol type="1"><li>若 <span class="math inline">\(X,Y\)</span> 的支撑集有界，则 <span class="math inline">\(\forall u,\,F_X(u)=F_Y(u)\)</span> 当且仅当 <span class="math inline">\(\forall r=0,1,2,\ldots,\,\E X^r=\E Y^r\)</span></li><li>若 <span class="math inline">\(X,Y\)</span> 矩母函数都存在，且对 <span class="math inline">\(0\)</span> 的某邻域内的任意 <span class="math inline">\(t\)</span>，都有 <span class="math inline">\(M_X(t)=M_Y(t)\)</span>，则 <span class="math inline">\(\forall u, F_X(u)=F_Y(u)\)</span></li></ol><h2 id="积分号下的求导">4 积分号下的求导</h2><p>统计学中经常会遇到交换积分与求导的顺序。</p><h3 id="leibnitz-法则">4.1 Leibnitz 法则</h3><p>若 <span class="math inline">\(f(x,\theta),a(\theta),b(\theta)\)</span> 都对 <span class="math inline">\(\theta\)</span> 可导，则： <span class="math display">\[\frac{\mathrm d}{\mathrm d\theta}\int_{a(\theta)}^{b(\theta)} f(x,\theta)\mathrm dx=f(b(\theta),\theta)\cdot\frac{\mathrm d}{\mathrm d\theta}b(\theta)-f(a(\theta),\theta)\cdot\frac{\mathrm d}{\mathrm d\theta}a(\theta)+\int_{a(\theta)}^{b(\theta)}\frac{\partial}{\partial\theta}f(x,\theta)\mathrm dx\]</span> 特别的，如果 <span class="math inline">\(a(\theta),b(\theta)\)</span> 是常函数，得到 Leibnitz 法则的一个特例： <span class="math display">\[\frac{\mathrm d}{\mathrm d\theta}\int_a^bf(x,\theta)\mathrm dx=\int_a^b\frac{\partial}{\partial \theta}f(x,\theta)\mathrm dx\]</span></p><h3 id="求导与积分互换的相关定理">4.2 求导与积分互换的相关定理</h3><p>将求导写作定义式，则： <span class="math display">\[\begin{align}&amp;\int_{-\infty}^{+\infty}\frac{\partial}{\partial \theta}f(x,\theta)\mathrm dx=\int_{-\infty}^{+\infty}\lim_{\delta\to0}\left[\frac{f(x,\theta+\delta)-f(x,\theta)}{\delta}\right]\mathrm dx\\&amp;\frac{\mathrm d}{\mathrm d\theta}\int_{-\infty}^{+\infty}f(x,\theta)\mathrm dx=\lim_{\delta\to0}\int_{-\infty}^{+\infty}\left[\frac{f(x,\theta+\delta)-f(x,\theta)}{\delta}\right]\mathrm dx\end{align}\]</span> 因此要证明求导和积分可以互换，只需证明求极限与积分可以互换。解决这个问题要用到超纲知识，因此这里直接给出结论。</p><p><strong>定理</strong>：设对于任意 <span class="math inline">\(x\)</span>，<span class="math inline">\(h(x,y)\)</span> 在 <span class="math inline">\(y_0\)</span> 处连续，并且存在函数 <span class="math inline">\(g(x)\)</span> 满足：</p><ol type="1"><li>对任意 <span class="math inline">\(x,y\)</span> 有 <span class="math inline">\(|h(x,y)|\leq g(x)\)</span>【<span class="math inline">\(g(x)\)</span> 将 <span class="math inline">\(h(x,y)\)</span> 控制住】</li><li><span class="math inline">\(\int_{-\infty}^{+\infty}g(x)\mathrm dx&lt;+\infty\)</span>【<span class="math inline">\(g(x)\)</span> 有有限积分】</li></ol><p>则： <span class="math display">\[\lim_{y\to y_0}\int_{-\infty}^{+\infty}h(x,y)\mathrm dx=\int_{-\infty}^{+\infty}\lim_{y\to y_0}h(x,y)\mathrm dx\]</span> <br/></p><p>将这个定理套用到求导与积分互换问题中：假设 <span class="math inline">\(f(x,\theta)\)</span> 在 <span class="math inline">\(\theta=\theta_0\)</span> 处可导，那么把 <span class="math inline">\((f(x,\theta_0+\delta)-f(x,\theta_0))/\delta\)</span> 视作 <span class="math inline">\(h(x,\delta)\)</span>，得到求导与积分互换的条件是：存在函数 <span class="math inline">\(g(x,\theta_0)\)</span> 和常数 <span class="math inline">\(\delta_0&gt;0\)</span>，使得</p><ol type="1"><li>对任意 <span class="math inline">\(x\)</span> 以及 <span class="math inline">\(|\delta|\leq\delta_0\)</span>，有 <span class="math inline">\(\left|\frac{f(x,\theta_0+\delta)-f(x,\theta_0)}{\delta}\right|\leq g(x,\theta_0)\)</span></li><li><span class="math inline">\(\int_{-\infty}^{+\infty}g(x,\theta_0)\mathrm dx&lt;+\infty\)</span></li></ol><p>注意我们现在取 <span class="math inline">\(\theta\)</span> 为某定值 <span class="math inline">\(\theta_0\)</span>；另外，<span class="math inline">\(\delta_0\)</span> 的引入是因为我们考虑的是导数，只需要足够小的 <span class="math inline">\(\delta\)</span> 而不需要对所有 <span class="math inline">\(\delta\)</span> 都成立。</p><p>上面第一个条件类似于 Lipschitz 条件，通过给出一阶导函数的界来约束函数的光滑性。</p><p>假若 <span class="math inline">\(f(x,\theta)\)</span> 在任意 <span class="math inline">\(\theta\)</span> 处可导，那么上述定理中的 <span class="math inline">\(\theta_0\)</span> 可以全部换成 <span class="math inline">\(\theta\)</span>。并且在这种情形下，条件 1 可以根据拉格朗日中值定理进一步改写为：对任意满足 <span class="math inline">\(|\theta&#39;-\theta|\leq \delta_0\)</span> 的 <span class="math inline">\(\theta&#39;\)</span> 有 <span class="math display">\[\left|\left.\frac{\partial}{\partial \theta}f(x,\theta)\right|_{\theta=\theta&#39;}\right|\leq g(x,\theta)\]</span></p><h3 id="求导与求和互换的相关定理">4.3 求导与求和互换的相关定理</h3><p><strong>定理</strong>：设级数 <span class="math inline">\(\sum_{x=0}^{+\infty}h(\theta,x)\)</span> 对任意实数区间 <span class="math inline">\((a,b)\)</span> 内的 <span class="math inline">\(\theta\)</span> 都收敛，并且：</p><ol type="1"><li>对任意 <span class="math inline">\(x\)</span>，<span class="math inline">\(\frac{\partial}{\partial \theta}h(\theta,x)\)</span> 都是 <span class="math inline">\(\theta\)</span> 的连续函数</li><li><span class="math inline">\(\sum_{x=0}^{+\infty}\frac{\partial}{\partial \theta}h(\theta,x)\)</span> 在 <span class="math inline">\((a,b)\)</span> 的任意闭有界子区间上都<strong>一致收敛</strong></li></ol><p>则： <span class="math display">\[\frac{\mathrm d}{\mathrm d\theta}\sum_{x=0}^{+\infty}h(\theta,x)=\sum_{x=0}^{+\infty}\frac{\partial}{\partial\theta}h(\theta,x)\]</span></p><h3 id="求和与积分互换的相关定理">4.4 求和与积分互换的相关定理</h3><p><strong>定理</strong>：设级数 <span class="math inline">\(\sum_{x=0}^{+\infty}h(\theta,x)\)</span> 在 <span class="math inline">\([a,b]\)</span> 上一致收敛，且对任意 <span class="math inline">\(x\)</span>，<span class="math inline">\(h(\theta,x)\)</span> 都是 <span class="math inline">\(\theta\)</span> 的连续函数，则： <span class="math display">\[\int_a^b\sum_{x=0}^{+\infty}h(\theta,x)\mathrm d\theta=\sum_{x=0}^{+\infty}\int_a^bh(\theta,x)\mathrm d\theta\]</span></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>统计推断</category>
      
    </categories>
    
    
    <tags>
      
      <tag>statistics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[统计推断]第一章·概率论</title>
    <link href="/blog-main/2022/02/24/%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD-%E7%AC%AC%E4%B8%80%E7%AB%A0%C2%B7%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    <url>/blog-main/2022/02/24/%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD-%E7%AC%AC%E4%B8%80%E7%AB%A0%C2%B7%E6%A6%82%E7%8E%87%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="概率论基础">1 概率论基础</h2><h3 id="公理化基础">1.1 公理化基础</h3><p><strong><span class="math inline">\(\sigma\)</span> algebra / Borel field</strong>: 若 <span class="math inline">\(S\)</span> 的一族子集 <span class="math inline">\(\mathcal B\)</span> 满足以下三个性质：</p><ol type="1"><li><span class="math inline">\(\varnothing\in\mathcal B\)</span></li><li>若 <span class="math inline">\(A\in\mathcal B\)</span>，则 <span class="math inline">\(A^C\in\mathcal B\)</span></li><li>若 <span class="math inline">\(A_1,A_2,\ldots\in\mathcal B\)</span>，则 <span class="math inline">\(\cup_{i=1}^\infty A_i\in\mathcal B\)</span></li></ol><p>则称 <span class="math inline">\(\mathcal B\)</span> 为一个 <span class="math inline">\(\sigma\)</span> 代数或一个 Borel 域。</p><p><span class="math inline">\(\{\varnothing,S\}\)</span> 称为平凡的 <span class="math inline">\(\sigma\)</span> 代数。</p><p><strong>概率公理（Kolmogorov 公理）</strong>：已知样本空间 <span class="math inline">\(S\)</span> 和 <span class="math inline">\(\sigma\)</span> 代数 <span class="math inline">\(\mathcal B\)</span>，若定义在 <span class="math inline">\(\mathcal B\)</span> 上的函数 <span class="math inline">\(P\)</span> 满足下列条件：</p><ol type="1"><li>【非负性】对任意 <span class="math inline">\(A\in\mathcal B\)</span>，<span class="math inline">\(P(A)\geq0\)</span></li><li>【归一性】<span class="math inline">\(P(S)=1\)</span></li><li>【可数可加性】若 <span class="math inline">\(A_1,A_2,\ldots\in\mathcal B\)</span> 且两两不交，则 <span class="math inline">\(P(\cup_{i=1}^\infty A_i)=\sum_{i=1}^\infty P(A_i)\)</span></li></ol><p>一些学者认为可数可加性并不显然，应该换成有限可加性。</p><h3 id="概率演算">1.2 概率演算</h3><p><strong>定理</strong>： <span class="math display">\[\begin{align}&amp;P(B\cap A^C)=P(B)-P(A\cap B)\\&amp;P(A\cup B)=P(A)+P(B)-P(A\cap B)\end{align}\]</span></p><p><strong>Bonferroni 不等式</strong>： <span class="math display">\[P(A\cap B)\geq P(A)+P(B)-1\]</span> 推广： <span class="math display">\[P\left(\bigcap_{i=1}^n A_i\right)\geq\sum_{i=1}^n P(A_i)-(n-1)\]</span> <strong>Boole 不等式</strong>： <span class="math display">\[P\left(\bigcup_{i=1}^\infty A_i\right)\leq \sum_{i=1}^\infty P(A_i)\]</span></p><h3 id="计数">1.3 计数</h3><p>从 <span class="math inline">\(n\)</span> 个对象中选 <span class="math inline">\(r\)</span> 个的方式：</p><table><tr><th align="center"></th><th align="center">无放回</th><th align="center">有放回</th></tr><tr><th align="center">有序</th><td align="center"><span class="math display">\[\frac{n!}{(n-r)!}\]</span></td><td align="center"><span class="math display">\[n^r\]</span></td></tr><tr><th align="center">无序</th><td align="center"><span class="math display">\[\binom{n}{r}\]</span></td><td align="center"><span class="math display">\[\binom{n+r-1}{r}\]</span></td></tr></table><p>有序或无放回的情形都比较容易，难一点的是无序且有放回的情形。可以把 <span class="math inline">\(n\)</span> 个对象看作 <span class="math inline">\(n\)</span> 个箱子，然后把 <span class="math inline">\(r\)</span> 个无差别的小球放到这些箱子中。这等价于在 <span class="math inline">\(r\)</span> 个小球之间插入 <span class="math inline">\(n-1\)</span> 个隔板，又等价于在 <span class="math inline">\(n+r-1\)</span> 个对象中，挑 <span class="math inline">\(r\)</span> 个当作小球，剩下 <span class="math inline">\(n-1\)</span> 个当作隔板。因此答案是 <span class="math inline">\(\binom{n+r-1}{r}\)</span>.</p><h2 id="条件概率与独立性">2 条件概率与独立性</h2><p><strong>条件概率</strong>： <span class="math display">\[P(A\mid B)=\frac{P(A\cap B)}{P(B)}\]</span></p><p><strong>Bayes 公式</strong>： <span class="math display">\[P(A_i\mid B)=\frac{P(B\mid A_i)P(A_i)}{\sum_{j}P(B\mid A_j)P(A_j)}\]</span> <strong>独立</strong>：称事件 <span class="math inline">\(A,B\)</span> 统计独立，若： <span class="math display">\[P(A\cap B)=P(A)P(B)\]</span> 称一列事件 <span class="math inline">\(A_1,\ldots,A_n\)</span> <strong>相互独立</strong>，若对任意 <span class="math inline">\(A_{i_1},\ldots,A_{i_k}\)</span>，都有； <span class="math display">\[P\left(\bigcap_{j=1}^k A_{i_j}\right)=\prod_{j=1}^kP(A_{i_j})\]</span></p><h2 id="随机变量">3 随机变量</h2><h3 id="累积分布函数cdf">3.1 累积分布函数（cdf）</h3><p><span class="math display">\[F_X(x)=P_X(X\leq x)\]</span></p><p>满足性质：</p><ol type="1"><li><span class="math inline">\(\lim\limits_{x\to-\infty} F(x)=0,\,\lim\limits_{x\to+\infty}F(x)=1\)</span></li><li><span class="math inline">\(F(x)\)</span> 是 <span class="math inline">\(x\)</span> 的单调递增函数</li><li><span class="math inline">\(F(x)\)</span> 右连续，即 <span class="math inline">\(\lim\limits_{x\to x_0^+}F(x)=F(x_0)\)</span></li></ol><p><strong>同分布</strong>：随机变量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 同分布，当且仅当 <span class="math inline">\(\forall x,\,F_X(x)=F_Y(y)\)</span>.</p><h3 id="概率密度函数pdf和概率质量函数pmf">3.2 概率密度函数（pdf）和概率质量函数（pmf）</h3><p>对于某离散随机变量 <span class="math inline">\(X\)</span>，其<strong>概率质量函数（pmf）</strong>为： <span class="math display">\[f_X(x)=P_X(X=x)\]</span> 对于某连续随机变量 <span class="math inline">\(X\)</span>，其<strong>概率密度函数（pdf）</strong>为满足下式的函数： <span class="math display">\[F_X(x)=\int_{-\infty}^x f_X(x)\mathrm dx\]</span> 由于 <span class="math inline">\(F_X(x)\)</span> 可能连续而不可导，因此上式并不总是成立。事实上确实存在这样的连续随机变量，对任意 <span class="math inline">\(f_X(x)\)</span> 都不满足上式，但这不在本书涉及范围内。在更深入的教材中，称满足上式的随机变量<strong>绝对连续</strong>。</p><div class="note note-success">            <p>例：<strong>logistic 分布</strong>（的一个特例）的 cdf 和 pdf 分别是： <span class="math display">\[\begin{align}&amp;F_X(x)=\frac{1}{1+e^{-x}}\\&amp;f_X(x)=\frac{\mathrm dF_X(x)}{\mathrm dx}=\frac{e^{-x}}{(1+e^{-x})^2}\end{align}\]</span> <img src="logistic.png" width=50% /></p>          </div>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>统计推断</category>
      
    </categories>
    
    
    <tags>
      
      <tag>statistics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>gitignore pattern format</title>
    <link href="/blog-main/2022/02/11/gitignore-pattern-format/"/>
    <url>/blog-main/2022/02/11/gitignore-pattern-format/</url>
    
    <content type="html"><![CDATA[<p>网上虽然能搜到很多讲解 gitignore 模式匹配格式的博客，但它们大都不全或者不太清楚，要彻底搞明白还得直接看官方文档啊！</p><h2 id="pattern-format">PATTERN FORMAT</h2><blockquote><p><strong>翻译</strong>自：https://git-scm.com/docs/gitignore PATTERN FORMAT 一节。</p></blockquote><ul><li><p>空行不匹配任何文件，因此可以用空行增加可读性。</p></li><li><p>以 <code>#</code> 开头的一行是注释，用 <code>\</code> 进行转义。</p></li><li><p>末尾的空格将被忽略，除非在空格前用 <code>\</code> 转义。</p></li><li><p>可选前缀 <code>!</code> 表示否定，若匹配的文件被<strong>之前的</strong>模式忽略了，则它将被重新包含回来；</p><p><u>但是，如果某文件的父目录被忽略了，则不能重新包含该文件</u>，这是因为 git 出于性能的考虑不会检索被忽略的目录下的文件；</p><p>用 <code>\</code> 进行转义。</p></li><li><p><code>/</code> 用作目录的分隔符，可能出现在模式串的开头、中间或结尾。</p><ul><li>若 <code>/</code> 出现在模式串开头或中间（或两者皆有），则模式串匹配<strong>相对</strong>于 <code>.gitignore</code> 所在位置的路径；否则模式串将匹配 <code>.gitignore</code> 位置以下的任何层级。</li><li>若 <code>/</code> 出现在模式串的结尾，则模式串只会匹配目录；否则模式串既会匹配目录，也会匹配文件。</li><li>举个例子，模式串 <code>doc/frotz/</code> 匹配 <code>doc/frotz</code> 目录，但不匹配 <code>a/doc/frotz</code> 目录；然而模式串 <code>frotz/</code> 会匹配 <code>frotz</code> 和 <code>a/frotz</code> 目录。</li></ul></li><li><p>一个星号 <code>*</code> 匹配<strong>除了 <code>/</code> 以外</strong>的任意字符串；一个问号 <code>?</code> 匹配<strong>除了 <code>/</code> 以外</strong>的任意单个字符；区间表示，如 <code>[a-zA-Z]</code>，可以用于匹配任何一个区间范围内的字符。</p></li><li><p>两个星号 <code>**</code> 匹配完整路径名，有一些特殊含义：</p><ul><li>以 <code>**</code> 开头并紧接一个 <code>/</code> 表示在任何目录下匹配。例如 <code>**/foo</code> 将匹配任意位置的 <code>foo</code> 文件或目录，与 <code>foo</code> 模式串效果相同；<code>**/foo/bar</code> 匹配任何在某 <code>foo</code> 目录下的 <code>bar</code> 文件或目录。</li><li>以 <code>/**</code> 结尾表示匹配目录下的任何东西。例如 <code>abc/**</code> 匹配所有在 <code>abc</code> 目录（相对于 <code>.gitignore</code>）下的任何文件，<strong>不限目录深度</strong>。</li><li><code>/**/</code> 表示匹配 <strong>0 个或多个</strong>目录层级。例如 <code>a/**/b</code> 能匹配 <code>a/b</code>、<code>a/x/b</code>、<code>a/x/y/b</code> 等等。</li><li>其他连续两个星号的用法将被视为一个星号的用法。</li></ul></li></ul><h2 id="常见用法">常见用法</h2><p>有了上一节的原理支撑，一些常见的需求就能顺手写出了。</p><ul><li><p>忽略项目中所有 <code>.DS_Store</code> 文件（mac 用户表示很赞）</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-title">.DS_Store</span><br></code></pre></td></tr></table></figure></li><li><p>忽略项目中所有 <code>ckpt</code> 目录（不把训练时保存的 checkpoint 加入 git）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">ckpt/<br></code></pre></td></tr></table></figure><p>同理，常用于忽略 <code>__pycache__</code> 目录、<code>.idea</code> 目录等配置性目录。</p></li><li><p>忽略 <code>/data/</code> 目录（不把数据集加入 git）</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs haskell">/<span class="hljs-class"><span class="hljs-keyword">data</span>/</span><br></code></pre></td></tr></table></figure></li><li><p>忽略目录 <code>foo</code> 下所有的 <code>model.pt</code>（训练好的模型太大了，不适于放入 git）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">/foo/**/model.pt<br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>技术栈</category>
      
      <category>Git</category>
      
    </categories>
    
    
    <tags>
      
      <tag>git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git基础学习</title>
    <link href="/blog-main/2022/01/10/Git%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"/>
    <url>/blog-main/2022/01/10/Git%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<blockquote><p>基本都参考自 <a href="https://git-scm.com/book/zh/v2">Git-book</a>。</p></blockquote><h2 id="什么是版本控制">什么是版本控制</h2><p><strong>本地版本控制系统</strong>：采用某种简单的数据库来记录文件的历次更新差异。</p><p><img src="https://git-scm.com/book/en/v2/images/local.png" width=50% /></p><p>缺点：无法让不同系统上的开发者协同工作。</p><p><strong>集中化的版本控制系统</strong>：一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。</p><p><img src="https://git-scm.com/book/en/v2/images/centralized.png" width=50% /></p><p>优点：每个人都可以在一定程度上看到项目中的其他人正在做些什么。 而管理员也可以轻松掌控每个开发者的权限，并且管理一个 CVCS 要远比在各个客户端上维护本地数据库来得轻松容易。</p><p>缺点：中央服务器的单点故障，有丢失所有历史更新记录的风险。</p><p><strong>分布式版本控制系统</strong>：客户端并不只提取最新版本的文件快照， 而是把代码仓库完整地镜像下来，包括完整的历史记录。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。</p><p><img src="https://git-scm.com/book/en/v2/images/distributed.png" width=50% /></p><h2 id="git-的特点">Git 的特点</h2><ol type="1"><li><p><strong>直接记录快照，而非差异比较。</strong></p><p>基于差异的版本控制：</p><p><img src="https://git-scm.com/book/en/v2/images/deltas.png" width=80% /></p><p>Git 对待数据更像是一个<strong>快照流</strong>：</p><p><img src="https://git-scm.com/book/en/v2/images/snapshots.png" width=80% /></p></li><li><p><strong>近乎所有操作都是本地执行</strong></p><p>速度快，离线的时候也能操作。</p></li><li><p><strong>Git 保持完整性</strong></p><p>Git 所有数据在存储前都计算校验和，然后以校验和来引用。这意味着不可能在 Git 不知情时更改任何文件内容或目录内容。</p><p>Git 计算校验和的机制是 SHA-1 Hash。</p></li><li><p><strong>Git 一般只添加数据</strong></p><p>Git 操作几乎只往 Git 数据库中<strong>添加</strong>数据。 你很难使用 Git 从数据库中删除数据，也就是说 Git 几乎不会执行任何可能导致文件不可恢复的操作。</p></li><li><p><strong>三种状态</strong></p><p>文件有三种状态：</p><ul><li><strong>已修改（modified）</strong>：已修改表示修改了文件，但还没保存到数据库中。</li><li><strong>已暂存（staged）</strong>：已暂存表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。</li><li><strong>已提交（committed）</strong>：已提交表示数据已经安全地保存在本地数据库中。</li></ul><p>因而，Git 项目拥有三个阶段：工作区、暂存区和（本地）Git 仓库。</p><p><img src="https://git-scm.com/book/en/v2/images/areas.png" width=60% /></p><p>基本的 Git 工作流程如下：</p><ol type="1"><li>在工作区中修改文件。</li><li>将你想要下次提交的更改选择性地暂存，这样只会将更改的部分添加到暂存区。</li><li>提交更新，找到暂存区的文件，将快照永久性存储到 Git 目录。</li></ol></li></ol><h2 id="git-配置">Git 配置</h2><p>可以使用 <code>git config</code> 命令来设置一些配置变量，这些变量存储在：</p><ul><li><code>/etc/gitconfig</code> 文件：系统上每一个用户的通用配置。执行 <code>git config</code> 时带上 <code>--system</code> 选项，需要管理员或超级用户权限。</li><li><code>~/.gitconfig</code> 或 <code>~/.config/git/config</code> 文件：只针对当前用户。传递 <code>--global</code> 选项，对系统上<strong>所有</strong>的仓库生效。</li><li>当前使用仓库的 Git 目录中的 <code>config</code> 文件（即 <code>.git/config</code>）：针对该仓库。 传递 <code>--local</code> 选项（其实默认就是它）。</li></ul><p>每一个级别的设置会覆盖上一级别的设置。</p><p>以下命令查看所有的配置以及它们所在的文件：</p><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs brainfuck"><span class="hljs-comment">$ git config</span> <span class="hljs-literal">--</span><span class="hljs-comment">list</span> <span class="hljs-literal">--</span><span class="hljs-comment">show</span><span class="hljs-literal">-</span><span class="hljs-comment">origin</span><br></code></pre></td></tr></table></figure><p><br/></p><p>安装完 Git 之后要设置用户名和邮箱，<strong>它们会写入到每一次提交中，不可更改</strong>：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">$ git config <span class="hljs-attr">--global</span> user<span class="hljs-selector-class">.name</span> <span class="hljs-string">&quot;John Doe&quot;</span><br>$ git config <span class="hljs-attr">--global</span> user<span class="hljs-selector-class">.email</span> johndoe@example.com<br></code></pre></td></tr></table></figure><blockquote><p>这里使用了 <code>--global</code> 选项，因此对系统上所有仓库有效。</p></blockquote><h2 id="获取-git-仓库">获取 Git 仓库</h2><p>有两种获取 Git 项目的方式：</p><ul><li><p>将尚未进行版本控制的本地目录转换为 Git 仓库：</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>git init<br></code></pre></td></tr></table></figure><p>该命令创建一个名为 <code>.git</code> 的子目录，含有初始化的 Git 仓库中所有的必须文件。</p></li><li><p>从其他服务器上 clone 一个已存在的 Git 仓库：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">git <span class="hljs-built_in">clone</span> https://github.com/libgit2/libgit2</span><br></code></pre></td></tr></table></figure><p>可以通过额外的参数在 clone 的时候自定义本地仓库的名字：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">git <span class="hljs-built_in">clone</span> https://github.com/libgit2/libgit2 mylibgit</span><br></code></pre></td></tr></table></figure><p>上面的例子使用的是 <code>https://</code> 协议，也可以使用 <code>git://</code> 协议或者 SSH 传输协议。</p></li></ul><h2 id="记录每次更新到仓库">记录每次更新到仓库</h2><p>工作目录下的每一个文件都不外乎这两种状态：<strong>已跟踪</strong>或<strong>未跟踪</strong>。已跟踪的文件是指那些被纳入了版本控制的文件，在上一次快照中有它们的记录，在工作一段时间后， 它们的状态可能是未修改，已修改或已放入暂存区。简而言之，已跟踪的文件就是 Git 已经知道的文件。</p><p><img src="https://git-scm.com/book/en/v2/images/lifecycle.png" width=70% /></p><p><strong>检查当前文件状态</strong></p><p>使用 <code>git status</code> 命令查看哪些文件处于什么状态，<code>git status -s</code> 或 <code>git status --short</code> 输出更紧凑的格式。</p><p><strong>跟踪新文件</strong></p><p>使用 <code>git add</code> 命令跟踪新文件，参数是文件或目录的路径，如果参数是目录的路径，则该命令将递归地跟踪该目录下的所有文件。</p><p><strong>暂存已修改文件</strong></p><p><code>git add</code> 还可以用来把已修改的文件放入暂存区。事实上它是一个多功能命令，理解为“精确地将内容添加到下一次提交中”。</p><p><strong>忽略文件</strong></p><p>文件 <code>.gitignore</code> 列出无需纳入 Git 管理的文件，格式规范如下：</p><ul><li>所有空行或者以 <code>#</code> 开头的行都会被 Git 忽略。</li><li>可以使用标准的 glob 模式匹配，它会递归地应用在整个工作区中。</li><li>匹配模式可以以（<code>/</code>）开头防止递归。</li><li>匹配模式可以以（<code>/</code>）结尾指定目录。</li><li>要忽略指定模式以外的文件或目录，可以在模式前加上叹号（<code>!</code>）取反。</li></ul><blockquote><p>glob 模式：shell 所使用的简化了的正则表达式</p><ul><li>星号（<code>*</code>）匹配零个或多个任意字符；</li><li><code>[abc]</code> 匹配任何一个列在方括号中的字符 （这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c）；</li><li>问号（<code>?</code>）只匹配一个任意字符；</li><li>如果在方括号中使用短划线分隔两个字符， 表示所有在这两个字符范围内的都可以匹配（比如 <code>[0-9]</code> 表示匹配所有 0 到 9 的数字）；</li><li>使用两个星号（<code>**</code>）表示匹配任意中间目录，比如 <code>a/**/z</code> 可以匹配 <code>a/z</code> 、 <code>a/b/z</code> 或 <code>a/b/c/z</code> 等。</li></ul></blockquote><p><strong>查看修改</strong></p><p>使用 <code>git diff</code> 命令（不带任何参数）比较<strong>工作目录</strong>中当前文件和<strong>暂存区</strong>快照之间的差异，也即修改之后还为暂存的变化内容。</p><p>使用 <code>git diff --staged</code> 或者 <code>git diff --cached</code>（<code>--staged</code> 和 <code>--cached</code> 是同义词）比较<strong>已暂存</strong>文件与<strong>上一次提交</strong>的文件的差异。</p><p><strong>提交更新</strong></p><p>使用 <code>git commit</code> 命令会启动文本编辑器来输入提交说明。默认的提交信息包含最后一次运行 <code>git status</code> 的输出（在注释行中）。</p><p>也可以使用 <code>-m</code> 选项将提交信息和命令放在同一行。</p><p>提交后 Git 会输出，当前是在哪个分支提交的，本次提交的完整 SHA-1 校验和是什么，以及在本次提交中，有多少文件修订过，多少行添加和删改过。</p><p><strong>跳过暂存区</strong></p><p>给 <code>git commit</code> 加上 <code>-a</code> 选项，Git 就会自动把所有<em>已跟踪</em>的文件暂存起来一并提交，从而跳过 <code>git add</code> 步骤。</p><p><strong>移除文件</strong></p><p>使用 <code>git rm</code> 命令可以从<strong>工作区</strong>和<strong>暂存区</strong>中一并删除指定的文件。</p><p>如果只是从工作区手动删除文件，则运行 <code>git status</code> 会在 “Changes not staged for commit” 部分看到相应信息。这时再运行 <code>git rm</code> 可以记录下此次删除文件的操作。如果要删除之前修改过或已经放入暂存区的文件，则必须使用强制删除选项 <code>-f</code>。这是一种安全特性，用于防止误删尚未添加到快照的数据，这样的数据不能被 Git 恢复。</p><p>使用 <code>git rm --cached</code> 命令只从<strong>暂存区</strong>删除指定文件，文件依然保留在工作区。</p><p><strong>移动文件</strong></p><p><code>git mv  file_from file_to</code></p><h2 id="查看提交历史">查看提交历史</h2><p><code>git log</code> 命令按时间先后顺序列出所有的提交（SHA-1 校验和、作者名字和邮箱、提交时间以及提交说明）。</p><p><code>git log -p</code> 或 <code>git log --patch</code> 会显示每次提交所引入的差异，也可以限制显示的日志条目数量，例如使用 <code>-2</code> 选项来只显示最近的两次提交。</p><p><code>git log --stat</code> 可以附带上每次提交的简略统计信息（列出所有被修改的文件、有多少文件被修改了以及被修改的文件的哪些行被移除或是添加了）。</p><p><code>--pretty</code> 选项使用不同于默认格式的方式展示提交历史。它有一些内置的子选项，例如 <code>oneline</code>、<code>short</code>、<code>full</code>、<code>fuller</code>、<code>format</code> 等。</p><p><code>--graph</code> 选项能形象地展示分支、合并的历史，与 <code>oneline</code> 或 <code>format</code> 结合使用更佳。</p><h2 id="撤销操作">撤销操作</h2><p>如果提交完了才发现漏掉了几个文件没有添加，或者提交信息写错了，可以用 <code>git commit --amend</code> 重新提交。这个命令会将暂存区中的文件提交。 如果自上次提交以来你还未做任何修改（例如，在上次提交后马上执行了此命令）， 那么快照会保持不变，而你所修改的只是提交信息。</p><p>取消暂存的文件：使用 <code>git reset HEAD &lt;file&gt;...</code> 来取消暂存（执行 <code>git status</code> 后有提示）。</p><p>撤销对文件的修改：使用 <code>git checkout -- &lt;file&gt;...</code> 恢复工作区文件到上一次提交的样子（执行 <code>git status</code> 后有提示）。</p><h2 id="远程仓库的使用">远程仓库的使用</h2><p><strong>查看远程仓库</strong></p><p>使用 <code>git remote</code> 命令查看已经配置的远程仓库服务器，它会列出指定的每一个远程服务器的简写。clone 下来的仓库服务器默认名字为 origin。</p><p>使用 <code>git remote -v</code> 可以显示简写和对应的 URL。</p><p><strong>添加远程仓库</strong></p><p>使用 <code>git remote add &lt;shortname&gt; &lt;url&gt;</code> 添加一个新的远程仓库，同时指定一个简写。</p><p><strong>从远程仓库 fetch 与 pull</strong></p><p>使用 <code>git fetch &lt;remote&gt;</code> 拉取所有你还没有的数据。执行完成后，你将会拥有那个远程仓库中所有分支的引用，可以随时合并或查看。<code>git fetch</code> 命令只会将数据下载到你的本地仓库——它并不会自动合并或修改你当前的工作。</p><p>如果当前分支设置了跟踪远程分支，那么可以用 <code>git pull</code> 命令来自动抓取后合并该远程分支到当前分支。</p><p><strong>推送到远程仓库</strong></p><p><code>git push &lt;remote&gt; &lt;branch&gt;</code></p><p><strong>查看某个远程仓库</strong></p><p>使用 <code>git remote show &lt;remote&gt;</code> 查看某一个远程仓库的更多信息。</p><p><strong>远程仓库的重命名和移除</strong></p><p>使用 <code>git remote rename</code> 修改某一个远程仓库的简写名。</p><p>使用 <code>git remote remove</code> 或 <code>git remote rm</code> 移除一个远程仓库。</p><h2 id="git-分支">Git 分支</h2><p><strong>新建分支</strong></p><p>为了形象地说明，假设我们暂存了三个文件，则 Git 会为每一个文件计算校验和，然后将当前版本的文件快照用 <strong>blob</strong> 对象保存下来；commit 时，Git 会计算每一个子目录的校验和，然后将这些校验和保存为 <strong>tree</strong> 对象。随后 Git 创建一个 <strong>commit</strong> 对象，包含指向上述 tree 对象的指针。</p><p><img src="https://git-scm.com/book/en/v2/images/commit-and-tree.png" /></p><p>做一些修改后再次提交，则这次产生的 commit 对象包含指向上一次的 commit 对象（父对象）的指针。</p><p><img src="https://git-scm.com/book/en/v2/images/commits-and-parents.png" /></p><p><strong>Git 分支的本质是指向提交对象的可变指针。</strong>Git 默认分支名字是 <code>master</code>，在多次提交操作之后，你其实已经有一个指向最后那个提交对象的 <code>master</code> 分支。 <code>master</code> 分支会在每次提交时自动向前移动。</p><p><img src="https://git-scm.com/book/en/v2/images/branch-and-history.png" width=60% /></p><p>使用 <code>git branch &lt;newbranchname&gt;</code> 命令创建一个分支，本质是创建了一个可移动的指针。为了知道当前在哪一个分支上，Git 维护一个特殊的 <code>HEAD</code> 指针，指向当前所在的本地分支。</p><p><img src="https://git-scm.com/book/en/v2/images/head-to-master.png" width=50% /></p><p>使用 <code>git log --decorate</code> 可以查看各个分支当前所指的对象。</p><p>使用 <code>git checkout</code> 切换到一个已存在的分支，本质是将 <code>HEAD</code> 指针切换到分支指针上。这时再提交内容时，<code>HEAD</code> 指针指向的那个分支指针向前移动，而其他分支指针不移动：</p><p><img src="https://git-scm.com/book/en/v2/images/advance-testing.png" width=65% /></p><p>再使用 <code>git checkout</code> 回到 <code>master</code> 分支时，除了 <code>HEAD</code> 指针指回 <code>master</code> 分支，工作区也会恢复成 <code>master</code> 分支指向的快照内容。这时再次修改，则提交历史会产生<strong>分叉</strong>。</p><p><img src="https://git-scm.com/book/en/v2/images/advance-master.png" width=65% /></p><p><code>git log --oneline --decorate --graph --all</code> 会输出你的提交历史、各个分支的指向以及项目的分支分叉情况。</p><p>注意，<code>git branch</code> 只创建分支而不切换，需要再执行 <code>git checkout</code>。如果想创建之后立即切换，可以使用 <code>git checkout -b &lt;newbranchname&gt;</code> 命令。</p><p>使用 <code>git branch -d</code> 删除分支。</p><p><strong>分支的合并</strong></p><p>使用 <code>git merge</code> 命令合并分支，假设当前在 A 分支，则 <code>git merge B</code> 会将 B 分支的内容合并到 A 中。如果 B 是 A 的直接后继，则 Git 要做的事情仅仅是将 A 指针移动到 B 指针相同的地方，这被称做 fast-forward；如果合并的两个分支有分叉，则 Git 会对待合并的两个分支以及它们的公共祖先做一个简单的三方合并：</p><p><img src="https://git-scm.com/book/en/v2/images/basic-merging-1.png" width=70% /></p><p><img src="https://git-scm.com/book/en/v2/images/basic-merging-2.png" width=70% /></p><p>Git 会对三方合并的结果做一个快照并创建一个新的提交指向它，这被称做一个合并提交，它不只有一个父提交。</p><p><strong>有冲突的分支合并</strong></p><p>如果两个不同的分支对同一个文件的同一个部分进行了不同的修改，合并时就会产生冲突。此时 Git 做了合并，但是没有自动地创建一个新的合并提交。Git 会暂停下来，等待你去解决合并产生的冲突。你可以在合并冲突后的任意时刻使用 <code>git status</code> 命令来查看那些因包含合并冲突而处于未合并（unmerged）状态的文件。</p><p>解决冲突后可以输入 <code>git commit</code> 来完成合并提交。</p><p><strong>分支管理</strong></p><p>不带任何参数的 <code>git branch</code> 会给出当前所有分支的列表，<code>*</code> 标识当前分支（<code>HEAD</code> 指针指向的分支）。</p><p><code>git branch -v</code> 命令可以查看每一个分支的最后一次提交。</p><p><code>git branch --merge</code> 查看哪些分支已经合并到当前分支，<code>git branch --no-merged</code> 查看尚未合并到当前分支的分支。</p><p><strong>远程分支</strong></p><p>远程跟踪分支以 <code>&lt;remote&gt;/&lt;branch&gt;</code> 的形式命名。正如前文所说，clone 的远程服务器默认取名为 <code>origin</code>。</p><p>假设你在本地的 <code>master</code> 分支上做了一些工作，而其他人 push 到 <code>origin</code> 并更新了它的 <code>master</code> 分支。<strong>但只要你保持不与 <code>origin</code> 服务器连接（并拉取数据），你的 <code>origin/master</code> 指针就不会移动。</strong></p><p><img src="https://git-scm.com/book/en/v2/images/remote-branches-2.png" width=70% /></p><p><code>git fetch &lt;remote&gt;</code> 命令抓取 remote 服务器上本地没有的数据并更新本地数据库，移动 <code>origin/master</code> 指针到更新之后的位置：</p><p><img src="https://git-scm.com/book/en/v2/images/remote-branches-3.png" width=70% /></p><p><strong>推送</strong></p><p><code>git push &lt;remote&gt; &lt;branch&gt;</code></p><p><code>git push &lt;remote&gt; &lt;localbranchname&gt;:&lt;remotebranchname&gt;</code></p><p><strong>删除远程分支</strong></p><p><code>git push &lt;remote&gt; --delete &lt;branch&gt;</code></p><p><strong>变基</strong></p><p>在 Git 中整合来自不同分支的修改主要有两种方法：<code>merge</code> 以及 <code>rebase</code>。</p><p>假设有这样的分支历史：</p><p><img src="https://git-scm.com/book/en/v2/images/basic-rebase-1.png" width=60% /></p><p>变基的操作是：首先 <code>checkout</code> 到 <code>experiment</code> 分支，然后 <code>git rebase master</code>。</p><p>它的原理是首先找到这两个分支（即当前分支 <code>experiment</code>、变基操作的目标基底分支 <code>master</code>） 的最近共同祖先 <code>C2</code>，然后对比当前分支相对于该祖先的历次提交，提取相应的修改并存为临时文件， 然后将当前分支指向目标基底 <code>C3</code>，最后以此将之前另存为临时文件的修改依序应用。</p><p><img src="https://git-scm.com/book/en/v2/images/basic-rebase-3.png" width=70% /></p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>技术栈</category>
      
      <category>Git</category>
      
    </categories>
    
    
    <tags>
      
      <tag>git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>A Survey Report on Image Inpainting</title>
    <link href="/blog-main/2021/12/24/A-Survey-Report-on-Image-Inpainting/"/>
    <url>/blog-main/2021/12/24/A-Survey-Report-on-Image-Inpainting/</url>
    
    <content type="html"><![CDATA[<object data="./GAN-based Image Inpainting.pdf" type="application/pdf" width="100%" height="1000px"></object>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>计算机视觉</category>
      
      <category>图像填充</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>操作系统折腾记（三）</title>
    <link href="/blog-main/2021/12/10/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%8A%98%E8%85%BE%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <url>/blog-main/2021/12/10/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%8A%98%E8%85%BE%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="wait">wait</h2><blockquote><p>https://www.ibm.com/docs/en/ztpf/2020?topic=apis-waitwait-status-information-from-child-process</p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;sys/wait.h&gt;</span></span><br><span class="hljs-type">int</span> <span class="hljs-title function_">wait</span><span class="hljs-params">(<span class="hljs-type">int</span> *stat_loc)</span>;<br></code></pre></td></tr></table></figure><p>等待任一子进程退出，如果正常获取到了子进程的退出信息（存入 <code>stat_loc</code> 指向的地址），返回该子进程的 pid；否则立即返回 -1 并设置 <code>errno</code>（据实践，不会更改 <code>*stat_loc</code> 原本的值，「立即返回」应该就是这个意思）。</p><p>注意，<code>wait</code> 的返回值只说明了它本身有没有被正常地执行，并不能说明子进程是怎样退出的。无论子进程是怎么挂掉的，只要 <code>wait</code> 成功地搞到了子进程挂掉的信息（<code>stat_loc</code>），就返回 <code>pid</code> (&gt;0)；什么时候搞不到呢，一是调用 <code>wait</code> 的进程本来就没有子进程，二是 <code>wait</code> 被信号中断了。</p><p>那么子进程有哪些退出状态呢？CSAPP 中说到，进程会因为三种原因终止：</p><ol type="1"><li>收到一个信号</li><li>从主程序返回</li><li>调用 <code>exit</code> 函数：有一个 exit code，如 <code>exit(0)</code>、<code>exit(-1)</code></li></ol><p>那我们怎么知道子进程是因为什么原因而退出的呢？看 <code>stat_loc</code> 呗！怎么个看法呢？见下文 <code>WIFEXITED &amp; WEXITSTATUS</code> 一节。</p><h2 id="waitpid">waitpid</h2><blockquote><p>https://www.ibm.com/docs/en/ztpf/2020?topic=apis-waitpidobtain-status-information-from-child-process</p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;sys/wait.h&gt;</span></span><br><span class="hljs-type">pid_t</span> <span class="hljs-title function_">waitpid</span><span class="hljs-params">(<span class="hljs-type">pid_t</span> pid, <span class="hljs-type">int</span> *stat_loc, <span class="hljs-type">int</span> options)</span>;<br></code></pre></td></tr></table></figure><p>类似于 <code>wait</code>，但功能更强大了。首先，<code>pid</code> 参数有以下两种选择：</p><ul><li><code>pid&gt;0</code>：等待集合就是 ID 为 <code>pid</code> 的子进程</li><li><code>pid=-1</code>：等待集合是所有 子进程</li></ul><p>其次，<code>options</code> 选项提供了 <code>waitpid</code> 行为的选择：</p><ul><li>默认为 <code>0</code>，则 <code>waitpid</code> 会<strong>挂起</strong>调用它的进程，直到等待集合中的某一个子进程终止；当然，如果本来就有终止的子进程，那么就立即返回了</li><li><code>WNOHANG</code>：如果等待集合中没有已经终止的子进程，就立即返回而不挂起等待</li><li>……</li></ul><h2 id="wifexited-wexitstatus">WIFEXITED &amp; WEXITSTATUS</h2><blockquote><p>https://www.ibm.com/docs/en/ztpf/2020?topic=zca-wifexitedquery-status-see-if-child-process-ended-normally</p><p>https://www.ibm.com/docs/en/ztpf/2020?topic=apis-wexitstatusobtain-exit-status-child-process</p><p>https://www.ibm.com/docs/en/ztpf/2020?topic=zca-wifsignaledquery-status-see-if-child-process-ended-abnormally</p><p>https://www.ibm.com/docs/en/ztpf/2020?topic=zca-wtermsig-determine-which-signal-caused-child-process-exit#cpp_wtermsig</p><p>https://blog.csdn.net/duyuguihua/article/details/38986197</p><p>https://blog.csdn.net/y396397735/article/details/53769865</p><p>https://stackoverflow.com/questions/47441871/why-should-we-check-wifexited-after-wait-in-order-to-kill-child-processes-in-lin</p></blockquote><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;sys/wait.h&gt;</span></span><br><span class="hljs-type">int</span> <span class="hljs-title function_">WIFEXITED</span><span class="hljs-params">(<span class="hljs-type">int</span> status)</span>;<br><span class="hljs-type">int</span> <span class="hljs-title function_">WEXITSTATUS</span><span class="hljs-params">(<span class="hljs-type">int</span> status)</span>;<br><span class="hljs-type">int</span> <span class="hljs-title function_">WIFSIGNALED</span><span class="hljs-params">(<span class="hljs-type">int</span> status)</span>;<br><span class="hljs-type">int</span> <span class="hljs-title function_">WTERMSIG</span><span class="hljs-params">(<span class="hljs-type">int</span> status)</span>;<br><span class="hljs-comment">// The status field that was filled in by the wait or waitpid function.</span><br></code></pre></td></tr></table></figure><p>我们知道，父进程调用 <code>wait</code> 或者 <code>waitpid</code> 时需要传入一个 <code>&amp;status</code>，意即获取子进程的退出状态。大多数情况下，我们并不关心子进程怎么挂掉的，所以经常这么写：<code>wait(NULL)</code> 或者 <code>wait(0)</code>。</p><p>但有的时候我们就是想看看 <code>status</code>。<code>status</code> 能指出子进程是怎样退出的、如果是 exit 的话返回值是啥、如果是被信号结束的话是被谁结束的……这些信息被编码进了一个 <code>int</code> 的二进制位中。但是每次找某个二进制位也太麻烦了吧，所以为了方便，人们定义了一套<strong>宏</strong>，用来解析 <code>status</code> 的信息：</p><ul><li><p><code>WIFEXITED(status)</code></p><p>如果子进程是调用 exit 退出的，返回 TRUE，接下来我们需要用 <code>WEXITSTATUS</code> 获取子进程的 exit code；否则返回 FALSE。</p></li><li><p><code>WEXITSTATUS(status)</code></p><p>返回子进程的 exit code，当然前提是 <code>WIFEXITED(status)</code> 是 TRUE，否则返回值没有任何意义。</p></li><li><p><code>WIFSIGNALED(status)</code></p><p>如果子进程是因为信号而退出的，返回 TRUE，接下来我们需要用 <code>WTERMSIG</code> 获取究竟是什么信号让它退出的；否则返回 FALSE。</p></li><li><p><code>WTERMSIG(status)</code></p><p>返回让子进程退出的信号对应的数值（什么信号对应什么数值在 <code>sys/signal.h</code> 中有定义），前提是 <code>WIFSIGNALED(status)</code> 是 TRUE，否则返回值没有任何意义。</p></li><li><p>……</p></li></ul><p>（注：C 语言没有 <code>bool</code> 类型，也没有 <code>true</code> 和 <code>false</code>，上文中的 TRUE 泛指非零值，FALSE 指 0 值）</p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>操作系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[xv6-mit-6.S081-2020]Lab10: mmap</title>
    <link href="/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab10-mmap/"/>
    <url>/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab10-mmap/</url>
    
    <content type="html"><![CDATA[<h1 id="lab-mmap">Lab: mmap</h1><p>https://pdos.csail.mit.edu/6.S081/2020/labs/mmap.html</p><p>代码：https://github.com/xyfJASON/xv6-mit-6.S081-2020/tree/mmap</p><span id="more"></span><p><br></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span> *<span class="hljs-title function_">mmap</span><span class="hljs-params">(<span class="hljs-type">void</span> *addr, <span class="hljs-type">size_t</span> length, <span class="hljs-type">int</span> prot, <span class="hljs-type">int</span> flags, <span class="hljs-type">int</span> fd, <span class="hljs-type">off_t</span> offset)</span>;<br><span class="hljs-type">int</span> <span class="hljs-title function_">munmap</span><span class="hljs-params">(<span class="hljs-type">void</span> *addr, <span class="hljs-type">size_t</span> len)</span>;<br></code></pre></td></tr></table></figure><p>本次实验只需实现简化版的 <code>mmap</code> 和 <code>munmap</code>：</p><ul><li>保证 <code>addr</code> 和 <code>offset</code> 都为 0，也即：在虚拟空间中找足够大的连续页面，将 <code>fd</code> 对应文件的前 <code>length</code> 字节存储在里面。</li><li><code>flags</code> 只需考虑 <code>MAP_SHARED</code> 和 <code>MAP_PRIVATE</code>；<code>prot</code> 只需考虑 <code>PROT_WRITE</code> 和 <code>PROT_READ</code>。</li><li>允许不同进程把同一个 <code>MAP_SHARED</code> 的文件映射到不同的物理页面上。</li><li><code>munmap</code> 释放的范围保证是一段 mmap 了的空间的开头一段、或结尾一段、或全部，不会从中间挖一个洞出来。</li></ul><p>即便做了上述简化，这次实验还是具有相当的难度，好在 mit 也给了很多提示。</p><p><br></p><p>首先设计一个数据结构 VMA，记录 mmap 分配的虚拟内存空间，包括地址、长度、权限等等信息：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">VMA</span> &#123;</span><br>  <span class="hljs-type">int</span> shared;<br>  <span class="hljs-type">int</span> perm;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">file</span> *<span class="hljs-title">filept</span>;</span><br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">VMA</span> *<span class="hljs-title">nxt</span>;</span><br>  <span class="hljs-type">int</span> valid;<br>  uint64 vmstart, vmend;<br>&#125;;<br></code></pre></td></tr></table></figure><p>这里，我并没有直接记录 <code>mmap</code> 输入的变量，而是做了一些转换方便后续操作：</p><ul><li><code>shared</code> 为 1 表示 <code>MAP_SHARED</code>，为 0 表示 <code>MAP_PRIVATE</code></li><li><code>perm</code> 指示 pte 的权限，即 <code>PTE_W</code> 或 <code>PTE_R</code> 或二者都有，根据 <code>prot</code> 设置</li><li><code>filept</code> 是指向文件的指针，指向当前进程打开的描述符为 <code>fd</code> 的文件</li><li><code>valid</code> 指示该 VMA 是否正在被某进程使用中</li><li><code>vmstart</code> 和 <code>vmend</code> 是根据 <code>addr</code> 和 <code>length</code> 计算出来的<strong>与一页对齐</strong>的 vma 地址范围，因为我们分配内存空间一定是按页分配的</li><li>每一个进程的 PCB 中记录它用到的所有 VMA——我用了一个链表，所以有一个 <code>nxt</code> 指针字段</li></ul><p>开一个 VMA 全局数组（16 个即可），当某进程执行 mmap 系统调用时就从该数组中找一个 valid 无效的 VMA 分配给该进程：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">struct</span> &#123;</span><br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">spinlock</span> <span class="hljs-title">lock</span>;</span><br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">VMA</span> <span class="hljs-title">vmas</span>[<span class="hljs-title">NVMA</span>];</span><br>&#125; vmatable;<br><br><span class="hljs-type">void</span><br><span class="hljs-title function_">vmainit</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  initlock(&amp;vmatable.lock, <span class="hljs-string">&quot;vmatable&quot;</span>);<br>&#125;<br><br><span class="hljs-keyword">struct</span> VMA*<br><span class="hljs-title function_">allocvma</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  acquire(&amp;vmatable.lock);<br>  <span class="hljs-type">int</span> i;<br>  <span class="hljs-keyword">for</span>(i = <span class="hljs-number">0</span>; i &lt; NVMA; i++)&#123;<br>    <span class="hljs-keyword">if</span>(vmatable.vmas[i].valid)<br>      <span class="hljs-keyword">continue</span>;<br>    vmatable.vmas[i].valid = <span class="hljs-number">1</span>;<br>    <span class="hljs-keyword">break</span>;<br>  &#125;<br>  release(&amp;vmatable.lock);<br>  <span class="hljs-keyword">if</span>(i == NVMA) panic(<span class="hljs-string">&quot;allocvma&quot;</span>);<br>  <span class="hljs-keyword">return</span> vmatable.vmas + i;<br>&#125;<br><br><span class="hljs-type">void</span><br><span class="hljs-title function_">deallocvma</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> VMA *vma)</span><br>&#123;<br>  acquire(&amp;vmatable.lock);<br>  vma-&gt;valid = <span class="hljs-number">0</span>;<br>  release(&amp;vmatable.lock);<br>&#125;<br></code></pre></td></tr></table></figure><p>给多个进程分配 vma 显然是需要上锁的操作，我懒得麻烦就直接上全局锁了。</p><p><br></p><p>把 mmap 和 munmap 系统调用的路径打通就不说了，现在考虑 <code>sys_mmap</code> 的实现。首先分配一个 vma，按照上文所述含义填充 vma 的各个字段，注意要增加文件的 reference count，然后把这个 vma 加到进程的 vma 链表里面。<code>mmap</code> 是 lazy allocation 的，所以我们这时候无需 kalloc 物理内存，只需返回起始地址。根据指导网站的描述，我们要「在进程的地址空间中找到一块不用的区域来映射文件」，这句话说的就非常玄学了。我参考了 https://xiayingp.gitbook.io/build_a_os/labs/untitled#define-vma-virtual-memory-area-per-process 的做法，从高地址处向低地址找这样的“不用区域”，为此在每个 PCB 里记录一下已分配区域的最低地址 curmax，下次就从 curmax 继续往下分配。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs c">uint64<br><span class="hljs-title function_">sys_mmap</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  uint64 addr;<br>  <span class="hljs-type">int</span> length, prot, flags, fd, offset;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">proc</span> *<span class="hljs-title">p</span> =</span> myproc();<br><br>  <span class="hljs-keyword">if</span>(argaddr(<span class="hljs-number">0</span>, &amp;addr) &lt; <span class="hljs-number">0</span> ||<br>     argint(<span class="hljs-number">1</span>, &amp;length) &lt; <span class="hljs-number">0</span> ||<br>     argint(<span class="hljs-number">2</span>, &amp;prot) &lt; <span class="hljs-number">0</span> ||<br>     argint(<span class="hljs-number">3</span>, &amp;flags) &lt; <span class="hljs-number">0</span> ||<br>     argint(<span class="hljs-number">4</span>, &amp;fd) &lt; <span class="hljs-number">0</span> ||<br>     argint(<span class="hljs-number">5</span>, &amp;offset) &lt; <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  <span class="hljs-keyword">if</span>(addr != <span class="hljs-number">0</span>) panic(<span class="hljs-string">&quot;sys_mmap&quot;</span>);<br>  <span class="hljs-keyword">if</span>(offset != <span class="hljs-number">0</span>) panic(<span class="hljs-string">&quot;sys_mmap&quot;</span>);<br>  <br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">file</span> *<span class="hljs-title">f</span> =</span> p-&gt;ofile[fd];<br>  <span class="hljs-keyword">if</span>((prot &amp; PROT_READ) &amp;&amp; !f-&gt;readable)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  <span class="hljs-keyword">if</span>((prot &amp; PROT_WRITE) &amp;&amp; (flags &amp; MAP_SHARED) &amp;&amp; !f-&gt;writable)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br><br>  <span class="hljs-comment">// alloc a vma</span><br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">VMA</span> *<span class="hljs-title">vma</span> =</span> allocvma();<br>  <span class="hljs-keyword">if</span>(!vma)  <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  <span class="hljs-comment">// fill in vma</span><br>  vma-&gt;perm = ((prot &amp; PROT_READ) ? PTE_R : <span class="hljs-number">0</span>) |<br>              ((prot &amp; PROT_WRITE) ? PTE_W : <span class="hljs-number">0</span>);<br>  vma-&gt;shared = (flags &amp; MAP_SHARED) ? <span class="hljs-number">1</span> : <span class="hljs-number">0</span>;<br>  vma-&gt;filept = f;<br>  filedup(f);<br>  <span class="hljs-comment">// add vma to process&#x27;s linklist</span><br>  vma-&gt;nxt = p-&gt;vmalist;<br>  p-&gt;vmalist = vma;<br>  <span class="hljs-comment">// find an unused region in the process&#x27;s address space in which to map the file</span><br>  uint64 vmaddr = PGROUNDDOWN(p-&gt;curmax - length);<br>  <span class="hljs-keyword">if</span>(vmaddr % PGSIZE != <span class="hljs-number">0</span>)  panic(<span class="hljs-string">&quot;sys_mmap&quot;</span>);<br>  vma-&gt;vmstart = vmaddr;<br>  vma-&gt;vmend = p-&gt;curmax;<br>  p-&gt;curmax = vmaddr;<br><br>  <span class="hljs-keyword">return</span> vmaddr;<br>&#125;<br></code></pre></td></tr></table></figure><p><br></p><p>如前所述，<code>mmap</code> 是 lazy allocation 的，所以我们在 usertrap 里处理 page fault：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">usertrap</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>  <span class="hljs-keyword">if</span>(r_scause() == <span class="hljs-number">8</span>)&#123;<br>    ...<br>  &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>((which_dev = devintr()) != <span class="hljs-number">0</span>)&#123;<br>    ...<br>  &#125; <span class="hljs-keyword">else</span> &#123;<br>    uint64 cause = r_scause();<br>    <span class="hljs-keyword">if</span>(cause == <span class="hljs-number">13</span> || cause == <span class="hljs-number">15</span>)&#123;<br>      <span class="hljs-comment">// page fault</span><br>      uint64 stval = r_stval();<br>      <span class="hljs-keyword">if</span>(mmaplazy(stval, cause) == <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">goto</span> brk;<br>    &#125;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;usertrap(): unexpected scause %p pid=%d\n&quot;</span>, r_scause(), p-&gt;pid);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;            sepc=%p stval=%p\n&quot;</span>, r_sepc(), r_stval());<br>    p-&gt;killed = <span class="hljs-number">1</span>;<br>  &#125;<br>brk:<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p><code>stval</code> 就是产生 page fault 的地址，我们不需要读入整个文件，只需要申请并映射 <code>stval</code> 所在的那一页并读入这一页对应的文件数据即可：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span><br><span class="hljs-title function_">mmaplazy</span><span class="hljs-params">(uint64 va, uint64 cause)</span><br>&#123;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">proc</span> *<span class="hljs-title">p</span> =</span> myproc();<br>  <span class="hljs-comment">// find the corresponding vma</span><br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">VMA</span> *<span class="hljs-title">vma</span> =</span> <span class="hljs-number">0</span>;<br>  <span class="hljs-keyword">for</span>(vma = p-&gt;vmalist; vma; vma = vma-&gt;nxt)<br>    <span class="hljs-keyword">if</span>(vma-&gt;valid &amp;&amp; vma-&gt;vmstart &lt;= va &amp;&amp; va &lt; vma-&gt;vmend)<br>      <span class="hljs-keyword">break</span>;<br>  <span class="hljs-keyword">if</span>(!vma)  <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  <span class="hljs-keyword">if</span>(cause == <span class="hljs-number">13</span> &amp;&amp; !(vma-&gt;perm &amp; PTE_R)) <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  <span class="hljs-keyword">if</span>(cause == <span class="hljs-number">15</span> &amp;&amp; !(vma-&gt;perm &amp; PTE_W)) <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  <span class="hljs-comment">// allocate physical memory and map pte</span><br>  <span class="hljs-type">char</span> *mem = kalloc();<br>  <span class="hljs-keyword">if</span>(mem == <span class="hljs-number">0</span>)  <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  <span class="hljs-built_in">memset</span>(mem, <span class="hljs-number">0</span>, PGSIZE);<br>  uint64 vaalign = PGROUNDDOWN(va);<br>  <span class="hljs-keyword">if</span>(mappages(p-&gt;pagetable, vaalign, PGSIZE, (uint64)mem, vma-&gt;perm|PTE_U|PTE_X) != <span class="hljs-number">0</span>)&#123;<br>    kfree(mem);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  &#125;<br>  <span class="hljs-comment">// read content of file into allocated memory</span><br>  mmapfileread(vma-&gt;filept, <span class="hljs-number">0</span>, (uint64)mem, vaalign - vma-&gt;vmstart, PGSIZE);<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br><br><span class="hljs-type">int</span> <span class="hljs-title function_">mmapfileread</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> file *f, <span class="hljs-type">int</span> user_dst, uint64 dst, uint off, uint n)</span>&#123; <br>  ilock(f-&gt;ip);<br>  <span class="hljs-type">int</span> ret = readi(f-&gt;ip, user_dst, dst, off, n);<br>  iunlock(f-&gt;ip);<br>  <span class="hljs-keyword">return</span> ret;<br>&#125;<br></code></pre></td></tr></table></figure><p>这里特别说明文件开始读的偏移位置 <code>off</code> 和读的长度 <code>n</code> 是怎么计算的，如图所示：</p><p><img src="1.png" /></p><p>蓝色 <code>va</code> 是发生 pagefault 的地方，因此灰色页就是我们要读的页。绿色线段是文件占的大小，它一定从 <code>vmstart</code> 开始。从图上能直观的看出，我们要读的文件偏移量就是 <code>vaalign-vmstart</code>，又由于读一页，所以 n 就是 <code>PGSIZE</code>。</p><p><br></p><p>munmap 的时候干这么几件事：</p><ol type="1"><li>找到包含释放空间的 vma</li><li>如果 vma 是 shared 的，则把现在的内容写回文件</li><li>解除页表的映射</li><li>分情况更新 vma：如果释放的只是 vma 区域的一部分，则更新 vma 的 <code>vmstart</code> 和 <code>vmend</code>；如果释放了整个 vma 区域，则把它从链表里拿出来、释放（valid 置零）、减少文件 reference count。</li></ol><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs c">uint64<br><span class="hljs-title function_">sys_munmap</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  uint64 addr;<br>  <span class="hljs-type">int</span> length;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">proc</span> *<span class="hljs-title">p</span> =</span> myproc();<br><br>  <span class="hljs-keyword">if</span>(argaddr(<span class="hljs-number">0</span>, &amp;addr) &lt; <span class="hljs-number">0</span> || argint(<span class="hljs-number">1</span>, &amp;length) &lt; <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br><br>  <span class="hljs-comment">// find the VMA for the address range</span><br>  uint64 st = PGROUNDDOWN(addr);<br>  uint64 ed = PGROUNDUP(addr + length);<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">VMA</span> *<span class="hljs-title">vma</span> =</span> <span class="hljs-number">0</span>, *pre = <span class="hljs-number">0</span>;<br>  <span class="hljs-keyword">for</span>(vma = p-&gt;vmalist; vma; vma = vma-&gt;nxt)&#123;<br>    <span class="hljs-keyword">if</span>(vma-&gt;valid &amp;&amp; vma-&gt;vmstart &lt;= st &amp;&amp; ed &lt;= vma-&gt;vmend)<br>      <span class="hljs-keyword">break</span>;<br>    pre = vma;<br>  &#125;<br>  <span class="hljs-keyword">if</span>(!vma)  <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br><br>  <span class="hljs-comment">// write back if needed</span><br>  <span class="hljs-keyword">if</span>(vma-&gt;shared &amp;&amp; (vma-&gt;perm | PTE_W))<br>    mmapfilewrite(vma-&gt;filept, st, ed - st);<br>  <span class="hljs-comment">// unmap specified pages</span><br>  <span class="hljs-keyword">for</span>(uint64 i = st; i &lt; ed; i += PGSIZE)&#123;<br>    <span class="hljs-keyword">if</span>(walkaddr(p-&gt;pagetable, i))&#123;<br>      <span class="hljs-comment">// need to check because vma is allocated lazily</span><br>      uvmunmap(p-&gt;pagetable, i, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>);<br>    &#125;<br>  &#125;<br>  <span class="hljs-comment">// update info in vma</span><br>  <span class="hljs-keyword">if</span>(vma-&gt;vmstart == st &amp;&amp; ed &lt; vma-&gt;vmend)<br>    vma-&gt;vmstart = PGROUNDDOWN(addr + length);<br>  <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(vma-&gt;vmstart &lt; st &amp;&amp; ed == vma-&gt;vmend)<br>    vma-&gt;vmend = PGROUNDUP(addr);<br>  <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(vma-&gt;vmstart == st &amp;&amp; ed == vma-&gt;vmend)&#123;<br>    <span class="hljs-keyword">if</span>(pre == <span class="hljs-number">0</span>)  p-&gt;vmalist = vma-&gt;nxt;<br>    <span class="hljs-keyword">else</span>  pre-&gt;nxt = vma-&gt;nxt, vma-&gt;nxt = <span class="hljs-number">0</span>;<br>    fileclose(vma-&gt;filept);<br>    deallocvma(vma);<br>  &#125;<br>  <span class="hljs-keyword">else</span> ;<br><br>  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>值得注意的是，解除页表映射前一定先 walk 一下，因为我们是 lazy allocation，释放空间里的页面不一定全部都已经建立了映射。</p><p>还有一个点，如果释放区域是 vma 开头或者结尾区域，那怎么调整 <code>vmstart</code> 或 <code>vmend</code> 呢？看图说话：</p><p><img src="2.png" /></p><p>理论上来说，释放了 vma 一部分后，这部分空间可以拿出来再次分配（垃圾回收），或者可以进行内存紧缩。我并没有实现它们，这是可以继续改进的一点。</p><p><br></p><p>最后，在 <code>exit</code> 中释放掉该进程的所有 vma，要干的事和 munmap 差不多：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">exit</span><span class="hljs-params">(<span class="hljs-type">int</span> status)</span><br>&#123;<br>  ...<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">VMA</span> *<span class="hljs-title">vma</span> =</span> <span class="hljs-number">0</span>, *nxtvma = <span class="hljs-number">0</span>;<br>  <span class="hljs-keyword">for</span>(vma = p-&gt;vmalist; vma; vma = nxtvma)&#123;<br>    nxtvma = vma-&gt;nxt;<br>    <span class="hljs-keyword">for</span>(uint64 i = vma-&gt;vmstart; i &lt; vma-&gt;vmend; i += PGSIZE)<br>      <span class="hljs-keyword">if</span>(walkaddr(p-&gt;pagetable, i))<br>        uvmunmap(p-&gt;pagetable, i, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>);<br>    vma-&gt;nxt = <span class="hljs-number">0</span>;<br>    fileclose(vma-&gt;filept);<br>    deallocvma(vma);<br>  &#125;<br>  p-&gt;vmalist = <span class="hljs-number">0</span>;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>以及在 <code>fork</code> 的时候把 vma 给到子进程，要干的事和 mmap 差不多：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span><br><span class="hljs-title function_">fork</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">VMA</span> *<span class="hljs-title">vma</span> =</span> <span class="hljs-number">0</span>, *pre = <span class="hljs-number">0</span>;<br>  np-&gt;vmalist = <span class="hljs-number">0</span>;<br>  <span class="hljs-keyword">for</span>(vma = p-&gt;vmalist; vma; vma = vma-&gt;nxt)&#123;<br>    <span class="hljs-comment">// alloc a vma</span><br>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">VMA</span> *<span class="hljs-title">newvma</span> =</span> allocvma();<br>    <span class="hljs-comment">// fill in newvma</span><br>    newvma-&gt;shared = vma-&gt;shared;<br>    newvma-&gt;perm = vma-&gt;perm;<br>    newvma-&gt;filept = vma-&gt;filept;<br>    newvma-&gt;vmstart = vma-&gt;vmstart;<br>    newvma-&gt;vmend = vma-&gt;vmend;<br>    filedup(vma-&gt;filept);<br>    <span class="hljs-comment">// add newvma to np&#x27;s linklist</span><br>    <span class="hljs-keyword">if</span>(pre == <span class="hljs-number">0</span>)  np-&gt;vmalist = newvma;<br>    <span class="hljs-keyword">else</span>  pre-&gt;nxt = newvma, newvma-&gt;nxt = <span class="hljs-number">0</span>;<br>    pre = newvma;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p><br></p><p>make grade 截图：</p><p><img src="result.png" /></p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>操作系统</category>
      
      <category>xv6-lab</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[xv6-mit-6.S081-2020]Lab9: fs</title>
    <link href="/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab9-fs/"/>
    <url>/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab9-fs/</url>
    
    <content type="html"><![CDATA[<h1 id="lab-file-system">Lab: file system</h1><p>https://pdos.csail.mit.edu/6.S081/2020/labs/fs.html</p><p>代码：https://github.com/xyfJASON/xv6-mit-6.S081-2020/tree/fs</p><span id="more"></span><p><br></p><p>xv6 文件系统的代码其实还是很繁杂的，大概像下面这样：</p><p><img src="fs.png" /></p><p>所以做实验前很容易一头雾水。但是完成这次实验其实用不着把文件系统的实现细节全搞清楚，理解几个关键点（特别是 inode 就行了）。</p><p>inode 是文件系统的核心，看了 mit 网课之后我画了一张图，希望有助于理解：</p><p><img src="inodes.png" /></p><h2 id="large-files">Large files</h2><p>任务：xv6 支持的最大文件大小由 inode 结构体决定——它包含 12 个 direct block number 和 1 个 indirect block number，后者指向的块里面含有 256 个 block number，所以一个文件最大占据 256+12=268 个磁盘块，也就是 268KB。这显然太小了，所以我们希望通过 doubly-indirect block 增大它——在 inode 里面，添加一个 doubly-indirect block（相应地，direct block 减少一个），并更改 bmap() 函数添加新的映射。</p><p>indirect 相当于一级索引，doubly-indirect 相当于二级索引，这个树形结构和多级页表很像。总的来说，这个任务很好理解，实现也不难，体现在代码里基本就是依葫芦画瓢和复制粘贴。</p><p>仿照 bmap() 中一级索引的查找方式，很容易写出二级索引的查找代码，只需注意 <code>bn / NINDIRECT</code> 是第一层的编号，<code>bn % NINDIRECT</code> 是第二层的编号：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">static</span> uint<br><span class="hljs-title function_">bmap</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> inode *ip, uint bn)</span><br>&#123;<br>  uint addr, *a;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">buf</span> *<span class="hljs-title">bp</span>;</span><br><br>  <span class="hljs-keyword">if</span>(bn &lt; NDIRECT)&#123;<br>      ...<br>  &#125;<br>  bn -= NDIRECT;<br><br>  <span class="hljs-keyword">if</span>(bn &lt; NINDIRECT)&#123;<br>      ...<br>  &#125;<br><br>  bn -= NINDIRECT;<br>  <span class="hljs-keyword">if</span>(bn &lt; NINDIRECT2)&#123;<br>    <span class="hljs-comment">// load doubly-indirect block, allocating if necessary.</span><br>    <span class="hljs-keyword">if</span>((addr = ip-&gt;addrs[NDIRECT+<span class="hljs-number">1</span>]) == <span class="hljs-number">0</span>)<br>      ip-&gt;addrs[NDIRECT+<span class="hljs-number">1</span>] = addr = balloc(ip-&gt;dev);<br>    bp = bread(ip-&gt;dev, addr);<br>    a = (uint*)bp-&gt;data;<br>    <span class="hljs-keyword">if</span>((addr = a[bn / NINDIRECT]) == <span class="hljs-number">0</span>)&#123;<br>      a[bn / NINDIRECT] = addr = balloc(ip-&gt;dev);<br>      log_write(bp);<br>    &#125;<br>    brelse(bp);<br>    bp = bread(ip-&gt;dev, addr);<br>    a = (uint*)bp-&gt;data;<br>    <span class="hljs-keyword">if</span>((addr = a[bn % NINDIRECT]) == <span class="hljs-number">0</span>)&#123;<br>      a[bn % NINDIRECT] = addr = balloc(ip-&gt;dev);<br>      log_write(bp);<br>    &#125;<br>    brelse(bp);<br>    <span class="hljs-keyword">return</span> addr;<br>  &#125;<br><br>  panic(<span class="hljs-string">&quot;bmap: out of range&quot;</span>);<br>&#125;<br></code></pre></td></tr></table></figure><p>另外还要更改 itrunc() 函数来释放所有块：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">itrunc</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> inode *ip)</span><br>&#123;<br>  ...<br><br>  <span class="hljs-keyword">if</span>(ip-&gt;addrs[NDIRECT])&#123;<br>      ...<br>  &#125;<br><br>  <span class="hljs-keyword">if</span>(ip-&gt;addrs[NDIRECT+<span class="hljs-number">1</span>])&#123;<br>    bp = bread(ip-&gt;dev, ip-&gt;addrs[NDIRECT+<span class="hljs-number">1</span>]);<br>    a = (uint*)bp-&gt;data;<br>    <span class="hljs-keyword">for</span>(j = <span class="hljs-number">0</span>; j &lt; NINDIRECT; j++)&#123;<br>      <span class="hljs-keyword">if</span>(a[j])&#123;<br>        <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">buf</span> *<span class="hljs-title">bp2</span> =</span> bread(ip-&gt;dev, a[j]);<br>        uint *a2 = (uint*)bp2-&gt;data;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> k = <span class="hljs-number">0</span>; k &lt; NINDIRECT; k++)&#123;<br>          <span class="hljs-keyword">if</span>(a2[k])<br>            bfree(ip-&gt;dev, a2[k]);<br>        &#125;<br>        brelse(bp2);<br>        bfree(ip-&gt;dev, a[j]);<br>        a[j] = <span class="hljs-number">0</span>;<br>      &#125;<br>    &#125;<br>    brelse(bp);<br>    bfree(ip-&gt;dev, ip-&gt;addrs[NDIRECT+<span class="hljs-number">1</span>]);<br>    ip-&gt;addrs[NDIRECT+<span class="hljs-number">1</span>] = <span class="hljs-number">0</span>;<br>  &#125;<br>  ...<br></code></pre></td></tr></table></figure><h2 id="symbolic-links">Symbolic links</h2><p>任务：实现 <code>symlink(char *target, char *path)</code> 系统调用，它在 <code>path</code> 处创建一个到文件名为 <code>target</code> 的软链接。</p><p>软链接基本可以理解为 Windows 下的快捷方式，它本身也是一个文件，但是指向另一个文件。当打开一个软链接文件时，如果打开模式没有设置 <code>O_NOFOLLOW</code>，就会打开链接到的文件——如果链接到的文件也是一个软链接，则继续往下找。</p><p>这次任务的核心在于 <code>sys_symlink（）</code>——也就是 <code>symlink</code> 系统调用的底层实现，和对 <code>sys_open（</code> 的改动，至于那些打通系统调用、添加标志位之类的代码就不细说了。</p><p>虽然 xv6 文件系统的代码非常繁杂，但是它还是提供了一些友好的接口——比如 <code>create()</code>。<code>create()</code> 函数定义在 <code>kernel/sysfile.c</code> 里，而不是作为文件系统的基本功能定义在 <code>kernel/fs.c</code> 里，从这一点就可以看出它的定位就是方便我们写代码。<strong>一个小坑是 <code>create()</code> 返回时是上了锁的，所以不要重复上锁。</strong>使用 <code>create()</code> 创建位于 <code>path</code> 的 symlink 文件之后，我们把 <code>target</code> 存到文件的数据块里——因为对于软链接文件来说，<code>target</code> 就是它的「数据」。写数据用 <code>writei()</code> 函数，接口也非常友好。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs c">uint64<br><span class="hljs-title function_">sys_symlink</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  <span class="hljs-type">char</span> target[MAXPATH], path[MAXPATH];<br>  <span class="hljs-keyword">if</span>(argstr(<span class="hljs-number">0</span>, target, MAXPATH) &lt; <span class="hljs-number">0</span> || argstr(<span class="hljs-number">1</span>, path, MAXPATH) &lt; <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br><br>  begin_op();<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">inode</span> *<span class="hljs-title">ip</span>;</span><br>  <span class="hljs-keyword">if</span>((ip = create(path, T_SYMLINK, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)) == <span class="hljs-number">0</span>)&#123;<br>    end_op();<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  &#125;<br>  <span class="hljs-keyword">if</span>(writei(ip, <span class="hljs-number">0</span>, (uint64)target, <span class="hljs-number">0</span>, MAXPATH) != MAXPATH)&#123;<br>    end_op();<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  &#125;<br>  iupdate(ip);<br>  iunlockput(ip);<br>  end_op();<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>然后改 <code>sys_open()</code>，当取到的文件时软链接并且没有设置 <code>O_NOFOLLOW</code> 时，就沿着链接一直往下找。和 <code>writei()</code> 对应的，用 <code>readi()</code> 读取文件的数据块即可得到链接地址。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs c">uint64<br><span class="hljs-title function_">sys_open</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>  <span class="hljs-type">int</span> cnt = <span class="hljs-number">0</span>;<br>  <span class="hljs-keyword">while</span>(ip-&gt;type == T_SYMLINK &amp;&amp; !(omode &amp; O_NOFOLLOW))&#123;<br>    <span class="hljs-keyword">if</span>((n = readi(ip, <span class="hljs-number">0</span>, (uint64)path, <span class="hljs-number">0</span>, MAXPATH)) != MAXPATH)&#123;<br>      iunlockput(ip);<br>      end_op();<br>      <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>    &#125;<br>    iunlockput(ip);<br>    <span class="hljs-keyword">if</span>((ip = namei(path)) == <span class="hljs-number">0</span> || ++cnt &gt; <span class="hljs-number">10</span>)&#123;<br>      end_op();<br>      <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>    &#125;<br>    ilock(ip);<br>  &#125;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p><br></p><p>make grade 截图：</p><p><img src="result.png" /></p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>操作系统</category>
      
      <category>xv6-lab</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[xv6-mit-6.S081-2020]Lab8: lock</title>
    <link href="/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab8-lock/"/>
    <url>/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab8-lock/</url>
    
    <content type="html"><![CDATA[<h1 id="lab-locks">Lab: locks</h1><p>https://pdos.csail.mit.edu/6.S081/2020/labs/lock.html</p><p>代码：https://github.com/xyfJASON/xv6-mit-6.S081-2020/tree/lock</p><span id="more"></span><p><br></p><p>这次实验中我们将重新设计 xv6 的<strong>内存分配</strong>和<strong>磁盘块缓存</strong>机制以提高它们的并行性。并行性的性能可以由锁争用的次数反应出来——差的并行代码会导致高锁争用。</p><h2 id="memory-allocator">Memory allocator</h2><p>任务：原本的 kalloc.c 导致高锁争用的原因是 xv6 只维护了一个空闲页面链表，该链表有一个锁。为了减少锁争用，我们可以给每一个 CPU 维护一个空闲页面链表，这样不同 CPU 就可以并行地内存分配和释放，因为它们之间相互独立。但是，当一个 CPU 的空闲页面被分配完之后，它需要从其他的 CPU 的空闲页面链表中<strong>窃取</strong>一部分空闲页面。窃取过程可能导致锁争用，但是不会很频繁。</p><p>我们的任务就是实现每一个 CPU 一个空闲链表，且在链表为空时窃取页面。所有锁的名字必须以 kmem 开头。kalloctest 检测是否减少了锁争用，usertests sbrkmuch 检测是否仍然能够分配所有内存。</p><p>首先把原来的一个 kmem 结构体改成一个数组 kmems，使每个 CPU 有一个对应的空闲页面链表：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">kmem</span>&#123;</span><br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">spinlock</span> <span class="hljs-title">lock</span>;</span><br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">run</span> *<span class="hljs-title">freelist</span>;</span><br>&#125; kmems[NCPU];<br></code></pre></td></tr></table></figure><p>然后每个 CPU 分别初始化：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">kinit</span><span class="hljs-params">()</span><br>&#123;<br>  <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; NCPU; i++)&#123;<br>    initlock(&amp;kmems[i].lock, <span class="hljs-string">&quot;kmem&quot;</span>);<br>    freerange(i == <span class="hljs-number">0</span> ? end : (<span class="hljs-type">void</span>*)PHYSTOP, (<span class="hljs-type">void</span>*)PHYSTOP);<br>  &#125;<br>  <span class="hljs-comment">/*initlock(&amp;kmem.lock, &quot;kmem&quot;);</span><br><span class="hljs-comment">  freerange(end, (void*)PHYSTOP);*/</span><br>&#125;<br></code></pre></td></tr></table></figure><p>这里我偷了个懒，先给 0 号 CPU 分配所有内存，其他 CPU 不分配。由于一个 CPU 没有空闲页面时会去其他 CPU 处窃取，所以我希望足够长时间之后，页面能够大致平均分配开（这其实是一个数学问题……有时间可以研究一下……）。</p><p>kfree 把释放的页面放进<strong>当前 CPU</strong> 的空闲页面链表。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">kfree</span><span class="hljs-params">(<span class="hljs-type">void</span> *pa)</span><br>&#123;<br>  ...<br>  r = (<span class="hljs-keyword">struct</span> run*)pa;<br>  push_off(); <span class="hljs-type">int</span> ci = cpuid(); pop_off();<br>  acquire(&amp;kmems[ci].lock);<br>  r-&gt;next = kmems[ci].freelist;<br>  kmems[ci].freelist = r;<br>  release(&amp;kmems[ci].lock);<br>&#125;<br></code></pre></td></tr></table></figure><p>kalloc 从当前 CPU 空闲页面链表中取一个页面，如果不存在则窃取（steal）：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span> *<br><span class="hljs-title function_">kalloc</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">run</span> *<span class="hljs-title">r</span>;</span><br>  ...<br>  push_off(); <span class="hljs-type">int</span> ci = cpuid(); pop_off();<br>  acquire(&amp;kmems[ci].lock);<br>  r = kmems[ci].freelist;<br>  <span class="hljs-keyword">if</span>(r)<br>    kmems[ci].freelist = r-&gt;next;<br>  <span class="hljs-keyword">else</span> <span class="hljs-comment">// Steal from other CPU when freelist is empty.</span><br>    r = steal(ci);<br>  release(&amp;kmems[ci].lock);<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>窃取我实现的很暴力，逐一检查各个 CPU 是否有空闲页面，有就窃取过来：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-keyword">struct</span> run *<br><span class="hljs-title function_">steal</span><span class="hljs-params">(<span class="hljs-type">int</span> ci)</span><br>&#123;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">run</span> *<span class="hljs-title">r</span> =</span> <span class="hljs-number">0</span>;<br>  <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; NCPU; i++)&#123;<br>    <span class="hljs-keyword">if</span>(i == ci) <span class="hljs-keyword">continue</span>;<br>    acquire(&amp;kmems[i].lock);<br>    <span class="hljs-keyword">if</span>((r = kmems[i].freelist))&#123;<br>      kmems[i].freelist = r-&gt;next;<br>      release(&amp;kmems[i].lock);<br>      <span class="hljs-keyword">break</span>;<br>    &#125;<br>    release(&amp;kmems[i].lock);<br>  &#125;<br>  <span class="hljs-keyword">return</span> r;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="buffer-cache">Buffer cache</h2><p>xv6 在 bio.c 中实现了磁盘块的缓存机制，它是一个双向链表，每个元素是一个缓存块。一个缓存块（struct buf, kernel/buf.h）不仅包含数据，还包含有效位 valid、脏位 disk、设备号、磁盘块号、被引用次数等信息。<strong>整个双向链表由一个自旋锁保护，每个缓存块都由一个睡眠锁保护。</strong></p><p>任务：由于整个缓存双向链表由一个自旋锁保护，所以多个进程反复读不同文件时会产生高锁争用。我们要更改缓存机制以减少锁争用。</p><p>我选择的方法是 Hash：原本只有一个双向链表维护所有缓存块，现在我开 NBUCKETS 个桶，每个桶里面维护一个双向链表。一个块应该去哪个桶里找由 Hash 确定。</p><p>Hash 函数是最简单的直接取模：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">define</span> NBUCKETS 13</span><br><span class="hljs-keyword">inline</span> uint <span class="hljs-title function_">myhash</span><span class="hljs-params">(uint blockno)</span>&#123;<br>  <span class="hljs-keyword">return</span> blockno % NBUCKETS;<br>&#125;<br></code></pre></td></tr></table></figure><p><br></p><p>然后修改 binit 给每个桶初始化：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">binit</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">buf</span> *<span class="hljs-title">b</span>;</span><br>  <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; NBUCKETS; i++)&#123;<br>    initlock(&amp;bcache.lock[i], <span class="hljs-string">&quot;bcache&quot;</span>);<br>    <span class="hljs-comment">// Create linked list of buffers</span><br>    bcache.head[i].prev = &amp;bcache.head[i];<br>    bcache.head[i].next = &amp;bcache.head[i];<br>  &#125;<br>  <span class="hljs-keyword">for</span>(b = bcache.buf; b &lt; bcache.buf+NBUF; b++)&#123;<br>    b-&gt;next = bcache.head[<span class="hljs-number">0</span>].next;<br>    b-&gt;prev = &amp;bcache.head[<span class="hljs-number">0</span>];<br>    initsleeplock(&amp;b-&gt;lock, <span class="hljs-string">&quot;buffer&quot;</span>);<br>    bcache.head[<span class="hljs-number">0</span>].next-&gt;prev = b;<br>    bcache.head[<span class="hljs-number">0</span>].next = b;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>和 kinit 一样我偷了个懒，先把所有缓存块给 0 号桶，因为当一个桶缓存块不够用时会去其他桶窃取。</p><p><br></p><p>接下来修改 bget。原本的 bget 在双向链表中<strong>扫描两次</strong>，第一次<strong>从前往后</strong>找是否命中，如果命中则返回命中的缓存块；如果没命中，则第二次<strong>从后往前</strong>找一块未被映射的缓存块（refcnt = 0），加上映射之后返回之。这样扫描实现了 LRU 机制。</p><p>为了修改，我们除了把上述双向链表改成当前桶的双向链表以外，还需处理一种情况：如果当前桶没有空闲的缓存块，那么去其他桶窃取一块。为了方便，我把「从某桶取出某未映射缓存」写作如下函数：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">// Extract and return available cache block from bucket b.</span><br><span class="hljs-keyword">struct</span> buf *<br><span class="hljs-title function_">bfind</span><span class="hljs-params">(<span class="hljs-type">int</span> i, <span class="hljs-type">int</span> needlock)</span>&#123;<br>  <span class="hljs-keyword">if</span>(needlock)<br>    acquire(&amp;bcache.lock[i]);<br>  <span class="hljs-keyword">for</span>(<span class="hljs-keyword">struct</span> buf *b = bcache.head[i].prev; b != &amp;bcache.head[i]; b = b-&gt;prev)&#123;<br>    <span class="hljs-keyword">if</span>(b-&gt;refcnt == <span class="hljs-number">0</span>)&#123;<br>      b-&gt;prev-&gt;next = b-&gt;next;<br>      b-&gt;next-&gt;prev = b-&gt;prev;<br>      b-&gt;prev = b-&gt;next = <span class="hljs-number">0</span>;<br>      <span class="hljs-keyword">if</span>(needlock)<br>        release(&amp;bcache.lock[i]);<br>      <span class="hljs-keyword">return</span> b;<br>    &#125;<br>  &#125;<br>  <span class="hljs-keyword">if</span>(needlock)<br>    release(&amp;bcache.lock[i]);<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>在 bget 中，先对当前桶执行 bfind，如果没找到则窃取——遍历其他桶执行 bfind，把返回的缓存块插入当前桶的双向链表。有一个坑是：如果 bfind 的是当前桶，小心不要重复上锁。</p><p>然后本实验最难的点来了：<strong>窃取过程可能发生死锁</strong>。</p><p>假设每个桶在窃取别人时都是从 0 号桶开始遍历，那么考虑如下情况：</p><p><img src="dead.png" /></p><p>0 号桶和 1 号桶都先执行 bget，把自己锁住；然后 0 号桶找 1 号桶要空闲块，于是申请 1 号桶的锁；1 号桶找 0 号桶要空闲块，于是申请 0 号桶的锁——这样就死锁了。但这时系统的 2 号桶可能有空闲块是可以给出来的。</p><p>要解决这个问题，我和同学经过了漫长的讨论，目前有两种方案。但是，<strong>panic 或者死锁很难很难很难很难复现，所以我也没法保证下述策略一定正确……</strong></p><h3 id="方案一更改遍历顺序">方案一：更改遍历顺序</h3><p>i 号桶在窃取时从 i+1 开始向后遍历，直到绕一圈回来。其实这样做也可能死锁，但是死锁的唯一情形是——当前没有任何桶有空闲块，这样的情形下 xv6 原本的策略是 panic，不比死锁好哪儿去。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">static</span> <span class="hljs-keyword">struct</span> buf*<br><span class="hljs-title function_">bget</span><span class="hljs-params">(uint dev, uint blockno)</span><br>&#123;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">buf</span> *<span class="hljs-title">b</span>;</span><br><br>  uint hashid = myhash(blockno);<br>  acquire(&amp;bcache.lock[hashid]);<br><br>  <span class="hljs-comment">// Is the block already cached?</span><br>  <span class="hljs-keyword">for</span>(b = bcache.head[hashid].next; b != &amp;bcache.head[hashid]; b = b-&gt;next)&#123;<br>    <span class="hljs-keyword">if</span>(b-&gt;dev == dev &amp;&amp; b-&gt;blockno == blockno)&#123;<br>      b-&gt;refcnt++;<br>      release(&amp;bcache.lock[hashid]);<br>      acquiresleep(&amp;b-&gt;lock);<br>      <span class="hljs-keyword">return</span> b;<br>    &#125;<br>  &#125;<br><br>  <span class="hljs-comment">// Not cached.</span><br>  <span class="hljs-comment">// Recycle the least recently used (LRU) unused buffer.</span><br>  b = bfind(hashid, <span class="hljs-number">0</span>);<br>  <span class="hljs-keyword">if</span>(!b)<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = (hashid + <span class="hljs-number">1</span>) % NBUCKETS; i != hashid; i = (i + <span class="hljs-number">1</span>) % NBUCKETS)<br>      <span class="hljs-keyword">if</span>((b = bfind(i, <span class="hljs-number">1</span>)))  <span class="hljs-keyword">break</span>;<br>  <span class="hljs-keyword">if</span>(b)&#123;<br>    b-&gt;next = bcache.head[hashid].next;<br>    b-&gt;prev = &amp;bcache.head[hashid];<br>    bcache.head[hashid].next-&gt;prev = b;<br>    bcache.head[hashid].next = b;<br><br>    b-&gt;dev = dev;<br>    b-&gt;blockno = blockno;<br>    b-&gt;valid = <span class="hljs-number">0</span>;<br>    b-&gt;refcnt = <span class="hljs-number">1</span>;<br>    release(&amp;bcache.lock[hashid]);<br>    acquiresleep(&amp;b-&gt;lock);<br>    <span class="hljs-keyword">return</span> b;<br>  &#125;<br>  panic(<span class="hljs-string">&quot;bget: no buffers&quot;</span>);<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="方案二加全局锁">方案二：加全局锁</h3><p>仔细思考死锁什么时候会发生：当我窃取别人时，别人也在窃取我。但是，我去窃取别人这个动作本身就说明了我自己都一贫如洗不能自给自足，别人还来窃取我，那必然是徒劳无功的。所以，如果我现在放开自己的锁，打开家门让别人来自己的桶里逛一圈，别人也拿不走什么东西。所以我们有了初步的解决方案：在窃取别人前释放自己的锁。</p><p>然而，当我们兴致勃勃地写完代码运行测试的时候，就会发现 <code>panic freeing free block</code>。</p><p>panic 的原因是我们没有保证 bget 的原子性，使得两个进程将同一个磁盘块做了两次缓存——在进程 1 释放锁之后，进程 2 也去访问同一个磁盘块，于是两个进程各自偷了一块缓存块，都对磁盘块做了缓存。由于这样很容易导致两个缓存不一致，因此是不被允许的。</p><p>要解决（缓解）这个问题，考虑引入全局锁——即对整个 bcache 上锁。假若一个进程在释放某桶的锁之后，能立即加上全局锁，就可以避免另一个进程来找同一个磁盘块。</p><p>但毕竟我们没法保证释放桶锁和加全局锁这两个操作是原子的，如果有个进程好巧不巧（概率很小）在这两个操作之间来查找同一个磁盘块，仍然会 panic。所以这种做法能够极大的缓解 panic 问题，但并没有根本上解决它。</p><p>针对重复缓存的问题，我想了个办法：在窃取到缓存块之后，先在桶里再次查找一番是否有已映射的缓存块，如果有，就不映射窃取来的缓存块了。好比我出门走了一趟，回来发现已经有人往我家里放了我要的东西。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">static</span> <span class="hljs-keyword">struct</span> buf*<br><span class="hljs-title function_">bget</span><span class="hljs-params">(uint dev, uint blockno)</span><br>&#123;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">buf</span> *<span class="hljs-title">b</span>;</span><br><br>  uint hashid = myhash(blockno);<br>  acquire(&amp;bcache.buclock[hashid]);<br><br>  <span class="hljs-comment">// Is the block already cached?</span><br>  <span class="hljs-keyword">for</span>(b = bcache.head[hashid].next; b != &amp;bcache.head[hashid]; b = b-&gt;next)&#123;<br>    <span class="hljs-keyword">if</span>(b-&gt;dev == dev &amp;&amp; b-&gt;blockno == blockno)&#123;<br>      b-&gt;refcnt++;<br>      release(&amp;bcache.buclock[hashid]);<br>      acquiresleep(&amp;b-&gt;lock);<br>      <span class="hljs-keyword">return</span> b;<br>    &#125;<br>  &#125;<br><br>  <span class="hljs-comment">// Not cached.</span><br>  <span class="hljs-comment">// Recycle the least recently used (LRU) unused buffer.</span><br>  b = bfind(hashid, <span class="hljs-number">0</span>);<br>  <span class="hljs-keyword">if</span>(!b)&#123;<br>    release(&amp;bcache.buclock[hashid]);<br>    acquire(&amp;bcache.lock);<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; NBUCKETS; i++)<br>      <span class="hljs-keyword">if</span>(i != hashid &amp;&amp; (b = bfind(i, <span class="hljs-number">1</span>)))  <span class="hljs-keyword">break</span>;<br>    release(&amp;bcache.lock);<br>    acquire(&amp;bcache.buclock[hashid]);<br>  &#125;<br>  <span class="hljs-keyword">if</span>(b)&#123;<br>    b-&gt;next = bcache.head[hashid].next;<br>    b-&gt;prev = &amp;bcache.head[hashid];<br>    bcache.head[hashid].next-&gt;prev = b;<br>    bcache.head[hashid].next = b;<br>  &#125;<br>  <span class="hljs-keyword">for</span>(<span class="hljs-keyword">struct</span> buf *bi = bcache.head[hashid].next; bi != &amp;bcache.head[hashid]; bi = bi-&gt;next)&#123;<br>    <span class="hljs-keyword">if</span>(bi-&gt;dev == dev &amp;&amp; bi-&gt;blockno == blockno)&#123;<br>      bi-&gt;refcnt++;<br>      release(&amp;bcache.buclock[hashid]);<br>      acquiresleep(&amp;bi-&gt;lock);<br>      <span class="hljs-keyword">return</span> bi;<br>    &#125;<br>  &#125;<br>  <span class="hljs-keyword">if</span>(b)&#123;<br>    b-&gt;dev = dev;<br>    b-&gt;blockno = blockno;<br>    b-&gt;valid = <span class="hljs-number">0</span>;<br>    b-&gt;refcnt = <span class="hljs-number">1</span>;<br>    release(&amp;bcache.buclock[hashid]);<br>    acquiresleep(&amp;b-&gt;lock);<br>    <span class="hljs-keyword">return</span> b;<br>  &#125;<br>  panic(<span class="hljs-string">&quot;bget: no buffers&quot;</span>);<br>&#125;<br></code></pre></td></tr></table></figure><p>我强调这种办法只是为了解决重复缓存的问题，但是会不会导致其他问题嘛……我也不敢保证……</p><p><br></p><p>brelse、bpin、bunpin 都只需要把原来的锁换成当前桶的锁即可，这里不再赘述。</p><p><br>make grade 截图：</p><p><img src="result.png" /></p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>操作系统</category>
      
      <category>xv6-lab</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[xv6-mit-6.S081-2020]Lab7: thread</title>
    <link href="/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab7-thread/"/>
    <url>/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab7-thread/</url>
    
    <content type="html"><![CDATA[<h1 id="lab-multithreading">Lab: Multithreading</h1><p>https://pdos.csail.mit.edu/6.S081/2020/labs/thread.html</p><p>代码：https://github.com/xyfJASON/xv6-mit-6.S081-2020/tree/thread</p><span id="more"></span><h2 id="uthread-switching-between-threads">Uthread: switching between threads</h2><p>任务：在<strong>用户层面</strong>实现线程切换机制。</p><p>我们需要补充完整 user/uthread.c 中的 thread_create() 和 thread_schedule()，以及 user/uthread_switch.S 中的 thread_switch。两个目标：</p><ul><li>当 thread_scheduler() 第一次跑某一线程时，该线程<strong>在自己的栈上</strong>执行传入的函数；</li><li>thread_switch 保存切换走的线程的寄存器，恢复要切换的线程的寄存器，并返回到线程上一次切走的位置。</li></ul><p>如果在做实验前看过 xv6 book 的第 7 章和相关代码，就会发现我们只需要模仿内核中进程切换的方式写这个任务。内核首先定义了一个 context 结构体，包含一系列 callee-save registers：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">// Saved registers for kernel context switches.</span><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">context</span> &#123;</span><br>  uint64 ra;<br>  uint64 sp;<br><br>  <span class="hljs-comment">// callee-saved</span><br>  uint64 s0;<br>  uint64 s1;<br>  uint64 s2;<br>  uint64 s3;<br>  uint64 s4;<br>  uint64 s5;<br>  uint64 s6;<br>  uint64 s7;<br>  uint64 s8;<br>  uint64 s9;<br>  uint64 s10;<br>  uint64 s11;<br>&#125;;<br></code></pre></td></tr></table></figure><blockquote><p>寄存器被分为两种类型——caller-save 和 callee-save，顾名思义，前者要求调用者保存和恢复相关寄存器，而后者要求被调用者保存和恢复相关寄存器，也就是说，从调用者的角度看，caller-save 寄存器在调用前后可能发生变化，而 callee-save 寄存器不会变化 。因此，切换时我们只需要恢复 callee-save 寄存器。</p><p>除了 callee-save 寄存器需要恢复以外，栈指针 sp 和返回地址 ra 也需要恢复，它们构成了上述 context 结构体的内容。</p></blockquote><p>完成上下文切换的是一段汇编代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs assembly"># Context switch<br>#<br>#   void swtch(struct context *old, struct context *new);<br>#<br># Save current registers in old. Load from new.<br><br><br>.globl swtch<br>swtch:<br>        sd ra, 0(a0)<br>        sd sp, 8(a0)<br>        sd s0, 16(a0)<br>        sd s1, 24(a0)<br>        sd s2, 32(a0)<br>        sd s3, 40(a0)<br>        sd s4, 48(a0)<br>        sd s5, 56(a0)<br>        sd s6, 64(a0)<br>        sd s7, 72(a0)<br>        sd s8, 80(a0)<br>        sd s9, 88(a0)<br>        sd s10, 96(a0)<br>        sd s11, 104(a0)<br><br>        ld ra, 0(a1)<br>        ld sp, 8(a1)<br>        ld s0, 16(a1)<br>        ld s1, 24(a1)<br>        ld s2, 32(a1)<br>        ld s3, 40(a1)<br>        ld s4, 48(a1)<br>        ld s5, 56(a1)<br>        ld s6, 64(a1)<br>        ld s7, 72(a1)<br>        ld s8, 80(a1)<br>        ld s9, 88(a1)<br>        ld s10, 96(a1)<br>        ld s11, 104(a1)<br><br>        ret<br></code></pre></td></tr></table></figure><p>C 程序只需要调用 <code>swtch(&amp;old_context, &amp;new_context)</code> 就可以实现切换了——<strong>原理是什么呢？</strong>在 C 代码被汇编成汇编代码时，函数调用的参数会被依次放入 a0、a1、……寄存器，因此，调用 swtch 时，a0 寄存器就是 <code>&amp;old_context</code>，a1 寄存器就是 <code>&amp;new_context</code>。由于一个 uint64 占 8 字节，所以 <code>0(a0)</code>、<code>8(a0)</code>、……就分别对应 old_context 结构体里的 <code>ra</code>、<code>sp</code>、……。</p><p><br></p><p>仿照上述代码，这个任务就很简单了：</p><ul><li>我们也定义一个 context 结构体，包含 ra、sp 和 callee-save registers，放进 struct thread 中；</li><li>uthread_switch.S 直接复制 kernel/swtch.S；</li><li>在 thread_schedule() 中调用 uthread_switch 并把旧的线程上下文 <code>&amp;t-&gt;context</code> 和新的线程上下文 <code>&amp;next_thread-&gt;context</code> 作为参数传入；</li><li>最后，在 thread_create() 中把传入的函数指针给到线程上下文的 <code>ra</code>——这样当这个线程被调度执行时就可以执行指定函数了；同时把栈指针给到线程上下文的 <code>sp</code>——<strong>注意，栈空间是按地址从大到小增长的，</strong>所以我们给的应该是 stack 数组的末尾指针。</li></ul><h2 id="using-threads">Using threads</h2><p>任务：在一台多核 Linux 或 MacOS 上使用 UNIX <code>pthread</code> 线程库写多线程的并行程序。</p><p>MIT 提供了一个代码 notxv6/ph.c，它开给定数量个线程，每个线程向 hash 表里面加许多 key（put 操作），然后从 hash 表里取出 key（get 操作），同时记录 put、get 的用时，以及缺失的 key——本来应该在 hash 表里，但是 get 不到。发生缺失的原因是 ph.c 没有在多线程时加锁，我们的任务就是把锁给加上。</p><p>相关接口：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">pthread_mutex_t</span> lock;            <span class="hljs-comment">// declare a lock</span><br>pthread_mutex_init(&amp;lock, <span class="hljs-literal">NULL</span>); <span class="hljs-comment">// initialize the lock</span><br>pthread_mutex_lock(&amp;lock);       <span class="hljs-comment">// acquire lock</span><br>pthread_mutex_unlock(&amp;lock);     <span class="hljs-comment">// release lock</span><br></code></pre></td></tr></table></figure><p>对于在学习操作系统理论课的时候就喜欢瞎折腾的我，这个任务就……蛮简单的（逃。给每个 hash 桶加一个锁，put 和 get 的时候先加锁，再操作，最后解锁就搞定了。</p><h2 id="barrier">Barrier</h2><p>任务：实现一个 barrier——执行到这里的线程必须等待，直到所有线程都执行到了这个地方。</p><p>和上一个任务一样，notxv6/barrier.c 已经写了一个不对的 barrier，它开给定数量个线程，每个线程做一个循环，循环的某处调用了 barrier()。我们期望所有线程都调用了 barrier() 之后才能继续执行。</p><p>相关接口：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scss"><span class="hljs-built_in">pthread_cond_wait</span>(&amp;cond, &amp;mutex);  <span class="hljs-comment">// go to sleep on cond, releasing lock mutex, acquiring upon wake up</span><br><span class="hljs-built_in">pthread_cond_broadcast</span>(&amp;cond);     <span class="hljs-comment">// wake up every thread sleeping on cond</span><br></code></pre></td></tr></table></figure><p>看过 xv6 book 第 7 章就能很好理解什么叫做在一个 condition 上 sleep、什么叫做 broadcast、以及为什么 wait 的时候要传入一个锁了，此不赘述。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">static</span> <span class="hljs-type">void</span><br><span class="hljs-title function_">barrier</span><span class="hljs-params">()</span><br>&#123;<br>  <span class="hljs-comment">// YOUR CODE HERE</span><br>  <span class="hljs-comment">//</span><br>  <span class="hljs-comment">// Block until all threads have called barrier() and</span><br>  <span class="hljs-comment">// then increment bstate.round.</span><br>  <span class="hljs-comment">//</span><br>  assert(pthread_mutex_lock(&amp;bstate.barrier_mutex) == <span class="hljs-number">0</span>);<br>  <span class="hljs-keyword">if</span>(++bstate.nthread == nthread)&#123;<br>    assert(pthread_cond_broadcast(&amp;bstate.barrier_cond) == <span class="hljs-number">0</span>);<br>    bstate.round++;<br>    bstate.nthread = <span class="hljs-number">0</span>;<br>  &#125;<br>  <span class="hljs-keyword">else</span><br>    assert(pthread_cond_wait(&amp;bstate.barrier_cond, &amp;bstate.barrier_mutex) == <span class="hljs-number">0</span>);<br>  assert(pthread_mutex_unlock(&amp;bstate.barrier_mutex) == <span class="hljs-number">0</span>);<br>&#125;<br></code></pre></td></tr></table></figure><p><br></p><p>make grade 截图：</p><p><img src="result.png" /></p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>操作系统</category>
      
      <category>xv6-lab</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[xv6-mit-6.S081-2020]Lab6: cow</title>
    <link href="/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab6-cow/"/>
    <url>/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab6-cow/</url>
    
    <content type="html"><![CDATA[<h1 id="lab-copy-on-write-fork-for-xv6">Lab: Copy-on-Write Fork for xv6</h1><p>https://pdos.csail.mit.edu/6.S081/2020/labs/cow.html</p><p>代码：https://github.com/xyfJASON/xv6-mit-6.S081-2020/tree/cow</p><span id="more"></span><p><br></p><p>在 xv6 系统中，fork() 系统调用会复制父进程的所有用户空间内存给子进程（正如我们在 pgtbl 实验中看到过的那样）。可是，如果父进程很大，复制过程会消耗很长的时间。更糟糕的是，这可能是无用功，例如 fork() 后子进程紧接着 exec()，那么刚复制下来的内存根本不会用到。另一方面，如果父子进程都要用到这块内存，那么复制又是必须的。</p><p>解决方法是 copy-on-write (COW)。COW fork() 只给子进程创建一个页表，其 PTE 指向父进程的物理页，然后给父子进程的 PTE 全部打上<strong>不可写</strong>的标记。于是，当父子进程之一试图写这些页面时会产生 page fault，内核的 page fault 处理程序检测到这种情况后，给产生异常的那个进程分配一个新的页面，把原页面复制过去，修改 PTE 并打上可写的标记。处理程序返回之后，进程就能继续正常执行了。</p><p>COW fork() 让释放物理页面变得很 tricky，因为一个物理页面可能被多个进程的页表同时指向，因此只当最后一个指针撤去后才能释放。</p><h2 id="implement-copy-on-write">Implement copy-on-write</h2><p>任务：实现上述 copy-on-write 过程，要求通过 cowtest 和 usertests。</p><p>这次实验给的评级是 hard，但是个人感觉比以前的 hard 要简单一点，一方面我们已经有了前面实验的基础，另一方面这次要完成的功能非常明确，步骤和需要注意的点也在指导网站上明确地列了出来。</p><p><br></p><p>RISC-V 的 PTE 有 10 个标志位，其中第 8、9 位是为用户保留的，因此我用第 8 位作为 PTE_COW 标志，表示该 PTE 是否需要 copy-on-write：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">define</span> PTE_COW (1L &lt;&lt; 8) <span class="hljs-comment">// copy-on-write</span></span><br></code></pre></td></tr></table></figure><p><br></p><p>我们查看 fork() 的代码，发现页表复制是由 uvmcopy() 实现的，因此我们需要改的地方其实是 uvmcopy()。（我顺便查了一下还有没有其他函数调用了 uvmcopy()，结果惊奇地发现只有 fork() 调用了它）我们将开辟内存并复制内容的代码注释掉，改为直接向原空间建立映射，并把 PTE_W 置零，把 PTE_COW 置一：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span><br><span class="hljs-title function_">uvmcopy</span><span class="hljs-params">(<span class="hljs-type">pagetable_t</span> old, <span class="hljs-type">pagetable_t</span> new, uint64 sz)</span><br>&#123;<br>  ...<br>    pa = PTE2PA(*pte);<br>    flags = PTE_FLAGS(*pte);<br><br>    flags = (flags | PTE_COW) &amp; (~PTE_W);<br>    *pte = PA2PTE(pa) | flags;<br>    <span class="hljs-keyword">if</span>(mappages(new, i, PGSIZE, pa, flags) != <span class="hljs-number">0</span>)<br>      <span class="hljs-keyword">goto</span> err;<br>    update_refcount(pa, <span class="hljs-number">1</span>);<br>    <span class="hljs-comment">/*</span><br><span class="hljs-comment">    if((mem = kalloc()) == 0)</span><br><span class="hljs-comment">      goto err;</span><br><span class="hljs-comment">    memmove(mem, (char*)pa, PGSIZE);</span><br><span class="hljs-comment">    if(mappages(new, i, PGSIZE, (uint64)mem, flags) != 0)&#123;</span><br><span class="hljs-comment">      kfree(mem);</span><br><span class="hljs-comment">      goto err;</span><br><span class="hljs-comment">    &#125;*/</span><br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>update_refcount(pa, 1) 的作用后文叙述。</p><p><br></p><p>现在，对于某个带有 PTE_COW 标记的 PTE 指向的页面，我们写它时会引起 page fault——因为它的 PTE_W 被置零了。和上一个实验（lazy allocation）一样，这个 page fault 在 usertrap() 中处理。当<strong>写</strong>页面发生异常时，scause 寄存器的值会被置为 15，stval 寄存器会存储导致异常的地址，所以我们根据这两个寄存器处理 copy-on-write：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">usertrap</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>  <span class="hljs-keyword">else</span> &#123;<br>    uint64 cause = r_scause();<br>    <span class="hljs-keyword">if</span>(cause == <span class="hljs-number">15</span>)&#123;<br>      <span class="hljs-comment">// page fault</span><br>      uint64 stval = r_stval();<br>      <span class="hljs-keyword">if</span>(handle_cow(p-&gt;pagetable, stval, <span class="hljs-number">0</span>) == <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">goto</span> brk;<br>    &#125;<br>    ...<br>  &#125;<br>brk:<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>其中 handle_cow() 函数处理 copy-on-write：它先解析虚拟地址，如果发现其 PTE 的 PTE_COW 被置位了，则开辟新的空间，并将 PTE 指向这个新的空间，同时把 PTE_W 置一、PTE_COW 置零。这样，当我们返回用户空间时，用户进程就能正常执行了：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">// Check if virtual address refers to a cow page and handle it.</span><br><span class="hljs-comment">// Store the physical address of the newly allocated page in parameter newpa.</span><br><span class="hljs-comment">// Return: -1 something wrong</span><br><span class="hljs-comment">//          0 copy-on-write success</span><br><span class="hljs-comment">//          1 no need to copy-on-write</span><br><span class="hljs-type">int</span><br><span class="hljs-title function_">handle_cow</span><span class="hljs-params">(<span class="hljs-type">pagetable_t</span> pagetable, uint64 va, uint64 *newpa)</span><br>&#123;<br>  <span class="hljs-type">pte_t</span> *pte = walk(pagetable, va, <span class="hljs-number">0</span>);<br>  <span class="hljs-keyword">if</span>(pte == <span class="hljs-number">0</span>)  <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  <span class="hljs-keyword">if</span>(*pte &amp; PTE_COW)&#123;<br>    <span class="hljs-comment">// copy-on-write</span><br>    uint flags = (PTE_FLAGS(*pte) &amp; (~PTE_COW)) | PTE_W;<br>    uint64 pa = PTE2PA(*pte);<br>    <span class="hljs-type">char</span> *mem = kalloc();<br>    <span class="hljs-keyword">if</span>(mem == <span class="hljs-number">0</span>)  <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>    memmove(mem, (<span class="hljs-type">char</span> *)pa, PGSIZE);<br>    *pte = PA2PTE((uint64)mem) | flags;<br>    kfree((<span class="hljs-type">void</span> *)pa);<br>    <span class="hljs-keyword">if</span>(newpa) *newpa = (uint64)mem;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>  &#125;<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>为了方便，我传入了一个指针以保存新的物理地址，待会儿会用到。</p><p><br></p><p>copy-on-write 之后，原来的页面怎么办呢？在上面的代码中，我调用了 kfree <strong>试图</strong>将其释放。但正如前文所述，只有当所有进程都不用一个页面时才能将其释放，所以我们需要开一个计数数组，对每一个页面统计有多少个进程指向了它。那一共有多少个页面呢？之前做第三个实验（page tables）的时候我们学习了，xv6（内核进程会用到）的内核空间是从 <code>KERNBASE</code> 到 <code>PHYSTOP</code> 的一段，所以一共有 <code>(PHYSTOP-KERNBASE)/PGSIZE</code> 页，并且物理地址 <code>pa</code> 在第 <code>(pa-KERNBASE)/PGSIZE</code> 页中，我们以此作为计数数组下标即可：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">define</span> idx(x) (((uint64)(x)-KERNBASE)/PGSIZE)</span><br><span class="hljs-type">int</span> refcount[idx(PHYSTOP)];<br></code></pre></td></tr></table></figure><p>每次 kalloc() 时，设置当前计数为 1；kfree 时，计数自减，如果减到了零，才释放内存：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">kfree</span><span class="hljs-params">(<span class="hljs-type">void</span> *pa)</span><br>&#123;<br>  ...<br>  <span class="hljs-keyword">if</span>(((uint64)pa % PGSIZE) != <span class="hljs-number">0</span> || (<span class="hljs-type">char</span>*)pa &lt; end || (uint64)pa &gt;= PHYSTOP)<br>    panic(<span class="hljs-string">&quot;kfree&quot;</span>);<br>  <span class="hljs-comment">// Free only when the last reference goes away.</span><br>  <span class="hljs-keyword">if</span>((uint64)pa &gt;= KERNBASE)&#123;<br>    <span class="hljs-keyword">if</span>(refcount[idx(pa)] &lt;= <span class="hljs-number">0</span>)<br>      panic(<span class="hljs-string">&quot;kfree: refcount&quot;</span>);<br>    <span class="hljs-keyword">if</span>(--refcount[idx(pa)]) <span class="hljs-keyword">return</span>;<br>  &#125;<br>  ...<br>&#125;<br><br><span class="hljs-type">void</span> *<br><span class="hljs-title function_">kalloc</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>  <span class="hljs-comment">// xyf</span><br>  <span class="hljs-keyword">if</span>(r)<br>    refcount[idx(r)] = <span class="hljs-number">1</span>;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>值得注意的是，kinit() 初始化是调用 kfree() 实现的，所以为了把计数数组初始化为零，可以在 freerange 中把所有页的计数置为 1，这样调用一次 kfree()，就能把计数置零、加入 kmem.freelist：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">freerange</span><span class="hljs-params">(<span class="hljs-type">void</span> *pa_start, <span class="hljs-type">void</span> *pa_end)</span><br>&#123;<br>  <span class="hljs-type">char</span> *p;<br>  p = (<span class="hljs-type">char</span>*)PGROUNDUP((uint64)pa_start);<br>  <span class="hljs-keyword">for</span>(; p + PGSIZE &lt;= (<span class="hljs-type">char</span>*)pa_end; p += PGSIZE)&#123;<br>    refcount[idx(p)] = <span class="hljs-number">1</span>;<br>    kfree(p);<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p><br></p><p>为了让 kalloc.c 之外的函数方便地对 refcount 操作，我写了一个更新函数：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">// Update refcount</span><br><span class="hljs-type">void</span><br><span class="hljs-title function_">update_refcount</span><span class="hljs-params">(uint64 pa, <span class="hljs-type">int</span> val)</span><br>&#123;<br>  <span class="hljs-keyword">if</span>((pa % PGSIZE) != <span class="hljs-number">0</span> || pa &lt; KERNBASE || pa &gt;= PHYSTOP)<br>    panic(<span class="hljs-string">&quot;update_refcount&quot;</span>);<br>  refcount[idx(pa)] += val;<br>  <span class="hljs-keyword">if</span>(refcount[idx(pa)] &lt; <span class="hljs-number">0</span>)<br>    panic(<span class="hljs-string">&quot;update_refcount: less than 0&quot;</span>);<br>&#125;<br></code></pre></td></tr></table></figure><p>上文 uvmcopy 就调用了它使得计数加一。</p><p><br></p><p>最后，我们还需要改 copyout，也用 handle_cow 函数即可，这里我设置的第三个参数就发挥了作用，当然不设置这个参数，之后 walkaddr 一下也行。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span><br><span class="hljs-title function_">copyout</span><span class="hljs-params">(<span class="hljs-type">pagetable_t</span> pagetable, uint64 dstva, <span class="hljs-type">char</span> *src, uint64 len)</span><br>&#123;<br>  ...<br>  <span class="hljs-keyword">while</span>(len &gt; <span class="hljs-number">0</span>)&#123;<br>    va0 = PGROUNDDOWN(dstva);<br>    pa0 = walkaddr(pagetable, va0);<br>    <span class="hljs-keyword">if</span>(pa0 == <span class="hljs-number">0</span>)<br>      <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>    <span class="hljs-keyword">if</span>(handle_cow(pagetable, va0, &amp;pa0) == <span class="hljs-number">-1</span>)<br>      <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>    ...<br>&#125;<br></code></pre></td></tr></table></figure><p>make grade 截图：</p><p><img src="result.png" /></p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>操作系统</category>
      
      <category>xv6-lab</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[xv6-mit-6.S081-2020]Lab5: lazy</title>
    <link href="/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab5-lazy/"/>
    <url>/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab5-lazy/</url>
    
    <content type="html"><![CDATA[<h1 id="lab-xv6-lazy-page-allocation">Lab: xv6 lazy page allocation</h1><p>https://pdos.csail.mit.edu/6.S081/2020/labs/lazy.html</p><p>代码：https://github.com/xyfJASON/xv6-mit-6.S081-2020/tree/lazy</p><span id="more"></span><p><br></p><p>许多 OS 都会为用户的堆内存实现懒分配——在用户程序用 sbrk() 申请更多的空间时，不真正开辟物理内存，而是把要用的用户虚拟地址在页表中设为 invalid，等到确实用到了这个虚拟地址，CPU 产生缺页错误，这时内核再分配物理内存。</p><p>可以看到，完成这次实验需要我们综合运用前两次实验（page tables，traps）的知识，但只要前两次实验认真做了，这次实验应该不难。</p><h2 id="eliminate-allocation-from-sbrk">Eliminate allocation from sbrk()</h2><p>任务：在 sys_sbrk (kernel/sysproc.c) 中修改 xv6 原本的 sbrk(n) 系统调用的实现。原本的 sbrk(n) 会让用户空间增长 n 个字节，返回新分配虚拟空间的首地址（即原用户空间大小）。新的 sbrk(n) 应该只给 <code>myproc()-&gt;sz</code> 加上 n，返回原用户空间大小，但是并没有实际开辟物理内存。</p><p>根据指导，注释掉 growproc 的调用，改变 sz 即可：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c">uint64<br><span class="hljs-title function_">sys_sbrk</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>  addr = myproc()-&gt;sz;<br>  myproc()-&gt;sz += n;<br><span class="hljs-comment">//  if(growproc(n) &lt; 0)</span><br><span class="hljs-comment">//    return -1;</span><br>  <span class="hljs-keyword">return</span> addr;<br>&#125;<br></code></pre></td></tr></table></figure><p>现在编译 xv6，输入 <code>echo hi</code>，则会出错：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros">$ echo hi<br>usertrap(): unexpected scause 0x000000000000000f <span class="hljs-attribute">pid</span>=3<br>            <span class="hljs-attribute">sepc</span>=0x00000000000012ac <span class="hljs-attribute">stval</span>=0x0000000000004008<br>panic: uvmunmap: <span class="hljs-keyword">not</span> mapped<br></code></pre></td></tr></table></figure><h2 id="lazy-allocation">Lazy allocation</h2><p>任务：改变 trap.c 的代码以回应用户空间的缺页错误，即新开辟一页的物理内存空间，返回用户空间继续执行。</p><p>我们首先在 usertrap 中处理缺页错误。缺页错误的代码是 13 或 15，当发生缺页错误时，判断是否是懒分配引起的（引起错误的地址在 p-&gt;sz 内），如果是，则用 kalloc 新开辟一页物理空间，并在页表中加上缺页的地址所在页面这一项。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">usertrap</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>  &#125; <span class="hljs-keyword">else</span> &#123;<br>    uint64 cause = r_scause();<br>    <span class="hljs-keyword">if</span>(cause == <span class="hljs-number">13</span> || cause == <span class="hljs-number">15</span>)&#123;<br>      <span class="hljs-comment">// page fault</span><br>      uint64 stval = r_stval();<br>      <span class="hljs-keyword">if</span>(stval &lt; p-&gt;sz)&#123;<br>        <span class="hljs-comment">// need lazy allocation</span><br>        <span class="hljs-type">char</span> *mem = kalloc();<br>        <span class="hljs-keyword">if</span>(mem)&#123;<br>          <span class="hljs-built_in">memset</span>(mem, <span class="hljs-number">0</span>, PGSIZE);<br>          <span class="hljs-keyword">if</span>(mappages(p-&gt;pagetable, PGROUNDDOWN(stval), PGSIZE, (uint64)mem, PTE_W|PTE_X|PTE_R|PTE_U) != <span class="hljs-number">0</span>)&#123;<br>            kfree(mem);<br>            uvmunmap(p-&gt;pagetable, PGROUNDDOWN(stval), <span class="hljs-number">1</span>, <span class="hljs-number">1</span>);<br>          &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">goto</span> brk;<br>        &#125;<br>      &#125;<br>    &#125;<br><br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;usertrap(): unexpected scause %p pid=%d\n&quot;</span>, r_scause(), p-&gt;pid);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;            sepc=%p stval=%p\n&quot;</span>, r_sepc(), r_stval());<br>    p-&gt;killed = <span class="hljs-number">1</span>;<br>  &#125;<br>brk:<br></code></pre></td></tr></table></figure><p>我们还需要更改 uvmunmap 的内容，这是因为加入懒分配之后，uvmunmap 可能会被要求解除本就不存在的映射、或者去找还没有创建的 pte。在原本的写法中这样会 panic，因此，我们要把 panic 改掉：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">uvmunmap</span><span class="hljs-params">(<span class="hljs-type">pagetable_t</span> pagetable, uint64 va, uint64 npages, <span class="hljs-type">int</span> do_free)</span><br>&#123;<br>  ...<br>    <span class="hljs-keyword">if</span>((pte = walk(pagetable, a, <span class="hljs-number">0</span>)) == <span class="hljs-number">0</span>)<br>      <span class="hljs-keyword">continue</span>; <span class="hljs-comment">// panic(&quot;uvmunmap: walk&quot;);</span><br>    <span class="hljs-keyword">if</span>((*pte &amp; PTE_V) == <span class="hljs-number">0</span>)<br>      <span class="hljs-keyword">continue</span>; <span class="hljs-comment">// panic(&quot;uvmunmap: not mapped&quot;);</span><br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>现在我们已经能正常执行 <code>echo hi</code> 了。</p><h2 id="lazytests-and-usertests">Lazytests and Usertests</h2><p>任务：通过 Lazytests 和 Usertests。</p><p>完成上一小节的任务后，我们的代码其实并不完善，还需要处理下列问题：</p><ol type="1"><li>处理负的 sbrk() 参数</li><li>如果导致缺页错误的虚拟地址高于任何 sbrk() 分配的内存地址，则杀死进程</li><li>在 fork() 中正确处理父进程到子进程的内存复制</li><li>处理以下情况：一个进程给系统调用（如 read / write）传入了一个合法的地址，但是地址的内存还没有分配</li><li>正确处理超出内存的情况：如果 kalloc() 在缺页错误处理中失败了，则杀死进程</li><li>处理缺页错误中访问用户栈之下的非法空间</li></ol><p>对于第 1 点，只需要参照 growproc 原本的写法，如果 n &lt; 0，则调用 uvmdealloc 回收空间：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c">uint64<br><span class="hljs-title function_">sys_sbrk</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">proc</span> *<span class="hljs-title">p</span> =</span> myproc();<br>  addr = p-&gt;sz;<br>  <span class="hljs-keyword">if</span>(n &gt; <span class="hljs-number">0</span>)<br>    p-&gt;sz += n;<br>  <span class="hljs-keyword">else</span><br>    p-&gt;sz = uvmdealloc(p-&gt;pagetable, addr, addr+n);<br>  <span class="hljs-keyword">return</span> addr;<br>&#125;<br></code></pre></td></tr></table></figure><p>对于第 2、5、6 点，只需要在上一小节的基础上加一点判断和 killed 设置即可，为了方便，这次把处理代码写成一个函数（kernel/proc.c 中）：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">// Handle page fault with lazy allocation</span><br><span class="hljs-type">int</span><br><span class="hljs-title function_">lazy_allocate</span><span class="hljs-params">(uint64 va)</span>&#123;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">proc</span> *<span class="hljs-title">p</span> =</span> myproc();<br>  <span class="hljs-keyword">if</span>(va &gt;= p-&gt;sz || va &lt; p-&gt;trapframe-&gt;sp)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  <span class="hljs-type">char</span> *mem = kalloc();<br>  <span class="hljs-keyword">if</span>(mem == <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  <span class="hljs-built_in">memset</span>(mem, <span class="hljs-number">0</span>, PGSIZE);<br>  <span class="hljs-keyword">if</span>(mappages(p-&gt;pagetable, PGROUNDDOWN(va), PGSIZE, (uint64)mem, PTE_W|PTE_X|PTE_R|PTE_U) != <span class="hljs-number">0</span>)&#123;<br>    kfree(mem);<br>    uvmunmap(p-&gt;pagetable, PGROUNDDOWN(va), <span class="hljs-number">1</span>, <span class="hljs-number">1</span>);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  &#125;<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>于是乎，在 usertrap 中我们只需要调用 lazy_allocate：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">usertrap</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>  &#125; <span class="hljs-keyword">else</span> &#123;<br>    uint64 cause = r_scause();<br>    <span class="hljs-keyword">if</span>(cause == <span class="hljs-number">13</span> || cause == <span class="hljs-number">15</span>)&#123;<br>      <span class="hljs-comment">// page fault</span><br>      uint64 stval = r_stval();<br>      <span class="hljs-keyword">if</span>(lazy_allocate(stval) == <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">goto</span> brk;<br>    &#125;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;usertrap(): unexpected scause %p pid=%d\n&quot;</span>, r_scause(), p-&gt;pid);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;            sepc=%p stval=%p\n&quot;</span>, r_sepc(), r_stval());<br>    p-&gt;killed = <span class="hljs-number">1</span>;<br>  &#125;<br>brk:<br></code></pre></td></tr></table></figure><p>对于第 3 点，我们查看 fork 的代码，发现内存复制是调用 uvmcopy 实现的，所以只需要像改 uvmunmap 一般改 uvmcopy 即可：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span><br><span class="hljs-title function_">uvmcopy</span><span class="hljs-params">(<span class="hljs-type">pagetable_t</span> old, <span class="hljs-type">pagetable_t</span> new, uint64 sz)</span><br>&#123;<br>  ...<br>  <span class="hljs-keyword">for</span>(i = <span class="hljs-number">0</span>; i &lt; sz; i += PGSIZE)&#123;<br>    <span class="hljs-keyword">if</span>((pte = walk(old, i, <span class="hljs-number">0</span>)) == <span class="hljs-number">0</span>)<br>      <span class="hljs-keyword">continue</span>; <span class="hljs-comment">// panic(&quot;uvmcopy: pte should exist&quot;);</span><br>    <span class="hljs-keyword">if</span>((*pte &amp; PTE_V) == <span class="hljs-number">0</span>)<br>      <span class="hljs-keyword">continue</span>; <span class="hljs-comment">// panic(&quot;uvmcopy: page not present&quot;);</span><br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>对于第 4 点，系统调用的时候 RISC-V 硬件不会引发缺页错误，因此操作系统必须处理这种情况。我们知道，那些参数包含地址的系统调用都会执行 argaddr() 函数，所以我们先找到它（kernel/syscall.c）。理论上，在这里处理缺页是可行的，但是我们把目光向上移，就会发现注释说：argaddr() 不检查是否合法，因为 copyin/copyout 会检查。好吧，那我们就去看看 copyin/copyout 是怎么检查的呗。看了一圈下来，我们可以发现，它们会调用 walkaddr()，如果 walkaddr 返回 0，那么就返回错误代码 -1。所以，<strong>本质上是 walkaddr() 在检查是否合法！</strong>Okay，定位了问题所在，我们只需要用上刚刚写的 lazy_allocate() 函数，略微修改 walkaddr() 即可：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c">uint64<br><span class="hljs-title function_">walkaddr</span><span class="hljs-params">(<span class="hljs-type">pagetable_t</span> pagetable, uint64 va)</span><br>&#123;<br>  ...<br>  pte = walk(pagetable, va, <span class="hljs-number">0</span>);<br>  <span class="hljs-keyword">if</span>(pte == <span class="hljs-number">0</span> || (*pte &amp; PTE_V) == <span class="hljs-number">0</span>)&#123;<br>    <span class="hljs-keyword">if</span>(lazy_allocate(va) != <span class="hljs-number">0</span>)<br>      <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>    pte = walk(pagetable, va, <span class="hljs-number">0</span>);<br>  &#125;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>make grade 截图：</p><p><img src="result.jpg" /></p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>操作系统</category>
      
      <category>xv6-lab</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[xv6-mit-6.S081-2020]Lab4: traps</title>
    <link href="/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab4-traps/"/>
    <url>/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab4-traps/</url>
    
    <content type="html"><![CDATA[<h1 id="lab-traps">Lab: traps</h1><p>https://pdos.csail.mit.edu/6.S081/2020/labs/traps.html</p><p>代码：https://github.com/xyfJASON/xv6-mit-6.S081-2020/tree/traps</p><span id="more"></span><h2 id="risc-v-assembly">RISC-V assembly</h2><p>我们计算机组成原理课上学过 RISC-V 的汇编语言，所以这一节相对轻松。</p><ol type="1"><li><p>Which registers contain arguments to functions? For example, which register holds 13 in main's call to <code>printf</code>?</p><p>user/call.asm 中有两行 <code>li a2,13</code> 和 <code>li a1,12</code>，我们可以看出 <code>a1,a2</code> 寄存器是存放 <code>printf</code> 的参数的地方。</p></li><li><p>Where is the call to function <code>f</code> in the assembly code for main? Where is the call to <code>g</code>? (Hint: the compiler may inline functions.)</p><p>刚刚我们已经看到，<code>li a,12</code> 直接把 <code>f(8)+1</code> 算出来了，所以是编译器内联了它；另外，<code>f</code> 函数里面的指令和 <code>g</code> 一模一样，说明编译器也内联了它。</p></li><li><p>At what address is the function <code>printf</code> located?</p><p>注释写的很清楚了：<code>0x630</code>；</p><p>如果没有注释的话也可以算出来，<code>auipc ra,0x0</code> 是将当前 pc 给了 ra，即 ra=<code>0x30</code>，那么 <code>jalr 1536(ra)</code> 能跳到 <code>printf</code>，说明其位置是 <code>0x30+1536=0x630</code>。</p></li><li><p>What value is in the register <code>ra</code> just after the <code>jalr</code> to <code>printf</code> in <code>main</code>?</p><p><code>jalr</code> 会将 pc+4 存储给指定的寄存器，反汇编语句里省略了指定寄存器，是因为默认给 ra，所以 ra=<code>0x38</code>。</p></li><li><p>Run the following code.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> i = <span class="hljs-number">0x00646c72</span>;<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;H%x Wo%s&quot;</span>, <span class="hljs-number">57616</span>, &amp;i);<br></code></pre></td></tr></table></figure><p>What is the output? <a href="http://web.cs.mun.ca/~michael/c/ascii-table.html">Here's an ASCII table</a> that maps bytes to characters.</p><p>The output depends on that fact that the RISC-V is little-endian. If the RISC-V were instead big-endian what would you set <code>i</code> to in order to yield the same output? Would you need to change <code>57616</code> to a different value?</p><p><a href="http://www.webopedia.com/TERM/b/big_endian.html">Here's a description of little- and big-endian</a> and <a href="http://www.networksorcery.com/enp/ien/ien137.txt">a more whimsical description</a>.</p><p>很有意思的一道题。<code>%x</code> 是按 16 进制输出，<code>57616=0xe110</code>，所以输出的前半段是 <code>He110</code>；</p><p>ASCII 码中 0x64 对应 <code>d</code>，0x6c 对应 <code>l</code>，0x72 对应 <code>r</code>，又 RISC-V 小端存储（低地址存 0x72），且 <code>%s</code> 从低地址开始读取数据输出，所以会输出 <code>rld</code>，于是输出的后半段就是 <code>World</code>。</p><p>如果是大端存储，那么 <code>i=0x726c6400</code>；但 <code>57616</code> 不用改动。</p></li><li><p>In the following code, what is going to be printed after <code>'y='</code>? (note: the answer is not a specific value.) Why does this happen?</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;x=%d y=%d&quot;</span>, <span class="hljs-number">3</span>);<br></code></pre></td></tr></table></figure><p>前面说了，<code>a2</code> 寄存器是 <code>printf</code> 指令的第 2 个参数，所以应该会输出 a2 寄存器的值。</p></li></ol><h2 id="backtrace">Backtrace</h2><p>任务：在 <code>kernelprintf.c</code> 中添加一个 <code>backtrace()</code>，用于在出错时输出这之前栈中的函数调用。编译器会在每个栈帧中存入一个帧指针，指向调用者的帧指针。<code>backtrace()</code> 应该用这些帧指针来遍历栈并输出每个栈帧的保存的返回地址。</p><p>看到题目的时候，我对帧指针这个术语很迷惑，直到我看到了它的缩写 <code>fp</code> 才反应过来这是当前函数的栈底……然后我又对栈帧这个术语很迷惑，于是看了 <a href="https://pdos.csail.mit.edu/6.828/2020/lec/l-riscv-slides.pdf">lecture notes</a>，哦，原来是当前函数用的这一段栈啊……notes 里面画的很清楚栈里面都有哪些内容：</p><p><img src="note.png" /></p><p>这些存放 prev frame 的地址可以看成构成了一个链表，所以写 backtrace 遍历这个链表即可。但是终止条件是什么呢？根据提示，xv6 给每个栈分配一页的大小，所以一直把它所在的页跳完了就终止。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">backtrace</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;backtrace:\n&quot;</span>);<br>  uint64 fp = r_fp();<br>  uint64 lim = PGROUNDUP(fp);<br>  <span class="hljs-keyword">for</span>(; fp &lt; lim; fp = *((uint64 *)(fp<span class="hljs-number">-16</span>)))<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%p\n&quot;</span>, *((uint64 *)(fp<span class="hljs-number">-8</span>)));<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="alarm">Alarm</h2><p>任务：给 xv6 加一个功能——在进程使用CPU时间时定期发出警告。这对于限制 CPU 密集型（计算密集型）进程的占用时间，或对于在计算过程中有其他定期动作的进程可能很有用。更广泛的说，我们将实现一个用户级中断/异常的处理程序。</p><p>我们需要添加一个 <code>sigalarm(interval, handler)</code> 系统调用。如果一个应用调用了 <code>sigalarm(n, fn)</code>，则该应用每耗时 <code>n</code> 个 ticks，内核应该使之调用 <code>fn</code>，<code>fn</code> 返回后该应用继续执行。如果一个应用调用 <code>sigalarm(0, 0)</code>，内核应该停止产生 alarm calls。</p><h3 id="test0-invoke-handler">test0: invoke handler</h3><p>这个任务本来是比较难的，但是指导网站写得真的太详细了，跟着指导一步步做就行。我感觉总体思想和 Lab2 的 trace 类似，把总 ticks 数、处理程序指针、剩余 ticks 数存放在 struct proc 中，视为当前进程的「属性」：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">proc</span> &#123;</span><br>  ...<br>  <span class="hljs-type">int</span> ticks;                   <span class="hljs-comment">// Ticks between two alarms.</span><br>  uint64 handler;              <span class="hljs-comment">// Alarm handler.</span><br>  <span class="hljs-type">int</span> remain_ticks;            <span class="hljs-comment">// Remaining ticks after last alarm.</span><br>&#125;;<br></code></pre></td></tr></table></figure><p>在 allocproc (kernel/proc.c) 中初始化：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">static</span> <span class="hljs-keyword">struct</span> proc*<br><span class="hljs-title function_">allocproc</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>  <span class="hljs-comment">// Initialize for alarm.</span><br>  p-&gt;ticks = p-&gt;remain_ticks = <span class="hljs-number">0</span>;<br>  p-&gt;handler = <span class="hljs-number">0</span>;<br>  <span class="hljs-keyword">return</span> p;<br>&#125;<br></code></pre></td></tr></table></figure><p>当进程调用 sigalarm 系统调用时设置它们：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs c">uint64<br><span class="hljs-title function_">sys_sigalarm</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  <span class="hljs-type">int</span> ticks;<br>  uint64 handler;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">proc</span> *<span class="hljs-title">p</span> =</span> myproc();<br><br>  <span class="hljs-keyword">if</span>(argint(<span class="hljs-number">0</span>, &amp;ticks) &lt; <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  <span class="hljs-keyword">if</span>(argaddr(<span class="hljs-number">1</span>, &amp;handler) &lt; <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  acquire(&amp;p-&gt;lock);<br>  p-&gt;ticks = ticks;<br>  p-&gt;handler = handler;<br>  p-&gt;remain_ticks = ticks;<br>  release(&amp;p-&gt;lock);<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>每一个 tick 过后硬件都会产生计时器中断（timer interrupt），所以我们在 kernel/trap.c 中 <code>if(which_dev == 2)</code> 语句下处理它。一个计时器中断发生后将剩余 ticks 减 1，如果减到 0 了，则在返回用户空间的时候让它返回到处理程序。从 xv6 book 第四章我们可以知道，内核只需要设置 sepc 即可控制返回的地址，这是在 usertrapret() 中执行的：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">usertrapret</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>  <span class="hljs-comment">// set S Exception Program Counter to the saved user pc.</span><br>  w_sepc(p-&gt;trapframe-&gt;epc);<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>所以我们只需要事先把 <code>p-&gt;trapframe-&gt;epc</code> 改成处理程序地址即可：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">usertrap</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>  <span class="hljs-keyword">if</span>(p-&gt;killed)<br>    <span class="hljs-built_in">exit</span>(<span class="hljs-number">-1</span>);<br><br>  <span class="hljs-comment">// This is a timer interrupt.</span><br>  <span class="hljs-keyword">if</span>(which_dev == <span class="hljs-number">2</span>)&#123;<br>    <span class="hljs-keyword">if</span>(p-&gt;ticks == <span class="hljs-number">0</span>)<br>      yield();<br>    p-&gt;remain_ticks--;<br>    <span class="hljs-keyword">if</span>(p-&gt;remain_ticks == <span class="hljs-number">0</span>)&#123;<br>      p-&gt;remain_ticks = p-&gt;ticks;<br>      p-&gt;trapframe-&gt;epc = p-&gt;handler; <span class="hljs-comment">// TODO</span><br>    &#125;<br>  &#125;<br><br>  usertrapret();<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="test1test2-resume-interrupted-code">test1/test2(): resume interrupted code</h3><p>test0 里面我们写的代码显然有问题：<code>p-&gt;trapframe-&gt;epc</code> 被覆盖了且从来没有恢复过。改正方法也很简单：事先复制一份，在 sigreturn 系统调用时恢复它。除了 epc 以外，由于处理程序还有可能更改寄存器，我们还需要保存寄存器，所以不妨直接把整个 trapframe 复制下来：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">// This is a timer interrupt.</span><br><span class="hljs-keyword">if</span>(which_dev == <span class="hljs-number">2</span>)&#123;<br>  <span class="hljs-keyword">if</span>(p-&gt;ticks == <span class="hljs-number">0</span>)<br>    yield();<br>  p-&gt;remain_ticks--;<br>  <span class="hljs-keyword">if</span>(p-&gt;remain_ticks == <span class="hljs-number">0</span>)&#123;<br>    p-&gt;remain_ticks = p-&gt;ticks;<br>    p-&gt;save_trapframe = (<span class="hljs-keyword">struct</span> trapframe *)kalloc();<br>    memmove(p-&gt;save_trapframe, p-&gt;trapframe, PGSIZE);<br>    p-&gt;trapframe-&gt;epc = p-&gt;handler;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>然后在 sigreturn 中还原 trapframe：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c">uint64<br><span class="hljs-title function_">sys_sigreturn</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">proc</span> *<span class="hljs-title">p</span> =</span> myproc();<br>  acquire(&amp;p-&gt;lock);<br>  <span class="hljs-keyword">if</span>(p-&gt;save_trapframe)&#123;<br>    memmove(p-&gt;trapframe, p-&gt;save_trapframe, PGSIZE);<br>    kfree(p-&gt;save_trapframe);<br>    p-&gt;save_trapframe = <span class="hljs-number">0</span>;<br>  &#125;<br>  release(&amp;p-&gt;lock);<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>现在我们能过 test1，但是不能过 test2，原因是我们没有保证：如果一个处理函数尚未返回，那么内核不应该再次调用它。要保证这一点，只需要加上一个判断条件：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">// This is a timer interrupt.</span><br><span class="hljs-keyword">if</span>(which_dev == <span class="hljs-number">2</span>)&#123;<br>  <span class="hljs-keyword">if</span>(p-&gt;ticks == <span class="hljs-number">0</span>)<br>    yield();<br>  p-&gt;remain_ticks--;<br>  <span class="hljs-keyword">if</span>(p-&gt;remain_ticks == <span class="hljs-number">0</span> &amp;&amp; p-&gt;save_trapframe == <span class="hljs-number">0</span>)&#123;<br>    p-&gt;remain_ticks = p-&gt;ticks;<br>    p-&gt;save_trapframe = (<span class="hljs-keyword">struct</span> trapframe *)kalloc();<br>    memmove(p-&gt;save_trapframe, p-&gt;trapframe, PGSIZE);<br>    p-&gt;trapframe-&gt;epc = p-&gt;handler;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>make grade 截图：</p><p><img src="result.png" /></p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>操作系统</category>
      
      <category>xv6-lab</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[xv6-mit-6.S081-2020]Lab3: pgtbl</title>
    <link href="/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab3-pgtbl/"/>
    <url>/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab3-pgtbl/</url>
    
    <content type="html"><![CDATA[<h1 id="lab-page-tables">Lab: page tables</h1><p>https://pdos.csail.mit.edu/6.S081/2020/labs/pgtbl.html</p><p>代码：https://github.com/xyfJASON/xv6-mit-6.S081-2020/tree/pgtbl</p><span id="more"></span><p><br></p><p>做这次实验需要对 xv6 的页表机制和代码有<strong>深刻</strong>的理解，必须先阅读 xv6 book 的第三章。</p><p>为了更形象的理解，我画了一张图：</p><p><img src="img.png" /></p><p>首先要明白的一点是，页表本身也是在内存中的，也是需要内存空间的，所以<strong>给页表开辟空间</strong>和<strong>建立映射（填写页表项）</strong>是两码事，同样的，<strong>释放页表项指向的空间</strong>、<strong>释放页表本身占有的空间</strong>、<strong>解除映射（页表项清零）</strong>也是不同的事儿。</p><p>其次，xv6 代码中只有 kalloc.c 是对物理内存进行直接管理的代码，要真正分配、释放内存空间，最底层一定是调用 kalloc() 或 kfree() 实现的。</p><p>最后，vm.c 正如其名，是管理虚拟内存的代码，包括（加粗重点）：</p><ul><li><p><strong>walk</strong>：在指定的页表中找到 pte 地址，若设置了 alloc 参数，可以<strong>为页表开辟内存空间</strong>（但是 walk 不会给叶子结点的页表项填写内容，即<strong>不会建立映射</strong>）；</p></li><li><p><strong>mappages</strong>：<strong>建立映射</strong>，如果页表本身不存在会先开辟空间，<strong>要求原本没有映射</strong>；</p></li><li><p><strong>freewalk</strong>：<strong>释放页表本身</strong>，<strong>要求所有映射已经被解除</strong>（页表项全部已经清零）；</p></li><li><p>walkaddr：在指定的用户页表中找虚拟地址对应的物理地址，如果页表不存在/映射不存在返回 0；</p></li><li><p><strong>kvminit</strong>：创建初始内核页表（<strong>开辟空间并建立映射</strong>）；</p></li><li><p><strong>kvminithart</strong>：为 CPU 设置 satp 寄存器，刷新 TLB；</p></li><li><p>kvmmap：对 mappages 的封装，专门为内核页表建立映射；</p></li><li><p>kvmpa：在内核页表中找虚拟地址对应的物理地址，只在为 kstack 找物理地址的时候才需要用；</p></li><li><p><strong>uvmunmap</strong>：<strong>解除映射</strong>（要求映射存在），若设置了 dofree 参数，会同时<strong>释放页表项指向的空间</strong>（注意不会释放页表本身）；</p></li><li><p>uvmcreate：创建一个新的用户页表，即为页表开辟空间；</p></li><li><p>uvminit：为第一个进程加载页表；</p></li><li><p>uvmalloc：如果 newsz &gt;= oldsz，则在 [oldsz, newsz) 区间内<strong>开辟内存、创建页表并建立映射</strong>。</p></li><li><p>uvmdealloc：如果 oldsz &gt; newsz，则将 [newsz, oldsz) 区间内的页表<strong>解除映射并释放页表项指向的空间</strong>；</p></li><li><p><strong>uvmfree</strong>：<strong>解除映射、释放页表项指向的空间、释放页表本身</strong>；</p></li><li><p>uvmcopy：把父进程的页表和页表项指向的物理空间复制给子进程；</p></li><li><p>uvmclear：把给一个 PTE 标志为 invalid，系统调用 exec 用它来设置 guard pages。</p></li><li><p>copyout：从内核 copy 给用户地址</p></li><li><p>copyin：从用户地址 copy 进内核</p></li><li><p>copyinstr：从用户地址 copy 一个字符串进内核</p></li></ul><p>它们的调用关系是：</p><p><img src="graph.png" /></p><p>再画一个表吧：</p><p><img src="table.png" /></p><h2 id="print-a-page-table">Print a page table</h2><p>任务：定义一个函数 vmprint()，它接受一个 pagetable_t 参数，按格式打印出该页表。在 exec.c 返回之前，如果当前进程是 1 号进程，则调用 vmprint() 打印其页表。</p><p>根据提示，我们参照 freewalk 的代码结构，可以知道如何递归遍历所有页表。值得说明的是，freewalk 中判断到达叶节点的方式是检查 PTE_R, PTE_W 或 PTE_X 是否至少有一个被置位了，如果都没有那说明不是叶节点。这样判断的依据是，中间节点标志位的设置是在 walk (kernel/vm.c) 函数中进行的，参看源码就知道，walk 函数只给 PTE_V 置位了，其余标志都是 0；而叶节点是我们设定的，一定会包含 PTE_R, PTE_W, PTE_X 中的至少一个。</p><p>但是，我实现 vmprint 时调用另实现的 printwalk 函数，后者可以接受当前层级作为参数，所以判断是否到达叶节点就没有必要这么麻烦了。</p><h2 id="a-kernel-page-table-per-process">A kernel page table per process</h2><p>任务：xv6 系统只有一个内核页表，只要它在内核中运行就用这个内核页表；同时每一个进程都有自己独立的用户页表。因此，当内核接受一个用户空间的指针（例如调用 write() 时传入的指针）时，必须先将其翻译成物理地址才能使用。此次任务是修改内核使得每一个进程都有一个上述<strong>全局内核页表的副本</strong>。</p><p>首先在 struct proc 中添加一项：pagetable_t k_pagetable，表示该进程的内核页表。接下来，按捺住冲动的内心，好好想一下我们应该做些什么：我们需要实现这个页表的「生命周期」——它随着进程的创建而创建、随着进程被释放而释放。</p><p>为此，我们回顾一下全局内核页表的「生命周期」（这一段在 xv6 book 的 3.3 节有详细阐述）：kernel/main.c 中，main 函数首先调用 <strong>kvminit()</strong> 创建初始全局内核页表（开辟空间+填充初始页表项，除内核栈以外），然后调用 <strong>kvminithart()</strong> 装载全局内核页表，最后调用 <strong>procinit()</strong> 为所有 NPROC 个进程开辟内核栈并重新装载。由于全局内核页表是始终存在的，所以没有释放的操作。</p><p>对应的，我们需要在 allocproc 中创建初始进程内核页表、映射到已经开辟出来的内核栈，在 scheduler 中进程切换时装载对应的进程内核页表，最后在 freeproc 中释放进程内核页表。这些操作的代码都在 kernel/proc.c 中实现。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">static</span> <span class="hljs-keyword">struct</span> proc *<br><span class="hljs-title function_">allocproc</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>  <span class="hljs-comment">// An empty kernel page table.</span><br>  p-&gt;k_pagetable = pvminit();<br>  <span class="hljs-keyword">if</span>(p-&gt;k_pagetable == <span class="hljs-number">0</span>)&#123;<br>    freeproc(p);<br>    release(&amp;p-&gt;lock);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>  &#125;<br>  <span class="hljs-comment">// Put kernel stack into process&#x27;s kernel page table.</span><br>  <span class="hljs-comment">// Note that the stack is built previously in procinit().</span><br>  pvmmap(p-&gt;k_pagetable, p-&gt;kstack, kvmpa(p-&gt;kstack), PGSIZE, PTE_R | PTE_W);<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">static</span> <span class="hljs-type">void</span><br><span class="hljs-title function_">freeproc</span><span class="hljs-params">(<span class="hljs-keyword">struct</span> proc *p)</span><br>&#123;<br>  ...<br>  <span class="hljs-keyword">if</span>(p-&gt;k_pagetable)<br>    freewalk_unmap(p-&gt;k_pagetable);<br>  p-&gt;k_pagetable = <span class="hljs-number">0</span>;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">scheduler</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>        p-&gt;state = RUNNING;<br>        c-&gt;proc = p;<br>        pvminithart(p-&gt;k_pagetable);<br>        swtch(&amp;c-&gt;context, &amp;p-&gt;context);<br>        kvminithart();<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure><p>其中，我们需要实现一些底层函数，我分别命名为 pvminit, pvminithart, pvmmap 和 freewalk_unmap，前三者是遵循 xv6 的命名方式（kvm 开头表示对全局内核页表的操作，uvm 开头表示对用户页表的操作，所以我用 pvm 开头表示对进程内核页表的操作），第四个函数的命名在下面说明。这四个底层函数都是在 kernel/vm.c 中实现的。</p><ul><li><p>pvminit：模仿 kvminit 为每个进程开辟内核页表空间并初始化，返回指向该页表的指针</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">pagetable_t</span><br><span class="hljs-title function_">pvminit</span><span class="hljs-params">()</span><br>&#123;<br>  <span class="hljs-type">pagetable_t</span> ret_pagetable = (<span class="hljs-type">pagetable_t</span>) kalloc();<br>  <span class="hljs-built_in">memset</span>(ret_pagetable, <span class="hljs-number">0</span>, PGSIZE);<br>  pvmmap(ret_pagetable, UART0, UART0, PGSIZE, PTE_R | PTE_W);<br>  pvmmap(ret_pagetable, VIRTIO0, VIRTIO0, PGSIZE, PTE_R | PTE_W);<br>  pvmmap(ret_pagetable, CLINT, CLINT, <span class="hljs-number">0x10000</span>, PTE_R | PTE_W);<br>  pvmmap(ret_pagetable, PLIC, PLIC, <span class="hljs-number">0x400000</span>, PTE_R | PTE_W);<br>  pvmmap(ret_pagetable, KERNBASE, KERNBASE, (uint64)etext-KERNBASE, PTE_R | PTE_X);<br>  pvmmap(ret_pagetable, (uint64)etext, (uint64)etext, PHYSTOP-(uint64)etext, PTE_R | PTE_W);<br>  pvmmap(ret_pagetable, TRAMPOLINE, (uint64)trampoline, PGSIZE, PTE_R | PTE_X);<br>  <span class="hljs-keyword">return</span> ret_pagetable;<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>pvminithart：模仿 kvminithart 设置 satp 寄存器和刷新 TLB</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">pvminithart</span><span class="hljs-params">(<span class="hljs-type">pagetable_t</span> pagetable)</span><br>&#123;<br>  w_satp(MAKE_SATP(pagetable));<br>  sfence_vma();<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>pvmmap：模仿 kvmmap 在传入的进程内核页表中添加新的映射</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">pvmmap</span><span class="hljs-params">(<span class="hljs-type">pagetable_t</span> pagetable, uint64 va, uint64 pa, uint64 sz, <span class="hljs-type">int</span> perm)</span><br>&#123;<br>  <span class="hljs-keyword">if</span>(mappages(pagetable, va, sz, pa, perm) != <span class="hljs-number">0</span>)<br>    panic(<span class="hljs-string">&quot;pvmmap&quot;</span>);<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>freewalk_unmap：释放进程内核页表的时候，我们只需要解除映射并释放掉页表本身占据的空间即可，<strong>不能够释放掉页表项指向的内存空间</strong>。可惜，vm.c 里面并没有支持我们这么干的函数（见上文的表格），所以我仿照 freewalk 写了一个 freewalk_unmap，命名很直观，即在 freewalk 的基础上增加解除映射功能：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">// Free page table and unmap PTE without freeing physical memory pages.</span><br><span class="hljs-type">void</span><br><span class="hljs-title function_">freewalk_unmap</span><span class="hljs-params">(<span class="hljs-type">pagetable_t</span> pagetable)</span><br>&#123;<br>  <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">512</span>; i++)&#123;<br>    <span class="hljs-type">pte_t</span> pte = pagetable[i];<br>    <span class="hljs-keyword">if</span>((pte &amp; PTE_V) &amp;&amp; (pte &amp; (PTE_R|PTE_W|PTE_X)) == <span class="hljs-number">0</span>)&#123;<br>      uint64 child = PTE2PA(pte);<br>      freewalk_unmap((<span class="hljs-type">pagetable_t</span>)child);<br>      pagetable[i] = <span class="hljs-number">0</span>;<br>    &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(pte &amp; PTE_V)&#123;<br>      pagetable[i] = <span class="hljs-number">0</span>;<br>    &#125;<br>  &#125;<br>  kfree((<span class="hljs-type">void</span>*)pagetable);<br>&#125;<br></code></pre></td></tr></table></figure></li></ul><h2 id="simplify-copyincopyinstr">Simplify copyin/copyinstr</h2><p>任务：原本的 copyin 获取指向用户地址的指针，因此需要在用户页表中查找对应的物理地址。现在，我们把每个进程的用户页表加进它的内核页表，这样 copyin 就可以直接将用户指针解引用。这么做的前提是用户虚拟地址不能覆盖掉内核虚拟地址中存放数据和代码的部分，幸运的是，xv6 的用户虚拟地址从 0 开始，而内核从更高的地址开始，所以用户页表可以合并进内核页表。但是，用户虚拟地址不能超过内核最低的虚拟地址，即 PLIC，我们写代码时需要保证这一点。</p><p>首先，修改 copyin 使之直接 return copyin_new，后者 xv6 已经实现好了；copyinstr 同理。</p><p>接下来实现用户页表复制到内核页表的底层操作，我将其取名为 pvmcopy。与 uvmcopy 不同，uvmcopy 复制了物理内存，毕竟父子进程的变量不共享；但是 pvmcopy 只需要复制 PTE。注意，由于内核页表项不能被用户进程访问，所以复制的时候去掉 PTE_U 标志位。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">// Copy content of user page table in range [st, ed) to kernel page table,</span><br><span class="hljs-comment">// and clear content of kernel page table in range [ed, clear_ed).</span><br><span class="hljs-comment">// Only copy PTE, do not copy physical memory.</span><br><span class="hljs-type">int</span><br><span class="hljs-title function_">pvmcopy</span><span class="hljs-params">(<span class="hljs-type">pagetable_t</span> u_pagetable, <span class="hljs-type">pagetable_t</span> k_pagetable, uint64 st, uint64 ed, uint64 clear_ed)</span><br>&#123;<br>  <span class="hljs-comment">// xyf</span><br>  <span class="hljs-keyword">if</span>(st &gt; ed || PGROUNDUP(ed) &gt;= PLIC || PGROUNDUP(clear_ed) &gt;= PLIC)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br><br>  <span class="hljs-comment">// Copy PTE in [st, ed)</span><br>  <span class="hljs-type">pte_t</span> *pte_u, *pte_k;<br>  <span class="hljs-keyword">for</span>(uint64 va = PGROUNDUP(st); va &lt; ed; va += PGSIZE)&#123;<br>    <span class="hljs-keyword">if</span>((pte_u = walk(u_pagetable, va, <span class="hljs-number">0</span>)) == <span class="hljs-number">0</span>)<br>      panic(<span class="hljs-string">&quot;pvmcopy: pte should exist&quot;</span>);<br>    <span class="hljs-keyword">if</span>((*pte_u &amp; PTE_V) == <span class="hljs-number">0</span>)<br>      panic(<span class="hljs-string">&quot;pvmcopy: page not present&quot;</span>);<br><br>    <span class="hljs-keyword">if</span>((pte_k = walk(k_pagetable, va, <span class="hljs-number">1</span>)) == <span class="hljs-number">0</span>)<br>      panic(<span class="hljs-string">&quot;pvmcopy: walk&quot;</span>);<br>    *pte_k = *pte_u &amp; (~PTE_U);<br>  &#125;<br>  <span class="hljs-comment">// Unmap PTE in [ed, clear_ed)</span><br>  <span class="hljs-keyword">for</span>(uint64 va = PGROUNDUP(ed); va &lt; clear_ed; va += PGSIZE)&#123;<br>    <span class="hljs-keyword">if</span>((pte_k = walk(k_pagetable, va, <span class="hljs-number">0</span>)) == <span class="hljs-number">0</span>)<br>      panic(<span class="hljs-string">&quot;pvmcopy: walk&quot;</span>);<br>    *pte_k = <span class="hljs-number">0</span>;<br>  &#125;<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>上面的代码将 u_pagetable 中涉及 [st, ed) 范围的页表项复制给 k_pagetable。另外，有些操作会清除用户页表的一些页表项，所以为了方便我加了一个 clear_ed，当然这个功能可以拆成另一个函数写。</p><p><br></p><p>现在，我们需要找到所有对用户页表进行修改的操作，用 pvmcopy 同步修改内核页表，这样的操作在 fork(), exec() 和 sbrk() 中：</p><ul><li><p>fork</p><p>fork 里面有一处涉及到修改 pagetable 的地方，是将父进程的用户页表（及物理空间）复制给子进程，这之后我们需要把子进程的新的用户页表复制到它的内核页表中：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-keyword">if</span>(pvmcopy(np-&gt;pagetable, np-&gt;k_pagetable, <span class="hljs-number">0</span>, np-&gt;sz, np-&gt;sz) &lt; <span class="hljs-number">0</span>)&#123;<br>  freeproc(np);<br>  release(&amp;np-&gt;lock);<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>exec</p><p>exec 会用新的进程<strong>彻底覆盖</strong>掉原来的进程，所以我们也应该用新的用户页表<strong>彻底替换</strong>掉原来的用户页表——彻底替换不仅要求复制新的，还要求清除旧的：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-keyword">if</span>(pvmcopy(pagetable, p-&gt;k_pagetable, <span class="hljs-number">0</span>, sz, oldsz) &lt; <span class="hljs-number">0</span>)<br>  <span class="hljs-keyword">goto</span> bad;<br></code></pre></td></tr></table></figure></li><li><p>sbrk</p><p>sbrk 其实就是 growproc，会把进程的用户空间从 sz 大小变成 sz+n 大小。如果 n&gt;0，用户空间增加，我们需要把增加的这一部分复制到内核页表中；如果 n&lt;0，用户空间减少，我们需要把减少的部分从内核页表中清除：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-keyword">if</span>(n &gt; <span class="hljs-number">0</span>)&#123;<br>  <span class="hljs-keyword">if</span>((sz = uvmalloc(p-&gt;pagetable, sz, sz + n)) == <span class="hljs-number">0</span>) &#123;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  &#125;<br>  <span class="hljs-keyword">if</span>(pvmcopy(p-&gt;pagetable, p-&gt;k_pagetable, p-&gt;sz, sz, sz) &lt; <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(n &lt; <span class="hljs-number">0</span>)&#123;<br>  sz = uvmdealloc(p-&gt;pagetable, sz, sz + n);<br>  <span class="hljs-keyword">if</span>(pvmcopy(p-&gt;pagetable, p-&gt;k_pagetable, sz, sz, p-&gt;sz) &lt; <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>&#125;<br></code></pre></td></tr></table></figure></li></ul><p><br></p><p>最后，别忘了在 userinit 中复制一份初始用户页表！（我就忘了，调了半天总是不对，吐血……）</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">// include user page table in kernel page table</span><br>pvmcopy(p-&gt;pagetable, p-&gt;k_pagetable, <span class="hljs-number">0</span>, p-&gt;sz, p-&gt;sz);<br></code></pre></td></tr></table></figure><p>然后这次实验就<s>愉快地</s>结束了。</p><p><br></p><p>make grade 结果截图：</p><p><img src="result.png" /></p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>操作系统</category>
      
      <category>xv6-lab</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[xv6-mit-6.S081-2020]Lab2: syscall</title>
    <link href="/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab2-syscall/"/>
    <url>/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab2-syscall/</url>
    
    <content type="html"><![CDATA[<h1 id="lab-system-calls">Lab: system calls</h1><p>https://pdos.csail.mit.edu/6.S081/2020/labs/syscall.html</p><p>代码：https://github.com/xyfJASON/xv6-mit-6.S081-2020/tree/syscall</p><span id="more"></span><p><br></p><h2 id="system-call-tracing">System call tracing</h2><p>任务：添加一个系统调用 trace，它接受一个整型参数 mask，mask 是一个二进制掩码，其每一位代表是否跟踪那一位表示的系统调用。例如 fork 是 1 号，那么 mask 的第 1 位就表示是否跟踪 fork。每一种系统调用的编号定义在 kernel/syscall.h 中。如果某一种系统调用被跟踪了，那么在它将要返回的时候，输出一行 <code>&lt;pid&gt;: systemcall &lt;syscall name&gt; -&gt; &lt;return value&gt;</code>。trace 能跟踪的进程包括调用它的那个进程，以及该进程所 fork 的子进程、子进程所 fork 的子子进程……</p><p>xv6 已经实现了一个 trace.c 用户程序，如果我们添加好了 trace 系统调用，那么这个用户程序就能正常的执行。（回顾一下，在上一个实验中，我们需要写一个<strong>用户程序 sleep</strong>，它调用<strong>系统调用 sleep</strong>；这个实验正好反过来，我们需要写一个<strong>系统调用 trace</strong>，供<strong>用户程序 trace</strong> 调用。）</p><p>现在我们需要好好研究一下 xv6 系统（基于 risc-v）究竟是如何完成系统调用的。【理了半天只总结了下面这些，如果有错误还请指出】</p><p>假设我们调用 sleep 系统调用，那么下面的事情将会依次发生：</p><ol type="1"><li>user/usys.S 中对应的汇编代码段将会得到执行：<strong>它会将对应的编号</strong>（sleep 是 13 号，见 kernel/syscall.h）<strong>传入 a7 寄存器</strong>，随后执行 ecall 汇编指令（见 user/usys.S）使得进程陷入内核态（supervised mode）；</li><li>陷入内核态时，kernel/trampoline.S 中 uservec 段的汇编代码将会得到执行。这一段代码的作用是：在 TRAPFRAME 这一段内存中保存用户的所有寄存器（<strong>因此 a7 寄存器中的系统编号 13 现在被存入了 TRAPFRAME</strong>），然后恢复一些内核所必要的寄存器，例如内核栈指针、hartid、内核页表的首地址……，最后调用 usertrap()；</li><li>usertrap() 函数（见 kernel/trap.c）处理所有用户空间导致的 trap，一共有三种可能——系统调用、设备引起的中断、异常。这次我们只需要关注系统调用那一个 if 代码块。它会把返回地址设为当前 pc 加 4，这样就能返回到 ecall 的下一条指令；最后它会调用 syscall()；</li><li>syscall() 就是一个高层封装，它首先获取系统调用编号 13（第 2 步已经存储到了 p-&gt;trapframe-&gt;a7 之中，p-&gt;trapframe 指向 TRAPFRAME 的物理地址），然后找到对应系统调用的处理函数 sys_sleep()；</li><li>sys_sleep()（见 kernel/sysproc.c）还是一层封装……它会从 p-&gt;trapframe-&gt;a0 中捞出 sleep 的参数，然后终于真真正正地跑确实让进程等待的 sleep 函数了（见 kernel/proc.c），最后把返回值放进 p-&gt;trapframe-&gt;a0 之中；</li><li>返回时，首先执行 usertrapret()（见 kernel/trap.c），然后执行 trampoline.S 中的 usenet 段汇编代码，此处不再赘述。</li></ol><p>一图以蔽之：</p><p><img src="path.png" /></p><p>好了，现在要实现 trace 系统调用，怎么办呢？</p><ol type="1"><li><p>最核心的一点是要意识到，trace 的功能是针对进程而言的，也就是说，trace 可以看成给进程打的一个标签，甚至可以看成进程具有的一个属性。所以，我们可以在 proc 结构体（kernel/proc.h），也就是 pcb 中直接加上 mask，表示当前进程的哪些系统调用被跟踪了：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">// Per-process state</span><br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">proc</span> &#123;</span><br>  ...<br>  <span class="hljs-type">int</span> mask;                    <span class="hljs-comment">// Trace mask</span><br>  ...<br>&#125;;<br></code></pre></td></tr></table></figure></li><li><p>然后由于 trace 对子进程具有传递性，所以每次 fork 子进程的时候，都要把这个 mask “标签”传下去，这在 fork 的具体实现（kernel/proc.c）中参照其他信息的复制方式加一行：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">int</span><br><span class="hljs-title function_">fork</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>  <span class="hljs-comment">// copy trace mask.</span><br>  np-&gt;mask = p-&gt;mask;<br>  ...<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>那 mask 在哪里被赋值的呢？获得参数的地方。在哪里获得参数？sys_trace 里面。所以参照其他调用写一份 sys_trace，把捞出来的参数赋给 mask：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c">uint64<br><span class="hljs-title function_">sys_trace</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  <span class="hljs-type">int</span> mask;<br>  <span class="hljs-keyword">if</span>(argint(<span class="hljs-number">0</span>, &amp;mask) &lt; <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  myproc()-&gt;mask = mask;<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>别忘了，trace 是要输出的。由上文可知，内核中处理系统调用的最高层封装是 syscall，在这一层我们已经可以知道是什么系统调用、系统调用的返回值是什么，这对 trace 的输出已经足够了。所以在 syscall 返回前，我们判断当前系统调用是否被 mask 标记了，如果是，则输出：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">void</span><br><span class="hljs-title function_">syscall</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  ...<br>  <span class="hljs-type">char</span> *syscall_names[NELEM(syscalls)+<span class="hljs-number">1</span>] = &#123;<br>  <span class="hljs-string">&quot;&quot;</span>,<br>  <span class="hljs-string">&quot;fork&quot;</span>,<br>  <span class="hljs-string">&quot;exit&quot;</span>,<br>  ...<br>  &#125;;<br>  <span class="hljs-comment">// if this proc is traced, print info</span><br>  <span class="hljs-keyword">if</span>((p-&gt;mask &gt;&gt; num) &amp; <span class="hljs-number">1</span>)<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d: syscall %s -&gt; %d\n&quot;</span>, p-&gt;pid, syscall_names[num], p-&gt;trapframe-&gt;a0);<br></code></pre></td></tr></table></figure></li><li><p>剩下的工作就简单了，只是把流程连起来而已，参照其他调用在 kernel/syscall.h、user/user.h、user/usys.pl（用于生成 user/usys.S 的脚本）里面加上 trace 相关部分即可。</p></li></ol><h2 id="sysinfo">Sysinfo</h2><p>任务：添加一个系统调用 sysinfo 用于收集系统信息，它接受一个指向 struct sysinfo（见 kernel/sysinfo.h）的指针，内核需要填上这个结构体的两个元素：freemem 表示空闲内存的字节数，nproc 表示 state 不是 UNUSED 的进程的数量。</p><p>在做了第一个任务之后，这个任务显得简单了许多，不过在码之前，先理一下 kernel 中各个文件之间的关系，因为着实有些混乱，如下图所示（以 fork 和 sleep 为例）：</p><figure><img src="rel.png" alt="kernel各文件关系" /><figcaption aria-hidden="true">kernel各文件关系</figcaption></figure><ol type="1"><li><p>柿子先挑软的捏，我们先把系统调用这条路打通，和之前一样，参照其他调用在 user/user.h、user/usys.pl、kernel/syscall.h、kernel/syscall.c 中补上 sysinfo；这一步完成了就能够正常编译了；</p></li><li><p>接下来我们在 kernel/sysproc.c 中实现一个 sys_sysinfo() 函数，这个函数获取指向 struct sysinfo 的指针，求出当前系统空闲内存、非 UNUSED 进程数量，存进这个指针指向的结构体中。看起来我们只需要实现两个底层功能——countfreebytes() 和 countproc()……似乎很完美？但是事情没有这么简单！问题在这个指针上，指针是我们传入的参数，指向的是用户空间的虚拟内存，但是 xv6 系统内核中的页表和用户空间中的页表不一样，所以不能直接用这个指针！（事实上，xv6 内核的虚拟内存直接映射到物理内存，但用户空间中虚拟内存是从 0 开始的）。为了解决这个问题，我们需要使用 copyout() 函数，可以参看 sys_fstat()（kernel/sysfile.c）和 filestat()（kernel/file.c）的实现。综上，我们的 sysproc.c 长这样：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs c">uint64<br><span class="hljs-title function_">sys_sysinfo</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">proc</span> *<span class="hljs-title">p</span> =</span> myproc();<br>  uint64 ptr; <span class="hljs-comment">// pointer to struct sysinfo</span><br>  <span class="hljs-keyword">if</span>(argaddr(<span class="hljs-number">0</span>, &amp;ptr) &lt; <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">sysinfo</span> <span class="hljs-title">si</span>;</span><br>  si.freemem = countfreebytes();<br>  si.nproc = countproc();<br>  <span class="hljs-keyword">if</span>(copyout(p-&gt;pagetable, ptr, (<span class="hljs-type">char</span> *)&amp;si, <span class="hljs-keyword">sizeof</span>(si)) &lt; <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>现在实现 countfreebytes()，指导网页提示我们在 kernel/kalloc.c 中实现它。阅读代码可以知道，一页有 4KB (PGSIZE)，空闲的页首构成一个链表 kmem.freelist，所以要求出空闲内存的字节数，数一数链表有多长即可：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">// Count number of bytes of free memory</span><br><span class="hljs-type">int</span><br><span class="hljs-title function_">countfreebytes</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  <span class="hljs-type">int</span> cnt = <span class="hljs-number">0</span>;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">run</span> *<span class="hljs-title">r</span> =</span> kmem.freelist;<br>  <span class="hljs-keyword">for</span>(; r; r = r-&gt;next)<br>    cnt += PGSIZE;<br>  <span class="hljs-keyword">return</span> cnt;<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>最后实现 countproc()，指导网页提示我们在 kernel/proc.c 中实现它。阅读代码并且参照其他函数，知道我们可以用一个 for 循环遍历所有进程，数一数这里面有多少个不是 UNUSED 即可：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-comment">// Count number of processes whose state is not UNUSED.</span><br><span class="hljs-type">int</span><br><span class="hljs-title function_">countproc</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  <span class="hljs-type">int</span> cnt = <span class="hljs-number">0</span>;<br>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">proc</span> *<span class="hljs-title">p</span>;</span><br>  <span class="hljs-keyword">for</span>(p = proc; p &lt; &amp;proc[NPROC]; p++)&#123;<br>    acquire(&amp;p-&gt;lock);<br>    cnt += (p-&gt;state != UNUSED);<br>    release(&amp;p-&gt;lock);<br>  &#125;<br>  <span class="hljs-keyword">return</span> cnt;<br>&#125;<br></code></pre></td></tr></table></figure></li></ol><p>make grade 截图：</p><p><img src="result.png" /></p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>操作系统</category>
      
      <category>xv6-lab</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[xv6-mit-6.S081-2020]Lab1: util</title>
    <link href="/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab1-util/"/>
    <url>/blog-main/2021/11/30/xv6-mit-6-S081-2020-Lab1-util/</url>
    
    <content type="html"><![CDATA[<h1 id="lab-xv6-and-unix-utilities">Lab: Xv6 and Unix utilities</h1><p>https://pdos.csail.mit.edu/6.S081/2020/labs/util.html</p><p>代码：https://github.com/xyfJASON/xv6-mit-6.S081-2020/tree/util</p><span id="more"></span><p><br></p><h2 id="sleep">sleep</h2><p>任务：暂停指定 ticks，1 个 tick 由 xv6 内核定义。</p><p>这个任务就是用来让我们熟悉 xv6 的运作方式的。user 目录下是用户程序，kernel 目录下是内核程序，本次实验只需要添加用户程序，不涉及内核。在写好一个用户程序（如 sleep.c）之后，修改 Makefile 文件，这样在编译之后 xv6 就能得到 sleep 可执行文件（类似于 linux 下 /bin/ 中的内容），从而可以通过 shell 运行它。</p><p>xv6 系统提供了系统调用 sleep()，因而我们的 sleep.c 用户程序只需调用它。为了避免混乱，在此说明一下：当我们给 shell 输入 sleep 命令时，实际上是在执行我们写的 <strong>sleep 用户程序</strong>，而这个用户程序通过 <strong>sleep 系统调用</strong>执行了内核的相关代码，完成了暂停的功能。至于系统调用的过程中发生了什么，就是下一次实验要考虑的内容了。</p><h2 id="pingpong">pingpong</h2><p>任务：父进程将“ping”写入管道，子进程从管道将其读出并打印。子进程从父进程收到字符串后，将“pong”写入另一个管道，然后由父进程从该管道读取并打印。</p><p>这个任务主要是学习使用管道的方法。我认为重点是要正确认识到文件描述符是什么，在此基础上管道就不难理解了。关于文件描述符的资料很多，在此不赘述。</p><p>最后记住不用的管道一定要 close 掉，否则读完了还在读、或写满了还在写就会一直被阻塞。</p><h2 id="primes">primes</h2><p>任务：使用管道实现质数筛选，输出2~35之间的所有质数。</p><p>筛选思路：主进程将所有数据输入到管道的左侧，第一个子进程从管道读出并筛选出2，排除掉2的倍数，其他数字再写入下一管道；第二个子进程读出并筛选出3，排除掉3的倍数，其他数字写入到下一管道；第三个子进程读出筛选出5，以此类推……</p><p><img src="https://hitsz-lab.gitee.io/os-labs-2021/lab1/part2.assets/image-20201017231043674.png" /></p><p>这是这次实验中最难的内容。不难想到可以用递归实现，怎样设计这个递归是一个难点。我的实现如下：</p><p>从原理图可以看见，整个过程可以分为若干个阶段：筛 2 阶段、筛 3 阶段、筛 5 阶段、……每一个阶段都会从上一个阶段读入若干待处理数字，认定第一个读入的数字为素数，并筛去它的倍数。我把每一个阶段都交给一个进程负责。因此，每一层递归我们要做的事情就有：创建向下一个阶段传输的管道 p，然后 fork，父进程从前一个管道（<strong>递归时将管道文件描述符作为参数</strong>）读入数字，如果当前数字不被筛去，则输出到 p；子进程递归这个过程。如果从前一个管道读入的时候发现写端（它的父进程）已经关闭了，那么 read 将返回 0，我们以此作为递归的终止条件。</p><p>需要注意的是，xv6 系统文件描述符有限，因此创建管道有数量的限制。素数筛到 31 已经是上限，如果发现程序筛到 29 还没问题，筛到 31 就出错，那需要好好考虑一下有没有不用的管道没关闭和递归的终止条件。</p><h2 id="find">find</h2><p>任务：在目录树中查找名称与字符串匹配的所有文件，输出文件的相对路径。</p><p>这个任务需要参考 ls.c 的实现，读懂 ls.c 的逻辑，不需要一路回看到源码，根据变量名和上下文猜就行了。然后在 ls.c 的基础上略作更改，在搜索到文件时改成判断是否与要找的文件名相同，相同则输出 path；在搜索到目录时改成递归 find 即可。</p><h2 id="xargs">xargs</h2><p>任务：从标准输入中读取一些行，对于每一行，将该行作为参数运行一次指定的命令。</p><p>这个任务最难的点在于搞懂 xargs 的工作方式。xargs 后接一个命令，然后从标准输入读入若干行参数，每读一行，就用这一行的参数运行一次命令。</p><p>xargs 一般用作管道的读端，这样在管道的写端运行一个命令，视命令的输出为参数，写入管道，然后 xargs 读入这些参数。例如 echo hello | xargs echo bye，管道的写端执行命令 echo hello，于是将 "hello" 写入管道，xargs 读入 "hello" 作为 echo bye 的参数，于是命令变成了 echo bye hello，因此这条命令的执行结果是 "bye hello"。</p><p>实现的重点主要是字符串的处理，要求把输入的字符串转化成 *argv[] 的形式，即一个字符串数组；然后创建一个子进程执行命令，注意 exec() 的第一个参数为执行的命令名称，第二个参数为参数列表且以 0 结尾；父进程等待子进程执行结束后继续。</p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>操作系统</category>
      
      <category>xv6-lab</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[Image Inpainting]基于CNN的十个经典模型</title>
    <link href="/blog-main/2021/10/18/Image-Inpainting-%E5%9F%BA%E4%BA%8ECNN%E7%9A%84%E5%8D%81%E4%B8%AA%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/"/>
    <url>/blog-main/2021/10/18/Image-Inpainting-%E5%9F%BA%E4%BA%8ECNN%E7%9A%84%E5%8D%81%E4%B8%AA%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="image-inpainting">Image Inpainting</h2><blockquote><p>碎碎念【2021.09.27】</p><p>今天终于鼓起勇气找导师寻求科研项目了。表达了对生成类模型的偏好之后，导师说他有一个博士生在做 blind image inpainting 的工作，我可以跟着做一下。好吧，全新的征程就此开启！</p><p>这之前我已经读了 CV 各个方向的经典模型论文，但是还真没接触过 image inpainting 这块。Image inpainting 说来简单，就是图像填充，在一张图像上放上 mask（可能不规则），要求模型把图像恢复出来。这玩意儿已经做了好几年了，但是有个问题，就是 mask 都是给定的，模型明确地知道哪些地方是需要填充的，哪些地方不是。这就和实际情况不太符合了，我们拿到一张被污染的图片想恢复它，才不会花时间把污染区域标出来呢！所以没有 mask 的 image inpainting，就叫做 blind image inpainting 了，就是我拿到的课题。Blind image inpainting 目前做的人还不多，最近的文章是 ECCV2020 港中文的 VCNet<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Wang, Yi, Ying-Cong Chen, Xin Tao, and Jiaya Jia. VCNet: A Robust Approach to Blind Image Inpainting. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXV 16*, pp. 752-768. Springer International Publishing, 2020.">[1]</span></a></sup>。Alright，那就开搞呗！</p></blockquote><p>本文主要关注于基于卷积网络的方法，它们大都发表在 2019 年之前。从 2020 年开始，随着 Transformer 进军 CV 届，大家都开始用 Transformer 做填充了，有关工作放在以后的文章总结。</p><p>首先说明几个术语。输入的有缺损的图像一般称为 corrupted / masked / deteriorated image，其中缺失的像素叫做 invalid / missing / hole pixels，没有缺失的叫做 valid / remaining / ground truth pixels，一般用一个 bool mask 来指示缺失像素位置。值得注意的是，大家默认 <strong>valid pixels 可以直接 copy 到模型的输出结果去</strong>，也会在测试的时候这么干，所以别傻乎乎地直接把输出拿去评测，白白亏几个点。</p><p>Image inpainting 有几个特点：</p><ul><li><p>Ill-posed，即如果缺失区域够大的话，理论上多种填充方式都是可行的。也因此，这个任务很难有好的评价指标，因为对比 ground truth 的指标，如 L1、MSE、PSNR、SSIM 等，必然对多样性填充不利。</p></li><li><p>全局信息很重要，因为缺损的地方不一定规则，可能好似涂鸦一般满图都是，如下图：</p><p><img src="https://miro.medium.com/max/1400/1*-FA14AJWebhtYg3RlHlNrQ.png" alt="https://miro.medium.com/max/1400/1*-FA14AJWebhtYg3RlHlNrQ.png" width=50% /></p><p>很明显，任何一个局部都有大量缺失像素，不利于我们填充。</p></li><li><p>高层次语义 &amp; 低层次纹理：一方面，我们希望模型学到高层次的特征，不能乱填，要保持语义的一致性；另一方面，模型也要学到低层次的纹理，保证局部真实性。</p></li></ul><p>填充图像最简单传统的方式是 copy-and-paste，核心思想是从图片本身的 remaining pixels 或者一个大数据集中找到最相似的 image patch 粘到缺失的地方。显然这个方法计算量大，涉及人工设计的距离指标，效果也不太行。但都 2021 年了，我们肯定用深度学习来搞啊！在资料<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Chu-Tak Li. “10 Papers You Must Read for Deep Image Inpainting”. https://towardsdatascience.com/10-papers-you-must-read-for-deep-image-inpainting-2e41c589ced0">[2]</span></a></sup>中，作者总结了 10 个必须知道的模型，下面我们来依次看看。</p><h2 id="context-encoder">Context Encoder</h2><p>2016 年的 <strong>Context Encoder</strong><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros, “[Context Encoders: Feature Learning by Inpainting](https://arxiv.org/abs/1604.07379),” *Proc. International Conference on Computer Vision and Pattern Recognition* (*CVPR*), 2016.">[3]</span></a></sup> 是第一个 GAN-based inpainting model，可谓是开山之作，也是这个任务的 baseline。作者的研究意图是：</p><ul><li><p>我们都知道 CNN 能提取图像特征，前面的层提取简单的低层次特征，后面的层提取复杂的高层次语义特征，也可以叫做 latent feature。作者想用这个高层次语义特征帮助还原缺失部分。</p></li><li><p>做 Inpainting 学到的特征能够深度刻画出原图的语义，因而对其他任务也有帮助。</p><blockquote><p>upd：没想到，2021 年何恺明提出 MAE 做自监督学习，其思想其实在 Context Encoder 中已经有所体现了。</p></blockquote></li></ul><p>Context Encoder 奠定了图像填充模型的整体框架——<strong>encoder-decoder 框架</strong>，即 encoder 提取 feature，decoder 重建图像。具体而言，encoder 用的 AlexNet（毕竟那年代没什么可用），decoder 基本就是 encoder 的对称结构，用 transposed convolution 上采样。</p><p>然后接下来是本模型的核心贡献：正如第一节所言，image inpainting 任务需要关联远处的空间信息（即全局视野），这是卷积不能做到的。那什么能做到呢？全连接（毕竟那年代只能想到全连接了）！可是全连接有个显著的问题：参数量太大了……<span class="math inline">\(CHW=512\times4\times 4\)</span> 的 feature map，全连接一波，就是 <span class="math inline">\((CHW)^2=67.1\times 10^6\)</span> 的参数量。所以作者提出了 <strong>Channel-wise Fully Connected Layer</strong>。这名字看起来好长一串，实际上很简单，就是每个 channel 独立地做全连接，这样 <span class="math inline">\(CHW\)</span> 的 feature map 只用 <span class="math inline">\(CH^2W^2\)</span> 的参数量就 ok。另外作者还在 channel-wise fc 之后接了一个 conv1x1 来让信息跨 channels 流动，就好似 depthwise separable conv 一样。用全连接引入全局信息实属时代限制，后面的工作会用 dilated conv、non-local、transformer 等更好的方法。</p><p><img src="https://miro.medium.com/max/1400/0*YrX17vrxzJtjXUcE.png" alt="https://miro.medium.com/max/1400/0*YrX17vrxzJtjXUcE.png" style="width:90%;" /></p><p>模型结构如上图所示，encoder 将输入图像从 128x128 下采样到 4x4，经过 channel-wise fc 获取全局信息，然后用 decoder 生成了 64x64 的中心缺失区域。生成的图像和真实的缺失图像送给判别器判别真假。</p><p>Loss 分成两部分：重构的 L2 loss 和 adversarial loss： <span class="math display">\[\begin{align}&amp;\mathcal L_{rec}(x)=||\hat M\odot(x-F((1-\hat M)\odot x))||_2^2\\&amp;\mathcal L_{adv}=\max_D\mathbb E_{x\in\mathcal X}[\log D(x)+\log(1-D(F((1-\hat M)\odot x)))]\\&amp;\mathcal L=\lambda_{rec}\mathcal L_{rec}+\lambda_{adv}\mathcal L_{adv}\end{align}\]</span> 如果只用 <span class="math inline">\(\mathcal L_{rec}\)</span> 的 L2 loss，结果会很模糊。<strong>这是因为 L2 倾向于给出较为平均化的结果，这样平均误差小并且比较“安全”。</strong>如果只用 <span class="math inline">\(\mathcal L_{adv}\)</span>，结果会很突兀，跟随便 copy 了一块差不多……</p><p>从 GAN 的角度看，整个 encoder-decoder 是一个<strong>加了 context 作为 condition</strong> 的生成器，但是没有给判别器加 condition。据作者所言，给判别器加 condition（即输入整张图像）以后太容易判别真假了，导致生成器的梯度消失难以训练。但不加 condition 会产生一个问题：由于判别器只能看到缺失区域而不是整张图像，<span class="math inline">\(\mathcal L_{adv}\)</span> 无法保证填充的语义是否正确，全局语义的一致性仅靠 <span class="math inline">\(\mathcal L_{rec}\)</span> 保证。这大概是为什么仅用 <span class="math inline">\(\mathcal L_{adv}\)</span> 结果很突兀的原因吧。</p><h2 id="msnps">MSNPS</h2><p><strong>MSNPS</strong> (Multi-Scale Neural Patch Synthesis)<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang, and Hao Li, “[High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis](https://arxiv.org/pdf/1611.09969.pdf),” *Proc. International Conference on Computer Vision and Pattern Recognition* (*CVPR*), 2017.">[4]</span></a></sup> 是 Context Encoder 的加强版，也是 2016 年的工作。作者研究的动机是：</p><ul><li>囿于内存限制和训练的困难，现有方法只能处理低分辨率情形</li><li>Context Encoder 明显还有改进空间——特别是 texture details 还需改进</li></ul><p>作者认为，textures and colors 可以看做是 styles，于是应用了 <strong>style transfer</strong> algorithm 来加强 Context Encoder 生成的图像的纹理细节。具体解决方法是：</p><ul><li>使用 Context Encoder 预测缺失部分</li><li>对 predicted pixels 和 valid pixels 使用 style transfer 方法，将 valid pixels 的 style 迁移到 predicted pixels 中</li><li>本文假定测试图像均是 512x512 的，中间有一个 256x256 的缺失矩形区域。流程分为<strong>三个阶段</strong>：输入首先被 resize 到 128x128（缺失区域 64x64），然后通过 Context Encoder 得到低分辨率的重构图像；接下来 upsample 成 256x256（缺失区域是<strong>粗糙</strong>填充的 128x128），二次重构图像；最后 upsample 成原大小并第三次重构图像（refinement）。</li></ul><p><img src="https://miro.medium.com/max/1400/0*apttmL1xdCRq7tKC.png" alt="https://miro.medium.com/max/1400/0*apttmL1xdCRq7tKC.png" style="width:90%;" /></p><p>MSNPS 整体结构如上图(bottom) 所示，Content Network 是一个略微修改的 Context Encoder，详细结构如上图(top) 所示；Texture Network 是一个 ImageNet 预训练的 VGG-19；本文的主要 insight 应该是 loss function 的设计。</p><p>Content Network 与 Context Encoder 的不同之处有两点：1. channel-wise fc 被替换成标准的 fc；2. 所有 ReLU 或者 Leaky ReLU 都被替换成 ELU。作者先按照训练 Context Encoder 的方法（L2 loss + Adversarial loss）独立训练 Content Network，训练好后其输出将被用来优化整个结构。</p><p>Texture Network 是和 Style Tranfer 相关的网络，其目的是保证 generated pixels 和 valid pixels 有着相似的 style。作者用了这样的观点：CNN 不同层次的 feature maps 能够反映图像的 style，如果两个图像在网络中的 feature maps 相似，那么它们有着相似的 style（说实话，这观点是一个 over-simplified claim）。怎么比较 feature maps 的相似度呢？用 Gram matrix。作者在 relu3_1 和 relu4_1 层强制缺失区域内的 feature maps 和缺失区域外的 feature maps 是相似的，以此试图达到相似 style 的目的。</p><p>Loss function 由 3 部分加权组成：content loss (L2), texture loss, TV loss (total variation loss)：</p><ul><li><p>Content loss: <span class="math display">\[E_c(h(x, R), h(x_i, R)) = ||h(x, R)-h(x_i, R)||_2^2\]</span> <span class="math inline">\(x\)</span> 是真实图像，<span class="math inline">\(x_i\)</span> 是第 <span class="math inline">\(i\)</span> 个阶段的输出图像，<span class="math inline">\(h(x_i, R)\)</span> 指 <span class="math inline">\(x_i\)</span> 在缺失区域 <span class="math inline">\(R\)</span> 内的部分。这就是 Context Encoder 中的 <span class="math inline">\(\mathcal L_{rec}\)</span>。</p></li><li><p>Texture loss: <span class="math display">\[E_t(\phi_t(x),R)=\frac{1}{|R^\phi|}\sum_{i\in R^\phi}||h(\phi_t(x), P_i)-h(\phi_t(x), P_{nn(i)})||_2^2\]</span> <span class="math inline">\(R^\phi\)</span> 是 feature maps 上对应缺失区域的部分，<span class="math inline">\(\phi_t(x)\)</span> 是 Texture Network 在输入 <span class="math inline">\(x\)</span> 时的输出，<span class="math inline">\(|R^\phi|\)</span> 指在缺失区域中抽样 patch 的数量，<span class="math inline">\(P_i\)</span> 是中心在 <span class="math inline">\(i\)</span> 处的 patch，<span class="math inline">\(nn(i)\)</span> 是最近邻位置： <span class="math display">\[nn(i)=\mathop{\arg\min}_{j\in\mathcal N(i)\wedge j\not\in R^{\phi}}||h(\phi_t(x),P_i)-h(\phi_t(x), P_j)||_2^2\]</span> 公式看起来很复杂，但其实很简单。首先把图像输入预训练的 VGG-19，得到 feature maps relu3_1 和 relu4_1。然后把 feature maps 分成对应缺失区域的部分 <span class="math inline">\(R^\phi\in\mathbb R^{c\times s\times s}\)</span> 和缺失区域外的部分。然后对于每个 patch，找到最相近的有效 patch，计算 2 范数。</p></li><li><p>TV loss： <span class="math display">\[\gamma(x)=\sum_{i,j}((x_{i, j+1}-x_{i, j})^2+(x_{i+1, j}-x_{i,j})^2)\]</span> 这是图像处理中常用的保证图像光滑性的玩意儿。</p></li></ul><p>该模型的缺点在于：1. 复杂场景依旧很难填充；2. 多阶段导致速度慢，且训练不是 end-to-end.</p><p>这篇论文还是有很多贡献的：它可以视为 <strong>coarse-to-fine</strong> 的前身，首先重建一个有一定 pixel-wise reconstruction accuracy 的模糊图像，然后 refine texture details；这篇文章里的 Texture loss 的思想也影响了后续工作的 Perceptual Loss / Style Loss 等。</p><h2 id="glcic">GLCIC</h2><p><strong>GLCIC</strong> (Globally and Locally Consistent Image Completion)<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa, “[Globally and Locally Consistent Image Completion](http://iizuka.cs.tsukuba.ac.jp/projects/completion/data/completion_sig2017.pdf),” *ACM Trans. on Graphics*, Vol. 36, №4, Article 107, Publication date: July 2017.">[5]</span></a></sup> 是早稻田大学在 2017 年的工作，是 Image Inpainting 的里程碑式成果。</p><p>作者提出 GLCIC 的动机是：</p><ul><li><p>Patch-based methods 有一个前提假设，那就是我们能从缺失区域外找到相似于填充内容的东西，这对于自然风景图片一般可行，但是对于诸如把人眼遮住的图像就不可行了。我们的模型需要具备生成 novel fragments 的能力；</p></li><li><p>现有 GAN-based 方法（前两种模型）用 discriminator 来加强填充区域的细节，有些方法（e.g. MSNPS）比较缺失区域内外的 neural responses 来保证纹理的一致性。如果我们同时考虑图像的局部和全局信息来强制得到 local consistency 和 global consistency 会怎样？</p></li><li><p>怎么处理高分辨率图像？MSNPS 的解决方案是反复迭代处理，这导致填充 1 张 512x512 的图片在 Titan X GPU 上需要 1min。我们能否只用 one single forward pass 就搞定？——要回答这个问题，我们要知道导致这个问题的原因——计算量太大！为什么计算量大？全连接！那怎么解决？全卷积！可是卷积不能引入远处的信息？Dilated Convolution!</p></li></ul><p>简单来说，本文提出的解决方案是：</p><ul><li><strong>Fully Convolutional Network with Dilated Convolution</strong>：引入 Dilated Convolution 代替 fc 层，这样既能够收集距离较远的信息（扩大了 receptive field），又能够保证全卷积以适应不同大小的图像输入</li><li>用两个 discriminator，一个看整张图像保证 global consistency，另一个看填充区域周围的局部保证 local consistency</li><li>最后引入了简单的后处理过程：Fast Marching method 和 Poisson image blending</li></ul><p><img src="https://miro.medium.com/max/1400/0*0jeG453RYjKsjjs0.png" alt="https://miro.medium.com/max/1400/0*0jeG453RYjKsjjs0.png" style="width:90%;" /></p><p>上图是整个网络的架构，由三部分组成：Completion Network (generator), Local Discriminator, Global Discriminator。两个 Discriminator 只在 train 阶段使用，不在 test 阶段使用。</p><p>Completion Network 是一个 FCN 全卷积网络，因此可以接受各种大小的图像，注意上图中输出图像和输入图像大小都是 <span class="math inline">\(H\times W\)</span>。</p><p>两个 Discriminator 架构基本相同，global 输入大小为 256x256，local 输入大小为 128x128。需要强调的是，<strong>训练阶段只有一个缺失区域，测试阶段却可能有很多缺失区域</strong>。另外对于 local discriminator 而言，训练时我们需要给它真实的图像和生成的图像，这里真实的图像是<strong>随机选取</strong>的 128x128 部分。</p><p>Loss function 和之前的一样，由两部分组成：L2 loss &amp; GAN loss <span class="math display">\[\begin{align}&amp;L(x,M_c)=||M_c\odot(C(x, M_c)-x)||^2\\&amp;\min_C\max_D\mathbb E[\log D(x, M_d)+\log(1-D(C(x, M_c), M_c))]\end{align}\]</span> <span class="math inline">\(C(x, M_c)\)</span> 指代 Completion Network，<span class="math inline">\(x\)</span> 是输入图像，<span class="math inline">\(M_c\)</span> 是 binary mask（1 缺失，0 valid）；L2 loss 只在缺失区域内计算，<strong>缺失区域外直接用原本的像素点覆盖</strong>。<span class="math inline">\(D(x,M_d)\)</span> 指代两个 Discriminator，<span class="math inline">\(M_d\)</span> 是针对 local discriminator 的随机 mask。二者加权和即是 joint loss： <span class="math display">\[\min_C\max_D\mathbb E[L(x, M_c)+\alpha\log D(x, M_d)+\alpha\log(1-D(C(x, M_c), M_c))]\]</span> <img src="https://miro.medium.com/max/1400/1*v_o4tmsJANoN4hQxtMrT_Q.png" alt="https://miro.medium.com/max/1400/1*v_o4tmsJANoN4hQxtMrT_Q.png" style="width:50%;" /></p><p>训练流程分为 3 步：</p><ol type="1"><li>只用 L2 loss 训练 completion network <span class="math inline">\(T_C\)</span> 轮；</li><li>训练 discriminators <span class="math inline">\(T_D\)</span> 轮；</li><li>交替训练 completion network 和 discriminators 若干轮，这次用 joint loss。</li></ol><p>先让 completion network 生成模糊但差不离的结果（参见 Context Encoder 的结果和解释），然后训练 discriminators 让它们跟上 completion network 的进度，最后交替训练细化生成结果。</p><h2 id="pggan">PGGAN</h2><p><strong>PGGAN</strong> (Patch-based Image Inpainting with GANs)<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ugur Demir, and Gozde Unal, “[Patch-Based Image Inpainting with Generative Adversarial Networks](https://arxiv.org/pdf/1803.07422.pdf),” https://arxiv.org/pdf/1803.07422.pdf.">[6]</span></a></sup> 是 2018 年提出的，可以看做 GLCIC 的变体。作者的动机是引入 residual connection 和 PatchGAN<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros, “[Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/pdf/1611.07004.pdf）),” *Proc. Computer Vision and Pattern Recognition* (*CVPR*), 21–26 Jul. 2017.">[7]</span></a></sup>。Residual 不解释了，现在的模型大多都有 residual connection 的影子，膜一膜何恺明 orz～～～在一般的 GAN 中，discriminator 对一张图像输出一个值表示真假，但是 PatchGAN 输出一个矩阵，矩阵每个元素表示对应 patch 的真假，见下图：</p><p><img src="https://miro.medium.com/max/1064/1*f-cmriSHtg8PKOXR-uWaEg.png" alt="https://miro.medium.com/max/1064/1*f-cmriSHtg8PKOXR-uWaEg.png" style="width:40%;" /></p><p>不得不说 PatchGAN 太适合 Image Inpainting 任务了，因为图像修复就是一部分为真、一部分为假。与 PatchGAN 相对应地，原来的 GAN 就被称作 Global GAN，或 G-GAN。由于 PatchGAN 关注的是 local consistency，所以与 GLCIC 类似地，为了得到 global consistency 还需要一个 G-GAN，这就是这篇论文提出的模型被称为 PGGAN 的原因。Global discriminator 和 PatchGAN discriminator 的前几层是共享的，见下文的图片。</p><p>另外，PGGAN 依旧沿袭了 GLCIC 的 Dilated Convolution，结合 residual connection 构成了 dilated residual block 用于 generator 中。</p><p><img src="https://miro.medium.com/max/1400/0*VqS399kNJmYwL1v9.png" alt="https://miro.medium.com/max/1400/0*VqS399kNJmYwL1v9.png" style="width:80%;" /></p><p>作者还在模型中使用了 interpolated convolution，并与 dilated convolution 的效果做比较。Interpolated convolution 首先用插值将输入 resize 到希望的大小，然后施加标准的卷积。</p><p>关于 loss function，这篇文章和以往没有大的变化： <span class="math display">\[\begin{align}&amp;\mathcal L_{rec}=\frac{1}{N}\sum_{n=1}^N\frac{1}{WHC}||y-x||_1\\&amp;\mathcal L_{GAN}(G, D)=\mathbb E_{x\sim p(x)}[\log D(x)]+\mathbb E_{y\sim P_G(\tilde x)}[\log(1-D(G(\tilde x)))]\\&amp;\mathcal L=\lambda_1 \mathcal L_{rec}+\lambda_2 \mathcal L_{g\_adv}+\lambda_3\mathcal L_{p\_adv}\end{align}\]</span></p><blockquote><p>吐个槽：为什么作者一会儿写成抽样的形式，一会儿写成期望的形式，就不能统一一下吗……</p></blockquote><h2 id="shift-net">Shift-Net</h2><p>2018 年的 <strong>Shift-Net</strong><sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="Zhaoyi Yan, Xiaoming Li, Mu Li, Wangmeng Zuo, and Shiguang Shan, “[Shift-Net: Image Inpainting via Deep Feature Rearrangement](https://arxiv.org/pdf/1801.09392.pdf),” *Proc. European Conference on Computer Vision* (*ECCV*), 2018.">[8]</span></a></sup>，可以看成 deep learning-based copy-and-paste。传统 copy-and-paste 方法有很好的纹理细节，毕竟我们是直接复制的，但是其语义一致性很差。另一方面，深度学习方法用 fc 或者 dilated conv 获取整张图像的上下文信息，能够具有较好的语义一致性，但是即便使用 adversarial loss 帮助细化，其细节也不能让人满意。所以作者希望结合二者的优点。</p><p>简要来说，作者提出了一个 <strong>guidance loss</strong> 来让模型在 decode 过程中学会填充缺失部分。另外，还引入了一个 <strong>shift-connection layer</strong> 来匹配缺失区域内部的特征和外部的特征，每个匹配的外部特征被 shift 到对应内部特征的位置上，和原来的特征做 concatenation。下面详细介绍之。</p><p><img src="https://miro.medium.com/max/4800/0*Mff7UlOA9Tq2mrlp.png" alt="https://miro.medium.com/max/4800/0*Mff7UlOA9Tq2mrlp.png" style="width:90%;" /></p><p>上图是 Shift-Net 的架构。注意如果没有 shift-connection layer，这就是一个标准的 <strong>U-Net</strong>。Discriminator 没有作出，但是就是一个 PatchGAN discriminator。</p><p><strong>Guidance loss</strong> 衡量缺失区域内的 decoded feature 和真实图像在缺失区域内的 encoded feature 的差异： <span class="math display">\[\mathcal L_g=\sum_{y\in\Omega}||(\Phi_{L-l}(I))_y-(\Phi_l(I^{gt}))_y||_2^2\]</span> 其中，<span class="math inline">\(\Omega\)</span> 是缺失区域，<span class="math inline">\(\bar\Omega\)</span> 是未缺失区域；对于 <span class="math inline">\(L\)</span> 层的 U-Net，<span class="math inline">\(\Phi_l(I)\)</span> 表示第 <span class="math inline">\(l\)</span> 层的 encoded feature，<span class="math inline">\(\Phi_{L-l}(I)\)</span> 表示第 <span class="math inline">\(L-l\)</span> 层的 decoded feature。我们最终目的是得到 <span class="math inline">\(I^{gt}\)</span>，因此我们期望 <span class="math inline">\(\Phi_l(I)\)</span> 和 <span class="math inline">\(\Phi_{L-l}(I)\)</span> 包含了所有 <span class="math inline">\(\Phi_l(I^{gt})\)</span> 的信息；又由于缺失区域内 <span class="math inline">\((\Phi_l(I))_y=0\)</span>，所以我们希望 <span class="math inline">\((\Phi_{L-l}(I))_y=(\Phi_l(I^{gt}))_y\)</span>，这就是上面损失函数的来源。</p><p>仅靠 Guidance loss 得到的特征比较模糊，作者提出 <strong>Shift-connection layer</strong> 来利用缺失区域外的特征以添加细节。 <span class="math display">\[x^*(y)=\arg\max_{x\in\bar\Omega}\frac{\left\langle(\Phi_{L-l}(I))_y,(\Phi_l(I))_x\right\rangle}{||(\Phi_{L-l}(I))_y||_2||(\Phi_l(I))_x||_2}\]</span> 简单来说，对于缺失区域内的位置 <span class="math inline">\(y\)</span>，找到这样一个位置 <span class="math inline">\(x^\ast\)</span>，它是 <span class="math inline">\((\Phi_{L-l}(I))_y\)</span> 和 <span class="math inline">\((\Phi_l(I))_x\)</span> 的余弦相似度最大的那个 <span class="math inline">\(x\)</span>。于是乎，对于每个 <span class="math inline">\(y\)</span>，能找到一个 shift vector <span class="math inline">\(u_y=x^\ast(y)-y\)</span>，指示和它最相似的位置的偏移量。</p><p>得到偏移量之后，我们就可以得到 shift 后的特征： <span class="math display">\[\left(\Phi^{shift}_{L-l}(I)\right)_y=(\Phi_l(I))_{y+u_y}\]</span> 然后将该特征 concatenate 到 decoded feature 上，如上文架构图所示。</p><p>Shift-Net 的损失函数非常标准： <span class="math display">\[\mathcal L=\mathcal L_{L_1}+\lambda_g\mathcal L_g+\lambda_{adv}\mathcal L_{adv}\]</span> 除了 Guidance loss，依旧使用了 L1 loss 和 Adversarial loss，论文所用超参数为 <span class="math inline">\(\lambda_g=0.01, \lambda_{adv}=0.002\)</span>。</p><p>值得注意的是，shift 操作要求一个 <span class="math inline">\(\arg\max\)</span>，这是一个不可导操作，需要我们手动修改梯度。事实上，找最近邻是一个 <strong>hard assignment</strong>，在后续的工作中，我们可以用 <strong>soft assignment</strong> 代替，即考虑所有缺失区域外的位置并分配权重，这样所有操作都是可导的（是不是有 self attention 的味道了~）。</p><h2 id="contextual-attention-deepfill-v1">Contextual Attention (DeepFill v1)</h2><p>刚说到 attention，这不就来了吗。2018 年的 <strong>DeepFill v1</strong><sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><span class="hint--top hint--rounded" aria-label="Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang, “[Generative Image Inpainting with Contextual Attention](https://arxiv.org/abs/1801.07892),” *Proc. Computer Vision and Pattern Recognition* (*CVPR*), 2018.">[9]</span></a></sup>，也叫做 Generative Image Inpainting with Contextual Attention (CA)，就是 Shift-Net 的加强版。论文作者发展了 copy-and-paste 并且提出了可微的、全卷积的 contextual attention layer，其思想是：找到缺失区域外的像素对缺失区域内的像素的贡献度，外部像素的信息和起来可以改善内部的特征。Shift connection 只看最相近的特征，是 hard assignment，不可微；CA 是 soft assignment，由权重指示贡献度，可微。</p><p>Contextual Attention 如下图所示：</p><p><img src="https://miro.medium.com/max/1400/1*M-3oTs0df0T5_jppYpSJTQ.png" alt="https://miro.medium.com/max/1400/1*M-3oTs0df0T5_jppYpSJTQ.png" style="width:50%;" /></p><p>下图带文字说明已经很详细了，我就不再赘述了。（好吧其实是我懒～）</p><p><img src="https://miro.medium.com/max/1400/1*BAqkrfKBYUQU7BeBOCqBQg.png" alt="https://miro.medium.com/max/1400/1*BAqkrfKBYUQU7BeBOCqBQg.png" style="width:80%;" /></p><p>上面得到了 attention map，但是考虑到相邻像素一般关系更强，所以我们按如下方法做 attention propagation： <span class="math display">\[\hat s_{x,y,x&#39;,y&#39;}=\sum_{i\in\{-k,\ldots,k\}}s^*_{x+i, y, x&#39;+i, y&#39;}\]</span> 怎么理解呢？</p><p><img src="attention propagation.png" width=50% /></p><p>设虚线下方是 invalid pixels，上方是 valid pixels，蓝色折线的同一侧相似度设为 1 ，不同侧相似度设为 0。考虑缺失区域的某一行 <span class="math inline">\(y\)</span> 和对应真是区域的某一行 <span class="math inline">\(y&#39;\)</span>，那么 <span class="math inline">\(s_{x,y,x,y&#39;}\)</span>（注意这里 <span class="math inline">\(x=x&#39;\)</span>）是 <span class="math inline">\(1,1,1,1,0,0,0,0,1,1,1,1\)</span>，如图所示。现在我们对 <span class="math inline">\(s\)</span> 做 <span class="math inline">\(k=1\)</span> 的 attention propagation，那么得到的新的 attention score 就是 <span class="math inline">\(3,3,2,1,0,0,1,2,3,3\)</span>，这说明了 attention propagation 的作用：让 <span class="math inline">\(s_{x,y,x,y&#39;}\)</span> 和 <span class="math inline">\(s_{x+1,y,x+1,y&#39;}\)</span> （相对地）变得更相近。</p><p>上面说的是 left-right propagation（因为 <span class="math inline">\(i\)</span> 加在 <span class="math inline">\(x\)</span> 上），同理再做一次 top-down propagation。这个操作可以通过卷积完成，用单位矩阵做卷积核。</p><p>网络架构如下图所示，是 two-stage coarse-to-fine，两个阶段的网络都是 FCN with dilated convolution，Contextual Attention 应用在了第二个 refinement generator 上。</p><p><img src="https://miro.medium.com/max/1400/0*kF1fiNjwP5mfghi9.png" alt="https://miro.medium.com/max/1400/0*kF1fiNjwP5mfghi9.png" style="width:90%;" /></p><p>具体应用方式如下图所示：</p><p><img src="https://miro.medium.com/max/1400/1*tpIs6Fs1B23QDYozBSlJHA.png" alt="https://miro.medium.com/max/1400/1*tpIs6Fs1B23QDYozBSlJHA.png" style="width:50%;" /></p><p>关于 loss function，作者依旧应用了 L1 loss 和 GAN loss。不过，L1 loss 用的是 spatially discounted L1 loss，对每一个像素做了加权：weight mask 的权重设置为 <span class="math inline">\(\gamma^l\)</span>，其中 <span class="math inline">\(l\)</span> 为缺失像素到最近的真实像素的距离，<span class="math inline">\(\gamma\)</span> 为常数，模型中设置为 <span class="math inline">\(0.99\)</span>。GAN loss 用的是 WGAN-GP loss，作者称该 loss 也是基于 L1 距离的，因此网络更容易训练并且训练过程更稳定。</p><p>有趣的是，作者做了丢弃 reconstruction loss 的 ablation study，不过结论是不能丢，这玩意儿很关键。</p><h2 id="gmcnn">GMCNN</h2><p><strong>GMCNN</strong> (Generative Multi-column Convolutional Neural Networks)<sup id="fnref:10" class="footnote-ref"><a href="#fn:10" rel="footnote"><span class="hint--top hint--rounded" aria-label="Yi Wang, Xin Tao, Xiaojuan Qi, Xiaoyong Shen, and Jiaya Jia, “[Image Inpainting via Generative Multi-column Convolutional Neural Networks](https://arxiv.org/pdf/1810.08771.pdf),” *Proc. Neural Information Processing Systems*, 2018.">[10]</span></a></sup> 同样是 2018 年的工作，这篇论文的主要贡献是：</p><ol type="1"><li>提出的网络具有三个分支，每个分支 dilated conv 的 kernel size 不同，从而获取不同大小的 receptive fields；</li><li>引入两个新的 loss function：confidence-driven reconstruction loss 和 implicit diversified Markov Random Field (ID-MRF) loss。前者是一个加权 L1 loss，后者让生成的 patches 找到最近的有效区域作为参考。</li></ol><p>下面我们来细看论文主要内容。</p><p><img src="https://miro.medium.com/max/1400/0*3FWWYZeJQH5zDd4e.png" alt="https://miro.medium.com/max/1400/0*3FWWYZeJQH5zDd4e.png" style="width:80%;" /></p><p>上图是 GMCNN 的网络结构，一个 generator 具有 3 条分支（3x3, 5x5, 7x7），两个 discriminators 分别看 global 和 local，一个预训练 VGG19 作为 MRF 的特征提取网络。</p><p>从 MRF 的角度，我们想最小化生成的特征和真实图像上与之最相近的特征之间的差异（<span class="math inline">\(\min\min\)</span>）。以前都用 cosine similarity，但是这种方式往往会给不同的 generated feature patches 以相同的最近真实 feature patch，导致模糊的结果；因此作者采用一种基于相对距离的方法。</p><p>设 <span class="math inline">\(\hat Y_g\)</span> 是缺失区域的生成内容，<span class="math inline">\(\hat Y^L_g\)</span> 和 <span class="math inline">\(Y^L\)</span> 是预训练网络的第 <span class="math inline">\(L\)</span> 层特征。对于从 <span class="math inline">\(\hat Y^L_g\)</span> 和 <span class="math inline">\(Y^L\)</span> 分别取出的特征 patch <span class="math inline">\(v,s\)</span>，定义相对相似度为： <span class="math display">\[RS(v, s)=\exp\left(\left(\frac{\mu(v, s)}{\max_{r\in\rho_v(Y^L)}\mu(v, r)+\epsilon}\right)/h\right)\]</span> 其中 <span class="math inline">\(\mu(.,.)\)</span> 是余弦相似度，<span class="math inline">\(\rho_v(Y^L)\)</span> 是 <span class="math inline">\(Y^L\)</span> 除开 <span class="math inline">\(v\)</span> 的区域。随后对其进行标准化： <span class="math display">\[\overline{RS}(v, s)=RS(v, s)/\sum_{r\in\rho_v(Y^L)}RS(v, r)\]</span> 现在，我们可以定义 <span class="math inline">\(\hat Y^L_g\)</span> 和 <span class="math inline">\(Y^L\)</span> 的 ID-MRF loss： <span class="math display">\[\mathcal L_M(L)=-\log\left(\frac{1}{Z}\sum_{s\in Y^L}\max_{v\in \hat Y^L_g}\overline{RS}(v, s)\right)\]</span></p><p>其中，<span class="math inline">\(\max_{v\in \hat Y_g^L}\overline{RS}(v, s)\)</span> 说明 <span class="math inline">\(s\)</span> 是与 <span class="math inline">\(v\)</span> 最相似的 patch。考虑极端情况：所有 <span class="math inline">\(v\)</span> 都和某一个 <span class="math inline">\(s\)</span> 最相似，那么 <span class="math inline">\(\max\overline{RS}(v, s)\)</span> 就会很小，loss 就会很大。因此，ID-MRF loss 能够尽量给不同的生成 patch 匹配不同的真实 patch。</p><p>作者用 VGG19 作为预训练网络，所以最后提出的 loss 为： <span class="math display">\[\mathcal L_{mrf}=\mathcal L_M(\mathrm{conv}4\_2)+\sum_{t=3}^4\mathcal L_M(\mathrm{conv}t\_2)\]</span> Spatial Variant Reconstruction Loss 是一个 weighted l1 loss，作者用高斯分布作为 weight，使得越接近边缘的像素权重较大，越中心的像素权重较小。</p><p>和 Contextual Attention 相似，作者采用了 improved WGAN loss 作为 adversarial loss，最终总的 loss function 为： <span class="math display">\[\mathcal L=\mathcal L_{c}+\lambda_{mrf}\mathcal L_{mrf}+\lambda_{adv}\mathcal L_{adv}\]</span> 在训练中，所有图片都 resize 到 256x256，中心缺失区域 128x128。</p><h2 id="partialconv">PartialConv</h2><p>前面的大多数模型都假设了缺失区域是一个或多个矩形，<strong>PartialConv</strong> <sup id="fnref:11" class="footnote-ref"><a href="#fn:11" rel="footnote"><span class="hint--top hint--rounded" aria-label="Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro, “[Image Inpainting for Irregular Holes Using Partial Convolution](https://arxiv.org/pdf/1804.07723.pdf),” *Proc. European Conference on Computer Vision* (*ECCV*), 2018.">[11]</span></a></sup>着眼于解决不规则 mask 的情形。</p><p>动机：</p><ol type="1"><li>以前的方法以相同的方式看待缺失区域和有效区域——都是填充了 [0, 255] 的像素，只不过缺失像素置为预定义的值（0 或者均值），在上面做卷积会导致卷积结果受到缺失区域内像素的影响，这不合理。</li><li>已有的方法假设缺失区域是矩形的，许多模型用的 local discriminator 接收的输入是矩形缺失部分，虽然 test 过程中不要求缺失区域是矩形，但是得到的结果就会不尽人意。</li></ol><blockquote><p>对于第 1 点，我之前有过误解，以为只要缺失区域在标准化后值是 0，即最开始时定义为均值，那么卷积过程中一直以 0 值参与运算，并且求导时对参数的梯度贡献也为 0，所以没有实质影响。但是事实上，这些 0 会影响结果的 scale——同样 3x3 的 kernel，覆盖的缺失像素多，卷出来的值就小；覆盖的缺失像素少，卷出来的值就大。所以 PartialConv 其实还按有效像素的个数 scale 了卷积结果的。</p></blockquote><p>Partial Convolutional Layer 到底是什么呢？设 <span class="math inline">\(W\)</span> 和 <span class="math inline">\(b\)</span> 是卷积 filter 的权重和偏置，<span class="math inline">\(X\)</span> 表示将进行卷积的像素值/特征值，<span class="math inline">\(M\)</span> 是 binary mask（0 表示缺失，1 表示有效）。Partial Convolution 计算如下： <span class="math display">\[x&#39;=\begin{cases}W^T(X\odot M)\dfrac{\text{sum}(\mathbf{1})}{\text{sum}(M)}+b,&amp;\text{if sum}(M)&gt;0\\0,&amp;\text{otherwise}\end{cases}\]</span> 其中 <span class="math inline">\(\mathbf{1}\)</span> 表示全 <span class="math inline">\(1\)</span> 的矩阵，大小和 <span class="math inline">\(M\)</span> 相同；<span class="math inline">\(\frac{\text{sum}(\mathbf{1})}{\text{sum}(M)}\)</span> 是一个缩放系数，因为不同输入参与计算的有效值数量不同。可以看见，卷积的结果只依赖于有效部分的输入值。</p><p><strong>每个 partial convolution layer 之后要更新 binary mask。</strong>更新方式很简单，如果某一个位置有至少一个有效输入对其有贡献，那么它的 mask 就是 <span class="math inline">\(1\)</span>： <span class="math display">\[m&#39;=\begin{cases}1,&amp;\text{if sum}(M)&gt;0\\0,&amp;\text{otherwise}\end{cases}\]</span> 下图是一个简单的例子：</p><p><img src="https://miro.medium.com/max/1400/1*NzvO_fEjdQ_JJjFUHUzV0g.png" alt="https://miro.medium.com/max/1400/1*NzvO_fEjdQ_JJjFUHUzV0g.png" style="width:80%;" /></p><p>模型架构并没有新奇的地方，作者采用了带有 skip connection 的 U-Net 结构，只不过把所有卷积层换成了 Partial Convolution。</p><p>关于 loss function，很有趣的是<strong>这个模型没有用到 discriminator</strong>，除了 standard L1 loss 和 variation loss (TV loss)，作者还采用了两个高层次特征的 loss（Perceptual loss、Style loss）来生成细节纹理。L1 loss 是老朋友了： <span class="math display">\[\begin{align}&amp;\mathcal L_{hole}=\frac{1}{N_{I_{gt}}}||(1-M)\odot(I_{out}-I_{gt})||_1\\&amp;\mathcal L_{valid}=\frac{1}{N_{I_{gt}}}||M\odot(I_{out}-I_{gt})||_1\end{align}\]</span> Perceptual loss (VGG loss) 的思想是：我们想让生成的图片经过预训练 VGG 的某些层之后得到的 feature 与真实图片的 feature 相近： <span class="math display">\[\mathcal L_{perceptual}=\sum_{p=0}^{P-1}\frac{||\Psi_{p}^{I_{out}}-\Psi_{p}^{I_{gt}}||_1}{N_{\Psi_{p}^{I_{gt}}}}+\sum_{p=0}^{P-1}\frac{||\Psi_{p}^{I_{comp}}-\Psi_{p}^{I_{gt}}||_1}{N_{\Psi_{p}^{I_{gt}}}}\]</span> 其中，<span class="math inline">\(I_{comp}\)</span> 是把 <span class="math inline">\(I_{out}\)</span> 在有效区域内的部分用真实像素替换的结果；<span class="math inline">\(\Psi_p^I\)</span> 是输入 <span class="math inline">\(I\)</span> 在 VGG 的 <span class="math inline">\(p\)</span> 层的特征图；<span class="math inline">\(N_{\Psi_p^I}\)</span> 是 <span class="math inline">\(\Psi_p^I\)</span> 的元素数量。和 L1 loss 在图像空间进行比较不同，Perceptual loss 是在高层次特征空间进行比较的，这样的好处是，如果图像有微小变化，比如平移了几个像素，对 L1 loss 的影响是巨大的，但是对提取的特征没有太大影响。</p><p>Style loss： <span class="math display">\[\begin{align}&amp;\mathcal L_{style_{out}}=\sum_{p=0}^{P-1}\frac{1}{C_pC_p}||K_p\left((\Psi_p^{I_{out}})^T(\Psi_p^{I_{out}})-(\Psi_p^{I_{gt}})^T(\Psi_p^{I_{gt}})\right)||_1\\&amp;\mathcal L_{style_{comp}}=\sum_{p=0}^{P-1}\frac{1}{C_pC_p}||K_p\left((\Psi_p^{I_{comp}})^T(\Psi_p^{I_{comp}})-(\Psi_p^{I_{gt}})^T(\Psi_p^{I_{gt}})\right)||_1\end{align}\]</span> 可以看见，feature map 依旧是由预训练 VGG 给出的。Gram matrix 包含了纹理、颜色等信息，因此计算 feature map 的 Gram matrix 的 L1 loss 使得生成区域的 style 尽可能接近真实图像。<span class="math inline">\(K_p\)</span> 是一个依赖于第 <span class="math inline">\(p\)</span> 层特征图大小的缩放系数。</p><p>Total Variation loss 保证图像的光滑性： <span class="math display">\[\mathcal L_{tv}=\sum_{(i, j)\in R,(i,j+1)\in R}\frac{||I_{comp}^{i, j+1}-I_{comp}^{i, j}||_1}{N_{I_{comp}}}+\sum_{(i, j)\in R,(i+1,j)\in R}\frac{||I_{comp}^{i+1,j}-I_{comp}^{i, j}||_1}{N_{I_{comp}}}\]</span> Final loss 为上述四个 loss 的加权和： <span class="math display">\[\mathcal L_{total}=\mathcal L_{valid}+6\mathcal L_{hole}+0.05\mathcal L_{perceptual}+120(\mathcal L_{style_{out}}+\mathcal L_{style_{comp}})+0.1\mathcal L_{tv}\]</span></p><h2 id="edgeconnect">EdgeConnect</h2><p>2019 年的 <strong>EdgeConnect</strong><sup id="fnref:12" class="footnote-ref"><a href="#fn:12" rel="footnote"><span class="hint--top hint--rounded" aria-label="Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi, Mehran Ebrahimi, “[EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning](https://arxiv.org/pdf/1901.00212.pdf),” *Proc. International Conference on Computer Vision* (*ICCV*), 2019.">[12]</span></a></sup>，又是一个 two-stage 的佳作。其思想是先生成 edge，再根据 edge 上色，效果图着实让人眼前一亮。</p><p><img src="https://miro.medium.com/max/1400/0*HDdl5vq7tMYPtiTi.png" alt="https://miro.medium.com/max/1400/0*HDdl5vq7tMYPtiTi.png" style="width:100%;" /></p><p>正如前文所说，现有的 image inpainting 方法容易生成模糊的图片——这是 L1 loss / L2 loss 造成的。为了进一步提升图像质量，我们可以提供<strong>先验信息</strong>——一个 edge map。我们把图像填充分成两步——先预测边 (edge prediction)，再填颜色 (color filling)，即 <strong>“lines first, color next”</strong> 策略。</p><p>这篇论文的网络结构和以往类似，用了两个 generators——分别生成边和最终图像、以及两个 discriminators——分别对边和最终图像判断，可以看成类似于 coarse-to-fine 的结构。Loss functions 也和之前的类似，对于第一个 generator，作者用了 adversarial loss 和 feature matching loss，后者类似于 VGG perceptual loss；对于第二个 generator，作者用了 style loss, perceptual loss, L1 loss 和 adversarial loss。</p><p>一堆 loss 里面只有 feature matching loss 需要讲一下，其他 loss 参看前面讲过的模型。Feature matching loss 如下： <span class="math display">\[L_{FM}=\mathbb E\left[\sum_{i=1}^L\frac{1}{N_i}||D_1^{(i)}(C_{gt})-D_1^{(i)}(C_{pred})||_1\right]\]</span> 其中，<span class="math inline">\(D_1\)</span> 是 edge map 的 discriminator，<span class="math inline">\(L\)</span> 是它的层数，<span class="math inline">\(C_{gt}\)</span> 和 <span class="math inline">\(C_{pred}\)</span> 分别是真实和预测的 edge map，<span class="math inline">\(N_i\)</span> 是第 <span class="math inline">\(i\)</span> 层元素数量。和 VGG perceptual loss 很相似，我们想让真实 edge map 和生成的 edge map 在 discriminator 内部提取的特征相近。那为什么它要用 discriminator 而不是 VGG 呢——因为 pretrained VGG 不是在 edge 上训练的，自然直接用效果不好。</p><p>在训练过程中，作者采用了谱归一化 <strong>Spectral Normalization (SN)</strong>。SN 被用在 edge generator、edge discriminator 和第二个 discriminator 上。</p><p>最后提一点，图像的真实 edge 是用 Canny edge detector 生成的。</p><h2 id="deepfill-v2">DeepFill v2</h2><p>本文讨论的最后一个模型是 2019 年的 <strong>DeepFill v2</strong><sup id="fnref:13" class="footnote-ref"><a href="#fn:13" rel="footnote"><span class="hint--top hint--rounded" aria-label="Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang, “[Free-Form Image Inpainting with Gated Convolution](https://arxiv.org/pdf/1806.03589.pdf),” *Proc. International Conference on Computer Vision* (*ICCV*), 2019.">[13]</span></a></sup>，也叫 GatedConv，也许是最具有应用性的模型。它可以看成 DeepFill v1, Partial Convolution 和 EdgeConnect 的加强版。简单来说，Contextual Attention (DeepFill v1) 和 user guidance (EdgeConnect) 都被应用在了 DeepFill v2 中。Partial Convolution 被修改为了 <strong>Gated Convolution</strong>，前者是用一个固定的规则更新 mask，而后者改为了一个可学习的方式更新，可以说，Gated Convolution 是 Partial Convolution 的可学习版本。</p><p>DeepFill v2 的网络结构与 DeepFill v1 相比没有大的改变，仍然是 coarse-to-fine 结构，除了卷积改成了 gated convolution：</p><p><img src="https://miro.medium.com/max/1400/1*UaweIaCSL8HmFG9jh-KGlA.png" alt="https://miro.medium.com/max/1400/1*UaweIaCSL8HmFG9jh-KGlA.png" style="width:80%;" /></p><p>可以看见，输入分别是 masked image, mask 和可选择的 sketch image，经过一个 coarse network 后得到 coarse result，然后该粗糙图像经过 CA 和 Gated Convolution 双分支的 refinement network 后得到精细图像。Discriminator 采用 PatchGAN 结构。另外，和 EdgeConnect 相同，谱归一化 SA 被应用在了 discriminator 中。</p><p>Gated Convolution 是本文的重点，下图展现了 Partial conv 和 Gated conv 的区别：</p><p><img src="https://miro.medium.com/max/1208/1*m2Tr4p0PFkBEN5ZJccsopA.png" alt="https://miro.medium.com/max/1208/1*m2Tr4p0PFkBEN5ZJccsopA.png" style="width:60%;" /></p><p>其实 Gated convolution 和 Spatial Attention 差不多，用一个卷积+sigmoid 来预测 gating，sigmoid 的输出在 <span class="math inline">\((0, 1)\)</span> 之间，所以 soft gating 可以看做 binary mask 的 soft 版本，其作为“重要度”（“有效度”）点乘在 feature 上。</p><p>Loss function 只有两个：L1 loss 和 GAN loss。GAN loss，更准确地说，SN-PatchGAN loss，对于生成网络如下所示： <span class="math display">\[\mathcal L_G=-\mathbb E_{z\sim \mathbb P_Z(z)}[D^{sn}(G(z))]\]</span> 可见其形式非常简洁，其本质上是一个 hinge loss。</p><h2 id="小结">小结</h2><p>学了 20 天（2021-09-27～2021-10-18）的 Image Inpainting，看了 10 个模型、8 篇论文，是时候做一个简单的 review 了。</p><p>Image Inpainting 任务有着自己鲜明的特点。想一想，拿到一张有缺失的图像，人是如何脑补出缺失的部分呢？没错，我们需要对整幅图有个宏观上的把握，然后依据未缺失的部分推测缺失内容。对于神经网络也是如此，要填充原图中缺失的部分，必须从未缺失部分「借到」信息。另外，Image Inpainting 任务有个显著的难点：在全局上，填补的内容不能和未缺失部分违和，同时在局部上，必须足够细致。综上，<strong>我认为做 Image Inpainting，就是要解决如下三个问题</strong>：</p><ul><li>如何获取远处信息</li><li>如何保证全局语义一致性（high-level 角度）</li><li>如何保证局部纹理真实性（low-level 角度）</li></ul><p>纵观我们目前看过的模型，它们都有机制——要么是 loss function、要么是网络架构（设计网络模块），来保证上述三点内容。用 loss function 是在训练阶段保证，用网络架构则在测试阶段也能发挥作用。这些机制如下所示：</p><ul><li>Loss functions<ul><li><strong>Reconstruction loss ( [weighted] L1 / L2 loss)</strong>：保证全局一致性，同时很重要的一点是，它倾向于填补模糊的图像，也就是对局部真实性有负面的作用</li><li><strong>Global GAN loss</strong>：保证全局一致性</li><li><strong>Local GAN loss / PatchGAN loss</strong>：保证局部真实性</li><li><strong>Perceptual loss</strong>：保证全局一致性和局部真实性</li><li>Style loss：保证局部真实性</li><li>ID-MRF loss：保证局部真实性</li></ul></li><li>Network modules<ul><li>Shift-connection：保证局部真实性</li><li>Contextual Attention：保证局部真实性</li></ul></li></ul><p>（注：未加粗的 loss function 私以为人工介入很强，个人不喜欢，并且 DeepFill v2 也并不采用它们）</p><p>另外，随着 Image Inpainting 方法的发展，mask 形状从最开始的规整的矩形，变成了多个矩形，最后变成了任意形状。</p><p>综上，我将这些模型总结成了下表：</p><table><thead><tr class="header"><th><strong>方法</strong></th><th><strong>insight</strong></th><th><strong>mask</strong> <strong>形状</strong></th><th><strong>如何获取远处信息</strong></th><th><strong>如何保证</strong> <strong>global consistency</strong></th><th><strong>如何保证</strong> <strong>local consistency</strong></th></tr></thead><tbody><tr class="odd"><td><strong>Context Encoder (L2 + Adv)</strong> <br>Context Encoders: Feature Learning by Inpainting</td><td>Encoder-decoder 架构 指出仅 L2 模糊的原因，用 GAN 来细化结果<br>channel-wise fc 获取全局信息</td><td>64x64 rect</td><td>Channel-wise fc</td><td>L2 loss (only in masked region)</td><td>Adversarial loss (only in masked region)</td></tr><tr class="even"><td><strong>Context Encoder (only L2)</strong> <br>Context Encoders: Feature Learning by Inpainting</td><td></td><td>multiple holes</td><td>Channel-wise fc</td><td>L2 loss (only in masked region)</td><td>无</td></tr><tr class="odd"><td><strong>GLCIC (aka. GLGAN)</strong> <br>Globally and Locally Consistent Image Completion</td><td>Global + Local discriminators FCN 使得输入大小可变<br>fc -&gt; dilated conv</td><td>train: one random rect，width &amp; height ~ [96, 128]（由训练方法和 local discriminator 架构决定的）<br>test: multiple holes</td><td>Dilated conv</td><td>L2 loss (only in masked region)<br>Global discriminator (adversarial loss)</td><td>Local discriminator (adversarial loss)</td></tr><tr class="even"><td><strong>PGGAN</strong> <br>Patch-Based Image Inpainting with Generative Adversarial Networks</td><td>Local discriminator -&gt; PatchGAN</td><td>free</td><td>Dilated conv</td><td>L1 loss<br>Global GAN (adversarial loss)</td><td>PatchGAN (adversarial loss)</td></tr><tr class="odd"><td><strong>Shift-Net</strong> <br>Shift-Net: Image Inpainting via Deep Feature Rearrangement</td><td>Shift-connect layer<br>Guidance loss</td><td>free</td><td>Shift-connection</td><td>L1 loss<br>Guidance loss</td><td>PatchGAN (adversarial loss)<br>Shift-connection</td></tr><tr class="even"><td><strong>Contextual Attention</strong> <strong>(DeepFill v1)</strong> <br>Generative Image Inpainting with Contextual Attention</td><td>Contextual Attention<br>Spatially discounted L1 loss<br>Coarse-to-fine 二级结构</td><td>train: one random rect<br>test: multiple holes</td><td>Contextual Attention Dilated conv</td><td>Spatially discounted L1 loss (weighted L1 loss)<br>Global WGAN-GP loss</td><td>Contextual Attention<br>Local WGAN-GP loss</td></tr><tr class="odd"><td><strong>GMCNN</strong> <br>Image Inpainting via Generative Multi-column Convolutional Neural Networks</td><td>Multi-column<br>ID-MRF loss</td><td>train: 256x256 rect</td><td>Dilated conv (3 branches)</td><td>Spatially Variant Reconstruction loss (weighted L1 loss)<br>Global WGAN-GP loss</td><td>Local WGAN-GP loss<br>ID-MRF loss</td></tr><tr class="even"><td><strong>PartialConv</strong> <br>Image Inpainting for Irregular Holes Using Partial Convolutions</td><td>Partial Convolution<br>No discriminator</td><td>free</td><td>/</td><td>L1 loss<br>Perceptual loss<br>TV loss</td><td>Perceptual loss<br>Style loss</td></tr><tr class="odd"><td><strong>EdgeConnect</strong> <br>EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning</td><td>lines first, color next</td><td>free</td><td>Dilated conv</td><td>L1 loss<br>Perceptual loss</td><td>Edge prior<br>Perceptual loss<br>Style loss<br>Adversarial loss (D2)</td></tr><tr class="even"><td><strong>DeepFill v2</strong> <br>Free-Form Image Inpainting with Gated Convolution</td><td>Gated Convolution</td><td>free</td><td>Contextual Attention<br>Dilated conv</td><td>L1 loss</td><td>PatchGAN (adversarial loss)<br>Contextual Attention</td></tr></tbody></table><p>可以做出它们的发展关系图（其实关系很难用一个箭头说明清楚，只能大致画画，看个乐呵就好）：</p><p><img src="models.png" width=100% /></p><p>发展过程中，有许多方法（loss、架构）被更好的方法取代了，也有很多延续到了现在。那当下我们还经常用的有哪些呢？我把它们总结如下：</p><ul><li>Losses<ul><li>Reconstruction loss</li><li>Adversarial loss</li><li>Perceptual loss</li></ul></li><li>Modules<ul><li>Dilated Convolution</li><li>Contextual Attention</li><li>Partial Convolution / Gated Convolution</li></ul></li><li>Basic architectures<ul><li>Encoder-decoder / U-Net</li><li>PatchGAN</li></ul></li></ul><hr /><p>学习每个模型的过程中，我时不时有些 idea，这里把它们收集起来好了：</p><ol type="1"><li><p>首先，在学前几个模型的时候，我老在想 Transformer 似乎很适合这个任务——因为 Transformer 关注全局信息，自注意力模块按相似度加权的做法和 Image Inpainting 也很搭。后来看到了 Contextual Attention，就意识到了这和 Transformer 异曲同工，核心思想是一致的。</p></li><li><p>这些模型基本都用了大比例的 reconstruction loss，但是从理论上讲，global discriminator 是可以对全局真实性进行限制的，那为什么大家普遍用了大比例的 l1 / l2 呢？我在 Contextual Attention 的论文里找到了答案，作者说不要 l1 loss 会导致很难训练。不过我想，既然 l1 / l2 loss 对局部真实性有负面作用，<strong>希望今后能找到一种方法丢掉 l1 / l2 loss， 或者改进使其没有/减弱负面影响</strong>。</p></li><li><p>除了 Context Encoder 这个古董模型以外，其他模型在测试阶段其实都能给任意形状 mask 作为输入，只不过嘛，对于那些训练的时候只能输入一个矩形 mask 的模型来讲，这么做训练和测试输入就不一致了，那效果可想而知也不会太好。（这一点在 PartialConv 论文里也说了）</p></li><li><p>Partial Convolution 中，mask 的更新是二值化的，这完全可以改进为实值权重，只要将 mask 的感受野相加后归一化即可，即把原更新方式： <span class="math display">\[m&#39;=\begin{cases}1&amp;\text{if}\;\text{sum}(M)&gt;0\\0&amp;\text{otherwise}\end{cases}\]</span> 改为： <span class="math display">\[\begin{align}&amp;\tilde m&#39;=\text{sum}(M)\\&amp;m&#39;=\text{Normalize}(\tilde m&#39;)\end{align}\]</span></p></li><li><p>Gated Convolution 是对 Partial Convolution 的改进，它的做法很「暴力」，直接把 mask 的更新变成一个可学习的过程： <span class="math display">\[m&#39;=\sigma(\text{conv}(M))\]</span> 可以认为是和 SE Block 类似的一种 Attention 方法。但是！SE Block 直接干是因为人们确实没法知道各个 channel 都学了些啥、哪些更重要，但 Partial Convolution 的 mask 是有实际意义的啊，直接干也许效果不差，但是可能训练变得很困难（猜想，没实验过）。换句话说，Partial Convolution 的 mask 更新方式是一种人为引入的<strong>归纳偏置</strong>，而有道理的归纳偏置往往有助于训练。所以，说了这么多，我想到的解决方法是：用类似于 residual connection 的思想结合 Partial Convolution（或者我刚说的修改版本）和 Gated Convolution，让 Gated Convolution <strong>学习残差</strong>，即： <span class="math display">\[m&#39;=\sigma\left(m&#39;_{Partial}+\text{conv}(M)\right)\]</span></p></li><li><p>大多数模型在复原 edge 方面表现不佳，例如一个柱子向下延伸，生成的图像中柱子的边缘会越来越糊。不过，EdgeConnect 已经大幅缓解了这个问题。</p></li><li><p>EdgeConnect 引入的 edge 可以看做是<strong>先验信息</strong>，其最终的填补结果非常依赖于 edge 的生成结果。Edge generator 能轻松生成简单结构的 edge，但是对于复杂结构，比如一整个狗头，效果也不佳。复杂结构的生成问题其实是一个现在依旧没有解决的问题。遵循引入先验信息的思路，私以为我们可以找一些其他的先验信息，比如语义分割的结果？分割结果描述轮廓，edge 描述形状细节，二者结合也许能使效果更好。</p></li></ol><h2 id="参考资料">参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Wang, Yi, Ying-Cong Chen, Xin Tao, and Jiaya Jia. VCNet: A Robust Approach to Blind Image Inpainting. In <em>Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXV 16</em>, pp. 752-768. Springer International Publishing, 2020. <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Chu-Tak Li. “10 Papers You Must Read for Deep Image Inpainting”. https://towardsdatascience.com/10-papers-you-must-read-for-deep-image-inpainting-2e41c589ced0 <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros, “<a href="https://arxiv.org/abs/1604.07379">Context Encoders: Feature Learning by Inpainting</a>,” <em>Proc. International Conference on Computer Vision and Pattern Recognition</em> (<em>CVPR</em>), 2016. <a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang, and Hao Li, “<a href="https://arxiv.org/pdf/1611.09969.pdf">High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis</a>,” <em>Proc. International Conference on Computer Vision and Pattern Recognition</em> (<em>CVPR</em>), 2017. <a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa, “<a href="http://iizuka.cs.tsukuba.ac.jp/projects/completion/data/completion_sig2017.pdf">Globally and Locally Consistent Image Completion</a>,” <em>ACM Trans. on Graphics</em>, Vol. 36, №4, Article 107, Publication date: July 2017. <a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Ugur Demir, and Gozde Unal, “<a href="https://arxiv.org/pdf/1803.07422.pdf">Patch-Based Image Inpainting with Generative Adversarial Networks</a>,” https://arxiv.org/pdf/1803.07422.pdf. <a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros, “<a href="https://arxiv.org/pdf/1611.07004.pdf）">Image-to-Image Translation with Conditional Adversarial Networks</a>,” <em>Proc. Computer Vision and Pattern Recognition</em> (<em>CVPR</em>), 21–26 Jul. 2017. <a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Zhaoyi Yan, Xiaoming Li, Mu Li, Wangmeng Zuo, and Shiguang Shan, “<a href="https://arxiv.org/pdf/1801.09392.pdf">Shift-Net: Image Inpainting via Deep Feature Rearrangement</a>,” <em>Proc. European Conference on Computer Vision</em> (<em>ECCV</em>), 2018. <a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang, “<a href="https://arxiv.org/abs/1801.07892">Generative Image Inpainting with Contextual Attention</a>,” <em>Proc. Computer Vision and Pattern Recognition</em> (<em>CVPR</em>), 2018. <a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:10" class="footnote-text"><span>Yi Wang, Xin Tao, Xiaojuan Qi, Xiaoyong Shen, and Jiaya Jia, “<a href="https://arxiv.org/pdf/1810.08771.pdf">Image Inpainting via Generative Multi-column Convolutional Neural Networks</a>,” <em>Proc. Neural Information Processing Systems</em>, 2018. <a href="#fnref:10" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:11" class="footnote-text"><span>Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro, “<a href="https://arxiv.org/pdf/1804.07723.pdf">Image Inpainting for Irregular Holes Using Partial Convolution</a>,” <em>Proc. European Conference on Computer Vision</em> (<em>ECCV</em>), 2018. <a href="#fnref:11" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:12" class="footnote-text"><span>Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi, Mehran Ebrahimi, “<a href="https://arxiv.org/pdf/1901.00212.pdf">EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning</a>,” <em>Proc. International Conference on Computer Vision</em> (<em>ICCV</em>), 2019. <a href="#fnref:12" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:13" class="footnote-text"><span>Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang, “<a href="https://arxiv.org/pdf/1806.03589.pdf">Free-Form Image Inpainting with Gated Convolution</a>,” <em>Proc. International Conference on Computer Vision</em> (<em>ICCV</em>), 2019. <a href="#fnref:13" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>计算机视觉</category>
      
      <category>图像填充</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>操作系统折腾记（二）</title>
    <link href="/blog-main/2021/10/06/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%8A%98%E8%85%BE%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <url>/blog-main/2021/10/06/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%8A%98%E8%85%BE%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>今天室友问了我一个有趣的问题。我们正在做 mit 的 xv6 2020 版实验，他写了一段代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;kernel/types.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;user/user.h&quot;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span>&#123;<br>  <span class="hljs-type">int</span> bar = <span class="hljs-number">0</span>;<br>  <span class="hljs-keyword">if</span>(fork() == <span class="hljs-number">0</span>)&#123;<br>    <span class="hljs-comment">// child</span><br>    bar = <span class="hljs-number">1</span>;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Child:  bar=%d, va=%p\n&quot;</span>, bar, &amp;bar);<br>    <span class="hljs-built_in">exit</span>(<span class="hljs-number">0</span>);<br>  &#125;<br>  <span class="hljs-keyword">else</span>&#123;<br>    wait(<span class="hljs-number">0</span>);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Parent: bar=%d, va=%p\n&quot;</span>, bar, &amp;bar);<br>  &#125;<br>  <span class="hljs-built_in">exit</span>(<span class="hljs-number">0</span>);<br>&#125;<br></code></pre></td></tr></table></figure><p>不妨设这个用户程序叫做 <code>vapa</code> 吧，编译、执行，得到结果：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">Child:  <span class="hljs-attribute">bar</span>=1, <span class="hljs-attribute">va</span>=0x0000000000002FCC<br>Parent: <span class="hljs-attribute">bar</span>=0, <span class="hljs-attribute">va</span>=0x0000000000002FCC<br></code></pre></td></tr></table></figure><p>Child 和 Parent 的 <code>bar</code> 变量值不同，这符合我们的预期——<code>fork</code> 的时候子进程复制父进程的变量值，之后对变量的修改是独立的。但是问题是——为什么两个 <code>bar</code> 的地址相同呢？</p><p>刚做了（其实还没完成）mit xv6 lab3 的我立刻产生了一个猜想——这个地址只是虚拟地址，<code>fork</code> 的时候，子进程会复制父进程的页表，修改变量的时候，只会更改它的物理地址，而不会（也没有必要）修改虚拟地址。</p><p>猜想需要验证才能成为真理，我试图在 <code>vapa.c</code> 里调用 <code>walkaddr</code> 获取物理地址，但是遇到了一堆麻烦——也是，访问物理地址这种事情，怎么会允许用户态的程序干呢。看来只能写一个系统调用 <code>getpa</code> 了——<code>getpa</code> 获取一个虚拟地址，返回物理地址（当然是在调用它的进程的页表中找）。</p><p>做过 mit xv6 lab2，所以写系统调用也不是什么难事：</p><ol type="1"><li><p>首先在 user/user.h、user/usys.pl、kernel/syscall.h、kernel/syscall.c 中参照其他系统调用补上 <code>getpa</code>；</p></li><li><p>然后在 kernel/sysproc.c 中实现一个 <code>sys_getpa</code>，由于内核在 kernel/vm.c 中已经为我们实现好了虚拟地址转物理地址的底层代码 <code>walkaddr</code>，所以直接调用之：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c">uint64<br><span class="hljs-title function_">sys_getpa</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span><br>&#123;<br>  uint64 va;<br>  <span class="hljs-keyword">if</span>(argaddr(<span class="hljs-number">0</span>, &amp;va) &lt; <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;<br>  <span class="hljs-keyword">return</span> walkaddr(myproc()-&gt;pagetable, va);<br>&#125;<br></code></pre></td></tr></table></figure></li></ol><p>如果我没记错的话，现在已经万事俱备了，于是我们用 <code>getpa</code> 在 <code>vapa.c</code> 中把物理地址一并输出：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;kernel/types.h&quot;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;user/user.h&quot;</span></span><br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span>&#123;<br>  <span class="hljs-type">int</span> bar = <span class="hljs-number">0</span>;<br>  <span class="hljs-keyword">if</span>(fork() == <span class="hljs-number">0</span>)&#123;<br>    <span class="hljs-comment">// child</span><br>    bar = <span class="hljs-number">1</span>;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Child:  bar=%d, va=%p, pa=%p\n&quot;</span>, bar, &amp;bar, getpa((uint64)&amp;bar));<br>    <span class="hljs-built_in">exit</span>(<span class="hljs-number">0</span>);<br>  &#125;<br>  <span class="hljs-keyword">else</span>&#123;<br>    wait(<span class="hljs-number">0</span>);<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Parent: bar=%d, va=%p, pa=%p\n&quot;</span>, bar, &amp;bar, getpa((uint64)&amp;bar));<br>  &#125;<br>  <span class="hljs-built_in">exit</span>(<span class="hljs-number">0</span>);<br>&#125;<br></code></pre></td></tr></table></figure><p>编译执行，得到结果：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">Child:  <span class="hljs-attribute">bar</span>=1, <span class="hljs-attribute">va</span>=0x0000000000002FBC, <span class="hljs-attribute">pa</span>=0x0000000087F4C000<br>Parent: <span class="hljs-attribute">bar</span>=0, <span class="hljs-attribute">va</span>=0x0000000000002FBC, <span class="hljs-attribute">pa</span>=0x0000000087F41000<br></code></pre></td></tr></table></figure><p>哈哈！物理地址果然变了，猜想正确！</p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>操作系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>panic!</title>
    <link href="/blog-main/2021/10/05/panic/"/>
    <url>/blog-main/2021/10/05/panic/</url>
    
    <content type="html"><![CDATA[<p>panic! panic!</p><p>我紧紧地盯着这五个字母，它们似乎也暗示着我的心情。panic，这是内核拼尽最后一丝力气在操作系统里给我留下的讯息，透过它，我看到了那被蚕食殆尽的内存空间，看到了野指针像野兽一般乱窜，看到了shell被四分五裂、再也无法响应键盘的呼唤……我想拯救它，于是我走进代码丛林，用可怜的手指四处敲敲补补——我砍下树枝，把它们粘在另一棵树上；我摘下红花瓣，涂成黄色给黏回去；有时，我还会依葫芦画瓢，把石灰岩打磨成鹅卵石的模样。当这一切都结束后，我灰头土脸地走出丛林，颤抖地按下回车键——</p><p>panic!</p><p>我看见 cpu 使用率骤然突破200%；我听到远程服务器风扇的嘶吼；我手下的键盘开始熔化、翻滚，那灼热的岩浆即将喷涌而出、势不可挡！ 我想，我只有一个办法了。我把目光投向内核，它正躲在阴影里瑟瑟发抖。</p><p>ctrl+a, x</p><p>内核再也不 panic 了。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>操作系统折腾记（一）</title>
    <link href="/blog-main/2021/09/20/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%8A%98%E8%85%BE%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/blog-main/2021/09/20/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%8A%98%E8%85%BE%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>学习操作系统的精髓在哪里？当然是自己折腾啊！（逃</p><p>学习线程的时候，老师在 ppt 中展示了这样一段代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;unistd.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;pthread.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;time.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> NUM_THREADS 5</span><br><br><span class="hljs-type">pthread_t</span> threads[NUM_THREADS];<br><span class="hljs-type">char</span> **ptr;<br><br><span class="hljs-type">void</span> *<span class="hljs-title function_">Hello</span><span class="hljs-params">(<span class="hljs-type">void</span> *vargp)</span> &#123;<br><span class="hljs-type">int</span> myid = *((<span class="hljs-type">int</span>*)vargp);<br><span class="hljs-type">static</span> <span class="hljs-type">int</span> cnt = <span class="hljs-number">0</span>;<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Hello from No.%d thread, tid=%lu, cnt=%d.\n&quot;</span>, myid, threads[myid], ++cnt);<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%s\n&quot;</span>, ptr[myid]);<br>pthread_exit(<span class="hljs-literal">NULL</span>);<br><span class="hljs-keyword">return</span> <span class="hljs-literal">NULL</span>;<br>&#125;<br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span>&#123;<br><span class="hljs-type">int</span> i;<br><span class="hljs-type">char</span> *msgs[NUM_THREADS] = &#123;<br><span class="hljs-string">&quot;Hello from A&quot;</span>,<br><span class="hljs-string">&quot;Hello from B&quot;</span>,<br><span class="hljs-string">&quot;Hello from C&quot;</span>,<br><span class="hljs-string">&quot;Hello from D&quot;</span>,<br><span class="hljs-string">&quot;Hello from E&quot;</span>&#125;;<br>ptr = msgs;<br><span class="hljs-type">int</span> taskids[NUM_THREADS];<br><span class="hljs-keyword">for</span>(i = <span class="hljs-number">0</span>; i &lt; NUM_THREADS; i++)&#123;<br>taskids[i] = i;<br>pthread_create(threads+i, <span class="hljs-literal">NULL</span>, Hello, taskids + i);<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;create No.%d thread, tid=%lu\n&quot;</span>, i, threads[i]);<br>&#125;<br>pthread_exit(<span class="hljs-literal">NULL</span>);<br>&#125;<br></code></pre></td></tr></table></figure><p>因为是学习用，所以这段代码有些奇怪的地方，比如在主线程里定义了 <code>msgs</code> 数组，然后拿一个全局指针 <code>ptr</code> 指过去，意思是让我们知道，这个 <code>msgs</code> 在对等线程中不能访问，但是对等线程可以通过全局的 <code>ptr</code> 访问到 <code>msgs</code>——但这些不重要，不是我们折腾的点儿。</p><p>老师在 ppt 中贴了 3 次运行的结果，如下图所示：</p><p><img src="ppt.png" /></p><p>意思很明确，就是线程的执行顺序是随机的，谁也不知道什么时候当前线程会被切断，转而执行另一个线程。比如，<code>Hello from No.1</code> 和 <code>Hello from B</code> 这两个 <code>printf</code> 之间，就很可能被切断。</p><p>然后我无意间发现了一个奇怪的现象。按理说，就算 <code>cnt</code> 自增后被切断了，那切回来输出时，<code>cnt</code> 只会变得更大而不会变小，换句话说，结果的 <code>count</code> 值应该是单调不减的才对，可为什么老师的结果截图中，输出 <code>count=5</code> 之后还会输出 <code>count=3</code> 呢？</p><p>和同学们讨论后，我们达成的一致是，<code>cnt</code> 在自增后，会将其值作为参数传递给 <code>printf</code> 的形参，这个过程是不会（或者很小可能）被切断的。随后，即使在执行 <code>printf</code> 过程中线程被切断了、<code>cnt</code> 被其他线程改变了，但是 <code>printf</code> 内的<strong>形参</strong>值已经不受影响了，于是依旧输出的是原本的 <code>cnt</code>。</p><p>不过俗话说得好，talk is cheap, show me the code，我们必须要实打实地折腾一番，于是我在虚拟机（Ubuntu 20.04）中编写了上述代码并执行，结果如下：</p><p><img src="my.png" /></p><p>没错，虽然 <code>cnt</code> 偶尔会被打乱，但运行了很多次，<code>cnt</code> 都遍历完了 1~5 所有的数字，虽然不能说一定，但是可以说大概率下，<code>cnt</code> 自增和传参给 <code>printf</code> 没有被切断。</p><p>正面的实验做了，可没有反面的实验总不放心。不是切不断吗，那就人为的切断它呗！切断的方式很简单，在 <code>cnt</code> 自增与传给 <code>printf</code> 之间 <code>sleep</code> 一会儿就行：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-type">static</span> <span class="hljs-type">int</span> cnt = <span class="hljs-number">0</span>;<br>++cnt;<br>sleep(<span class="hljs-number">0.1</span>);<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Hello from No.%d thread, tid=%lu, cnt=%d.\n&quot;</span>, myid, threads[myid], cnt);<br></code></pre></td></tr></table></figure><p>运行结果如下：</p><p><img src="cut1.png" /></p><p><code>count</code> 的值单调不减，印证了我们的想法，nice！</p><p>但是解决一个旧问题，又出现了一个新问题。从老师的截图中可以看到，<code>count</code> 值和 <code>No</code> 序号神奇地一一对应了，而我们的运行结果并不对应。一个可能的解释是，在老师的机子上，线程被创建和 <code>cnt</code> 自增之间不会被切断，不过……这真的合理吗？</p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>操作系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>什么是数据集的分布</title>
    <link href="/blog-main/2021/08/25/%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%88%86%E5%B8%83/"/>
    <url>/blog-main/2021/08/25/%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%88%86%E5%B8%83/</url>
    
    <content type="html"><![CDATA[<p>最近看 GAN 相关的论文，总出现「分布」这个词，什么「数据集的分布 <span class="math inline">\(p_\text{data}(x)\)</span>」<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... &amp; Bengio, Y. (2014). Generative adversarial nets. *Advances in neural information processing systems*, *27*.">[1]</span></a></sup> 啦、「目标域 <span class="math inline">\(Y\)</span> 的分布」<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Zhu, J. Y., Park, T., Isola, P., &amp; Efros, A. A. (2017). Unpaired image-to-image translation using cycle-consistent adversarial networks. In *Proceedings of the IEEE international conference on computer vision* (pp. 2223-2232).">[2]</span></a></sup> 啦～所以对于 CV 而言，究竟什么叫做「分布」呢？</p><p>在概率论中我们都学过概率分布，最能体现概率分布的就是分布列 <span class="math inline">\(p_X(x)\)</span>（对于离散随机变量）或概率密度函数 <span class="math inline">\(f_X(x)\)</span>（对于连续随机变量）或累积分布函数 <span class="math inline">\(F_X(x)\)</span>（离散、连续都适用）了。我们当年先学习一元随机变量，然后过渡到二元随机变量，于是出现了联合分布 <span class="math inline">\(p_{X,Y}(x,y)\)</span>、边缘分布 <span class="math inline">\(p_X(x)\)</span> 等。如果我们进一步扩展，设 <span class="math inline">\(X\)</span> 是一个<strong>随机向量</strong>，那么 <span class="math inline">\(p_X(x)\)</span> 可以是这个随机向量的概率分布，也可以理解为 <span class="math inline">\(X\)</span> 各个分量的联合分布。</p><p>这么看来，当我们说到「分布」的时候，似乎指的是一个 <span class="math inline">\(\mathbb R^n\mapsto \mathbb R\)</span> 的函数，从输入的随机向量映射到一个正实数值，这当然没错，不过这个函数不是很直观。为此，我们可以在脑海中想象一个 <span class="math inline">\(\mathbb R^n\)</span> 空间，在这个空间里撒点，概率大的地方点密集一些，概率小的地方点稀疏一些～嗯不错，现在脑海里就有了一个非常直观的画面了。</p><p><img src="distribution1.png" style="zoom:30%;" /></p><p>Okay，现在我们在 CV 领域，更准确来讲是 GAN 的领域细想一下这个问题。GAN 解决一个什么问题？在 GAN 的论文<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... &amp; Bengio, Y. (2014). Generative adversarial nets. *Advances in neural information processing systems*, *27*.">[1]</span></a></sup> 中已经说的很明白了：让生成网络 <span class="math inline">\(G\)</span> 的输出分布和数据集的分布相同。为了训练 <span class="math inline">\(G\)</span>，我们引入一个判别网络 <span class="math inline">\(D\)</span> 来判断 <span class="math inline">\(G\)</span> 的输出分布究竟和数据集的分布是否相同，通过损失将梯度一路传回 <span class="math inline">\(G\)</span> 达到训练的效果。那么什么叫做数据集的分布呢？</p><p>想象一个“图像空间”，它可以是 <span class="math inline">\(32\times32\)</span> 维、每一维在 <span class="math inline">\(256^3\)</span> 中取值的离散空间（也即是把 <span class="math inline">\(32\times 32\)</span> 的图像压扁成向量视作输入）；也可以是 <span class="math inline">\(100\)</span> 维随机向量在经过复杂的非线性变换后得到的空间（也即是 GAN 中 <span class="math inline">\(G\)</span> 网络的输出空间）……总之，在这样的空间里，大部分图像都是没有意义的噪声图像，就像老电视机屏幕的雪花一样，没有哪个图像数据集会收录这样的噪声；反过来想，一个数据集收录的所有图像，就在这个空间中占据了一些小地方，想象在这些地方撒了一些密集的点，这就是所谓数据集的分布了。</p><p>那什么是某种「域」的分布呢？「域」这东西蛮悬的，基本靠上下文猜作者是啥意思。如果我想把猫变成狗，我可以说我的 GAN 从猫这个域映射到了狗这个域；我让一个垮着脸的人笑了起来，我说微笑的脸是一个域。反正域类似一个子空间，这里面的图像有某种共同特征，比如猫域里都是猫。域的分布就是说，猫域在整个图像空间里占据了一小小块特定的位置：</p><p><img src="distribution2.png" style="zoom:30%;" /></p><p>就酱！</p><h2 id="参考资料">参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... &amp; Bengio, Y. (2014). Generative adversarial nets. <em>Advances in neural information processing systems</em>, <em>27</em>. <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Zhu, J. Y., Park, T., Isola, P., &amp; Efros, A. A. (2017). Unpaired image-to-image translation using cycle-consistent adversarial networks. In <em>Proceedings of the IEEE international conference on computer vision</em> (pp. 2223-2232). <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>计算机视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>交叉熵与极大似然</title>
    <link href="/blog-main/2021/08/20/%E4%BA%A4%E5%8F%89%E7%86%B5%E4%B8%8E%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6/"/>
    <url>/blog-main/2021/08/20/%E4%BA%A4%E5%8F%89%E7%86%B5%E4%B8%8E%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>交叉熵这玩意儿，最开始接触是在 Andrew Ng 的机器学习课程中讲二分类问题的时候，当时并没有觉得有多难，用极大似然对其进行推导也非常的顺畅。后来直接用到多分类问题中，我也没细想，毕竟在 pytorch 中就是调用一个 <code>CrossEntropyLoss()</code> 的事儿。不过昨天我又重新想了想怎么从极大似然法推导交叉熵，这一想，就把整个人给绕进去了，困扰了大半天，终于想通了之前的理解哪里出了问题，遂写下这篇文章记录一下。</p><h2 id="交叉熵定义">交叉熵定义</h2><p>假设 <span class="math inline">\(X\)</span> 是离散的随机变量，我们有两个分布列 <span class="math inline">\(p(x),q(x)\)</span>，交叉熵定义如下： <span class="math display">\[H(q, p)=-\sum_xq(x)\log p(x)\tag{1}\]</span> 我们常说交叉熵能衡量两个分布之间的相似程度，也即交叉熵越小，<span class="math inline">\(p,q\)</span> 两个分布越相似。事实上，如果我们固定 <span class="math inline">\(q\)</span>，那么可以证明：<span class="math inline">\(H(q,p)\)</span> 在 <span class="math inline">\(p=q\)</span> 时达到最小。（详见参考资料<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="为什么交叉熵（cross-entropy）可以用于计算代价？ - 灵剑的回答 - 知乎 https://www.zhihu.com/question/65288314/answer/849294209">[2]</span></a></sup>）</p><h2 id="极大似然法推导交叉熵">极大似然法推导交叉熵</h2><p>交叉熵能用极大似然法解释。回忆极大似然法：我们有若干未知的参数 <span class="math inline">\(\theta\)</span>，现在想对其进行估计，那么抽取一个样本 <span class="math inline">\(\{x^{(1)},\ldots,x^{(n)}\}\)</span>，计算在参数 <span class="math inline">\(\theta\)</span> 下这个样本被抽取出来的概率。这个概率是参数 <span class="math inline">\(\theta\)</span> 的函数，称之为似然函数，当似然函数（或对数似然函数）取到最大值时，对应的参数值就是我们的估计值。</p><p>欲用极大似然法解释交叉熵，我们可以把 <span class="math inline">\(p(x)\)</span> 视为想去估计的「参数」，<span class="math inline">\(q(x)\)</span> 视为样本的真实概率分布。为方便，假设 <span class="math inline">\(X\)</span> 是离散的随机变量。我们现在抽取一个容量为 <span class="math inline">\(n\)</span> 的样本，设其中 <span class="math inline">\(X=k\)</span> 出现了 <span class="math inline">\(n_k\)</span> 次，那么<strong>在参数 <span class="math inline">\(p(x)\)</span>下</strong>，这个样本被抽取出来的概率是 <span class="math inline">\(L(p)=\prod\limits_k p(k)^{n_k}\)</span>，对数似然为：<span class="math inline">\(\log L(p)=\sum\limits_kn_k\log p(k)\)</span>。除以样本量再取个负号，得到： <span class="math display">\[-\frac{1}{n}\log L(p)=-\sum_k\frac{n_k}{n}\log p(k)\approx-\sum_k q(k)\log p(k)\tag{2}\]</span> 其中，<span class="math inline">\(\frac{n_k}{n}\approx q(k)\)</span> 是因为 <span class="math inline">\(q\)</span> 才是样本的真实分布。这样我们就得到了交叉熵的形式，让交叉熵最小，就是让似然函数最大，也就是让 <span class="math inline">\(p(x)\)</span> 更接近真实分布 <span class="math inline">\(q(x)\)</span>。</p><h2 id="分类问题中的交叉熵损失函数">分类问题中的交叉熵损失函数</h2><p>现在在分类问题的背景下看交叉熵与极大似然。</p><p><strong>考虑一个特定的输入</strong> <span class="math inline">\(x\)</span>，不妨设是一只猫咪吧。其类别的真实分布是一个 <span class="math inline">\(\text{onehot}\)</span> 向量（仅在“猫”那一维为 <span class="math inline">\(1\)</span>，其余为 <span class="math inline">\(0\)</span>），预测分布为 pred 向量。正如前文所述，交叉熵衡量两个分布的相似程度，因而我们可以用交叉熵 <span class="math display">\[H(\text{onehot},\text{pred})=-\sum_{c\in\text{classes}}\text{onehot}_c\cdot\log(\text{pred}_c)=-\log(\text{pred}_{猫})\tag{3}\]</span> 作损失函数。可以发现，交叉熵用在分类问题中形式变得很简单，这归功于 <span class="math inline">\(\text{onehot}\)</span> 这个其实完全没有随机性的概率分布。<strong>换句话说，我们常常用的交叉熵损失函数，其实是特殊的、指定了其中一个分布为 <span class="math inline">\(\text{one-hot}\)</span> 形式的交叉熵</strong>。</p><p>在分类问题的背景下，用极大似然法解释上述交叉熵，其实是一件很无聊的事，主要是因为真实分布是 <span class="math inline">\(\text{onehot}\)</span>。极大似然法需要我们抽一个样本出来（设样本量为 <span class="math inline">\(n\)</span>），这里的样本空间就是所有类别，但由于我们现在考虑的是一个特定的输入 <span class="math inline">\(x\)</span>（猫咪），所以我们的样本一定是：猫、猫、……、猫。于是似然函数就是 <span class="math inline">\(L(\text{pred})=(\text{pred}_{猫})^n\)</span>，对数似然就是 <span class="math inline">\(\log L(\text{pred})=n\log(\text{pred}_猫)\)</span>，除以样本量取个负，就得到了 <span class="math display">\[-\frac{1}{n}\log L(\text{pred})=-\frac{n\log(\text{pred}_猫)}{n}=-\log(\text{pred}_猫)\tag{4}\]</span> <br></p><p>如果只考虑<strong>二分类问题</strong>，上文依旧是成立的，但是二分类嘛，我们没必要输出一个大小为 <span class="math inline">\(2\)</span> 的向量，只用输出一个概率值 <span class="math inline">\(p\)</span> 就 OK 了。所以可以对上文的交叉熵形式做一个改写： <span class="math display">\[\begin{align}\text{BCE}(y,p)&amp;=\begin{cases}-\log p&amp;\text{if }y=1\\-\log(1-p)&amp;\text{if }y=0\end{cases}\\&amp;=-y\log p-(1-y)\log(1-p)\tag{5}\end{align}\]</span> 其中 <span class="math inline">\(y\in\{0, 1\},\,p\in(0, 1)\)</span>，就得到了我们非常熟悉的 Binary Cross-Entropy 的形式。</p><p><br></p><p>那为什么我之前被困扰了大半天呢？因为我把样本空间弄混了。极大似然法<strong>这个方法本身</strong>要求我们做一个采样，而这里采样的对象，其实是某一个特定输入的真实标签，然后颇为无聊地反复采若干次……我之前以为是训练模型的过程中采出的 minibatch，结果当然就死活想不通。<strong>所以一定要注意「训练过程中对数据集采样得到 minibatch」 和「极大似然法中的采样」是不同的。</strong></p><p><br></p><p>不过，前者也有值得一说的地方，设 minibatch 是 <span class="math inline">\(\{(x^{(i)},y^{(i)})\mid i=1,\ldots,m\}\)</span>，用 <span class="math inline">\(M\)</span> 表示模型（<span class="math inline">\(M(x)\)</span> 为输入 <span class="math inline">\(x\)</span> 时模型的输出），则在训练过程中，计算一个 iteration 的平均 <span class="math inline">\(\text{Loss}\)</span> 是这样的： <span class="math display">\[\text{Loss}=\frac{1}{m}\sum_{i=1}^mH\left(y^{(i)},M(x^{(i)})\right)=\frac{1}{m}\sum_{i=1}^m-\log\left(M(x^{(i)})_{y^{(i)}}\right)\tag{6}\]</span> 由于采样再求平均，其实是在近似一个期望，所以更通用的，上式可以写作： <span class="math display">\[\text{Loss}=\mathbb E_{x,y\sim p_\text{data}(x,y)}\left[-\log\left(M(x)_{y}\right)\right]\tag{7}\]</span> 其中 <span class="math inline">\(p_\text{data}(x,y)\)</span> 表示数据集的分布。这个样子好像在哪里见过？没错，GAN 的论文<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Generative Adversarial Nets https://arxiv.org/abs/1406.2661">[1]</span></a></sup>中作者就是写了这么个期望：</p><p><span class="math display">\[V(D, G)=\mathbb E_{x\sim p_\text{data}(x)}[-\log(D(x))]+\mathbb E_{z\sim p_\text{z}(z)}[-\log(1-D(G(z)))]\tag{8}\]</span> 只不过 GAN 中判别网络 D 只需要二分类，所以把两类拆开分别写出来罢了。</p><h2 id="参考资料">参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Generative Adversarial Nets https://arxiv.org/abs/1406.2661 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>为什么交叉熵（cross-entropy）可以用于计算代价？ - 灵剑的回答 - 知乎 https://www.zhihu.com/question/65288314/answer/849294209 <a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>机器学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>什么是图像</title>
    <link href="/blog-main/2021/07/03/%E4%BB%80%E4%B9%88%E6%98%AF%E5%9B%BE%E5%83%8F/"/>
    <url>/blog-main/2021/07/03/%E4%BB%80%E4%B9%88%E6%98%AF%E5%9B%BE%E5%83%8F/</url>
    
    <content type="html"><![CDATA[<h2 id="像素坐标">像素坐标</h2><p>一张图片是一个高 <span class="math inline">\(H\)</span>、宽 <span class="math inline">\(W\)</span> 的像素矩阵，其大小简称作 <span class="math inline">\(W\times H\)</span>。</p><p>在图片上建立坐标系，是以<strong>左上角</strong>为原点，<span class="math inline">\(x\)</span> 轴正半轴向<strong>右</strong>延伸、<span class="math inline">\(y\)</span> 轴正半轴向<strong>下</strong>延伸，如图所示：</p><p><img src="coord.png" alt="图源：https://interfacelift.com/wallpaper/details/3852/red_demons.html" style="zoom:40%;" /></p><p>图像上一个像素点的坐标就是指某像素点在上述坐标系中的坐标，如下图所示，图片大小 <span class="math inline">\(5120\times 2880\)</span>，狐狸耳朵尖的坐标是 <span class="math inline">\((1206,948)\)</span>。</p><p>在目标检测任务中，经常与检测框打交道。检测框一般有两种表示方式：<span class="math inline">\((x_\min,y_\min,x_\max,y_\max)\)</span> 和 <span class="math inline">\((x_\min,y_\min,w,h)\)</span>，均是相对上述坐标系而言的，例如下图中，检测框的两种表示方式分别是 <span class="math inline">\((3444,435,4771,2332)\)</span> 和 <span class="math inline">\((3444,435,1327,1897)\)</span>。</p><p><img src="coord2.png" alt="图源：https://interfacelift.com/wallpaper/details/3852/red_demons.html" style="zoom:40%;" /></p><hr /><h2 id="图像的加载">图像的加载</h2><h3 id="概述">概述</h3><p>众所周知，一张图片就是一个像素矩阵，且对于 RGB 格式而言，每个像素点都有 3 个值：R、G、B，它们的取值范围都是 <span class="math inline">\([0,255]\)</span>，分别可以用 1 个字节存储。因而，一个高 <span class="math inline">\(H\)</span> 宽 <span class="math inline">\(W\)</span> 通道数 <span class="math inline">\(C\)</span> 的图片，就需要 <span class="math inline">\(C\times H\times W\)</span> 个字节表示出来。这个 <span class="math inline">\(CHW\)</span> 矩阵通常就是我们神经网络的输入。</p><p>但是在磁盘上，一张图片的存在形式并不一定是这个 <span class="math inline">\(CHW\)</span> 矩阵，而是被编码（encode）成了各种格式，例如 <code>jpeg</code>、<code>png</code>、<code>bmp</code> 等等。<code>jpeg</code> 和 <code>png</code> 都是会压缩图片的，所以它们实际占的存储空间会比 <span class="math inline">\(C\times H\times W\)</span> 小。从被编码的格式恢复到 <span class="math inline">\(CHW\)</span> 矩阵的过程称作解码（decode）。</p><p>举个例子，这张图是图像处理领域最经典的图了：</p><figure><img src="lena.jpeg" alt="lena.jpeg" /><figcaption aria-hidden="true">lena.jpeg</figcaption></figure><p>查看图片信息，可以看到它的尺寸是 <span class="math inline">\(200\times 200\)</span>，色彩空间是 <span class="math inline">\(\text{RGB}\)</span>，那么它解码之后应占用 <span class="math inline">\(200\times200\times3=120\text{KB}\)</span> 的空间，但它在磁盘上只占用了 <span class="math inline">\(11\text{KB}\)</span>；类似地，这张图片的 png 格式占用磁盘空间为 <span class="math inline">\(91\text{KB}\)</span>。</p><h3 id="torchvision.io"><code>torchvision.io</code></h3><p><code>torchvision.io</code>（<a href="https://pytorch.org/vision/stable/io.html#image">文档</a>）提供了许多图片加载和存储的函数，它们的作用就是让我们能够对图像编码、解码以及对图像文件读写。我画了一个图直观理解：</p><p><img src="torchvision.png" alt="" style="zoom:40%;" /></p><p>注意到文档里 <code>decode_jpeg</code> 有一个与众不同的参数：</p><blockquote><p><strong>device</strong> (<em>str</em> <em>or</em> <em>torch.device</em>) – The device on which the decoded image will be stored. If a cuda device is specified, the image will be decoded with <a href="https://developer.nvidia.com/nvjpeg">nvjpeg</a>. This is only supported for CUDA version &gt;= 10.1</p></blockquote><p><code>nvjpeg</code> 是 nvidia 的 gpu 上加速 jpeg 图像解码、编码和转码的库。所以我们在写 <code>DataSet</code> 时用这个库，可能会加快数据的读取，因而加快训练。</p><h3 id="实验">实验</h3><p>用几个常用的库：<code>cv2</code>、<code>matplotlib.pylot</code>、<code>PIL</code>、<code>skimage</code> 和 <code>torchvision.io</code>，读取图像文件为矩阵形式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">st_time = time.time()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>):<br>    img = cv2.imread(paths[i])<br><span class="hljs-built_in">print</span>(time.time() - st_time, <span class="hljs-string">&#x27;\tcv2&#x27;</span>)<br><br>st_time = time.time()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>):<br>    img = plt.imread(paths[i])<br><span class="hljs-built_in">print</span>(time.time() - st_time, <span class="hljs-string">&#x27;\tplt&#x27;</span>)<br><br>st_time = time.time()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>):<br>    img = np.array(Image.<span class="hljs-built_in">open</span>(paths[i]))<br><span class="hljs-built_in">print</span>(time.time() - st_time, <span class="hljs-string">&#x27;\tPIL&#x27;</span>)<br><br>st_time = time.time()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>):<br>    img = skimage.io.imread(paths[i])<br><span class="hljs-built_in">print</span>(time.time() - st_time, <span class="hljs-string">&#x27;\tskimage&#x27;</span>)<br><br>st_time = time.time()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>):<br>    img = torchvision.io.read_image(paths[i])<br><span class="hljs-built_in">print</span>(time.time() - st_time, <span class="hljs-string">&#x27;\ttorchvision&#x27;</span>)<br></code></pre></td></tr></table></figure><p>本地（cpu）运行结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">1</span>.<span class="hljs-number">4684679508209229</span> cv2<br><span class="hljs-attribute">2</span>.<span class="hljs-number">0799291133880615</span> plt<br><span class="hljs-attribute">2</span>.<span class="hljs-number">2181248664855957</span> PIL<br><span class="hljs-attribute">2</span>.<span class="hljs-number">2203478813171387</span> skimage<br><span class="hljs-attribute">1</span>.<span class="hljs-number">4794716835021973</span> torchvision<br></code></pre></td></tr></table></figure><p>可以看到，<code>plt</code>、<code>PIL</code> 和 <code>skimage</code> 落后 <code>cv2</code> 和 <code>torchvision</code> 很多，而后两者之间 <code>cv2</code> 略胜一筹。我反复运行了多次均是类似的结果。</p><p>有趣的是，这段代码在服务器上的运行结果是：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">1</span>.<span class="hljs-number">348557472229004</span> cv2<br><span class="hljs-attribute">1</span>.<span class="hljs-number">6608097553253174</span> plt<br><span class="hljs-attribute">1</span>.<span class="hljs-number">7273526191711426</span> PIL<br><span class="hljs-attribute">1</span>.<span class="hljs-number">660323143005371</span> skimage<br><span class="hljs-attribute">0</span>.<span class="hljs-number">5512754917144775</span> torchvision<br></code></pre></td></tr></table></figure><p><code>torchvision</code> 完胜其他方式。</p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>计算机视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[CS231n]9·Deep Reinforcement Learning</title>
    <link href="/blog-main/2021/04/06/CS231n-9%C2%B7Deep-Reinforcement-Learning/"/>
    <url>/blog-main/2021/04/06/CS231n-9%C2%B7Deep-Reinforcement-Learning/</url>
    
    <content type="html"><![CDATA[<p><a href="https://cs231n.github.io/">CS231n Convolutional Neural Networks for Visual Recognition</a></p><p>https://www.bilibili.com/video/BV1nJ411z7fe</p><span id="more"></span><h2 id="reinforcement-learning">Reinforcement Learning</h2><p>强化学习要解决的问题类似于<strong>玩一个游戏</strong>：Environment 告诉 Agent 当前状态 <span class="math inline">\(s_t\)</span>，Agent 据此做出一个动作 <span class="math inline">\(a_t\)</span>，Environment 为该动作给出 <span class="math inline">\(r_t\)</span> 的奖励，并更新状态为 <span class="math inline">\(s_{t+1}\)</span>，然后反复上述过程。</p><p><img src="rl.png" width="50%" height="50%" /></p><h2 id="markov-decision-process">Markov Decision Process</h2><p>强化学习可以用马尔可夫过程进行数学形式化的表述，因为它满足 <strong>Markov property</strong>，即当前状态完全定义了未来的状态，或者说未来的状态仅受当前状态、而不受过去状态的影响。</p><p>马尔可夫过程定义为：<span class="math inline">\((\mathcal S,\mathcal A,\mathcal R,\mathbb P,\gamma)\)</span>，其中 <span class="math inline">\(\mathcal S\)</span> 表示所有可能的状态集合，<span class="math inline">\(\mathcal A\)</span> 表示所有可能的动作集合，<span class="math inline">\(\mathcal R\)</span> 表示奖励在给定 <span class="math inline">\(\mathcal S\)</span> 和 <span class="math inline">\(\mathcal A\)</span> 下的分布，<span class="math inline">\(\mathbb P\)</span> 表示状态的转移概率，<span class="math inline">\(\gamma\)</span> 是 discount factor，用于对近期奖励和远期奖励进行加权。</p><p>马尔可夫过程的流程是：</p><ul><li>在 <span class="math inline">\(t=0\)</span> 时刻，environment 对初始状态进行采样 <span class="math inline">\(s_0\sim p(s_0)\)</span>；</li><li>接下来，反复执行以下步骤直到结束：<ul><li>Agent 选择动作 <span class="math inline">\(a_t\)</span>；</li><li>Environment 对奖励进行采样 <span class="math inline">\(r_t\sim \mathcal R(\bullet\mid s_t,a_t)\)</span>；</li><li>Environment 对下一个状态进行采样 <span class="math inline">\(s_{t+1}\sim\mathbb P(\bullet\mid s_t,a_t)\)</span>；</li><li>Agent 收到奖励 <span class="math inline">\(r_t\)</span> 和下一个状态 <span class="math inline">\(s_{t+1}\)</span>。</li></ul></li></ul><p>一个决策 <span class="math inline">\(\pi\)</span> 就是一个从 <span class="math inline">\(\mathcal S\)</span> 到 <span class="math inline">\(\mathcal A\)</span> 的函数，决定在每一个状态下执行什么动作，而我们的目标就是找到最佳的决策 <span class="math inline">\(\pi^*\)</span> 以使得累计奖励 <span class="math inline">\(\sum\limits_{t\geqslant 0}\gamma^t r_t\)</span> 最大。但是，由于我们的众多变量都是随机变量，所以我们实际上要最大化的是累计奖励的期望，即： <span class="math display">\[\pi^*=\arg\max_\pi\mathbb E\left[\sum_{t\geqslant0}\gamma^tr_t\mid \pi\right]\]</span> 其中 <span class="math inline">\(s_0\sim p(s_0),\,a_t\sim\pi(\bullet\mid s_t),\,s_{t+1}\sim \mathbb P(\bullet\mid s_t,a_t)\)</span>.</p><p><br></p><p>服从一个决策将会产生一个样本路径（轨迹）：<span class="math inline">\(s_0,a_0,r_0,s_1,a_1,r_1,\ldots\)</span>，我们需要一种方式量化从这个初始状态 <span class="math inline">\(s_0\)</span> 开始的好坏程度，因此定义 <strong>value function</strong>： <span class="math display">\[V^{\pi}(s)=\mathbb E\left[\sum_{t\geqslant 0}\gamma ^tr_t\mid s=s_0,\pi\right]\]</span> 进一步的，我们还可以定义从初始状态 <span class="math inline">\(s_0\)</span> 和初始动作 <span class="math inline">\(a_0\)</span> 开始服从某一个决策 <span class="math inline">\(\pi\)</span> 的好坏程度，称为 <strong>Q-value function</strong>： <span class="math display">\[Q^{\pi}(s,a)=\mathbb E\left[\sum_{t\geqslant 0}\gamma^tr_t\mid s_0=s,a_0=a,\pi\right]\]</span> 那么，最佳的 Q-value function <span class="math inline">\(Q^*\)</span> 就是在给定 <span class="math inline">\((s,a)\)</span> 下找到使得 <span class="math inline">\(Q^{\pi}(s,a)\)</span> 最大的决策方式 <span class="math inline">\(\pi\)</span>： <span class="math display">\[Q^*(s,a)=\max_\pi\mathbb E\left[\sum_{t\geqslant 0}\gamma^tr_t\mid s_0=s,a_0=a,\pi\right]\]</span> 而一个核心的等式——<strong>Bellman Equation</strong> 告诉我们 <span class="math inline">\(Q^*\)</span> 满足： <span class="math display">\[Q^*(s,a)=\mathbb E_{s&#39;\sim \mathcal E}\left[r+\gamma\max_{a&#39;}Q^*(s&#39;,a&#39;)\mid s,a\right]\]</span></p><blockquote><p>推导： <span class="math display">\[\begin{align}Q^\pi(s,a)&amp;=\mathbb E\left[r_{t}+\gamma r_{t+1}+\gamma^2r_{t+2}+\cdots\mid s_0=s,a_0=a,\pi\right]\\&amp;=\mathbb E[r_t+\gamma(r_{t+1}+\gamma r_{t+2}+\cdots)\mid s_0=s,a_0=a,\pi]\\&amp;=\mathbb E[r_t+\gamma Q^{\pi}(s&#39;,a&#39;)\mid s_0=s,a_0=a,\pi]\end{align}\]</span> 其中，<span class="math inline">\(s&#39;,a&#39;\)</span> 是下一时刻的状态和动作。因此，<span class="math inline">\(Q^\pi(s,a)\)</span> 最大值就是当前奖励 <span class="math inline">\(r_t\)</span> 加上在接下来的状态中用最佳的动作得到的最大收益。</p></blockquote><p><br></p><p>为了求解上述最佳 Q-value functino <span class="math inline">\(Q^*\)</span>，根据 Bellman Equation，我们可以迭代求解，迭代格式为： <span class="math display">\[Q_{i+1}(s, a)=\mathbb E\left[r+\gamma \max_{a&#39;}Q_i(s&#39;,a&#39;)\mid s,a\right]\]</span> 可以证明，该迭代格式收敛到 <span class="math inline">\(Q^*\)</span>.</p><p>这么做理论上可行，但是实际中我们需要遍历所有可能的 <span class="math inline">\((s,a)\)</span> 对，这是不可行的。为了解决这个问题，我们可以找一个函数去近似 <span class="math inline">\(Q(s,a)\)</span>，比如用一个神经网络——这就引出了 Q-Learning。</p><h2 id="q-learning">Q-Learning</h2><p>正如前文所述，Q-Learning 的目的是用一个神经网络近似 <span class="math inline">\(Q^*(s,a)\)</span>，即： <span class="math display">\[Q(s,a;\theta)\approx Q^*(s,a)\]</span> 其中 <span class="math inline">\(\theta\)</span> 是我们的网络的参数。</p><p>要训练这个网络，关键是定义损失函数。由于 <span class="math inline">\(Q^*\)</span> 需要满足 Bellman Equation，我们可以用当前网络到 Bellman Equation 的 MSE 误差作为损失函数： <span class="math display">\[\begin{align}&amp;L_i(\theta_i)=\mathbb E_{s,a\sim \rho(\bullet)}\left[(y_i-Q(s,a;\theta_i))^2\right]\\\text{where}\;\;&amp;y_i=\mathbb E_{s\sim \mathcal{E}}\left[r+\gamma\max_{a&#39;}Q(s&#39;,a&#39;;\theta_{i-1})\mid s,a\right]\end{align}\]</span> 在训练玩游戏时，一个自然的想法是用最近的若干连续的画面帧作为神经网络的输入，但这么做将会导致一些问题：首先这些样本将高度相关，以至于学习的效率低；其次，当前网络的参数将决定接下来输入样本的分布，例如当前的动作是向左走，那么接下来的训练样本将大量采样自画面左侧。为了解决这些问题，我们使用 <strong>experience replay</strong>：维护一个 replay memory table，其存储的是 <span class="math inline">\((s_t,a_t,r_t,s_{t+1})\)</span> 组，在训练过程中不断更新其内容；在训练时，一个 minibatch 是从这个 table 中随机选取的，而非采取若干连续的状态，这样就解决了上述问题。</p><h2 id="policy-gradients">Policy Gradients</h2><p>Q-Learning 的问题在于 Q-function 可以非常复杂，而我们需要的策略可能仅仅是一个非常简单的动作而已。这就引出了 Policy Gradients.</p><p>首先，我们定义决策空间为：<span class="math inline">\(\Pi=\{\pi_\theta,\theta\in\mathbb R^m\}\)</span>. 对每个决策，我们定义其值为： <span class="math display">\[J(\theta)=\mathbb E\left[\sum_{t\geqslant 0}\gamma^tr_t\mid \pi_\theta\right]\]</span> 我们的目标是最大化 <span class="math inline">\(J(\theta)\)</span>，即找到 <span class="math inline">\(\theta^*=\arg\max\limits_\theta J(\theta)\)</span>. 容易想到我们对 <span class="math inline">\(\theta\)</span> 做梯度上升即可。</p><p><span class="math inline">\(J(\theta)\)</span> 可写作： <span class="math display">\[J(\theta)=\mathbb E_{\tau\sim p(\tau;\theta)}[r(\tau)]=\int_\tau r(\tau)p(\tau;\theta)\mathrm d\tau\]</span> 其中 <span class="math inline">\(\tau=(s_0,a_0,r_0,s_1,\ldots)\)</span> 是一个决策轨迹，<span class="math inline">\(r(\tau)\)</span> 为其奖励。现在我们求解其梯度： <span class="math display">\[\begin{align}\nabla_\theta J(\theta)&amp;=\int_\tau r(\tau)\nabla_\theta p(\tau;\theta)\mathrm d\tau\\&amp;=\int_\tau r(\tau) p(\tau;\theta)\nabla_\theta\ln p(\tau;\theta)\mathrm d\tau\\&amp;=\mathbb E_{\tau\sim p(\tau;\theta)}[r(\tau)\nabla_\theta\ln p(\tau;\theta)]\end{align}\]</span> 由于 <span class="math inline">\(p(\tau;\theta)=\prod\limits_{t\geqslant0}p(s_{t+1}\mid s_t,a_t)\pi_\theta(a_t\mid s_t)\)</span>，故： <span class="math display">\[\begin{align}\nabla_\theta \ln p(\tau;\theta)&amp;=\nabla_\theta \sum_{t\geqslant 0}\ln p(s_{t+1}\mid s_t,a_t)+\ln \pi_\theta(a_t\mid s_t)\\&amp;=\nabla_\theta \sum_{t\geqslant 0}\ln\pi_\theta(a_t\mid s_t)\end{align}\]</span> 于是乎，我们可以对轨迹 <span class="math inline">\(\tau\)</span> 进行采样，然后计算： <span class="math display">\[\nabla_\theta J(\theta)\approx\sum_{t\geqslant 0}r(\tau)\nabla_\theta \ln\pi_\theta(a_t\mid s_t)\]</span> 但是这个 gradient estimator 很难达到我们想要的效果，因为其方差很大。为此，人们提出了若干减小方差的方法：</p><ul><li><p>Idea 1： <span class="math display">\[\nabla_\theta J(\theta)\approx \sum_{t\geqslant 0}\left(\sum_{t&#39;\geqslant t} r_{t&#39;}\right)\nabla_\theta\ln\pi_\theta(a_t\mid s_t)\]</span> 用从当前决策开始直到结束的过程中得到的奖励代替 <span class="math inline">\(r(\tau)\)</span>。</p></li><li><p>Idea 2： <span class="math display">\[\nabla_\theta J(\theta)\approx \sum_{t\geqslant 0}\left(\sum_{t&#39;\geqslant t} \gamma^{t&#39;-t}r_{t&#39;}\right)\nabla_\theta\ln\pi_\theta(a_t\mid s_t)\]</span> 加入一个 discount factor <span class="math inline">\(\gamma\)</span>.</p></li><li><p>Idea：引入 baseline function： <span class="math display">\[\nabla_\theta J(\theta)\approx \sum_{t\geqslant 0}\left(\sum_{t&#39;\geqslant t} \gamma^{t&#39;-t}r_{t&#39;}-b(s_t)\right)\nabla_\theta\ln\pi_\theta(a_t\mid s_t)\]</span></p></li></ul><p>如何选取 baseline 呢？一个最简单的方法是取已经遍历过的决策轨迹的奖励的移动平均。不过利用 Q-value function，我们能得到更好的 baseline： <span class="math display">\[\nabla_\theta J(\theta)\approx \sum_{t\geqslant 0}\left(Q^{\pi_\theta}(s_t,a_t)-V^{\pi_\theta}(s_t)\right)\nabla_\theta\ln\pi_\theta(a_t\mid s_t)\]</span> 直观上讲，我们希望 <span class="math inline">\(A^{\pi_\theta}(s_t,a_t)=Q^{\pi_\theta}(s_t,a_t)-V^{\pi_\theta}(s_t)\)</span> 越大越好，因为这表示我们选取的动作 <span class="math inline">\(a_t\)</span> 使得这之后我们得到的奖励期望值大于随便取一个动作得到的奖励期望值，我们称 <span class="math inline">\(A^\pi(s,a)\)</span> 为 <strong>advantage function.</strong></p><p>现在我们得到了 Actor-Critic Algorithm，即结合 Policy Gradients 和 Q-learning 的算法：同时训练一个 actor，即决策，和 critic，即 Q-function——Actor 决定做一个动作，critic 告诉它这个动作的优劣以及应该如何调整；同时 critic 无需对所有 <span class="math inline">\((s,a)\)</span> 对进行学习，而只需要学习 actor 做出的那些决策。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Stanford CS231n</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>梧桐山顶的乐队</title>
    <link href="/blog-main/2021/04/04/%E6%A2%A7%E6%A1%90%E5%B1%B1%E9%A1%B6%E7%9A%84%E4%B9%90%E9%98%9F/"/>
    <url>/blog-main/2021/04/04/%E6%A2%A7%E6%A1%90%E5%B1%B1%E9%A1%B6%E7%9A%84%E4%B9%90%E9%98%9F/</url>
    
    <content type="html"><![CDATA[<p>登上顶峰俯瞰整座城市，期望中，我应该感到的是历经艰苦后的成就感和开阔视野带来的震撼感。确实如此，但除此之外，他的存在给我的这次登顶添加了更多的思考与感悟。</p><p><img src="psc.jpeg" /></p><p>他戴着一顶帽子、一个面罩、堆叠两只墨镜，整个面部捂的严严实实，看不见一丁点面容。从他的口中我了解到，他是一个小乐队的一员，日常是周转在深圳市的各个区找场地开演唱会。每场演唱会来的人不多，几百近千人罢了，但他们乐队总会寻找力所能及的好的场地条件。他说，他的梦想是开一次万人演唱会。他说话时的语气带着一丝小心，但是说出“万人演唱会”这几个字时却又非常坚定，即使他清楚地知道，他现在的能力尚不足以做到这一点。</p><p>他上午11点来到山顶上，接好有些年代的二手音箱，将一直演唱到下午5点，目的是售卖￥20一张、永久有效的演唱会门票。我非常不解，他完全可以在闹市区演唱，吸引更多的人流；也可以选择在互联网上抛头露面，轻松得到巨大的流量；但他却选择用一种不仅朴实、而且艰辛的方式——爬上一座城市的最高峰——来诠释他坚定的信念和对梦想的执着。都是爬到了山顶的人，都知道这一路的不易，也因此，一种莫名的共情油然而生。</p><p>我听了一首原创歌，内容大抵是为了梦想而孤独漂泊。这时再回头眺望整座城市，昏暗的天空下，镜头被无限地放大，我仿佛看到了每天穿梭在这钢筋水泥之中，与他一样，为梦想而奔波的无数。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[CS231n]8·Generative Models</title>
    <link href="/blog-main/2021/03/28/CS231n-8%C2%B7Generative-Models/"/>
    <url>/blog-main/2021/03/28/CS231n-8%C2%B7Generative-Models/</url>
    
    <content type="html"><![CDATA[<p><a href="https://cs231n.github.io/">CS231n Convolutional Neural Networks for Visual Recognition</a></p><p>https://www.bilibili.com/video/BV1nJ411z7fe</p><span id="more"></span><h2 id="generative-models">Generative Models</h2><p>Generative models 隶属于 unsupervised learning 的范畴，其数据集没有标签。其目的是根据输入学会一种数据集的分布，并生成具有这种分布的新的图像。</p><p><img src="generative.png" width="60%" height="60%" /></p><hr /><h2 id="pixel-rnn-pixel-cnn">Pixel RNN &amp; Pixel CNN</h2><p>Pixel RNN 和 Pixel CNN 都属于 <strong>Fully visible belief network</strong>，其思想是对于图像 <span class="math inline">\(x\)</span>，计算其似然 <span class="math inline">\(p_\theta(x)\)</span>： <span class="math display">\[p_\theta(x)=\prod_{i=1}^n p_\theta(x_i\mid x_1,\ldots,x_{i-1})\]</span> 其中 <span class="math inline">\(x_i\)</span> 是 <span class="math inline">\(x\)</span> 的一个像素。<span class="math inline">\(p_\theta(x_i\mid x_1,\ldots,x_{i-1})\)</span> 含义是，在已经生成像素点 <span class="math inline">\(x_1\ldots x_{i-1}\)</span> 的条件下，下一个像素点为 <span class="math inline">\(x_i\)</span> 的概率。那么根据极大似然法的思想，我们想生成一个好的图像 <span class="math inline">\(x\)</span>，目标就是最大化似然函数 <span class="math inline">\(p_\theta(x)\)</span>.</p><p>这跟网络有什么关系呢？上式的计算未免过于复杂，而我们知道神经网络善于对一个复杂的计算过程建模，因此我们可以设计一些网络达到目的。</p><h3 id="pixel-rnn">Pixel RNN</h3><p>我们从左上角开始，按照下图所示顺序生成新的像素：</p><p><img src="pixelrnn.png" width="30%" height="30%" /></p><p>自然而然地，这个时序过程可以用 RNN/LSTM 来完成。</p><p>其缺点是每个像素是顺次生成的，这导致网络的生成速度较慢，其训练速度也慢。</p><h3 id="pixel-cnn">Pixel CNN</h3><p>把 RNN 换成 CNN，根据周围像素生成新的像素。</p><p><img src="pixelcnn.png" width="40%" height="40%" /></p><p>训练比 Pixel RNN 快，但生成依旧是顺次生成，速度依旧较慢。</p><hr /><h2 id="variational-autoencoders-vae">Variational Autoencoders (VAE)</h2><h3 id="autoencoders">Autoencoders</h3><p>在学习 VAE 之前，我们首先需要了解 Autoencoders.</p><p>Autoencoders 是一种 unsupervised 的 dimensionality reduction 的方法。其思想是，用一个 CNN 将输入数据 <span class="math inline">\(x\)</span> 降维为 <span class="math inline">\(z\)</span>，然后再用一个 CNN 将降维后的数据 <span class="math inline">\(z\)</span> 恢复为原大小 <span class="math inline">\(\hat x\)</span>，并定义 loss function 为：<span class="math inline">\(||x-\hat x||^2\)</span>，这样在训练后，前一个 CNN 就可以作为数据降维的 encoder 了。注意这个过程并没有使用标签，所以这是 unsupervised 的。</p><p>Autoencoders 可以用于 supervised model 的初始化，帮助模型的学习。</p><p><img src="autoencoders.png" width="40%" height="40%" /></p><h3 id="vae">VAE</h3><p>我们假设真实数据 <span class="math inline">\(\{x^{(i)}\}_{i=1}^N\)</span> 是由一个未知的、隐藏的 <span class="math inline">\(z\)</span> 采样得到的，也就是说，给定 <span class="math inline">\(z^{(i)}\)</span>，<span class="math inline">\(x^{(i)}\)</span> 采样自概率分布 <span class="math inline">\(p_\theta(x\mid z^{(i)})\)</span>，而这里的 <span class="math inline">\(z^{(i)}\)</span> 又采样自一个先验概率分布：<span class="math inline">\(p_\theta(z)\)</span>. <img src="vae.png" width="40%" height="40%" /></p><p>我们可以合理地选取高斯分布为 <span class="math inline">\(z\)</span> 的先验分布；又由于 <span class="math inline">\(p_\theta(x\mid z^{(i)})\)</span> 是一个复杂的东西，所以我们可以用一个 decoder network 对它进行建模。那如何训练这个神经网络呢？不同于 Fully visible belief network，VAE 将似然写作： <span class="math display">\[p_\theta(x)=\int p_\theta(z)p_\theta(x\mid z)\mathrm dz\]</span></p><p>根据极大似然法的思想，最大化这个似然函数就是训练神经网络的过程了。</p><p>这时问题出现了，因为积分的存在，我们无法处理 <span class="math inline">\(p_\theta(x)\)</span> 这个函数，也就无法训练神经网络；我们也没法处理后验分布：<span class="math inline">\(p_\theta(z\mid x)=p_\theta(x\mid z)p_\theta(z)/p_\theta(x)\)</span>。</p><p>对于第二个问题，我们再定义一个神经网络 <span class="math inline">\(q_\phi(z\mid x)\)</span> 去近似 <span class="math inline">\(p_\theta(z\mid x)\)</span>：</p><p><img src="vae2.png" width="70%" height="70%" /></p><p>两个神经网络的输出都是均值和方差，如此，在 inference 阶段，我们可以选取以该均值和方差为统计量的正态分布作为采样的概率分布。</p><p>对第一个问题的解决方法是，我们训练一个 <span class="math inline">\(p_\theta(x)\)</span> 的下界： <span class="math display">\[\begin{align}\log p_\theta(x^{(i)})&amp;=\mathbb E_{z\sim q_\phi(z\mid x^{(i)})}\left[\log p_\theta(x^{(i)})\right]\\&amp;=\mathbb E_z\left[\log\frac{p_\theta({x^{(i)}\mid z})p_\theta(z)}{p_\theta(z\mid x^{(i)})}\right]\\&amp;=\mathbb E_z\left[\log\frac{p_\theta({x^{(i)}\mid z})p_\theta(z)}{p_\theta(z\mid x^{(i)})}\frac{q_\phi(z\mid x^{(i)})}{q_\phi(z\mid x^{(i)})}\right]\\&amp;=\mathbb E_z\left[\log p_\theta(x^{(i)}\mid z)\right]-\mathbb E_z\left[\log\frac{q_\phi(z\mid x^{(i)})}{p_\theta(z)}\right]+\mathbb E_z\left[\log\frac{q_\phi(z\mid x^{(i)})}{p_\theta(z\mid x^{(i)})}\right]\\&amp;=\mathbb E_z\left[\log p_\theta(x^{(i)}\mid z)\right]-D_{KL}(q_\phi(z\mid x^{(i)})||p_\theta(z))+D_{KL}(q_\phi(z\mid x^{(i)})||p_\theta(z\mid x^{(i)}))\\&amp;\geqslant \mathbb E_z\left[\log p_\theta(x^{(i)}\mid z)\right]-D_{KL}(q_\phi(z\mid x^{(i)})||p_\theta(z))\end{align}\]</span> 于是乎，我们训练神经网络的过程，就是最大化这个下界 <span class="math inline">\(\mathcal L(x^{(i)},\theta,\phi)=\mathbb E_z\left[\log p_\theta(x^{(i)}\mid z)\right]-D_{KL}(q_\phi(z\mid x^{(i)})||p_\theta(z))\)</span> 的过程。如何理解这个过程呢？最大化 <span class="math inline">\(\mathcal L(x^{(i)},\theta,\phi)\)</span>，就要最大化第一项——即努力重新构造出输入数据，以及最小化第二项——即努力使得近似后验分布接近于我们预定的先验分布。综上，我们的训练过程如下：</p><p><img src="vae3.png" width="80%" height="80%" /></p><p><br></p><p>现在我们训练好了一个 VAE 网络，就可以用它来生成数据了。从 <span class="math inline">\(z\sim N(0,I)\)</span> 对 <span class="math inline">\(z\)</span> 进行采样，然后用训练的 decoder network 得到 <span class="math inline">\(\mu_{x\mid z}\)</span> 和 <span class="math inline">\(\Sigma_{x\mid z}\)</span>，随后从 <span class="math inline">\(x\mid z\sim N(\mu_{x\mid z},\Sigma_{x\mid z})\)</span> 得到生成的数据 <span class="math inline">\(\hat x\)</span>：</p><p><img src="vae4.png" width="40%" height="40%" /></p><p>以下是生成 MNIST 数字的 VAE，选取 <span class="math inline">\(z\)</span> 为 <span class="math inline">\(2\)</span> 维时可以得到：</p><p><img src="vae5.png" width="40%" height="40%" /></p><p>可以看到数字的渐变过程，还是蛮有趣的。</p><hr /><h2 id="gans">GANs</h2><p>PixelCNNs 和 VAEs 都显式地对概率密度函数 <span class="math inline">\(p_\theta(x)\)</span> 进行了定义，我们是否可以不给出一个显式的概率密度函数呢？GANs 网络就是这样的。</p><p>我们没有直接的方法从训练集里找出一个概率分布并据此采样以生成新的图像，但我们能从一个简单分布采样，例如随机噪声；随后我们不断改变这个简单的分布，以最终逼近真正的分布。这个复杂的过程显然用神经网络建模是最好不过的了：</p><p><img src="gan.png" width="40%" height="40%" /></p><p>那么我们如何训练这个神经网络呢？方法是用两个神经网络进行博弈——Generator network 负责生成新的图像，Discriminator network 负责辨别输入图像是真的还是假的（输出真的概率）。训练时 Generator 的目标是尽可能地骗过 Discriminator，而 Discriminator 的目标就是不被 Generator 骗到。</p><p><img src="gan2.png" width="60%" height="60%" /></p><p>如此，两个网络在对抗中共同成长，一路相爱相杀，最后都能取得较好的成效。</p><p><br></p><p>我们的目标函数定义为 Minimax objective function： <span class="math display">\[\min_{\theta_g}\max_{\theta_d}\left[\mathbb E_{x\sim p_{data}}\ln D_{\theta_d}(x)+\mathbb E_{z\sim p(z)}\ln(1-D_{\theta_d}(G_{\theta_g}(z))) \right]\]</span> 先看内层 <span class="math inline">\(\max\)</span> 的部分，第一项中，<span class="math inline">\(x\)</span> 取自真实分布，<span class="math inline">\(D_{\theta_d}(x)\)</span> 是 Discriminator 认为真的概率，所以这一项是要最大化真实图像是真的的概率；第二项中，<span class="math inline">\(z\)</span> 取自生成网络，<span class="math inline">\(1-D_{\theta_d}(G_{\theta_g}(z))\)</span> 是 Discriminator 认为假的概率，所以这一项是要最大化假图像是假的的概率。因此，内层值越高，代表 Discriminator 越准确，这正好不是 Generator 希望看到的，所以外层套一个 <span class="math inline">\(\min\)</span>，表示 Generator 希望 Discriminator 的最大得分尽可能低。这正是所谓的 minimax 算法。</p><p>训练时这个目标函数可以拆成两部分，对 Discirminator，用梯度上升使得： <span class="math display">\[\max_{\theta_d}\left[\mathbb E_{x\sim p_{data}}\ln D_{\theta_d}(x)+\mathbb E_{z\sim p(z)}\ln(1-D_{\theta_d}(G_{\theta_g}(z)))\right]\]</span> 对 Generator，用梯度下降使得： <span class="math display">\[\min_{\theta_g}\mathbb E_{z\sim p(z)}\ln(1-D_{\theta_d}(G_{\theta_g}(z)))\]</span> 因为前一项与 <span class="math inline">\(\theta_g\)</span> 无关，所以只有这后一项。</p><p>然而在实践中，优化这个目标函数并不能工作得很好。这是因为 <span class="math inline">\(y=\ln(1-x)\)</span> 的特性是在 <span class="math inline">\(x\)</span> 接近 <span class="math inline">\(0\)</span> 时梯度较小，<span class="math inline">\(x\)</span> 接近 <span class="math inline">\(1\)</span> 时梯度很大，于是在 Generator 这里，生成的图像很假的时候学习较慢，生成的图像已经很逼真的时候学习反而很快，不符合我们的预期。因此，我们改用梯度上升训练 Generator，使得： <span class="math display">\[\max_{\theta_g}\mathbb E_{z\sim p(z)}\ln(D_{\theta_d}(G_{\theta_g}(z)))\]</span> 这样梯度就符合我们的预期了。</p><p>总结一下，训练 GANs 的流程为：</p><p><img src="gantrain.png" width="80%" height="80%" /></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Stanford CS231n</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[CS231n]7·Visualizing and Understanding</title>
    <link href="/blog-main/2021/03/16/CS231n-7%C2%B7Visualizing-and-Understanding/"/>
    <url>/blog-main/2021/03/16/CS231n-7%C2%B7Visualizing-and-Understanding/</url>
    
    <content type="html"><![CDATA[<p><a href="https://cs231n.github.io/">CS231n Convolutional Neural Networks for Visual Recognition</a></p><p>https://www.bilibili.com/video/BV1nJ411z7fe</p><span id="more"></span><h2 id="first-layer-visualize-filters">First Layer: Visualize Filters</h2><p>对于第一层卷积层，我们可以可视化每个 filter 的权重，并得到一系列可解释的图片：</p><p><img src="first.png" width="80%" height="80%" /></p><p>可以看出，第一层卷积层在寻找一些点与线，以及对立的颜色，可以认为是在寻找物体与物体之间的分割线。</p><p>为什么可视化 filter 的权重是有道理的呢？filter 所做的就是与所有训练数据进行内积运算，而我们知道，两个向量越相近，其内积结果越大，因此，filter 图片上的一条白线，可以认为是众多数据中存在这么一条线，使得 filter 学到了这一特征。</p><hr /><h2 id="last-layer">Last Layer</h2><h3 id="nearst-neighbors">Nearst Neighbors</h3><p>最后一层的输出其实可以看作是每个图像的特征向量，因此我们可以画出 nearst neighbors：</p><p><img src="last-nn.png" width="80%" height="80%" /></p><p>可以看见，这比我们在第一节课直接用像素空间进行距离的衡量要准确很多。</p><h3 id="dimensionality-reduction">Dimensionality Reduction</h3><p>将最后一层的向量使用降维方法，例如简单的 PCA 或更强大的 <strong>t-SNE</strong> 降维至 <span class="math inline">\(2\)</span> 维并作图。</p><p>在 MNIST 上，我们可以得到：</p><p><img src="last-dr.png" width="50%" height="50%" /></p><p>在 CIFAR-10 上，我们把降维后的点放置在网格中（准确来讲，是找出与每个网格点中心最相似的图片），得到如下可视化结果（高清版本参见 http://cs.stanford.edu/people/karpathy/cnnembed/）：</p><p><img src="last-dr2.png" width="80%" height="80%" /></p><hr /><h2 id="maximally-activating-patches">Maximally Activating Patches</h2><p>从网络中选择某层的某个 channel，输入若干图片并记录这个 channel 上的数值。由于使用的是卷积神经网络，所以每一个神经元其实感受的是输入图像的某一些小片。将最大激活的图像小片可视化出来，可以得到：</p><p><img src="map.png" width="50%" height="50%" /></p><hr /><h2 id="occlusion-experiments">Occlusion Experiments</h2><p>遮盖住输入图像的一部分之后再输入进 CNN，记录分类结果的概率。可以据此做出遮盖不同位置导致不同概率的热点图，如下：</p><p><img src="occlusion.png" width="80%" height="80%" /></p><p>显然，那些导致分类概率急剧变小的像素就是图像里关键的地方。</p><hr /><h2 id="saliency-maps">Saliency Maps</h2><p>Saliency Maps 和 Occlusion 的目的一样，都是想知道哪些像素对分类结果很重要。</p><p>计算每一类的得分对图像像素的梯度，取绝对值并取 RGB 三个 channel 中的最大值。这样做让我们知道如果一个像素发生了微小扰动，相应类的分类得分将发生多大的变化。如果变化很大，自然这个像素对该分类非常重要。</p><p><img src="saliency.png" width="80%" height="80%" /></p><p><br></p><p>由此，我们可以想到把 saliency maps 用在 semantic segmentation 中，并且这样做不需要 semantic segmentation 的训练数据。使用 <strong>GrabCut</strong> 算法可以做到这一点：</p><p><img src="grabcut.png" width="80%" height="80%" /></p><hr /><h2 id="intermediate-features-via-guided-backprop">Intermediate features via (guided) backprop</h2><p>借助和 Saliency Maps 类似的想法，如果我们取出神经网络中的某一个神经元并计算其对输入图像各个像素的梯度，那我们也能知道哪些像素对这个神经元非常重要。然而，与通常的 back propagation 不同，我们这时使用 guided backprop，只对通过 ReLU 的正的梯度进行反向传播，这样得到的图像更加清晰。</p><p><img src="gb.png" width="80%" height="80%" /></p><p><img src="gb2.png" width="80%" height="80%" /></p><hr /><h2 id="gradient-ascent">Gradient Ascent</h2><p>对于神经网络中的一个神经元，我们想知道什么类型的输入能够激活这个神经元。注意现在和之前的区别在于，我们现在不依赖于输入的数据，而是生成一个使得神经元激活的输入类型。为解决这个问题，我们可以采取 Gradient Ascent.</p><p>在 Gradient Descent 中，我们改变参数的值来最小化结果；而 Gradient Ascent 是一个相反的过程，我们固定参数而去改变输入图像的像素，以使得神经元被最大激活或者某一类的得分最大。同时我们需要正则化项，以避免我们生成的图像对我们的网络结构过拟合： <span class="math display">\[I^*=\arg\max_If(I)+R(I)\]</span> 其中 <span class="math inline">\(f(I)\)</span> 表示神经元的值或者某一类的得分，而 <span class="math inline">\(R(I)\)</span> 是正则化项。</p><hr /><h2 id="fooling-images-adversarial-examples">Fooling Images / Adversarial Examples</h2><p>首先任选一个输入图像、任选一类，然后更改图像来使得选定类别的分值最大，反复此操作直到网络被 fool.</p><p>结果我们发现，一个被更改为错误分类的图像其实并没有肉眼可见的变化。</p><p><img src="fool.png" width="80%" height="80%" /></p><hr /><h2 id="deepdream-amplify-existing-features">DeepDream: Amplify existing features</h2><p>对于给定一张图像和选定 CNN 中的某层，反复执行：</p><ol type="1"><li>前向传播到选定的层；</li><li>设置该层的梯度等于其激活值；</li><li>反向传播，计算对输入图像的梯度；</li><li>更新图像</li></ol><p>这样做以使得图像的特征被增强。</p><hr /><h2 id="section">...</h2>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Stanford CS231n</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[CS231n]6·Detection and Segmentation</title>
    <link href="/blog-main/2021/03/11/CS231n-6%C2%B7Detection-and-Segmentation/"/>
    <url>/blog-main/2021/03/11/CS231n-6%C2%B7Detection-and-Segmentation/</url>
    
    <content type="html"><![CDATA[<p><a href="https://cs231n.github.io/">CS231n Convolutional Neural Networks for Visual Recognition</a></p><p>https://www.bilibili.com/video/BV1nJ411z7fe</p><span id="more"></span><h2 id="overview">Overview</h2><p><img src="cvtasks.png" width="80%" height="80%" /></p><hr /><h2 id="semantic-segmentation">Semantic Segmentation</h2><p>对图像的每一个像素进行分类。</p><h3 id="idea-1-sliding-window">Idea 1: Sliding Window</h3><p><img src="slide.png" width="80%" height="80%" /></p><p>对每一个像素点用一个神经网络做一次分类。效率很差，进行了许多重复计算，nobody use it.</p><h3 id="idea-2-fully-convolutional">Idea 2: Fully Convolutional</h3><p><img src="fullyconv.png" width="80%" height="80%" /></p><p>经过一个卷积神经网络，每一个卷积层都<strong>保持原图像的大小</strong>，最终对每个像素都得到其各分类的得分，从而得到每个像素的分类结果。</p><p>问题：卷积非常耗时，因为每一个卷积层都保持原图像大小。</p><p><br></p><p>为了解决这个问题，我们可以先通过 <span class="math inline">\(\text{max-pool}\)</span> 或者 <span class="math inline">\(\text{conv}\)</span> 层进行 <strong>downsampling</strong>，然后再 <strong>upsampling</strong> 回原大小：</p><p><img src="fullyconv2.png" width="80%" height="80%" /></p><p>我们很清楚如何 downsampling，问题在于如何进行 <strong>upsampling</strong>：</p><ul><li><p><strong>Unpooling</strong>：</p><p><img src="unpooling.png" width="80%" height="80%" /></p></li><li><p><strong>Max-unpooling</strong>：</p><p><img src="maxunpooling.png" width="80%" height="80%" /></p></li><li><p><strong>Transpose Convolution</strong>：</p><p>视输入矩阵为 filter 的权重，按权重将 filter 累加到输出矩阵中。</p><p><img src="transconv.png" width="80%" height="80%" /></p><p><strong>Transpose Convolution</strong> 只能恢复卷积前后的大小，并不能恢复值，其原理是：</p><p>在 Convolution 操作时，我们可以将输入图像拉伸为 <span class="math inline">\(1\)</span> 维向量 <span class="math inline">\(X\in\mathbb R^{D}\)</span>，输出图像拉伸为 <span class="math inline">\(1\)</span> 维向量 <span class="math inline">\(Y\in\mathbb R^{D&#39;}\)</span>，则卷积核可以写作一个稀疏矩阵 <span class="math inline">\(C\in \mathbb R^{D&#39;\times D}\)</span>，使得 <span class="math inline">\(CX=Y\)</span>. 在 Upsampling 中，已知 <span class="math inline">\(C,Y\)</span>，欲得到 <span class="math inline">\(X\)</span>，Transpose Convolution 的操作是：<span class="math inline">\(X=C^TY\)</span>，这也是其名称的由来。显然这样做并没有恢复值，除非 <span class="math inline">\(CC^T=I\)</span>.</p></li></ul><hr /><h2 id="object-detection">Object Detection</h2><h3 id="idea-1-regression">Idea 1: Regression</h3><p>构造神经网络输出检测物体的类别以及框住物体的矩形框的坐标 <span class="math inline">\((x,y)\)</span> 和大小 <span class="math inline">\((w,h)\)</span>.</p><p>由于在 Object Detection 中，我们并不能实现知道输入图像中有多少个 object，所以其输出大小是不固定的，并且当 object 很多时输出将很大。因此这不是一个很好的方法。</p><h3 id="idea2-sliding-window">Idea2: Sliding Window</h3><p>滑动一个矩形框，每次对矩形框内的图像进行分类。</p><p><img src="obde_sliding.png" width="80%" height="80%" /></p><p>该方法也有很明显的问题：矩形框的位置和大小都是未知的，我们需要暴力遍历各种矩形框，每次都通过一个巨大的 CNN 网络得到类别，这无疑十分耗时。</p><h3 id="idea-3-region-proposals-r-cnn">Idea 3: Region Proposals &amp; R-CNN</h3><ul><li><p><strong>Region Proposals</strong>: 先通过一个网络给出物体可能存在的若干矩形范围，后续工作只需在这些范围内进行。<strong>Selective Search</strong> 是一种给出 Region Proposals 的方法，其运行速度非常快。</p></li><li><p><strong>R-CNN</strong>：在每一个给定的矩形范围内，我们可以构建 CNN 网络，输出该矩形内物体的类别以及这个矩形区域应该如何修正。注意由于每个矩形大小不同，而我们的网络要求特定大小的输入，所以我们会先对这些矩形进行变形使之符合网络的输入要求。具体见下图：</p><p><img src="R-CNN.png" width="80%" height="80%" /></p><p>该方法的不足在于，对每个矩形区域进行计算依然十分耗时耗力。</p></li><li><p><strong>Fast R-CNN</strong>：Idea 是我们先把图像输入进 CNN 网络，再对网络的输出结果进行矩形区域划分，这样图像只会经过一次 CNN 网络，从而提高运行速度。</p><p><img src="Fast R-CNN.png" width="60%" height="60%" /></p><p>这里有个问题是如何在 CNN 网络的输出结果上进行 Region Proposals：</p><ul><li><p><strong>RoI (Region of Intrest) Pool</strong>：首先把原图像上的矩形区域按比例投影到输出矩阵上，然后取整使矩形端点落在整点上，对于这个矩形，我们将其尽可能地等分成需要的形状，并在每一块中作 <span class="math inline">\(\text{max-pool}\)</span>，最终得到该区域的 feature.</p><p><img src="RoI pool.png" width="100%" height="100%" /></p><p>由于我们在这个过程中进行了两次近似——取整和划分，所以这种方法的问题在于我们得到的区域特征稍有偏离。</p></li><li><p><strong>RoI Align</strong>：仍然把原图像上的矩形区域投影到输出矩阵上，然后直接等距离划分成需要的形状；现在考察每一个小块，它们的顶点不一定在整点上，我们在其中取 <span class="math inline">\(4\)</span> 个位置，对每个位置用它周围的 <span class="math inline">\(4\)</span> 个整点的值作双线性插值，然后取 <span class="math inline">\(\text{max-pool}\)</span>，最终得到该区域的 feature.</p><p><img src="RoI align.png" width="100%" height="100%" /></p></li></ul></li><li><p><strong>Faster R-CNN</strong>：实测中发现，在 Fast R-CNN 中，Region Proposals 占了大部分时间。为了提高这部分的速度，Faster R-CNN 用一个 <strong>Region Proposal Network</strong> 在 CNN 的输出矩阵上作矩形区域划分，其余部分和 Fast R-CNN 相同。</p><p><img src="Faster R-CNN.png" width="60%" height="60%" /></p><p>Region Proposal Network (RPN) 的原理如下：</p><ol type="1"><li>想象以 feature map 的每一个位置为中心有固定大小的 anchor box，使用一个 CNN 预测这些 anchor box 中是否有物体，以及对 box 四边的修正值。</li><li>对于每一个位置，计算 <span class="math inline">\(K\)</span> 个不同的 anchor box，这样我们得到了 <span class="math inline">\(K\times H\times W\)</span> 个数值表示该 anchor 处是否有物体，以及 <span class="math inline">\(4K\times H\times W\)</span> 个数值表示 box 四边的修正值。</li><li>取得分最高的约 <span class="math inline">\(300\)</span> 个 anchor box 作为我们的 region proposals.</li></ol><p><img src="RPN.png" width="100%" height="100%" /></p><p>在训练时，我们会将四个 loss 一起训练—— RPN 二分类（判断是否是物体）、RPN anchor box 位置的 loss、最终分类（判断是何物）的 loss、最终 box 的坐标的 loss。</p></li><li><p><strong>YOLO / SSD / RetinaNet</strong>：Faster R-CNN 其实有两个阶段：第一个阶段是得到 region proposals，通过一个基础的 CNN 和 RPN 完成，第二个阶段是对每一个 region 进行分类以及修正边界，通过 RoI pool / align 之后输入到 CNN 完成。而 YOLO / SSD / RetinaNet 等网络只用一个阶段，具体的内容不在此课程中讲授。</p></li></ul><p><br></p><p>可以看出，Object Detection 有很多选择：首先，基础的网络有很多选择（VGG16 / ResNet-101 / Inception V2……）；接下来，我们可以选择二阶段的 Faster R-CNN 或者单阶段的 YOLO / SSD 或者混合的 R-FCN……这其中就有许多的 trade-offs. 例如，Faster R-CNN 比 SSD 更慢，但是准确率比后者更高；又如网络越大越深，效果越好，但训练也越难，耗时也越多……</p><p>这里有一篇综述论文：<a href="https://arxiv.org/pdf/1905.05055v2.pdf">Zou et al, “Object Detection in 20 Years: A Survey”, arXiv 2019</a></p><hr /><h2 id="instance-segmentation">Instance Segmentation</h2><h3 id="mask-r-cnn">Mask R-CNN</h3><p><img src="Mask R-CNN.png" width="80%" height="80%" /></p><p>类似于之前的 Faster R-CNN，不过最后一个卷积网络不是对区域内容进行分类，而是对每一类输出一个 mask。</p><p>Mask R-CNN 还可以做 pose estimation，即预测人体的姿势。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Stanford CS231n</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[CS231n]5·Recurrent Neural Networks</title>
    <link href="/blog-main/2021/03/06/CS231n-5%C2%B7Recurrent-Neural-Networks/"/>
    <url>/blog-main/2021/03/06/CS231n-5%C2%B7Recurrent-Neural-Networks/</url>
    
    <content type="html"><![CDATA[<p><a href="https://cs231n.github.io/">CS231n Convolutional Neural Networks for Visual Recognition</a></p><p>https://www.bilibili.com/video/BV1nJ411z7fe</p><span id="more"></span><h2 id="rnn-process-sequences">RNN: Process Sequences</h2><h3 id="overview">Overview</h3><p>所谓循环神经网络，可以看作是有时序性的神经网络，有时序电路的那种感觉。</p><p><img src="RNN.png" /></p><p>上图中，横向从左到右可以看作为若干时刻。普通的神经网络是 one to one 的结构——一个输入层，经过一系列隐藏层，到达一个输出层，这些步骤都是在一个时刻完成的；而 RNN 可以处理序列型的数据，其可以是 one to many, many to one, many to many 等结构。</p><p>one to many: 某一时刻给一个输入，在之后的若干时刻都有输出。典型例子是 Image Captioning，即输入一个图像、生成描述该图像的文字。</p><p>many to one: 在连续的几个时刻给输入，直到最后一个时刻给出输出。典型例子是 Audio Prediction。</p><p>many to many: 在连续的几个时刻给输入，输入完成后在之后的若干时刻都有输出。典型例子是 Video Captioning，即生成描述视频的文字。</p><p>many to many: 在连续的几个时刻给输入，同时不断地输出。典型例子是 Video classification on frame level。</p><h3 id="forward">Forward</h3><p><img src="h.png" width="70%" height="70%" /></p><p>RNN 向前传播的 key idea 是：每一个神经元有一个“隐藏”的和时序相关的向量 <span class="math inline">\(h_t\)</span>，它根据某<strong>不随时序变化</strong>的参数 <span class="math inline">\(W\)</span> 和当前的输入 <span class="math inline">\(x_t\)</span> 更新，即： <span class="math display">\[h_t=f_W(h_{t-1}, x_t)\]</span> 随后可以根据情况（one to many / many to one / many to many, etc.）决定如何用 <span class="math inline">\(h_t\)</span> 去更新输出 <span class="math inline">\(y_t\)</span>.</p><p>例如，一个简单的情形可以是： <span class="math display">\[\begin{align}h_t&amp;=\tanh(W_{hh}h_{t-1}+W_{xh}x_t)\\y_t&amp;=W_{hy}h_t\end{align}\]</span></p><h3 id="computational-graph">Computational Graph</h3><p>为了方便 Backpropagation 的推导，computational graph 是非常重要的技巧。显然，对于不同结构（one to many / many to one / many to many, etc.），它们的 computational graph 会不同，但大同小异：</p><table><thead><tr class="header"><th><img src="cg%20mto.png" /></th><th><img src="cg%20mtm.png" /></th></tr></thead><tbody><tr class="odd"><td><img src="cg%20otm.png" /></td><td><img src="cg%20otm2.png" /></td></tr></tbody></table><p>注意，在 one to many 结构中，我们可以用前一时刻的输出作为下一时刻的输入。</p><p>另外我们还可以把 many to one 和 one to many 连起来，形成 sequence to sequence 的效果。</p><p><img src="cg sts.png" width="70%" height="70%" /></p><h3 id="backpropagation">Backpropagation</h3><p><img src="bp.png" width="70%" height="70%" /></p><p>向前传播是按照时序计算的，于是反向传播就逆着时序传播。但是这里有一个问题，如果时序序列很长，这个过程会占用很大的内存。解决方案是 <strong>Truncated</strong> Backpropagation:</p><p><img src="Truncated.png" /></p><p>把整个时序分段，每次向前传播一段后就对这段反向传播。</p><h3 id="rnn-tradeoffs">RNN Tradeoffs</h3><p>RNN Advantages:</p><ul><li>Can process any length input</li><li>Computation for step t can (in theory) use information from many steps back</li><li>Model size doesn’t increase for longer input</li><li>Same weights applied on every timestep, so there is symmetry in how inputs are processed.</li></ul><p>RNN Disadvantages:</p><ul><li>Recurrent computation is slow</li><li>In practice, difficult to access information from many steps back</li></ul><hr /><h2 id="lstm-long-short-term-memory">LSTM (Long Short Term Memory)</h2><h3 id="rnn-gradient-flow">RNN Gradient Flow</h3><p>RNN 的在一个时钟中的更新为： <span class="math display">\[h_t=\tanh(W_{hh}h_{t-1}+W_{xh}x_t)=\tanh\left(W\begin{pmatrix}h_{t-1}\\x_t\end{pmatrix}\right)\]</span> <img src="rnngf1.png" width="30%" height="30%" /></p><p>在这一个时钟中，我们有： <span class="math display">\[\frac{\partial h_t}{\partial h_{t-1}}=\tanh&#39;\left(W_{hh}h_{t-1}+W_{xh}x_t\right)W_{hh}\]</span> 考虑整个时序：</p><p><img src="rnngf2.png" width="100%" height="100%" /></p><p>我们 Backpropagation 的目的是找到 <span class="math inline">\(\partial L/\partial W\)</span>： <span class="math display">\[\frac{\partial L}{\partial W}=\sum_{t=1}^T\frac{\partial L_t}{\partial W}\]</span> 若仅考虑 <span class="math inline">\(\partial L_T/\partial W\)</span>： <span class="math display">\[\begin{align}\frac{\partial L_T}{\partial W}&amp;=\frac{\partial L_T}{\partial h_T}\frac{\partial h_T}{\partial h_{T-1}}\cdots\frac{\partial h_{2}}{\partial h_1}\frac{\partial h_1}{\partial W}\\&amp;=\frac{\partial L_T}{\partial h_T}\left(\prod_{t=2}^T\frac{\partial h_t}{\partial h_{t-1}}\right)\frac{\partial h_1}{\partial W}\\&amp;=\frac{\partial L_T}{\partial h_T}\left(\prod_{t=2}^T\tanh&#39;(W_{hh}h_{t-1}+W_{xh}x_t)\right)W_{hh}^{T-1}\frac{\partial h_1}{\partial W}\\\end{align}\]</span> 由于 <span class="math inline">\(\tanh&#39;(x)\leqslant 1\)</span>（当且仅当 <span class="math inline">\(x=0\)</span> 时取等），所以上式中括号内的乘积将非常小，这导致 <strong>Vanishing gradients</strong> 梯度消失。</p><p>即便不考虑括号那一项，注意 <span class="math inline">\(W_{hh}^{T-1}\)</span> 这一项，如果 <span class="math inline">\(W_{hh}\)</span> 的最大奇异值 <span class="math inline">\(&gt;1\)</span>，该项将很大，导致 <strong>Exploding gradients</strong> 梯度爆炸；而如果 <span class="math inline">\(W_{hh}\)</span> 最大奇异值 <span class="math inline">\(&lt;1\)</span>，该项将很小，导致 <strong>Vanishing gradients</strong>.</p><p>总而言之，梯度在 RNN 中的传播是困难的，于是我们思考改进 RNN 的结构来解决这个问题。</p><h3 id="lstm">LSTM</h3><p>LSTM 在普通 RNN 的基础上多加了四个中间变量，将一个时钟中的更新定义为： <span class="math display">\[\begin{cases}&amp;\begin{pmatrix}i\\f\\o\\g\end{pmatrix}=\begin{pmatrix}\sigma\\\sigma\\\sigma\\\tanh\end{pmatrix}W\begin{pmatrix}h_{t-1}\\x_t\end{pmatrix}\\&amp;c_t=f\odot c_{t-1}+i\odot g\\&amp;h_t=o\odot \tanh(c_t)\end{cases}\]</span> 其中：</p><ul><li><span class="math inline">\(i\)</span>: Input gate, whether to write to cell</li><li><span class="math inline">\(f\)</span>: Forget gate, whether to erase cell</li><li><span class="math inline">\(o\)</span>: Output gate, how much to reveal cell</li><li><span class="math inline">\(g\)</span>: how much to write to cell</li></ul><p><img src="lstm.png" width="40%" height="40%" /></p><p>注意，计算上述四个 gate 各自的 <span class="math inline">\(W\)</span> 是不同的，而上式中的 <span class="math inline">\(W\)</span> 表示把它们写在一起的矩阵。</p><p><br></p><p>在一个时钟中，LSTM 的从 <span class="math inline">\(c_t\)</span> 到 <span class="math inline">\(c_{t-1}\)</span> 的梯度更新为：</p><p><img src="lstmgf1.png" width="30%" height="30%" /></p><p>整个时序上，gradient flow 显得很顺畅：</p><p><img src="lstmgf2.png" width="90%" height="90%" /></p><p>虽然 LSTM 不能保证不会发生 exploding gradients 或 vanishing gradients，但是它的 gradient flow 机制确实使得神经网络更容易训练。梯度在 LSTM 中的反向传播好似走了一条 high way，这一点上 LSTM 与 ResNet 有异曲同工之妙。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Stanford CS231n</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[CS231n]4·Convoluntional Neural Networks</title>
    <link href="/blog-main/2021/03/03/CS231n-4%C2%B7Convoluntional-Neural-Networks/"/>
    <url>/blog-main/2021/03/03/CS231n-4%C2%B7Convoluntional-Neural-Networks/</url>
    
    <content type="html"><![CDATA[<p><a href="https://cs231n.github.io/">CS231n Convolutional Neural Networks for Visual Recognition</a></p><p>https://www.bilibili.com/video/BV1nJ411z7fe</p><span id="more"></span><h2 id="architecture-overview">Architecture Overview</h2><p>在图像处理的任务中，图片的特征具有局部性，而全连接的传统神经网络引入了太多冗余的参数，既浪费又难以训练。因此，CNN 被提出以解决这个问题。CNN 的架构和传统神经网络架构类似，都具有输入层、隐藏层和输出层。不同的是，CNN 中每一层都是 <span class="math inline">\(3\)</span> 维的神经元，如下图所示：</p><p><img src="conv.jpeg" width="80%" height="80%" /></p><p>例如，CIFAR-10 数据集的输入可以是 <span class="math inline">\(32\times32\times3\)</span> 的，即 <span class="math inline">\(\text{RGB}\)</span> 三个通道分别有 <span class="math inline">\(32\times32\)</span> 的像素；其输出可以是 <span class="math inline">\(1\times1\times10\)</span>，分别表示 <span class="math inline">\(10\)</span> 个分类上的得分。</p><p>下面来详细分析 CNN 中各种层的结构。</p><hr /><h2 id="layers-used-to-build-cnn">Layers used to build CNN</h2><p>CNN 中主要有三种层：<strong>Convolutinal Layer, Pooling Layer, Fully-Connected Layer</strong>，将它们堆叠在一起就可以形成一个 CNN 的架构。</p><p>例如，[INPUT - CONV - RELU - POLL - FC] 就是一种简单的 CNN 架构。其中，CONV / FC 层具有需要学习的参数，而 RELU / POOL 层是固定的函数；CONV / FC / POOL 层都有超参数，而 RELU 没有。</p><p><img src="layers.jpeg" width="80%" height="80%" /></p><h3 id="convolutional-layer">Convolutional Layer</h3><p>Convolutional Layer 是 CNN 的核心。其基本思想很简单，就是用一个<strong>卷积核</strong>（<strong>filter</strong> / <strong>kernel</strong>）在输入图像上扫一遍，得到输出图像，其中卷积核的参数就是神经网络要学习的参数。设输入深度为 <span class="math inline">\(D_1\)</span>，则这个卷积核的深度也是 <span class="math inline">\(D_1\)</span>，即将输入图像某一区域<strong>所有层</strong>的数据一起做线性组合；如果输入的深度是 <span class="math inline">\(D_2\)</span>，那么我们需要 <span class="math inline">\(D_2\)</span> 个不同的卷积核，形成输出的不同层。</p><p><img src="filter.png" width="100%" height="100%" /></p><p>正式地说，设输入是一个 <span class="math inline">\(W_1\times H_1\times D_1\)</span> 的 <span class="math inline">\(3\)</span> 维数组（即上一层），Convolutional Layer 有以下几个超参数：</p><ul><li><span class="math inline">\(K\)</span>：filters 的个数</li><li><span class="math inline">\(F\)</span>：filters 的边长</li><li><span class="math inline">\(S\)</span>：filters 进行扫描时的步长</li><li><span class="math inline">\(P\)</span>：zero padding，在输入图像的边缘补充的长度</li></ul><p>输出一个 <span class="math inline">\(W_2\times H_2\times D_2\)</span> 的 <span class="math inline">\(3\)</span> 维数组，其中： <span class="math display">\[\begin{align}W_2&amp;=(W_1-F+2P)/S+1\\H_2&amp;=(H_1-F+2P)/S+1\\D_2&amp;=K\end{align}\]</span> 在进行卷积操作时，我们需要注意计算得到的 <span class="math inline">\(W_2\)</span> 和 <span class="math inline">\(H_2\)</span> 是不是整数，如果不是，需要调整 <span class="math inline">\(P\)</span> 或者 <span class="math inline">\(S\)</span>。另外，注意对于输出的每一个 depth slice，其上的神经元是由同一个卷积核算出来的，分享了同样的参数，这有助于减少 CNN 的参数数量。</p><p><br></p><p><strong>im2col</strong> 是实现卷积操作的一个方法，正如其名 image to column，它将二维图像变成一个列向量以方便地实施卷积操作。举例说明，假设输入图像是 <span class="math inline">\([227\times227\times3]\)</span>，使用 <span class="math inline">\([11\times11\times3]\)</span> 的卷积核并取步长为 <span class="math inline">\(4\)</span>，想要得到 <span class="math inline">\([55\times 55\times96]\)</span> 的输出。那么，我们将 <span class="math inline">\([11\times 11\times 3]\)</span> 的卷积核拉伸成 <span class="math inline">\(11\times11\times3=363\)</span> 维的<strong>行向量</strong>，各卷积核拼接起来得到 <span class="math inline">\([96\times 363]\)</span> 的参数矩阵；然后把输入图像中对应 <span class="math inline">\(55\times 55\)</span> 个位置的 <span class="math inline">\([11\times11\times3]\)</span> 全拿出来拉伸成<strong>列向量</strong>并顺次拼接成 <span class="math inline">\([363\times 3025]\)</span> 的矩阵；二者相乘即可得到 <span class="math inline">\([96\times 3025]\)</span> 的矩阵，每一行的列向量还原成 <span class="math inline">\([55\times 55]\)</span> 即得到输出结果。</p><h3 id="pooling-layer">Pooling Layer</h3><p>插入 Pooling Layer 是为了减小当前层的大小，与 Convolutional Layer 类似，它用一个 filter 扫描整个图像，然后做某种无参数的计算，如取 <span class="math inline">\(\max\)</span> (<em>max pooling</em>) 或者 <span class="math inline">\(\text{L2 norm}\)</span> (<em>L2-norm pooling</em>) 或者平均值 (<em>average pooling</em>).</p><p><img src="pool.jpeg" width="80%" height="80%" /></p><h3 id="fully-connected-layer">Fully-connected Layer</h3><p>CNN 中的全连接层和传统神经网络的全连接层一模一样，一般最后连接输出层的时候用全连接层。</p><p>FC Layer 可以和 CONV Layer 相互转化：</p><ul><li>CONV 转 FC：只需要强行设定 FC 中某些权重为 <span class="math inline">\(0\)</span>；</li><li>FC 转 CONV：设定 CONV 卷积核大小与输入相同，那么一次卷积运算相当于 FC 前一层到后一层的某一个神经元的运算。</li></ul><hr /><h2 id="cnn-architectures">CNN Architectures</h2><p>上一节讲了 CNN 中主要的三种层，这一节讲如何把它们的组合在一起形成 CNN.</p><h3 id="layer-patterns">Layer Patterns</h3><p>最常见的各种层的组合是：</p><p><strong>INPUT</strong> -&gt; [ [ <strong>CONV</strong> -&gt; <strong>RELU</strong> ] * N -&gt; <strong>POOL</strong>? ] * M -&gt; [<strong>FC</strong> -&gt; <strong>RELU</strong>] * K -&gt; <strong>FC</strong></p><p>即若干 CONV-RELU layers，选择性地接 POOL layer，重复该结构直到最后接上传统的神经网络。</p><p><br></p><p>实践中注意一点：相比用一个具有较大 filter 的卷积层，更好的是使用多个具有较小 filter 的卷积层堆叠起来，因为一方面，堆叠起来的层引入更多非线性因素，使神经网络更强大；另一方面，后者具有更少的参数。</p><h3 id="layer-sizing-patterns">Layer Sizing Patterns</h3><ul><li><strong>input layer</strong>：最好能被 <span class="math inline">\(2\)</span> 整除很多次；</li><li><strong>conv layers</strong>：应使用小的 filters（如 <span class="math inline">\(3\times 3\)</span> 或最多 <span class="math inline">\(5\times 5\)</span>），使用步长 <span class="math inline">\(S=1\)</span>，并且使用 zero padding 保证输出图像的大小和输入的大小相等（取 <span class="math inline">\(P=(F-1)/2\)</span> 即可），这样能够防止略过图像边缘的信息，而把缩小 feature map 的任务交给 Pooling layer；</li><li><strong>pool layers</strong>：负责缩小输入的大小。最常用一个 <span class="math inline">\(2\times 2\)</span> 的 filter 以步长为 <span class="math inline">\(2\)</span> 扫描一遍，丢掉恰好 <span class="math inline">\(75\%\)</span> 的结果；也可以使用 <span class="math inline">\(3\times 3\)</span> 的 filter，但是基本不用 <span class="math inline">\(&gt;3\)</span> 的 filter，因为这样太过 aggresive 而使得结果变差。</li></ul><h3 id="case-studies">Case studies</h3><p>有一些经典的 CNN 结构有一个名字，例如：</p><ul><li><strong>LeNet</strong>：由 Yann LeCun 在 1990's 实现的。<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">Gradient-Based Learning Applied to Document Recognition</a></li><li><strong>AlexNet</strong>：由 Alex Krizhevsky, Ilya Sutskever 和 Geoff Hinton 实现，在 2012 年 ImageNet ILSVRC 比赛中以绝对优势夺冠，并使得 CNN 从此广受欢迎。<a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a></li><li><strong>ZF Net</strong>：由 Matthew Zeiler 和 Rob Fergus 发明，在 ILSVRC 2013 夺冠。<a href="https://arxiv.org/abs/1311.2901">Visualizing and Understanding Convolutional Networks</a></li><li><strong>GoogLeNet</strong>：由 Szeged 等人实现，在 ILSVRC 2014 夺冠。<a href="https://arxiv.org/abs/1409.4842">Going Deeper with Convolutions</a></li><li><strong>VGGNet</strong>：由 Karen Simonyan 和 Andrew Zisserman 实现，是 ILSVRC 2014 的第二名。<a href="https://arxiv.org/abs/1409.1556">Very Deep Convolutional Networks for Large-Scale Visual Recognition</a></li><li><strong>ResNet</strong>：由何恺明等人实现，在 ILSVRC 2015 夺冠。<a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Stanford CS231n</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[CS231n]3·Neural Networks</title>
    <link href="/blog-main/2021/02/24/CS231n-3%C2%B7Neural-Networks/"/>
    <url>/blog-main/2021/02/24/CS231n-3%C2%B7Neural-Networks/</url>
    
    <content type="html"><![CDATA[<p><a href="https://cs231n.github.io/">CS231n Convolutional Neural Networks for Visual Recognition</a></p><p>https://www.bilibili.com/video/BV1nJ411z7fe</p><span id="more"></span><h2 id="modeling-one-neuron">Modeling one neuron</h2><p><img src="neuron.png" width="80%" height="80%" /></p><p>即取输入的线性组合（加上偏置项），再对其取 <strong>activation function</strong> <span class="math inline">\(f\)</span> 之后输出。</p><h3 id="commonly-used-activation-functions">Commonly used activation functions</h3><ul><li><p><strong>Sigmoid</strong>： <span class="math display">\[\sigma(x)=\frac{1}{1+e^{-x}}\]</span> Sigmoid 函数曾经很常用，但最近很少用了，这是因为它有两个大弊端：</p><ul><li>Sigmoids 会 saturate，导致<strong>梯度消失</strong>。由于 <span class="math inline">\(\sigma(x)\)</span> 在 <span class="math inline">\(|x|\)</span> 较大时梯度非常小，而神经网络反向传播的本质是链式求导法则，要把梯度一级一级地乘起来，所以一旦遇到梯度很小的地方，梯度就无法继续反向传播了，这将导致前层的神经元的参数几乎不怎么更新。</li><li>Sigmoid 的输出不是以 <span class="math inline">\(0\)</span> 为中心的，这意味着后一层神经元接受的输入都是大于 <span class="math inline">\(0\)</span> 的数，从而导致参数 <span class="math inline">\(w\)</span> 的梯度全为正或者全为负。</li></ul></li><li><p><strong>Tanh</strong>： <span class="math display">\[\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\]</span> 其实 <span class="math inline">\(\tanh(x)\)</span> 仅仅是 <span class="math inline">\(\sigma(x)\)</span> 放缩和移动后的结果： <span class="math display">\[\tanh(x)=2\sigma(2x)-1\]</span> 所以依然存在梯度消失的问题，但是不同于 sigmoid 的是它的值域为 <span class="math inline">\((-1,1)\)</span>，以 <span class="math inline">\(0\)</span> 为中心。因此实践中，<span class="math inline">\(\tanh(x)\)</span> 往往比 sigmoid 更被大家喜爱。</p></li><li><p><strong>ReLU</strong>：Rectified Linear Unit. <span class="math display">\[f(x)=\max(0,x)\]</span></p><ul><li>(+) 与 sigmoid 或 tanh 相比，ReLU 大大加速了 <span class="math inline">\(\text{SGD}\)</span> 的收敛速度；</li><li>(+) ReLU 的计算非常简单（取 <span class="math inline">\(\max\)</span> 即可），而 sigmoid 和 tanh 涉及到指数等比较耗时的计算；</li><li>(-) 使用 ReLU 容易导致神经元 "die". 如果学习率设置的过大，容易使得参数变化过大，导致对数据集中的所有点 <span class="math inline">\(wx+b\)</span> 都小于 <span class="math inline">\(0\)</span>，于是梯度不可逆地变成 <span class="math inline">\(0\)</span>.</li></ul></li><li><p><strong>Leaky ReLU</strong>： <span class="math display">\[f(x)=[x&lt;0](\alpha x)+[x\geqslant 0]x\]</span> 其中，<span class="math inline">\(\alpha\)</span> 是一个较小的常数。这么做是为了解决 ReLU 中神经元 die 的问题。在 PReLU 中，<span class="math inline">\(\alpha\)</span> 被视作神经元的一个参数。</p></li><li><p><strong>Maxout</strong>： <span class="math display">\[f(x)=\max(w_1x+b_1,w_2x+b_2)\]</span> ReLU / Leaky ReLU 是 Maxout 的特殊情况。</p></li></ul><h3 id="single-neuron-as-a-linear-classifier">Single neuron as a linear classifier</h3><p>选取合适的损失函数后，一个神经元可以用来实现一个 linear classifier.</p><ul><li><strong>Binary Softmax classifier</strong>：视神经元的输出 <span class="math inline">\(\sigma\left(\sum_iw_ix_i+b\right)\)</span> 为分类为正类的概率，采取 cross-entropy loss 训练，则我们得到了 binary Softmax classifier，也即 <strong>logistic regression</strong>.</li><li><strong>Binary SVM classifier</strong>：使用 max-margin hinge loss 训练，则我们得到 binary Support Vector Machine.</li></ul><hr /><h2 id="neural-network-architectures">Neural Network architectures</h2><p>一个 input layer，一个 output layer，若干 hidden layer. 对于最普通的神经网络而言，层与层之间全连接。</p><p><img src="neural network.png" width="80%" height="80%" /></p><p>输出层的神经元经常没有 activation function，这样输出可以被视作分类的得分或者一些取实值的目标（比如回归值）。</p><p><br></p><p>一个单层的神经网络就可以任意近似任何函数，严格证明在 Approximation by superpositions of a sigmoidal function (1989) 这篇论文中，直观解释可以在博客 <a href="http://neuralnetworksanddeeplearning.com/chap4.html">Neural Networks and Deep Learning</a> 中找到。虽然如此，实践中多层的神经网络效果更好，也更容易学习。</p><p>神经网络中，隐藏层神经元数量越多，其表达能力越强大：</p><p><img src="hidden neurons.png" width="70%" height="70%" /></p><p>当然也意味着越容易过拟合，可以采取正则化的方式避免过拟合：</p><p><img src="overfitting.png" width="70%" height="70%" /></p><hr /><h2 id="data-preprocessing">Data Preprocessing</h2><ul><li><p><strong>Mean subtraction</strong>：将所有数据减去平均值，即中心化。</p></li><li><p><strong>Normalization</strong>：将数据的各维度缩放到大致相同的规模。一般有两种方式：一是对中心化后的数据除以各维度的标准差，使得各维度的数据均值为 <span class="math inline">\(0\)</span>，方差为 <span class="math inline">\(1\)</span>；二是将最大值和最小值缩放为 <span class="math inline">\(1\)</span> 和 <span class="math inline">\(-1\)</span>，这样所有数据都在 <span class="math inline">\([-1,1]\)</span> 之中。</p><p><img src="data.png" width="80%" height="80%" /></p></li><li><p><strong>PCA</strong>：主成分分析 (PCA) 是一种常用的数据降维方法，具体内容可见<a href="https://xyfjason.top/2021/01/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8-%E5%8D%81%E4%BA%8C%C2%B7%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/">机器学习笔记</a>。</p></li><li><p><strong>Whitening</strong>：假设数据是一个多元高斯分布，则 whitening 将数据变为均值为 <span class="math inline">\(0\)</span>，协方差矩阵为单位矩阵的多元高斯分布。就具体实现而言，在主成分分析中，如果没有使维度减小，则其效果为去相关性 (decorrelate)；对去相关性后的数据除以各维度的特征值就得到了处理后的数据。</p><p><img src="whiten.png" width="80%" height="80%" /></p></li></ul><blockquote><p>在实际应用中，PCA 和 Whitening 不会在卷积神经网络中用到，但是中心化以及 normalization 很重要。</p></blockquote><p><em>在数据预处理时，非常重要的一点是预处理的统计量应该由 traning data 计算而来，然后应用到 validation / test data 上</em>。<strong>不能够</strong>先在整个数据集上预处理，再划分 train / val / test.</p><hr /><h2 id="weight-initialization">Weight Initialization</h2><ul><li><p><strong>陷阱：all zero initialization</strong>. 全部初始化为 <span class="math inline">\(0\)</span> 是错误的做法，因为这样同一层的所有神经元都没有区别，它们将输出同样的值、具有同样的梯度、进行同样的更新（因为它们是对称的），这是没有意义的。</p></li><li><p><strong>Small random numbers</strong>：初始化为接近 <span class="math inline">\(0\)</span> 的随机数是经常使用的方法。一般而言，可以取 <span class="math inline">\(0.01\)</span> 倍的标准正态分布或者均匀分布等。</p></li><li><p><strong>Calibrating the variances with <span class="math inline">\(1/\sqrt n\)</span></strong>：上一个方法存在一个问题，即神经网络输出的方差将随着输入量的增大而增大。在 <a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a> 中作者举了一个<a href="http://neuralnetworksanddeeplearning.com/chap3.html#weight_initialization">例子</a>说明这一点：不妨假设 <span class="math inline">\(1000\)</span> 个输入中有 <span class="math inline">\(500\)</span> 个 <span class="math inline">\(0\)</span> 和 <span class="math inline">\(500\)</span> 个 <span class="math inline">\(1\)</span>，考察第一个隐藏层中的一个神经元，<span class="math inline">\(z=\sum_jw_jx_j+b\)</span> 将是 <span class="math inline">\(501\)</span> 个标准正态分布之和，故服从 <span class="math inline">\(N(0,501)\)</span>，方差高达 <span class="math inline">\(501\)</span>！为了解决这个问题，我们计算： <span class="math display">\[\newcommand{\var}{\text{var}}\newcommand{\E}{\mathbb{E}}\begin{align}\var(z)&amp;\approx\var\left(\sum_jw_jx_j\right)&amp;&amp;\text{for simplicity, ignore bias}\\&amp;=\sum_{j}\var(w_jx_j)&amp;&amp;\text{independence}\\&amp;=\sum_j(\E [w_j])^2\var(x_j)+(\E[x_j])^2\var(w_j)+\var(x_j)\var(w_j)\\&amp;=\sum_j\var(x_j)\var(w_j)&amp;&amp;\text{assume }\E(x)=\E(w)=0\\&amp;=n\ \var(w)\ \var(x)\end{align}\]</span></p><p>因此，若要 <span class="math inline">\(\var(z)=\var(x)\)</span>，需要 <span class="math inline">\(\var(w)=1/n\)</span>，故我们只需要将初始化的值除以 <span class="math inline">\(1/\sqrt n\)</span> 即可。实践中这样做能加快收敛。</p><blockquote><p>在 <a href="https://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a> by Glorot et al. 这篇论文中，作者最终建议用 <span class="math inline">\(\var(w)=2/(n_\text{in}+n_\text{out})\)</span> 来初始化；在 <a href="https://link.zhihu.com/?target=http%3A//arxiv-web3.library.cornell.edu/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a> by He et al. 中，作者对 ReLU 神经网络进行分析，得到结论是方差应初始化为 <span class="math inline">\(2/n\)</span>.</p><p><strong>实践中，建议采用 ReLU 神经元并将权重方差设为 <span class="math inline">\(2/n\)</span>.</strong></p></blockquote></li><li><p><strong>Sparse initialization</strong>：另一种初始化方式是将权重矩阵初始化为 <span class="math inline">\(0\)</span>，但是为了打破对称性，每个神经元随机地连接下一层若干神经元（连接数目可由一个小的高斯分布生成）。</p></li><li><p><strong>Initializing the biases</strong>：常常将偏置初始化为 <span class="math inline">\(0\)</span>.</p></li><li><p><strong>Batch Normalization</strong>：在全连接层/卷积层与 activation function 之间插入一个 BatchNorm layer，因为 normalization 是可导的操作，这么做是可行的，具体查看论文：<a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>.</p></li></ul><hr /><h2 id="regularization">Regularization</h2><ul><li><p><strong>L2 regularization</strong>：最常见的正则化方法，向 loss function 加上 <span class="math inline">\(\frac{1}{2}\lambda w^2\)</span>. 其特点是使得网络倾向于<em>分散</em>的权重。</p></li><li><p><strong>L1 regularization</strong>：相对常见的正则化方法，加上 <span class="math inline">\(\lambda |w|\)</span>. 其特点是使得网络的权重倾向于稀疏（即许多非常接近 <span class="math inline">\(0\)</span> 的数字，与 L2 正好相反）。结合 L1 和 L2 我们得到 Elastic net regularization，即向 loss function 加上 <span class="math inline">\(\lambda_1|w|+\lambda_2 w^2\)</span>. 一般而言，如果不是特别关注某些特征的选择，L2 比 L1 效果会更好。</p></li><li><p><strong>Max norm constraints</strong>：给神经元的权重向量 <span class="math inline">\(w\)</span> 以约束：<span class="math inline">\(||w||_2&lt;c\)</span>，<span class="math inline">\(c\)</span> 一般取 <span class="math inline">\(3\)</span> 或 <span class="math inline">\(4\)</span>。</p></li><li><p><strong>Dropout</strong>：及其有效和简单的正则化方法，在 <a href="https://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ersalakhu/papers/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a> by Srivastava et al. 中提出，能作为其他正则化方法的补充。实现方法是在训练中让神经元以概率 <span class="math inline">\(p\)</span>（一个超参数）激活或设置为 <span class="math inline">\(0\)</span>.</p><p><img src="dropout.jpeg" width="80%" height="80%" /></p></li></ul><p>我们无需对 bias 进行正则化，因为它们没有与数据直接相乘，并且正则化 bias 对结果的影响也微乎其微。</p><hr /><h2 id="loss-functions">Loss functions</h2><h3 id="classification">Classification</h3><p>常用 SVM loss: <span class="math display">\[L_i=\sum_{j\neq y_i}\max(0,f_j-f_{y_i}+1)\]</span> 和 cross-entropy loss: <span class="math display">\[L_i=-\ln\left(\cfrac{e^{f_{y_i}}}{\sum_je^{f_j}}\right)\]</span></p><h3 id="attribute-classification">Attribute classification</h3><p>不一定被分为某一类，而可以分到若干类，设 <span class="math inline">\(y_{ij}\)</span> 表示第 <span class="math inline">\(i\)</span> 个图像是否属于第 <span class="math inline">\(j\)</span> 类。我们可以对每一类训练一个 logistic regression classifier，然后用负的对数似然作为损失函数： <span class="math display">\[L_i=-\sum_j y_{ij}\ln\sigma(f_j)+(1-y_{ij})\ln(1-\sigma(f_j))\]</span></p><h3 id="regression">Regression</h3><p>可采取 L2 或 L1 距离作为损失函数，即： <span class="math display">\[L_i=||f-y_i||_2^2=\sum_j(f_j-(y_{i})_j)^2\]</span> 或 <span class="math display">\[L_i=||f-y_i||_1=\sum_j|f_j-(y_i)_j|\]</span></p><hr /><h2 id="gradient-checks">Gradient Checks</h2><p>神经网络反向传播过程很容易写出 bug，为了检验我们计算的梯度是否正确，可以将 analytic gradient 和 numerical gradient 进行比较。这里有一些实现的 tips：</p><ul><li><p>计算 numerical gradient 时，使用 centered formula，即： <span class="math display">\[\frac{\mathrm df(x)}{\mathrm dx}\approx\frac{f(x+h)-f(x-h)}{2h}\]</span> 而非 <span class="math inline">\(\frac{\mathrm df(x)}{\mathrm dx}\approx\frac{f(x+h)-f(x)}{h}\)</span>，这是因为前者误差为 <span class="math inline">\(O(h^2)\)</span>，而后者有 <span class="math inline">\(O(h)\)</span>. （泰勒展开即可证）</p></li><li><p>使用相对误差作为比较基准。如果使用绝对误差，比如 <span class="math inline">\(10^{-4}\)</span>，那么在梯度本来就小于 <span class="math inline">\(10^{-4}\)</span> 的地方，这个阈值没有什么作用。因此，我们使用相对误差： <span class="math display">\[\frac{|f&#39;_{\text{analytic}}-f&#39;_\text{numerical}|}{\max\left(|f&#39;_\text{analytic}|,|f&#39;_\text{numerical}|\right)}\]</span> 分母用 <span class="math inline">\(\max\)</span> 或者加和或者取任意一个都是可以的，但是要小心除零。</p></li><li><p>注意目标函数的不可导点。跨越了不可导点的 numerical gradient 可能和实际的 gradient 有较大差距，如果误差较大需要检查是不是这个原因。</p></li><li><p>只使用少量数据。解决上一条问题的一个方法是 gradient checks 时只用少量的数据，这样目标函数的不可导处将减少；同时只用检验 <span class="math inline">\(2\sim3\)</span> 处的梯度就够了。</p></li><li><p><span class="math inline">\(h\)</span> 不宜设置得太小。理论上 <span class="math inline">\(h\)</span> 越小越精确，但由于计算机浮点数的精度误差，太小了可能反而不精确。</p></li><li><p>小心正则化项占了梯度的主要部分。如果正则化项在梯度中占比很大，它可能掩盖住了 back propagation 的错误。一个方式是在梯度检验时去掉正则化。</p></li><li><p>记住去掉 dropout/augmentations. 梯度检验时需要去除带有<strong>随机性质</strong>的量，比如 dropout 或者数据的随机 augmentation. 提前设置一个随机数种子也是可以的。</p></li></ul><hr /><h2 id="sanity-checks">Sanity Checks</h2><p>在实施学习之前，可以先进行 sanity checks，以保证我们的代码是正确的。</p><ul><li>按概率推测损失函数初始值。例如一个十分类问题，随机初始化后我们应期望得到 <span class="math inline">\(1/10\)</span> 的正确率，也即负对数概率的 loss function 大致为：<span class="math inline">\(-\ln(0.1)=2.302\)</span>.</li><li>过拟合一个极小的数据集。用一个极小的数据集去训练，在去除正则化的情况下，我们应该期望得到 <span class="math inline">\(100\%\)</span> 的正确率或 loss function 为 <span class="math inline">\(0\)</span>，即在这个极小的数据集上过拟合。</li></ul><hr /><h2 id="babysitting-the-learning-process">Babysitting the learning process</h2><p>学习过程中我们可以监测一些值随着迭代次数的增加的变化。</p><ul><li><p><strong>Loss function</strong>. 作 loss function - epoch 图能直观的告诉我们学习的状况，帮助我们调整学习率等参数。</p><p><img src="loss.jpeg" width="80%" height="80%" /></p></li><li><p><strong>Train/val accuracy</strong>. 作训练集和验证集上的准确率的图能直观的告诉我们是否过拟合/欠拟合，帮助我们调整正则化项。</p><p><img src="train val accuracy.jpeg" width="80%" height="80%" /></p></li><li><p><strong>Ratio of weights: updates</strong>. 检测参数更新的幅度，帮助我们判断学习率是否合适，合适的幅度应该在 <span class="math inline">\(10^{-3}\)</span> 左右。</p></li><li><p><strong>First-layer Visualizations</strong>. 在图像处理的网络中，我们可以把第一层的神经元的参数还原成图像可视化。</p><p><img src="first.jpeg" width="80%" height="80%" /></p></li></ul><hr /><h2 id="parameter-updates">Parameter updates</h2><h3 id="sgd-and-bells-and-whistels">SGD and bells and whistels</h3><ul><li><p><strong>Vanilla update</strong>. 朴素的随机梯度下降</p></li><li><p><strong>Momentum update</strong>. 朴素的 SGD 在每一处都向梯度下降的方向更新，就像一个人下山一样。而如果我们做另一个比喻，想象一个小球从山坡滚下，它在每一处将具有保持原方向的惯性，这就是 momentum SGD 的灵感来源。具体地，momentum SGD 更新如下： <span class="math display">\[\begin{align}v&amp;:=\mu v-\alpha\ \mathrm dx\\x&amp;:=x+v\end{align}\]</span> 其中，<span class="math inline">\(\alpha\)</span> 为学习率，<span class="math inline">\(\mu\)</span> 可以看作是摩擦因子。</p></li><li><p><strong>Nesterov Momentum</strong>. 这是另一种 momentum update. 不同于上一条，Nesterov Momentum 计算随“惯性”移动一段距离之后的梯度再更新，见下图：</p><p><img src="momentum.jpeg" width="80%" height="80%" /></p><p>具体地，Nesterov Momentum 更新如下： <span class="math display">\[\begin{align}x_\text{ahead}&amp;:=x+\mu v\\v &amp;:= \mu v-\alpha\ \mathrm dx_\text{ahead}\\x&amp;:=x+v\end{align}\]</span></p></li></ul><h3 id="annealing-the-learning-rate">Annealing the learning rate</h3><p>在学习过程中不断减小 learning rate 往往有帮助，一下三种方式是常用的 learning rate decay 实现方式：</p><ul><li><strong>Step decay</strong>. 每迭代几次后将 learning rate 减小，例如每 <span class="math inline">\(20\)</span> 次迭代后减小到原来的 <span class="math inline">\(0.1\)</span>，或者每 <span class="math inline">\(5\)</span> 次迭代减小一半等。这些参数依赖于具体问题和模型。一种启发式方法是当 validation error 停止减小时就减小 learning rate.</li><li><strong>Exponential decay</strong>. 依 <span class="math inline">\(\alpha=\alpha_0e^{-kt}\)</span> 减小，其中 <span class="math inline">\(\alpha_0,k\)</span> 是超参数，<span class="math inline">\(t\)</span> 是迭代次数。</li><li><span class="math inline">\(1/t\)</span> <strong>decay</strong>. 依 <span class="math inline">\(\alpha=\alpha_0/(1+kt)\)</span> 减小。</li></ul><p>实践中，Step decay 稍稍更受喜爱一些，因为其解释性比其他的强。</p><h3 id="second-order-methods">Second order methods</h3><p>梯度下降及其延伸算法只需要用到一阶导数，即梯度。还有些最优化算法需要用到二阶导数。</p><ul><li><p><strong>Newton's method</strong>. <span class="math display">\[x:=x-[Hf(x)]^{-1}\nabla f(x)\]</span> 其中，<span class="math inline">\(Hf(x)\)</span> 表示 Hessian matrix，<span class="math inline">\(\nabla f(x)\)</span> 表示 <span class="math inline">\(f(x)\)</span> 的梯度向量。</p><p>注意到上述更新中不含有学习率这一超参数，这也是其优于一阶方法所在。</p><p><br></p><p>但是，Newton's method 有一个及其显著的缺点，即 Hessian matrix 过于庞大，<span class="math inline">\(n\)</span> 个参数的 Hessian matrix 是 <span class="math inline">\(n\times n\)</span> 的，计算耗时且难以存储。因此，出现了众多 <em>quasi-Newton</em> methods，例如 <span class="math inline">\(\text{L-BFGS}\)</span>，它们不需要将 Hessian matrix 完整计算出来.</p></li></ul><h3 id="per-parameter-adaptive-learning-rate-methods">Per-parameter adaptive learning rate methods</h3><p>在之前的算法中，学习率是全局的且对所有参数都一样。调整学习率是一个耗时耗力的过程，因此人们发明了许多自动调整学习率的方法，且这些学习率甚至能对每一个参数进行调整。</p><p>虽然这些方法也有超参数，但它们与纯粹的学习率相比，能在更大的范围内表现较好，因而更好调参。</p><ul><li><p><strong>Adagrad</strong>： <span class="math display">\[\begin{align}\text{cache}&amp;:=\text{cache}+(\mathrm dx)^2\\x&amp;:=x-\alpha\ \frac{\mathrm dx}{\sqrt{\text{cache}}+\text{eps}}\end{align}\]</span> 注意，上式中对向量的运算符定义为与 <code>numpy</code> 相同。</p><p>对上式的理解是：<span class="math inline">\(\text{cache}\)</span> 一路记录了每个参数的梯度平方和，如果某个参数有较大的梯度，那么它的学习率将减小；相反，梯度较小的参数将有较大的学习率。</p></li><li><p><strong>RMSprop</strong>：RMSprop 在 Adagrad 的基础上进行了简单的修改： <span class="math display">\[\begin{align}\text{cache}&amp;:=\text{decay rate}\times \text{cache}+(1-\text{decay rate})\times (\mathrm dx)^2\\x&amp;:=x-\alpha\ \frac{\mathrm dx}{\sqrt{\text{cache}}+\text{eps}}\end{align}\]</span> 也即是增加了 <span class="math inline">\(\text{decay rate}\)</span> 这一超参数，一般取值为 <span class="math inline">\(0.9,0.99,0.999\)</span>.</p></li><li><p><strong>Adam</strong>：可以看作是 RMSprop 带 momentum 的版本，其（简化的）更新如下： <span class="math display">\[\begin{align}m&amp;:=\beta_1m+(1-\beta_1)\mathrm dx\\v&amp;:=\beta_2v+(1-\beta_2)(\mathrm dx)^2\\x&amp;:=x-\alpha\ \frac{m}{\sqrt{v}+\text{eps}}\end{align}\]</span> 典型的参数是：<span class="math inline">\(\text{eps}=10^{-8},\beta_1=0.9,\beta_2=0.999\)</span>.</p><p>完整的 Adam 算法存在一个 <em>bias correction</em> 机制： <span class="math display">\[\begin{align}m&amp;:=\beta_1m+(1-\beta_1)\mathrm dx\\\color{purple}{mt}&amp;\color{purple}{:={m}/({1-\beta_1^t})}\\v&amp;:=\beta_2v+(1-\beta_2)(\mathrm dx)^2\\\color{purple}{vt}&amp;\color{purple}{:=v/(1-\beta_2^t)}\\x&amp;:=x-\alpha\ \frac{\color{purple}{mt}}{\sqrt{\color{purple}{vt}}+\text{eps}}\end{align}\]</span></p></li></ul><hr /><h2 id="hyperparameter-optimization">Hyperparameter optimization</h2><p>超参数的优化是一个黑盒优化问题，其优化的目标函数是神经网络的性能，因而有一个显著的特点：得到每一组超参数的结果会花费巨额的时间。因此我们并不能直接套用一般的优化方法对超参数进行优化。一般的，超参数优化方法有以下几种：</p><ul><li><p><strong>Grid search</strong>：即每个超参数设置几个值，暴力遍历所有的可能。</p></li><li><p><strong>Random search</strong>：在论文 <a href="https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">Random Search for Hyper-Parameter Optimization</a> by Bergstra and Bengio 中，作者论证了随机搜索比暴力遍历更为优秀，且更容易实现。</p><p><img src="grid.jpeg" width="80%" height="80%" /></p></li><li><p><strong>Bayesian Hyperparameter Optimation</strong>：利用贝叶斯方法进行超参数的寻找，核心是 exploration - exploitation trade-off.</p></li></ul><hr /><h2 id="model-ensembles">Model Ensembles</h2><p>实践中，一个提升神经网络性能的方法是训练多个独立的模型，使用它们的平均预测结果。这些独立的模型可以是不同的模型，同一种模型、不同初始化条件，甚至是同一个模型在训练的不同时间点处等。除了对模型结果进行平均，还可以对各模型的参数进行平均，有时也能提高神经网络性能。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Stanford CS231n</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[CS231n]2·Linear Classification</title>
    <link href="/blog-main/2021/02/22/CS231n-2%C2%B7Linear-Classification/"/>
    <url>/blog-main/2021/02/22/CS231n-2%C2%B7Linear-Classification/</url>
    
    <content type="html"><![CDATA[<p><a href="https://cs231n.github.io/">CS231n Convolutional Neural Networks for Visual Recognition</a></p><p>https://www.bilibili.com/video/BV1nJ411z7fe</p><span id="more"></span><h2 id="linear-classifier">Linear Classifier</h2><p>设数据集为 <span class="math inline">\(\{(x_i,y_i)\mid i=1,2,\ldots,N\}\)</span>，其中 <span class="math inline">\(x_i\in\mathbb R^D,\,y_i\in\{1,\ldots,K\}\)</span>.</p><p>定义 <strong>score function</strong>: <span class="math inline">\(f:\mathbb R^D\mapsto \mathbb R^K\)</span>，即输入一个由图片像素构成的 <span class="math inline">\(D\)</span> 维向量，输出这个图片属于各个类别的 score. 我们认为 score 越高，图片越可能属于这个类别。</p><p>对于 linear classifier 来说，它的 score function 很简单： <span class="math display">\[f(x;W,b)=Wx+b\]</span> 其中，<span class="math inline">\(W,b\)</span> 是参数，分别称作 weights 和 bias. 显然，这里 <span class="math inline">\(W\in\mathbb R^{K\times D},\,b\in\mathbb R^K\)</span>.</p><p>如果单独看第 <span class="math inline">\(i\)</span> 类的得分，为 <span class="math inline">\(W\)</span> 的第 <span class="math inline">\(i\)</span> 行与 <span class="math inline">\(x\)</span> 的内积加上 <span class="math inline">\(b\)</span> 的第 <span class="math inline">\(i\)</span> 个元素。可以发现类别之间的得分相互独立，写成矩阵只是为了方便而已。</p><p><br></p><p>为了理解 linear classifier，注意 <span class="math inline">\(z=wx+b\)</span> 是一个 <span class="math inline">\(\mathbb R^{D+1}\)</span> 中的超平面，法向量为 <span class="math inline">\((w,-1)\)</span>，截距为 <span class="math inline">\(b\)</span>. 输入的 <span class="math inline">\(x\)</span> 在超平面上对应位置的“高度”越高，就越可能属于这一超平面代表的那一类。下面是 <span class="math inline">\(D=2\)</span> 的一个例子，线条表示超平面与 <span class="math inline">\(z=0\)</span> 的交线，沿着箭头方向走，<span class="math inline">\(wx+b\)</span> 变大。</p><p><img src="linear classifier.png" height="50%" width="50%" /></p><p><br></p><p><span class="math inline">\(W,b\)</span> 是我们要训练的参数，为了训练它们，我们需要定义 <strong>Loss function</strong>. 这样训练 <span class="math inline">\(W,b\)</span> 就是最小化 Loss function 的过程，机器学习问题最终归结为一个优化问题。</p><hr /><h2 id="multiclass-svm">Multiclass SVM</h2><p>一种 loss function 是 <strong>Multiclass SVM Loss</strong>. 对于数据 <span class="math inline">\((x_i,y_i)\)</span> 来说，其 loss function 定义为： <span class="math display">\[L_i=\sum_{j\neq y_i}\max(0,s_j-s_{y_i}+\Delta)\]</span> 其中，<span class="math inline">\(s_j\)</span> 表示 <span class="math inline">\(x_i\)</span> 在第 <span class="math inline">\(j\)</span> 类上的得分，即 <span class="math inline">\(s_j=f(x_i;W,b)_j\)</span>.</p><p>对这个 loss function 的理解是，如果正确分类的得分 <span class="math inline">\(s_{y_j}\)</span> 比错误分类的得分 <span class="math inline">\(s_j\)</span> 还高一个 <span class="math inline">\(\Delta\)</span>，那么损失就是 <span class="math inline">\(0\)</span>；否则，损失是 <span class="math inline">\(s_j+\Delta\)</span> 比 <span class="math inline">\(s_{y_j}\)</span> 多出的部分。由于这个函数的图像形状像铰链（合叶），所以这种 loss function 也称作 <strong>hinge loss</strong>.</p><p><br></p><p>在机器学习中学过的正则化也要用上，若使用 <span class="math inline">\(\text{L2 regularization}\)</span>，则 Multiclass SVM Loss 的完整形式是： <span class="math display">\[L=\frac{1}{N}\sum_{i=1}^N\sum_{j\neq y_i}\max(0,f(x_i;W,b)_j-f(x_i;W,b)_{y_i}+\Delta)+\lambda\sum_k\sum_lW_{kl}^2\]</span> 值得注意的是，上式中的 <span class="math inline">\(\Delta\)</span> 并不是需要调节的 hyperparameter，取 <span class="math inline">\(\Delta=1\)</span> 即可。这是因为 <span class="math inline">\(W\)</span> 的整体放缩可以在不改变 <span class="math inline">\(s_j\)</span> 和 <span class="math inline">\(s_{y_i}\)</span> 的大小关系的条件下改变它们的差值，所以 <span class="math inline">\(\Delta\)</span> 取值并不影响结果。</p><hr /><h2 id="softmax-classifier">Softmax classifier</h2><p><strong>Softmax classifier</strong> 是 binary Logistic Regression classifier 在多分类上的扩展，不同于 multiclass SVM loss，softmax classifier 的输出有一个概率的解释。</p><p>对于第 <span class="math inline">\(i\)</span> 个数据，设 <span class="math inline">\(f_j\)</span> 表示它在第 <span class="math inline">\(j\)</span> 类上的得分，则 softmax classifier 视之为尚未标准化的对数概率，并采取 <strong>cross-entropy loss</strong> 作为 loss function： <span class="math display">\[L_i=-\log\left(\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}\right)\]</span> 其中，函数 <span class="math inline">\(f_j(z)={e^{z_j}}/{\sum_ke^{z_k}}\)</span> 称作 <strong>softmax function</strong>.</p><p>和之前一样，总的 loss function 定义为各 <span class="math inline">\(L_i\)</span> 的平均值加上正则化项。</p><p><br></p><p>从信息论角度理解，设真实概率分布为 <span class="math inline">\(p\)</span>，估计概率分布为 <span class="math inline">\(q\)</span>，则定义 cross-entropy 为： <span class="math display">\[H(p,q)=-\sum_x p(x)\log q(x)\]</span> 在图像分类问题中，我们估计的各个分类上的概率分布为 <span class="math inline">\(q=e^{f_{y_i}}/\sum_je^{f_j}\)</span>，而真实分布为 <span class="math inline">\(p=[0,\ldots,1,\ldots,0]\)</span>（即正确的分类为 <span class="math inline">\(1\)</span>，其余为 <span class="math inline">\(0\)</span> ）。Softmax classifier 最小化的就是 <span class="math inline">\(H(p,q)\)</span>.</p><p><br></p><p>从概率论角度理解， <span class="math display">\[\mathbb P(y_i\mid x_i;W)=\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}\]</span> 可以解释为参数为 <span class="math inline">\(W\)</span> 时，输入为 <span class="math inline">\(x_i\)</span> 的条件下输出为 <span class="math inline">\(y_i\)</span> 的概率。于是最小化 <span class="math inline">\(L_i\)</span> 等价于实施极大似然估计（Maximum Likelihood Estimation）。</p><p><br></p><p>值得注意的是，在编写代码时，<span class="math inline">\(e^{f_j}\)</span> 可能太大以至于计算精度较低，但是注意到： <span class="math display">\[\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}=\frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}=\frac{e^{f_{y_i}+\ln C}}{\sum_j e^{f_j+\ln C}}\]</span> 所以我们可以取 <span class="math inline">\(\ln C=-\max\limits_j f_j\)</span> 来解决这个问题。</p><hr /><h2 id="svm-vs.-softmax">SVM vs. Softmax</h2><p><img src="svm vs softmax.png" height="80%" width="80%" /></p><p>它们的实际效果往往差不多。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Stanford CS231n</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[CS231n]1·Image Classification</title>
    <link href="/blog-main/2021/02/21/CS231n-1%C2%B7Image-Classification/"/>
    <url>/blog-main/2021/02/21/CS231n-1%C2%B7Image-Classification/</url>
    
    <content type="html"><![CDATA[<p><a href="https://cs231n.github.io/">CS231n Convolutional Neural Networks for Visual Recognition</a></p><p>https://www.bilibili.com/video/BV1nJ411z7fe</p><span id="more"></span><h2 id="data-driven-approach">Data-Driven Approach</h2><p>在统计机器学习领域，不同于传统的算法，我们的算法是<strong>数据驱动</strong>的。以图像分类为例，数据驱动方法有以下 <span class="math inline">\(3\)</span> 个步骤：</p><ol type="1"><li>收集数据并标注；</li><li>使用机器学习方法训练一个分类器；</li><li>使用该分类器对新的图片进行分类。</li></ol><p>实现上来讲，我们的算法至少包含两个函数：<code>train</code> 和 <code>predict</code>，前者进行机器学习训练，后者使用训练好的模型进行预测。</p><hr /><h2 id="nearest-neighbor">Nearest Neighbor</h2><p>最简单的分类器莫过于<strong>最近邻</strong>了。仍然以图像分类为例，最近邻的 <code>train</code> 过程仅仅是记录下所有数据及其标签，<code>predict</code> 过程是选取数据集中与当前图片最相近的图片，并以其标签作为结果输出。</p><p>尽管最近邻很简单，但是仍然有一个问题需要解决：怎么评判两张图片的相似程度。由于图片可以看作像素点组成的三个向量（<span class="math inline">\(\text{RGB}\)</span> 各一个），所以问题转化为两个向量相似程度的评判。常用的有：</p><ul><li><p><span class="math inline">\(\text{L1 distance}\)</span>： <span class="math display">\[d_1(I_1,I_2)=\sum_p|I_1^p-I_2^p|\]</span> 这里 <span class="math inline">\(I_1,I_2\)</span> 是要比较的两个向量，上标 <span class="math inline">\(p\)</span> 表示向量的第 <span class="math inline">\(p\)</span> 个元素。该距离也称作曼哈顿距离。</p></li><li><p><span class="math inline">\(\text{L2 distance}\)</span>： <span class="math display">\[d_2(I_1,I_2)=\sqrt{\sum_p(I_1^p-I_2^p)^2}\]</span> 即欧几里得距离。</p></li></ul><p>显而易见，最近邻算法准确度很差。如果出现一个噪声点，它将对周围一圈产生误导。</p><p><img src="nn.png" height="30%" width="30%" /></p><hr /><h2 id="k-nearest-neighbor">K-Nearest Neighbor</h2><p>为了解决最近邻的问题，我们可以查看 <span class="math inline">\(k\)</span> 个最近的点，并以其中最多的标签作为结果。这就是 <span class="math inline">\(k\)</span> 近邻算法。</p><h3 id="hyperparameters">Hyperparameters</h3><p>在 <span class="math inline">\(k\)</span> 近邻算法中，<span class="math inline">\(k\)</span> 如何取值是我们设定的，而非模型学习出的，这类参数被称作<strong>超参数</strong>。</p><p><span class="math inline">\(k\)</span> 近邻算法的另一个超参数是距离函数的选取，选 <span class="math inline">\(\text{L1 distance}\)</span> 和选 <span class="math inline">\(\text{L2 distance}\)</span> 的结果会不同。</p><p><img src="knn k.png" height="80%" width="80%" /></p><p><img src="nn l1 l2.png" height="70%" width="70%" /></p><p><br></p><p>为了找寻最好的超参数，我们需要将数据集分为 <span class="math inline">\(3\)</span> 类：training set, validation set, test set. 选取一组超参数后，我们在 training set 上训练模型，在 validation set 上测试该模型的效果，调整超参数使得效果达到最佳，最后以在 test set 上的效果作为该模型的真正效果。</p><p><br></p><p><strong>交叉验证</strong>：在数据集不是很多的时候，我们可以选择 <span class="math inline">\(k\)</span> 折交叉验证的方式代替上述方式。我们分出一部分 test set 后，把剩下的数据集分成 <span class="math inline">\(k\)</span> 份（fold），循环地将每一份都视为 validation set、其他份视为 training set 进行 <span class="math inline">\(k\)</span> 次训练，以 <span class="math inline">\(k\)</span> 个训练效果的平均视为当前超参数下的训练效果，然后对超参数进行调整使得效果达到最佳，同样最后以在 test set 上的效果作为真正效果。</p><p><img src="cross validation.png" height="50%" width="50%" /></p><h3 id="knn-on-images-never-used">KNN on images never used</h3><p><span class="math inline">\(\text{KNN}\)</span> 算法非常简单，但是问题也非常突出：</p><ol type="1"><li>它基本没有训练过程，仅仅是把数据集记录下来，而在使用阶段需要遍历整个数据集，非常耗时（如果使用 <span class="math inline">\(\text{KD-Tree}\)</span> 实现，随机数据下复杂度是 <span class="math inline">\(\log\)</span> 的，要好一些，但是仍然不满意）；这与我们需要的正好相反——我们愿意花费较多的时间对模型进行训练，但一旦训练好了，在对新数据预测时要很快地出结果。</li><li>不同的图像之间可以有相同的 <span class="math inline">\(\text{distance}\)</span>，这种测度方式对图像而言不太好。</li><li>当向量维度很高时，我们的数据点在高维空间中的分布是稀疏的，所以所谓的“最近点”可能其实并没有多么的相似。</li></ol>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Stanford CS231n</category>
      
    </categories>
    
    
    <tags>
      
      <tag>computer vision</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[吴恩达机器学习]总结与使用scikit-learn </title>
    <link href="/blog-main/2021/02/01/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%80%BB%E7%BB%93%E4%B8%8E%E4%BD%BF%E7%94%A8scikit-learn/"/>
    <url>/blog-main/2021/02/01/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%80%BB%E7%BB%93%E4%B8%8E%E4%BD%BF%E7%94%A8scikit-learn/</url>
    
    <content type="html"><![CDATA[<p>吴恩达机器学习系列课程：<a href="https://www.bilibili.com/video/BV164411b7dx" class="uri">https://www.bilibili.com/video/BV164411b7dx</a></p><span id="more"></span><h2 id="总结">总结</h2><p>吴恩达老师的机器学习系列课程到这里就结束了，<span class="math inline">\(40\)</span> 天里，我学到了许多有趣的、极具吸引力的机器学习知识，相信若干年后想起这段时光，仍然会感谢吴恩达老师带我入门了这个领域。</p><p>在这门课上，我们学习了以下内容：</p><ul><li>监督学习 Supervised learning<ul><li>线性回归 Linear regression【笔记一、二、三】</li><li>逻辑回归 Logistic regression【笔记四、五、六】</li><li>(BP)神经网络 Neural networks【笔记七、八】</li><li>支持向量机 Support vector machines【笔记十】</li></ul></li><li>无监督学习 Unsupervised learning<ul><li>K-Means【笔记十一】</li><li>主成分分析 Principal component analysis【笔记十二】</li><li>异常检测 Anomaly detection【笔记十三】</li></ul></li><li>特殊应用/特殊专题<ul><li>推荐系统 Recommender systems（协同过滤 Collaborative filtering）【笔记十四】</li><li>大规模机器学习 Large scale machine learning</li></ul></li><li>建立机器学习模型时的一些建议<ul><li>高方差与高偏差 Bias / variance【笔记九】</li><li>正则化 Regularization【笔记五】</li><li>对学习算法的评价：precision, recall, f1 score</li><li>学习曲线 Learning curves【笔记九】</li><li>误差分析 Error analysis</li><li>上界分析 Ceiling analysis</li></ul></li></ul><h2 id="使用-scikit-learn-进行机器学习">使用 <code>scikit-learn</code> 进行机器学习</h2><p>学习过程中的代码基本都是自己实现的，运行效率和使用容易程度上不敢恭维……现在清楚了原理之后，就可以放心大胆地调包了😂</p><h3 id="线性回归">线性回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">class sklearn.linear_model.LinearRegression(*, fit_intercept=True, </span><br><span class="hljs-string">normalize=False, copy_X=True, n_jobs=None, positive=False)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<br><br>data = np.loadtxt(<span class="hljs-string">&#x27;data/ex1data1.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br>y = data[:, -<span class="hljs-number">1</span>]<br>X = data[:, :-<span class="hljs-number">1</span>]<br><br>reg = LinearRegression(normalize=<span class="hljs-literal">True</span>)<br>reg.fit(X, y)<br><span class="hljs-built_in">print</span>(reg.score(X, y))<br><br>ax = plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>ax.set_xlabel(<span class="hljs-string">&#x27;x&#x27;</span>)<br>ax.set_ylabel(<span class="hljs-string">&#x27;y&#x27;</span>)<br>ax.set_title(<span class="hljs-string">&#x27;Linear Regression&#x27;</span>)<br>ax.plot(X.flatten(), y, <span class="hljs-string">&#x27;x&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br>ax.plot(X, reg.predict(X))<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="LinearRegression.png" width="50%" height="50%" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">class sklearn.linear_model.LinearRegression(*, fit_intercept=True, </span><br><span class="hljs-string">normalize=False, copy_X=True, n_jobs=None, positive=False)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression<br><br>data = np.loadtxt(<span class="hljs-string">&#x27;data/ex1data2.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br>y = data[:, -<span class="hljs-number">1</span>]<br>X = data[:, :-<span class="hljs-number">1</span>]<br><br>reg = LinearRegression(normalize=<span class="hljs-literal">True</span>)<br>reg.fit(X, y)<br><span class="hljs-built_in">print</span>(reg.score(X, y))<br><span class="hljs-built_in">print</span>(reg.predict(X))<br></code></pre></td></tr></table></figure><h3 id="逻辑回归">逻辑回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">class sklearn.linear_model.LogisticRegression(penalty=&#x27;l2&#x27;, *, dual=False, </span><br><span class="hljs-string">tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, </span><br><span class="hljs-string">class_weight=None, random_state=None, solver=&#x27;lbfgs&#x27;, max_iter=100, </span><br><span class="hljs-string">multi_class=&#x27;auto&#x27;, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><br>data = np.loadtxt(<span class="hljs-string">&#x27;data/ex2data1.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br>y = data[:, -<span class="hljs-number">1</span>]<br>X = data[:, :-<span class="hljs-number">1</span>]<br><br>clf = LogisticRegression()<br>clf.fit(X, y)<br><span class="hljs-built_in">print</span>(clf.score(X, y))<br><br>ax = plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>ax.set_xlabel(<span class="hljs-string">&#x27;x1&#x27;</span>)<br>ax.set_ylabel(<span class="hljs-string">&#x27;x2&#x27;</span>)<br>ax.set_title(<span class="hljs-string">&#x27;Logistic Regression&#x27;</span>)<br>ax.plot(X[y==<span class="hljs-number">0</span>][:, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">0</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;x&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br>ax.plot(X[y==<span class="hljs-number">1</span>][:, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">1</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>)<br>x1, x2 = np.meshgrid(np.linspace(X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>(), X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>(), <span class="hljs-number">100</span>), <br>np.linspace(X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>(), X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>(), <span class="hljs-number">100</span>))<br>ax.contour(x1, x2, <br>clf.predict_proba(np.array([x1, x2]).\<br>transpose(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>).reshape((<span class="hljs-number">10000</span>, <span class="hljs-number">2</span>)))[:, <span class="hljs-number">0</span>].reshape(<span class="hljs-number">100</span>, <span class="hljs-number">100</span>), [<span class="hljs-number">0.5</span>])<br>ax.axis(<span class="hljs-string">&#x27;square&#x27;</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="LogisticRegression.png" width="50%" height="50%" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">class sklearn.linear_model.LogisticRegression(penalty=&#x27;l2&#x27;, *, dual=False, </span><br><span class="hljs-string">tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, </span><br><span class="hljs-string">class_weight=None, random_state=None, solver=&#x27;lbfgs&#x27;, max_iter=100, </span><br><span class="hljs-string">multi_class=&#x27;auto&#x27;, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)</span><br><span class="hljs-string"></span><br><span class="hljs-string">class sklearn.preprocessing.PolynomialFeatures(degree=2, *, </span><br><span class="hljs-string">interaction_only=False, include_bias=True, order=&#x27;C&#x27;)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegressionCV<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> PolynomialFeatures<br><br>data = np.loadtxt(<span class="hljs-string">&#x27;data/ex2data2.txt&#x27;</span>, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br>y = data[:, -<span class="hljs-number">1</span>]<br>X = data[:, :-<span class="hljs-number">1</span>]<br>poly = PolynomialFeatures(degree=<span class="hljs-number">5</span>)<br>newX = poly.fit_transform(X)<br><br>fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>)<br>fig.suptitle(<span class="hljs-string">&#x27;Logistic Regression&#x27;</span>)<br>Cs = [<span class="hljs-number">0.01</span>, <span class="hljs-number">1</span>, <span class="hljs-number">300</span>, -<span class="hljs-number">1</span>]<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>C = Cs[i]<br><span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">3</span>:<br>clf = LogisticRegression(C=C, max_iter=<span class="hljs-number">1000</span>)<br><span class="hljs-keyword">else</span>:<br>clf = LogisticRegressionCV(max_iter=<span class="hljs-number">1000</span>)<br>clf.fit(newX, y)<br><span class="hljs-built_in">print</span>(clf.score(newX, y))<br><br><span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">3</span>:<br>ax[i].set_title(<span class="hljs-string">&#x27;C=&#123;0&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(C))<br><span class="hljs-keyword">else</span>:<br>ax[i].set_title(<span class="hljs-string">&#x27;LogisticRegressionCV&#x27;</span>)<br>ax[i].plot(X[y==<span class="hljs-number">0</span>][:, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">0</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;x&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br>ax[i].plot(X[y==<span class="hljs-number">1</span>][:, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">1</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>)<br>x1, x2 = np.meshgrid(np.linspace(X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>(), X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>(), <span class="hljs-number">100</span>), <br>np.linspace(X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>(), X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>(), <span class="hljs-number">100</span>))<br>ax[i].contour(x1, x2, clf.predict_proba(<br>poly.transform(np.array([x1, x2]).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>).reshape((<span class="hljs-number">10000</span>, <span class="hljs-number">2</span>)))<br>)[:, <span class="hljs-number">0</span>].reshape(<span class="hljs-number">100</span>, <span class="hljs-number">100</span>), [<span class="hljs-number">0.5</span>])<br>ax[i].axis(<span class="hljs-string">&#x27;square&#x27;</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="LogisticRegression2.png" width="100%" height="100%" /></p><h3 id="多项式回归">多项式回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">class sklearn.linear_model.Ridge(alpha=1.0, *, fit_intercept=True, normalize=False, </span><br><span class="hljs-string">copy_X=True, max_iter=None, tol=0.001, solver=&#x27;auto&#x27;, random_state=None)</span><br><span class="hljs-string"></span><br><span class="hljs-string">class sklearn.preprocessing.PolynomialFeatures(degree=2, *, </span><br><span class="hljs-string">interaction_only=False, include_bias=True, order=&#x27;C&#x27;)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> Ridge<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> RidgeCV<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> PolynomialFeatures<br><br>data = loadmat(<span class="hljs-string">&#x27;data/ex5data1.mat&#x27;</span>)<br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>]<br>y = data[<span class="hljs-string">&#x27;y&#x27;</span>]<br>poly = PolynomialFeatures(degree=<span class="hljs-number">8</span>)<br>newX = poly.fit_transform(X)<br><br>alphas = [<span class="hljs-number">0</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">10</span>, -<span class="hljs-number">1</span>]<br>fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>)<br>fig.suptitle(<span class="hljs-string">&#x27;Polynomial Regression&#x27;</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>alpha = alphas[i]<br><span class="hljs-keyword">if</span> alpha == -<span class="hljs-number">1</span>:<br>reg = RidgeCV(normalize=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">else</span>:<br>reg = Ridge(alpha=alpha, normalize=<span class="hljs-literal">True</span>)<br>reg.fit(newX, y)<br><span class="hljs-built_in">print</span>(reg.score(newX, y))<br><br><span class="hljs-keyword">if</span> alpha &gt;= <span class="hljs-number">0</span>:<br>ax[i].set_title(<span class="hljs-string">&#x27;alpha=&#123;0&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(alpha))<br><span class="hljs-keyword">else</span>:<br>ax[i].set_title(<span class="hljs-string">&#x27;RidgeCV&#x27;</span>)<br>ax[i].plot(X.flatten(), y, <span class="hljs-string">&#x27;x&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br>ax[i].plot(np.linspace(X.<span class="hljs-built_in">min</span>()-<span class="hljs-number">5</span>, X.<span class="hljs-built_in">max</span>()+<span class="hljs-number">5</span>, <span class="hljs-number">100</span>), \<br>reg.predict(poly.transform( np.linspace(X.<span class="hljs-built_in">min</span>()-<span class="hljs-number">5</span>, X.<span class="hljs-built_in">max</span>()+<span class="hljs-number">5</span>, <span class="hljs-number">100</span>).reshape((<span class="hljs-number">100</span>, <span class="hljs-number">1</span>))) ))<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="PolynomialRegression.png" width="100%" height="100%" /></p><h3 id="bp-神经网络">BP 神经网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=100, activation=&#x27;relu&#x27;, *, </span><br><span class="hljs-string">solver=&#x27;adam&#x27;, alpha=0.0001, batch_size=&#x27;auto&#x27;, learning_rate=&#x27;constant&#x27;, </span><br><span class="hljs-string">learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, </span><br><span class="hljs-string">tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, </span><br><span class="hljs-string">early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, </span><br><span class="hljs-string">n_iter_no_change=10, max_fun=15000)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.neural_network <span class="hljs-keyword">import</span> MLPClassifier<br><br>data = loadmat(<span class="hljs-string">&#x27;data/ex4data1.mat&#x27;</span>)<br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>]<br>m = X.shape[<span class="hljs-number">0</span>]<br>X = np.transpose(X.reshape(m, <span class="hljs-number">20</span>, <span class="hljs-number">20</span>), [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]).reshape(m, <span class="hljs-number">400</span>)<br>y = data[<span class="hljs-string">&#x27;y&#x27;</span>]<br>y[y==<span class="hljs-number">10</span>] = <span class="hljs-number">0</span><br>y = y.flatten()<br>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = <span class="hljs-number">0.8</span>)<br><br>clf = MLPClassifier(hidden_layer_sizes=(<span class="hljs-number">25</span>,), <br>random_state=<span class="hljs-literal">True</span>, <br>max_iter=<span class="hljs-number">1000</span>)<br>clf.fit(X_train, y_train)<br><span class="hljs-built_in">print</span>(clf.score(X_test, y_test))<br></code></pre></td></tr></table></figure><h3 id="支持向量机">支持向量机</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">class sklearn.svm.LinearSVC(penalty=&#x27;l2&#x27;, loss=&#x27;squared_hinge&#x27;, *, dual=True, </span><br><span class="hljs-string">tol=0.0001, C=1.0, multi_class=&#x27;ovr&#x27;, fit_intercept=True, intercept_scaling=1, </span><br><span class="hljs-string">class_weight=None, verbose=0, random_state=None, max_iter=1000)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> LinearSVC<br><br>data = loadmat(<span class="hljs-string">&#x27;data/ex6data1.mat&#x27;</span>)<br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>]<br>y = data[<span class="hljs-string">&#x27;y&#x27;</span>].flatten()<br><br>Cs = [<span class="hljs-number">1</span>, <span class="hljs-number">100</span>]<br>fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>fig.suptitle(<span class="hljs-string">&#x27;LinearSVC&#x27;</span>)<br>x1, x2 = np.meshgrid(np.linspace(X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>()-<span class="hljs-number">0.5</span>, X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>()+<span class="hljs-number">0.5</span>, <span class="hljs-number">100</span>), <br>np.linspace(X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>()-<span class="hljs-number">0.5</span>, X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>()+<span class="hljs-number">0.5</span>, <span class="hljs-number">100</span>))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):<br>C = Cs[i]<br>clf = LinearSVC(C=C)<br>clf.fit(X, y)<br><span class="hljs-built_in">print</span>(clf.predict(X[:<span class="hljs-number">10</span>]))<br><br>ax[i].plot(X[y==<span class="hljs-number">0</span>][:, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">0</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;x&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br>ax[i].plot(X[y==<span class="hljs-number">1</span>][:, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">1</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>)<br>ax[i].contour(x1, x2, clf.decision_function(<br>np.array([x1, x2]).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>).reshape(<span class="hljs-number">10000</span>, <span class="hljs-number">2</span>)<br>).reshape(<span class="hljs-number">100</span>, <span class="hljs-number">100</span>), [<span class="hljs-number">0</span>])<br>ax[i].set_xlabel(<span class="hljs-string">&#x27;x1&#x27;</span>)<br>ax[i].set_ylabel(<span class="hljs-string">&#x27;x2&#x27;</span>)<br>ax[i].set_title(<span class="hljs-string">&#x27;C=&#123;0&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(C))<br>ax[i].axis(<span class="hljs-string">&#x27;square&#x27;</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="LinearSVC.png" width="100%" height="100%" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">class sklearn.svm.SVC(*, C=1.0, kernel=&#x27;rbf&#x27;, degree=3, gamma=&#x27;scale&#x27;, </span><br><span class="hljs-string">coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, </span><br><span class="hljs-string">class_weight=None, verbose=False, max_iter=- 1, decision_function_shape=&#x27;ovr&#x27;, </span><br><span class="hljs-string">break_ties=False, random_state=None)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br><br>data = loadmat(<span class="hljs-string">&#x27;data/ex6data2.mat&#x27;</span>)<br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>]<br>y = data[<span class="hljs-string">&#x27;y&#x27;</span>].flatten()<br><br>Cs, gammas = [<span class="hljs-number">1</span>, <span class="hljs-number">100</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">30</span>]<br>fig, ax = plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br>fig.suptitle(<span class="hljs-string">&#x27;SVC&#x27;</span>)<br>x1, x2 = np.meshgrid(np.linspace(X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>(), X[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>(), <span class="hljs-number">100</span>), <br>np.linspace(X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>(), X[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>(), <span class="hljs-number">100</span>))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):<br><span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):<br>C, gamma = Cs[i], gammas[j]<br>clf = SVC(C=C, kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>, probability=<span class="hljs-literal">True</span>, gamma=gamma)<br>clf.fit(X, y)<br><br>ax[i][j].plot(X[y==<span class="hljs-number">0</span>][:, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">0</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;x&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>, alpha=<span class="hljs-number">0.5</span>)<br>ax[i][j].plot(X[y==<span class="hljs-number">1</span>][:, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">1</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>, alpha=<span class="hljs-number">0.5</span>)<br>ax[i][j].contour(x1, x2, clf.decision_function(<br>np.array([x1, x2]).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>).reshape(<span class="hljs-number">10000</span>, <span class="hljs-number">2</span>)<br>).reshape(<span class="hljs-number">100</span>, <span class="hljs-number">100</span>), [<span class="hljs-number">0</span>])<br>ax[i][j].set_title(<span class="hljs-string">&#x27;C=&#123;0&#125;, gamma=&#123;1&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(C, gamma))<br><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="SVC.png" width="100%" height="100%" /></p><h3 id="k-means">K-Means</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">class sklearn.cluster.KMeans(n_clusters=8, *, init=&#x27;k-means++&#x27;, n_init=10, </span><br><span class="hljs-string">max_iter=300, tol=0.0001, precompute_distances=&#x27;deprecated&#x27;, verbose=0, </span><br><span class="hljs-string">random_state=None, copy_x=True, n_jobs=&#x27;deprecated&#x27;, algorithm=&#x27;auto&#x27;)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br><br>data = loadmat(<span class="hljs-string">&#x27;data/ex7data2.mat&#x27;</span>)<br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>]<br><br>clt = KMeans(n_clusters=<span class="hljs-number">3</span>)<br>belong = clt.fit_predict(X)<br><br>ax = plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>ax.plot(X[belong==<span class="hljs-number">0</span>][:, <span class="hljs-number">0</span>], X[belong==<span class="hljs-number">0</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>)<br>ax.plot(X[belong==<span class="hljs-number">1</span>][:, <span class="hljs-number">0</span>], X[belong==<span class="hljs-number">1</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>)<br>ax.plot(X[belong==<span class="hljs-number">2</span>][:, <span class="hljs-number">0</span>], X[belong==<span class="hljs-number">2</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;green&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>)<br>ax.set_title(<span class="hljs-string">&#x27;KMeans&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="KMeans.png" width="50%" height="50%" /></p><h3 id="主成分分析">主成分分析</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">class sklearn.decomposition.PCA(n_components=None, *, copy=True, whiten=False, </span><br><span class="hljs-string">svd_solver=&#x27;auto&#x27;, tol=0.0, iterated_power=&#x27;auto&#x27;, random_state=None)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br><br>data = loadmat(<span class="hljs-string">&#x27;data/ex7data1.mat&#x27;</span>)<br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>]<br><br>pca = PCA(n_components=<span class="hljs-number">1</span>)<br>pca.fit(X)<br>redX = pca.transform(X) <span class="hljs-comment"># reduced X</span><br>recX = pca.inverse_transform(redX) <span class="hljs-comment"># recovered X</span><br><br>ax = plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>ax.plot(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;black&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>)<br>ax.plot(recX[:, <span class="hljs-number">0</span>], recX[:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>)<br>ax.set_title(<span class="hljs-string">&#x27;PCA&#x27;</span>)<br>ax.axis(<span class="hljs-string">&#x27;square&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="PCA.png" width="50%" height="50%" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">class sklearn.decomposition.PCA(n_components=None, *, copy=True, whiten=False, </span><br><span class="hljs-string">svd_solver=&#x27;auto&#x27;, tol=0.0, iterated_power=&#x27;auto&#x27;, random_state=None)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br><br>X = loadmat(<span class="hljs-string">&#x27;data/ex7faces.mat&#x27;</span>)[<span class="hljs-string">&#x27;X&#x27;</span>]<br>X = np.transpose(X.reshape((<span class="hljs-number">5000</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)), [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]).reshape(<span class="hljs-number">5000</span>, <span class="hljs-number">1024</span>)<br>X = -X<br><br>pca = PCA(n_components=<span class="hljs-number">0.99</span>)<br>pca.fit(X)<br>redX = pca.transform(X) <span class="hljs-comment"># reduced X</span><br>recX = pca.inverse_transform(redX) <span class="hljs-comment"># recovered X</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">show_a_face</span>(<span class="hljs-params">face, ax</span>):<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">face.shape: (1024, )</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>ax.matshow(face.reshape((<span class="hljs-number">32</span>, <span class="hljs-number">32</span>)), cmap=matplotlib.cm.binary)<br>ax.axis(<span class="hljs-string">&#x27;off&#x27;</span>)<br><br>fig, ax = plt.subplots(<span class="hljs-number">5</span>)<br>fig.suptitle(<span class="hljs-string">&#x27;99% variance&#x27;</span>, fontweight=<span class="hljs-string">&#x27;bold&#x27;</span>)<br>fig.subplots_adjust(hspace=<span class="hljs-number">0</span>, wspace=<span class="hljs-literal">None</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>show_a_face(recX[i], ax[i])<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="PCA2.png" width="50%" height="50%" /></p><h3 id="异常检测">异常检测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">class sklearn.covariance.EllipticEnvelope(*, store_precision=True, assume_centered=False, </span><br><span class="hljs-string">support_fraction=None, contamination=0.1, random_state=None)</span><br><span class="hljs-string"></span><br><span class="hljs-string">sklearn.metrics.f1_score(y_true, y_pred, *, labels=None, pos_label=1, </span><br><span class="hljs-string">average=&#x27;binary&#x27;, sample_weight=None, zero_division=&#x27;warn&#x27;)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">from</span> sklearn.covariance <span class="hljs-keyword">import</span> EllipticEnvelope<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> f1_score<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>data = loadmat(<span class="hljs-string">&#x27;data/ex8data1.mat&#x27;</span>)<br>X, Xval, yval = data[<span class="hljs-string">&#x27;X&#x27;</span>], data[<span class="hljs-string">&#x27;Xval&#x27;</span>], data[<span class="hljs-string">&#x27;yval&#x27;</span>].flatten()<br><br>bestc, bestf1 = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">100</span>):<br>ano = EllipticEnvelope(contamination=c)<br>ano.fit(X)<br>pred = ano.predict(X)<br>pred[pred==<span class="hljs-number">1</span>] = <span class="hljs-number">0</span><br>pred[pred==-<span class="hljs-number">1</span>] = <span class="hljs-number">1</span><br>f1 = f1_score(yval, pred)<br><span class="hljs-keyword">if</span> f1 &gt; bestf1:<br>bestc, bestf1 = c, f1<br><br>ano = EllipticEnvelope(contamination=bestc)<br><span class="hljs-built_in">print</span>(bestc)<br>ano.fit(X)<br>pred = ano.predict(X)<br><br>ax = plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>ax.set_xlabel(<span class="hljs-string">&#x27;Latency (ms)&#x27;</span>)<br>ax.set_ylabel(<span class="hljs-string">&#x27;Throughput (mb/s)&#x27;</span>)<br>ax.plot(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;x&#x27;</span>, alpha=<span class="hljs-number">0.5</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>)<br>ax.plot(X[pred==-<span class="hljs-number">1</span>][:, <span class="hljs-number">0</span>], X[pred==-<span class="hljs-number">1</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>, ms=<span class="hljs-number">10</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="AnomalyDetection.png" width="50%" height="50%" /></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Andrew Ng</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[吴恩达机器学习]14·推荐系统</title>
    <link href="/blog-main/2021/01/30/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-14%C2%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    <url>/blog-main/2021/01/30/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-14%C2%B7%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<p>吴恩达机器学习系列课程：<a href="https://www.bilibili.com/video/BV164411b7dx" class="uri">https://www.bilibili.com/video/BV164411b7dx</a></p><span id="more"></span><h2 id="基于内容的推荐算法">基于内容的推荐算法</h2><p>以向用户推荐电影为例，假设我们对每部电影构建了一个特征向量，并且已知每个用户对某些电影的评分，那么对于某个用户而言，我们可以将电影的特征向量看作自变量 <span class="math inline">\(x\)</span>，他的评分看作因变量 <span class="math inline">\(y\)</span>，然后做<strong>线性回归</strong>。</p><p>具体地，设一共有 <span class="math inline">\(n_u\)</span> 个用户，<span class="math inline">\(n_m\)</span> 部电影，第 <span class="math inline">\(i\)</span> 部电影的特征向量为 <span class="math inline">\(x^{(i)}\in\mathbb R^{n+1}\)</span>（包含偏置项），<span class="math inline">\(r(i,j)\)</span> 表示用户 <span class="math inline">\(j\)</span> 是否对第 <span class="math inline">\(i\)</span> 部电影进行了评分，如果评了分，设评分为 <span class="math inline">\(y^{(i,j)}\)</span>. 那么对于第 <span class="math inline">\(j\)</span> 个用户，线性回归的目标就是学习一个参数 <span class="math inline">\(\theta^{(j)}\)</span>，使得： <span class="math display">\[\min_{\theta^{(j)}}\frac{1}{2}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\sum_{k=1}^{n}(\theta_k^{(j)})^2\]</span> 由于每个用户的线性回归都是独立的，所以我们可以放在一起训练： <span class="math display">\[\min_{\theta^{(1)},\ldots,\theta^{(n_u)}}J(\theta^{(1)},\ldots,\theta^{(n_u)}):=\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2\]</span> 训练过程可能用到导函数： <span class="math display">\[\frac{\partial J}{\partial\theta^{(j)}_k}=\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)\theta^{(j)}_k+\lambda\theta_k^{(j)}[k&gt;0]\]</span> 那么对于一个特征向量为 <span class="math inline">\(x\)</span> 的电影，用户 <span class="math inline">\(j\)</span> 对它的评分的预测值就是：<span class="math inline">\((\theta^{(j)})^Tx\)</span>.</p><p>基于内容的推荐算法的缺点在于，我们需要知道每部电影的特征向量，然而这一点通常很难做到。所以我们需要不是基于内容的推荐算法。</p><h2 id="协同过滤算法">协同过滤算法</h2><h3 id="初始版本">初始版本</h3><p>现在我们不知道每部电影的特征向量，但是我们可以询问用户以得到用户的参数 <span class="math inline">\(\theta^{(j)}\)</span>（譬如用户对不同类型电影的偏好），然后反过来，用 <span class="math inline">\(\theta^{(j)}\)</span> 去训练出 <span class="math inline">\(x^{(i)}\)</span>，得到每部电影的特征。具体地，对于第 <span class="math inline">\(i\)</span> 部电影，我们可以学习它的特征 <span class="math inline">\(x^{(i)}\)</span>，使得： <span class="math display">\[\min_{\theta^{(j)}}\frac{1}{2}\sum_{j:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\sum_{k=1}^{n}(x_k^{(i)})^2\]</span> 由于每部电影的线性回归是独立的，所以我们可以放在一起训练： <span class="math display">\[\min_{x^{(1)},\ldots,x^{(n_m)}}J(x^{(1)},\ldots,x^{(n_m)}):=\frac{1}{2}\sum_{i=1}^{n_m}\sum_{j:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^2\]</span> 训练过程可能用到导函数： <span class="math display">\[\frac{\partial J}{\partial x^{(i)}_k}=\sum_{j:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)\theta^{(i)}_k+\lambda x_k^{(i)}[k&gt;0]\]</span> <br></p><p>总结一下，已知 <span class="math inline">\(\theta^{(j)}\)</span>，我们可以学习 <span class="math inline">\(x^{(i)}\)</span>；已知 <span class="math inline">\(x^{(i)}\)</span>，我们可以学习 <span class="math inline">\(\theta^{(j)}\)</span>. 于是我们有了一个大胆的想法——随机化一个 <span class="math inline">\(\theta^{(j)}\)</span>，学习出 <span class="math inline">\(x^{(i)}\)</span>，再用学习出的 <span class="math inline">\(x^{(i)}\)</span> 去学习 <span class="math inline">\(\theta^{(j)}\)</span>，再用新的 <span class="math inline">\(\theta^{(j)}\)</span> 去学习 <span class="math inline">\(x^{(i)}\)</span>……如此<strong>反复迭代</strong>，最终得到稳定的电影特征和用户参数。这就是最初始版本的协同过滤算法。</p><h3 id="改进版本">改进版本</h3><p>事实上，我们没有反复迭代的必要。观察用 <span class="math inline">\(\theta^{(j)}\)</span> 训练 <span class="math inline">\(x^{(i)}\)</span> 的优化目标和用 <span class="math inline">\(x^{(i)}\)</span> 训练 <span class="math inline">\(\theta^{(j)}\)</span> 的优化目标，我们可以发现，它们的非正则化项其实是相同的，都是：<span class="math inline">\(\sum\limits_{(i,j):r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2\)</span>. 所以，我们将两个优化目标综合起来，优化以下函数即可： <span class="math display">\[\begin{align}&amp;\min_{x^{(1)},\ldots,x^{(n_m)}\\\theta^{(1)},\ldots,\theta^{(n_u)}}J(x^{(1)},\ldots,x^{(n_m)},\theta^{(1)},\ldots,\theta^{(n_u)})\\=&amp;\frac{1}{2}\sum_{(i,j):r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i)}\right)^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n(x^{(i)}_k)^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^n(\theta^{(j)}_k)^2\end{align}\]</span> 值得注意的是，在综合起来之前，<span class="math inline">\(n\)</span> 是我们人为选定的特征维度数，是一个定值；而现在，<span class="math inline">\(n\)</span> 变成了一个超参数，因此我们也<strong>没有必要加上偏置项</strong>，所以这里 <span class="math inline">\(x^{(i)}\in\mathbb R^n,\theta^{(j)}\in\mathbb R^n\)</span>.</p><p>上式的导函数为： <span class="math display">\[\begin{align}&amp;\frac{\partial J}{\partial\theta^{(j)}_k}=\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)\theta^{(j)}_k+\lambda\theta_k^{(j)}\\&amp;\frac{\partial J}{\partial x^{(i)}_k}=\sum_{j:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)x^{(i)}_k+\lambda x_k^{(i)}\end{align}\]</span> 我们现在可以梯度下降或者用其他算法（如 <span class="math inline">\(\text{LBFGS}\)</span> 等）完成优化了。</p><h3 id="向量化版本">向量化版本</h3><p>为了代码的运行效率，将该算法向量化是必要的。</p><p>构建矩阵 <span class="math inline">\(Y:=\begin{bmatrix}y^{(i,j)}\end{bmatrix}\in\mathbb R^{n_m\times n_u}\)</span>，即第 <span class="math inline">\(i\)</span> 行第 <span class="math inline">\(j\)</span> 列表示用户 <span class="math inline">\(j\)</span> 对电影 <span class="math inline">\(i\)</span> 的评分；矩阵 <span class="math inline">\(X:=\begin{bmatrix}(x^{(1)})^T\\ \vdots\\ (x^{(n_m)})^T\end{bmatrix}\in\mathbb R^{n_m\times n}\)</span>，即第 <span class="math inline">\(i\)</span> 行表示电影 <span class="math inline">\(i\)</span> 的特征向量；矩阵 <span class="math inline">\(\Theta:=\begin{bmatrix}(\theta^{(1)})^T\\ \vdots\\ (\theta^{(n_u)})^T\end{bmatrix}\in\mathbb R^{n_u\times n}\)</span>，即第 <span class="math inline">\(j\)</span> 行表示用户 <span class="math inline">\(j\)</span> 的参数向量。如此，线性回归的预测值可以构成矩阵： <span class="math display">\[X\Theta^T\in\mathbb R^{n_m\times n_u}\]</span> 利用 <code>numpy</code> 的语法可以简单地写出代价函数及其导函数的向量化版本，详见代码。</p><h2 id="实现">实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">from</span> scipy.optimize <span class="hljs-keyword">import</span> minimize<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">J</span>(<span class="hljs-params">Y, R, X, Theta, lamb</span>):<br><span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * (np.<span class="hljs-built_in">sum</span>(((X @ Theta.T - Y) * R) ** <span class="hljs-number">2</span>) + \<br>lamb * np.<span class="hljs-built_in">sum</span>(X ** <span class="hljs-number">2</span>) + lamb * np.<span class="hljs-built_in">sum</span>(Theta ** <span class="hljs-number">2</span>))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partJ</span>(<span class="hljs-params">Y, R, X, Theta, lamb</span>):<br><span class="hljs-keyword">return</span> np.concatenate((<br>( ((X @ Theta.T - Y) * R) @ Theta + lamb * X ).flatten(), <br>( ((X @ Theta.T - Y) * R).T @ X + lamb * Theta ).flatten()<br>))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">Y, R, lamb, n</span>):<br>(n_m, n_u) = Y.shape<br>xt = np.empty(n*n_m+n*n_u)<br><span class="hljs-keyword">return</span> minimize(fun = <span class="hljs-keyword">lambda</span> xt, Y, R, lamb : J(Y, R, xt[:n*n_m].reshape((n_m, n)), \<br>xt[n*n_m:].reshape((n_u, n)), lamb), <br>x0 = np.random.randn(n*n_m+n*n_u), <br>args = (Y, R, lamb), <br>method = <span class="hljs-string">&#x27;TNC&#x27;</span>, <br>jac = <span class="hljs-keyword">lambda</span> xt, Y, R, lamb: partJ(Y, R, xt[:n*n_m].reshape((n_m, n)), \<br>xt[n*n_m:].reshape((n_u, n)), lamb)<br>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">Y, R, xt, n</span>):<br>(n_m, n_u) = Y.shape<br>X, Theta = xt[:n*n_m].reshape((n_m, n)), xt[n*n_m:].reshape((n_u, n))<br><span class="hljs-keyword">return</span> X @ Theta.T<br><br>data = loadmat(<span class="hljs-string">&#x27;ex8_movies.mat&#x27;</span>)<br>Y = data[<span class="hljs-string">&#x27;Y&#x27;</span>]<br>R = data[<span class="hljs-string">&#x27;R&#x27;</span>]<br>Ymean = Y.mean(axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br>res = train(Y-Ymean, R, lamb=<span class="hljs-number">1</span>, n=<span class="hljs-number">50</span>)<br><span class="hljs-built_in">print</span>(res)<br>np.save(<span class="hljs-string">&#x27;xt.npy&#x27;</span>, res.x)<br>xt = res.x<br></code></pre></td></tr></table></figure><p>优化结果为：</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs subunit">    fun: 11078.825074101991<br>    jac: array([ 4.75273039e<span class="hljs-string">-07</span>, <span class="hljs-string">-8</span>.33595791e<span class="hljs-string">-07</span>, <span class="hljs-string">-4</span>.85091646e<span class="hljs-string">-07</span>, ...,<br>       9.67607422e<span class="hljs-string">-07</span>,  3.80539749e<span class="hljs-string">-06</span>,  1.86386969e<span class="hljs-string">-06</span>])<br>message: &#x27;Converged (|f_n-f_(n<span class="hljs-string">-1</span>)| ~= 0)&#x27;<br>   nfev: 14671<br>    nit: 463<br> status: 1<br><span class="hljs-keyword">success: </span>True<br>      x: array([ 0.30269431, <span class="hljs-string">-1</span>.3577984 , <span class="hljs-string">-0</span>.19821756, ...,  0.12718836,<br>      <span class="hljs-string">-0</span>.40793964, <span class="hljs-string">-0</span>.60753772])<br></code></pre></td></tr></table></figure><p>用该参数找到第一个用户预测评分最高的 <span class="math inline">\(10\)</span> 部电影：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">xt = np.load(<span class="hljs-string">&#x27;xt.npy&#x27;</span>)<br>pred = predict(Y, R, xt, n=<span class="hljs-number">50</span>) + Ymean<br>movie_list = []<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;movie_ids.txt&#x27;</span>, encoding=<span class="hljs-string">&#x27;latin-1&#x27;</span>) <span class="hljs-keyword">as</span> file:<br><span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> file:<br>movie_list.append(<span class="hljs-string">&#x27; &#x27;</span>.join(line.strip().split(<span class="hljs-string">&#x27; &#x27;</span>)[<span class="hljs-number">1</span>: ]))<br>movie_list = np.array(movie_list)<br>idx = np.argsort(pred[:, <span class="hljs-number">0</span>])[::-<span class="hljs-number">1</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Top 10 movies for user 1:&#x27;</span>)<br><span class="hljs-keyword">for</span> movie <span class="hljs-keyword">in</span> movie_list[idx][:<span class="hljs-number">10</span>]:<br><span class="hljs-built_in">print</span>(movie)<br></code></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Top</span> <span class="hljs-number">10</span> movies for user <span class="hljs-number">1</span>:<br><span class="hljs-attribute">Titanic</span> (<span class="hljs-number">1997</span>)<br><span class="hljs-attribute">In</span> the Name of the Father (<span class="hljs-number">1993</span>)<br><span class="hljs-attribute">Philadelphia</span> (<span class="hljs-number">1993</span>)<br><span class="hljs-attribute">Duck</span> Soup (<span class="hljs-number">1933</span>)<br><span class="hljs-attribute">Ice</span> Storm, The (<span class="hljs-number">1997</span>)<br><span class="hljs-attribute">Saint</span>, The (<span class="hljs-number">1997</span>)<br><span class="hljs-attribute">William</span> Shakespeare&#x27;s Romeo and Juliet (<span class="hljs-number">1996</span>)<br><span class="hljs-attribute">Boot</span>, Das (<span class="hljs-number">1981</span>)<br><span class="hljs-attribute">People</span> vs. Larry Flynt, The (<span class="hljs-number">1996</span>)<br><span class="hljs-attribute">Manhattan</span> (<span class="hljs-number">1979</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Andrew Ng</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[吴恩达机器学习]13·异常检测</title>
    <link href="/blog-main/2021/01/29/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-13%C2%B7%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    <url>/blog-main/2021/01/29/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-13%C2%B7%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<p>吴恩达机器学习系列课程：<a href="https://www.bilibili.com/video/BV164411b7dx" class="uri">https://www.bilibili.com/video/BV164411b7dx</a></p><span id="more"></span><h2 id="多元正态分布高斯分布">多元正态分布（高斯分布）</h2><p>多元正态分布的概率密度函数： <span class="math display">\[p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)\]</span> 其中，<span class="math inline">\(x\in\mathbb R^n\)</span> 是 <span class="math inline">\(n\)</span> 维随机变量，<span class="math inline">\(\mu\in\mathbb R^n\)</span> 是 <span class="math inline">\(x\)</span> 的均值，<span class="math inline">\(\Sigma\in\mathbb R^{n\times n}\)</span> 是协方差矩阵。</p><p>特别地，当 <span class="math inline">\(x\)</span> 的各个维度不相关时，上述联合概率密度函数等于各分量的概率密度函数之积，即： <span class="math display">\[p(x;\mu,\Sigma)=\prod_{i=1}^np(x_i;\mu_i,\sigma_i^2)\]</span></p><h2 id="异常检测">异常检测</h2><p>异常检测的原理非常简单：假设有正常的数据集 <span class="math inline">\(\{x^{(1)},x^{(2)},\ldots,x^{(m)}\}\)</span>，我们构建一个多元正态分布，其均值为样本均值，协方差矩阵为样本的协方差矩阵，即： <span class="math display">\[\begin{align}\mu&amp;=\frac{1}{m}\sum\limits_{i=1}^mx^{(i)}\\\Sigma&amp;=\frac{1}{m}\sum\limits_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T\\p(x;\mu,\Sigma)&amp;=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)\end{align}\]</span> 或者直接认定 <span class="math inline">\(x\)</span> 各维度不相关，取： <span class="math display">\[p(x;\mu,\Sigma)=\prod_{i=1}^np(x_i;\mu_i,\sigma_i^2)\]</span></p><blockquote><p>后者在计算上更快，且允许 <span class="math inline">\(m\leqslant n\)</span> 的情况；而前者在 <span class="math inline">\(m&gt;n\)</span> 时 <span class="math inline">\(\Sigma\)</span> 不可逆。</p></blockquote><p>对于要检测的数据 <span class="math inline">\(x\)</span>，如果 <span class="math inline">\(p(x;\mu,\Sigma)\)</span> 小于某个阈值 <span class="math inline">\(\varepsilon\)</span>，那么就认为该数据是异常数据，否则正常。</p><p><br></p><p>如果我们有标注过的数据（标注是否异常），则可以将数据划分为训练集、验证集和测试集。训练集包含大部分正常数据，并据此构建出正态分布模型；验证集和测试集包含正常和异常数据，我们可以根据验证集的结果调整参数 <span class="math inline">\(\varepsilon\)</span>，最后在测试集上进行测试。</p><blockquote><p>注意，由于异常检测通常有偏（正常数据远远多于异常数据），所以测试结果应该取 <span class="math inline">\(\text{precision},\text{recall},\text{F1}\)</span> 等值。</p></blockquote><h2 id="实现">实现</h2><p>训练集即据此构建的二元正态分布：</p><p><img src="Figure_0.png" height="50%" width="50%" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">Xmeans = X.mean(axis=<span class="hljs-number">0</span>)<br>Xcov = ((X - Xmeans).T @ (X - Xmeans)) / X.shape[<span class="hljs-number">0</span>]<br>normDist = multivariate_normal(mean=Xmeans, cov=Xcov)<br></code></pre></td></tr></table></figure><p>根据验证集找到的最佳 <span class="math inline">\(\varepsilon\)</span> 约为：<span class="math inline">\(2.72\times 10^{-5}\)</span>；此时测试集上 <span class="math inline">\(\text{F1}\)</span> 值约为：<span class="math inline">\(0.75\)</span>.</p><p><img src="Figure_1.png" height="50%" width="50%" /></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Andrew Ng</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[吴恩达机器学习]12·主成分分析</title>
    <link href="/blog-main/2021/01/26/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-12%C2%B7%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    <url>/blog-main/2021/01/26/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-12%C2%B7%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>吴恩达机器学习系列课程：<a href="https://www.bilibili.com/video/BV164411b7dx" class="uri">https://www.bilibili.com/video/BV164411b7dx</a></p><p>参考文章：<a href="https://www.zhihu.com/question/41120789/answer/481966094" class="uri">https://www.zhihu.com/question/41120789/answer/481966094</a></p><span id="more"></span><h2 id="数据降维">数据降维</h2><p>我们的数据特征常常包含众多维度，但它们中的有些维度其实没有存在的必要。最极端的情况就是某一维度是其他若干维度的线性组合，那么这一维度就完全可以丢掉；但现实不会这么精准，如果某一维度是其他若干维度的线性组合加上微小的扰动，我们其实也可以将其丢掉。这就是数据降维。</p><p><img src="data%20compression.png" /></p><p>数据降维有众多算法可以完成，主成分分析即是其中之一。</p><h2 id="主成分分析">主成分分析</h2><p>主成分分析（Principal Component Analysis）的基本思想是：假设原始数据的特征有 <span class="math inline">\(n\)</span> 维，我们想将其缩减到 <span class="math inline">\(k\)</span> 维，那么我们只需要在原来的 <span class="math inline">\(n\)</span> 维空间中找到一个 <span class="math inline">\(k\)</span> 维的子空间，使得所有数据到这个子空间的距离平方和最小；此时，原数据在这个子空间上的投影就是我们新的 <span class="math inline">\(k\)</span> 维的数据。</p><h3 id="数学推导">数学推导</h3><p>为方便，我们首先将数据中心化，即使得数据的平均值在原点处。<u>一个获得所需要的 <span class="math inline">\(k\)</span> 维子空间的简单方式是：找到一个合适的 <span class="math inline">\(n\)</span> 维空间，直接选取前 <span class="math inline">\(k\)</span> 维作为子空间</u>。具体而言，对于一个数据点 <span class="math inline">\(x=\begin{bmatrix}x_1\\x_2\\\vdots\\x_n\end{bmatrix}\in\mathbb R^n\)</span>，设我们要找的 <span class="math inline">\(n\)</span> 维空间的规范正交基为 <span class="math inline">\(u_1,u_2,\ldots,u_n\)</span>，则 <span class="math inline">\(x\)</span> 在其中的新坐标为： <span class="math display">\[y_j=x\cdot u_j=x_1u_{j1}+x_2u_{j2}+\cdots+x_nu_{jn}\]</span> 它到前 <span class="math inline">\(k\)</span> 维形成的子空间（即以 <span class="math inline">\(u_1,u_2,\ldots,u_k\)</span> 为基底的子空间）的距离之平方为： <span class="math display">\[{y_{k+1}}^2+{y_{k+2}}^2+\cdots+{y_n}^2\]</span> 假设我们有 <span class="math inline">\(m\)</span> 个数据 <span class="math inline">\(x^{(1)},x^{(2)},\ldots,x^{(m)}\)</span>，于是我们的优化目标为： <span class="math display">\[\min \sum_{i=1}^m {y^{(i)}_{k+1}}^2+{y^{(i)}_{k+2}}^2+\cdots+{y^{(i)}_{n}}^2\]</span> 又由于在不同的基下 <span class="math inline">\({||x||}^2\)</span> 都是一个定值，于是最小化上述距离等价于： <span class="math display">\[\max \sum_{i=1}^m{y^{(i)}_1}^2+{y^{(i)}_2}^2+\cdots+{y^{(i)}_k}^2\]</span> 其充分条件为： <span class="math display">\[\max \sum_{i=1}^m {y_r^{(i)}}^2,\quad r=1,2,\ldots,k\]</span> 这就是我们要解决的问题。</p><p><br></p><p>由于 <span class="math display">\[\begin{align}\sum_{i=1}^m{y^{(i)}_r}^2&amp;=\sum_{i=1}^m(x^{(i)}\cdot u_r)^2\\&amp;=\sum_{i=1}^m\left(u_r^Tx^{(i)}\right)\left({x^{(i)}}^Tu_r\right)\\&amp;=u_r^T\left(\sum_{i=1}^mx^{(i)}{x^{(i)}}^T\right) u_r\end{align}\]</span> 这是一个正定二次型，<span class="math inline">\(\sum\limits_{i=1}^mx^{(i)}{x^{(i)}}^T\)</span> 是一个正定矩阵，可以进行奇异值分解： <span class="math display">\[\sum_{i=1}^mx^{(i)}{x^{(i)}}^T=U\Sigma U^T\]</span> 其中，<span class="math inline">\(U\)</span> 是正交矩阵，<span class="math inline">\(\Sigma\)</span> 是对角矩阵 <span class="math inline">\(\begin{bmatrix}\sigma_1&amp;\cdots&amp;0\\\vdots&amp;\ddots&amp;\vdots\\0&amp;\cdots&amp;\sigma_n\end{bmatrix}\)</span>，<span class="math inline">\(\sigma_1,\ldots,\sigma_n\)</span> 是奇异值，<span class="math inline">\(\sigma_1&gt;\cdots&gt;\sigma_n\)</span>.</p><p>令 <span class="math inline">\(v_r=U^Tu_r\)</span>，由于 <span class="math inline">\(U\)</span> 正交，所以 <span class="math inline">\(v_r\)</span> 也是单位向量，代回得到： <span class="math display">\[\begin{align}\sum_{i=1}^m{y^{(i)}_r}^2&amp;=u_r^TU\Sigma U^Tu_r\\&amp;=(U^T u_r)^T\Sigma(U^Tu_r)\\&amp;=v_r^T\Sigma v_r\\&amp;=\sigma_1v_{r1}^2+\sigma_2v_{r2}^2+\cdots+\sigma_nv_{rn}^2\end{align}\]</span> 所以我们的优化目标变成了： <span class="math display">\[\begin{align}&amp;\max\sum_{i=1}^n\sigma_iv_{ri}^2\\&amp;\text{s.t.}\begin{cases}\sum\limits_{i=1}^nv_{ri}^2=1\\\sigma_1&gt;\cdots&gt;\sigma_n\end{cases}\end{align}\]</span> 很显然，它的解是：<span class="math inline">\(v_{r1}=1,v_{r2}=\cdots=v_{rn}=0\)</span>，即 <span class="math inline">\(v_r=\begin{bmatrix}1\\0\\\vdots\\0\end{bmatrix}\)</span>。又由于 <span class="math inline">\(u_r=Uv_r\)</span>，<u>所以我们要找的 <span class="math inline">\(n\)</span> 维空间的各个基向量就是矩阵 <span class="math inline">\(\sum\limits_{i=1}^mx^{(i)}{x^{(i)}}^T\)</span> 的各个奇异值对应的奇异向量</u>，我们要降维到的 <span class="math inline">\(k\)</span> 维子空间的各个基向量就是前 <span class="math inline">\(k\)</span> 个奇异向量，对原来的数据进行基变换，就得到了降维后的数据。</p><h3 id="步骤总结">步骤总结</h3><p>总结一下，主成分分析的推导过程稍显复杂，但是它的实现很简单，主要是以下步骤：</p><ol type="1"><li><p>计算矩阵 <span class="math inline">\(\sum\limits_{i=1}^mx^{(i)}{x^{(i)}}^T\)</span>；</p><p>更简单的表达是：设矩阵 <span class="math inline">\(X=\begin{bmatrix}{x^{(1)}}^T\\{x^{(2)}}^T\\\vdots\\{x^{(m)}}^T\end{bmatrix}\)</span> 为数据集，那么计算矩阵 <span class="math inline">\(X^TX\)</span> 即可。</p></li><li><p>进行奇异值分解，得到奇异向量；</p></li><li><p>选取前 <span class="math inline">\(k\)</span> 个奇异向量作为降维后的空间的基向量，构成基变换矩阵 <span class="math inline">\(C\)</span>；</p></li><li><p>对于原数据 <span class="math inline">\(x\)</span>，取 <span class="math inline">\(z=C^Tx\)</span> 为其降维后的数据。</p><p>更简单的表达是：取 <span class="math inline">\(Z=XC\)</span>，则 <span class="math inline">\(Z\)</span> 是降维后的数据集。</p></li></ol><h3 id="主成分数量的选择">主成分数量的选择</h3><p>那么在实践中，我们到底选择多大的 <span class="math inline">\(k\)</span> 值比较好呢？对此，我们定义一个平均误差为： <span class="math display">\[\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_\text{approx}^{(i)}||^2\]</span> 其中，<span class="math inline">\(x_\text{approx}^{(i)}\)</span> 表示数据 <span class="math inline">\(x^{(i)}\)</span> 在我们找到的 <span class="math inline">\(k\)</span> 维子空间上的投影。再定义一个总方差为： <span class="math display">\[\frac{1}{m}\sum_{i=1}^m||x^{(i)}||^2\]</span> 则一般的，我们会选择最小的 <span class="math inline">\(k\)</span> 使得： <span class="math display">\[\frac{\frac{1}{m}\sum\limits_{i=1}^m||x^{(i)}-x_\text{approx}^{(i)}||^2}{\frac{1}{m}\sum\limits_{i=1}^m||x^{(i)}||^2}\leqslant 0.01\]</span> 并称之为「<span class="math inline">\(99\%\)</span> 的方差得以保留」。</p><p>这个式子看起来并不好计算，但事实上，借助我们的奇异值，可以证明，对于给定的 <span class="math inline">\(k\)</span>： <span class="math display">\[\frac{\frac{1}{m}\sum\limits_{i=1}^m||x^{(i)}-x_\text{approx}^{(i)}||^2}{\frac{1}{m}\sum\limits_{i=1}^m||x^{(i)}||^2}=1-\frac{\sum\limits_{i=1}^k\sigma_k}{\sum\limits_{i=1}^n\sigma_i}\]</span> 这就好算了。</p><h2 id="实现">实现</h2><h3 id="二维压缩为一维">二维压缩为一维</h3><p>二维平面上的原始数据：</p><p><img src="Figure_0.png" width="50%" height="50%" /></p><p>主成分分析代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">PCA</span>(<span class="hljs-params">X, dim = -<span class="hljs-number">1</span></span>):<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">X is the input data: (m, n)</span><br><span class="hljs-string"></span><br><span class="hljs-string">dim is the dimension after reduction</span><br><span class="hljs-string">if dim=-1, then the program select the smallest dim</span><br><span class="hljs-string">such that 99% of variance is retained</span><br><span class="hljs-string"></span><br><span class="hljs-string">return the data after reduction: (m, dim)</span><br><span class="hljs-string">and the data recovered from reduced data: (m, n)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>Xmean = np.empty((<span class="hljs-number">1</span>, X.shape[<span class="hljs-number">1</span>]))<br>Xstd = np.empty((<span class="hljs-number">1</span>, X.shape[<span class="hljs-number">1</span>]))<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">normalization</span>(<span class="hljs-params">X, k</span>):<br><span class="hljs-keyword">global</span> Xmean, Xstd<br><span class="hljs-keyword">if</span> k == <span class="hljs-number">1</span>:<br>Xmean = np.mean(X, axis=<span class="hljs-number">0</span>)<br>Xstd = np.std(X, axis=<span class="hljs-number">0</span>, ddof=<span class="hljs-number">1</span>)<br><span class="hljs-keyword">return</span> (X - Xmean) / Xstd<br><span class="hljs-keyword">else</span>:<br><span class="hljs-keyword">return</span> X * Xstd + Xmean<br><br>Xnorm = normalization(X, <span class="hljs-number">1</span>)<br>u, s, v = np.linalg.svd(Xnorm.T @ Xnorm)<br><span class="hljs-keyword">if</span> dim == -<span class="hljs-number">1</span>:<br>dim = <span class="hljs-number">1</span><br><span class="hljs-keyword">while</span> s[:dim].<span class="hljs-built_in">sum</span>() / s.<span class="hljs-built_in">sum</span>() &lt; <span class="hljs-number">0.99</span>:<br>dim += <span class="hljs-number">1</span><br><span class="hljs-keyword">return</span> Xnorm @ u[:, :dim], normalization(Xnorm @ u[:, :dim] @ u[:, :dim].T, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>得到映射的点：</p><p><img src="Figure_1.png" width="50%" height="50%" /></p><h3 id="人脸特征压缩">人脸特征压缩</h3><p>我们给出了 <span class="math inline">\(5000\)</span> 张人脸照片，每张照片含有 <span class="math inline">\(32\times32\)</span> 的灰度像素，形成维度为 <span class="math inline">\(1024\)</span> 的向量作为其特征。前 <span class="math inline">\(100\)</span> 张照片如图所示：</p><p><img src="Figure_2.png" /></p><p>现在将其压缩为 <span class="math inline">\(\text{dim}={36,100}\)</span> 维的数据，恢复后结果如下：</p><p><img src="Figure_dim=36.png" /></p><p><img src="Figure_dim=100.png" /></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Andrew Ng</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[吴恩达机器学习]11·K-means聚类</title>
    <link href="/blog-main/2021/01/25/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-11%C2%B7K-means%E8%81%9A%E7%B1%BB/"/>
    <url>/blog-main/2021/01/25/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-11%C2%B7K-means%E8%81%9A%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<p>吴恩达机器学习系列课程：<a href="https://www.bilibili.com/video/BV164411b7dx" class="uri">https://www.bilibili.com/video/BV164411b7dx</a></p><span id="more"></span><h2 id="聚类问题">聚类问题</h2><p>聚类问题属于无监督学习的范畴，与有监督学习不同，无监督学习的数据不再包含标注的标签。聚类问题就是在无标注的情况下将数据集分为若干类的问题。</p><h2 id="textk-means-算法"><span class="math inline">\(\text{K-means}\)</span> 算法</h2><p><span class="math inline">\(\textbf{K-means}\)</span> 算法是解决聚类问题的一种算法，其基本思想非常简单：假设我们要将数据分为 <span class="math inline">\(K\)</span> 类，首先我们随机 <span class="math inline">\(K\)</span> 个聚类中心，然后反复执行以下步骤：</p><ol type="1"><li>根据数据点到这 <span class="math inline">\(K\)</span> 个聚类中心的距离进行分类（距离哪个中心小就分为哪一类）；</li><li>将聚类中心重置为它所代表的那一类的所有点的平均位置。</li></ol><p>直到聚类中心不再改变，算法结束。</p><p><br></p><p>对于 <span class="math inline">\(\textbf{K-means}\)</span> 算法，我们可以定义一个代价函数，为各数据点到它所属于的聚类中心的距离之平方和。很容易证明，1、2 两个步骤都是在减小这个代价，所以正确实现的 <span class="math inline">\(\textbf{K-means}\)</span> 算法的代价应随着迭代次数增加而减小。</p><p><br></p><p>注意：</p><ol type="1"><li>在实践中，我们可以任取 <span class="math inline">\(K\)</span> 个数据点作为聚类中心；</li><li>如果更新聚类中心时，没有数据属于某一聚类中心，则可以将该聚类中心删去（这样分类数会减少）或者置于随机位置上（保持分类数不变）；</li><li>执行一次 <span class="math inline">\(\textbf{K-means}\)</span> 的结果依赖于聚类中心的选取方式，因此有可能得到一个局部最优解。所以我们可以多次执行算法，取代价最小的结果为最终结果。</li></ol><h2 id="实现">实现</h2><h3 id="平面点集分类">平面点集分类</h3><p>首先看看数据集的样子：</p><p><img src="Figure_0.png" width="50%" height="50%" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>X = loadmat(<span class="hljs-string">&#x27;ex7data2.mat&#x27;</span>)[<span class="hljs-string">&#x27;X&#x27;</span>]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">J</span>(<span class="hljs-params">X, cluster, centroid</span>):<br>res = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">0</span>]):<br>res += np.dot(X[i]-centroid[cluster[i]], X[i]-centroid[cluster[i]])<br><span class="hljs-keyword">return</span> res<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">K_means</span>(<span class="hljs-params">K, X, iteration=<span class="hljs-number">100</span></span>):<br>(m, n) = X.shape<br>bestCluster, bestCentroid, bestJ = np.empty(m), np.empty((K, n)), np.inf<br><span class="hljs-keyword">for</span> <span class="hljs-built_in">iter</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iteration):<br>centroid = X[np.random.randint(<span class="hljs-number">0</span>, m, K)]<br>cluster = np.empty(m, dtype=<span class="hljs-string">&#x27;int&#x27;</span>)<br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>ncentroid = np.zeros((K, n))<br>cnt = np.zeros(K)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>cluster[i] = np.argmin(np.<span class="hljs-built_in">sum</span>((X[i]-centroid)**<span class="hljs-number">2</span>, axis=<span class="hljs-number">1</span>))<br>ncentroid[cluster[i]] += X[i]<br>cnt[cluster[i]] += <span class="hljs-number">1</span><br>ncentroid[cnt!=<span class="hljs-number">0</span>] /= cnt[cnt!=<span class="hljs-number">0</span>][:, np.newaxis]<br>ncentroid[cnt==<span class="hljs-number">0</span>] = X[np.random.randint(<span class="hljs-number">0</span>, m, <span class="hljs-built_in">len</span>(cnt[cnt==<span class="hljs-number">0</span>]))]<br><span class="hljs-keyword">if</span> (centroid == ncentroid).<span class="hljs-built_in">all</span>():<br><span class="hljs-keyword">break</span><br>centroid = ncentroid.copy()<br>cost = J(X, cluster, centroid)<br><span class="hljs-keyword">if</span> cost &lt; bestJ:<br>bestCluster, bestCentroid, bestJ = cluster.copy(), centroid.copy(), cost.copy()<br><span class="hljs-keyword">return</span> bestCluster, bestCentroid<br><br>cl, ce = K_means(<span class="hljs-number">3</span>, X, iteration=<span class="hljs-number">100</span>)<br><br>ax = plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>ax.set_xlabel(<span class="hljs-string">&#x27;x1&#x27;</span>)<br>ax.set_ylabel(<span class="hljs-string">&#x27;x2&#x27;</span>)<br>ax.plot(X[cl==<span class="hljs-number">0</span>][:, <span class="hljs-number">0</span>], X[cl==<span class="hljs-number">0</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>, alpha=<span class="hljs-number">0.4</span>)<br>ax.plot(X[cl==<span class="hljs-number">1</span>][:, <span class="hljs-number">0</span>], X[cl==<span class="hljs-number">1</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;green&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>, alpha=<span class="hljs-number">0.4</span>)<br>ax.plot(X[cl==<span class="hljs-number">2</span>][:, <span class="hljs-number">0</span>], X[cl==<span class="hljs-number">2</span>][:, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;o&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>, markerfacecolor=<span class="hljs-string">&#x27;none&#x27;</span>, alpha=<span class="hljs-number">0.4</span>)<br>ax.plot(ce[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], ce[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;*&#x27;</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>, ms=<span class="hljs-number">10</span>)<br>ax.plot(ce[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], ce[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;*&#x27;</span>, color=<span class="hljs-string">&#x27;green&#x27;</span>, ms=<span class="hljs-number">10</span>)<br>ax.plot(ce[<span class="hljs-number">2</span>, <span class="hljs-number">0</span>], ce[<span class="hljs-number">2</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;*&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>, ms=<span class="hljs-number">10</span>)<br>ax.plot([], [], <span class="hljs-string">&#x27;*&#x27;</span>, color=<span class="hljs-string">&#x27;black&#x27;</span>, ms=<span class="hljs-number">10</span>, label=<span class="hljs-string">&#x27;cluster centroid&#x27;</span>)<br>ax.legend()<br><br>plt.show()<br><br></code></pre></td></tr></table></figure><p>结果如下：</p><p><img src="Figure_1.png" width="50%" height="50%" /></p><h3 id="图像压缩">图像压缩</h3><p>图像是有若干像素组成的，每个像素存放 <span class="math inline">\(3\)</span> 个字节的信息代表其 <span class="math inline">\(\text{RGB}\)</span> 颜色，一张图片可能含有成百上千种颜色，如果我们只用 <span class="math inline">\(16\)</span> 种颜色表示它，那么我们只需要在对应像素位置存放一个 <span class="math inline">\(4\)</span> 位二进制数表示是第几种颜色，这样就把图像压缩到了原来的 <span class="math inline">\(\frac{1}{6}\)</span> 大小。</p><p>现在我们用 <span class="math inline">\(\textbf{K-means}\)</span> 算法去得到这 <span class="math inline">\(16\)</span> 种颜色。</p><p>我们使用的图像含有 <span class="math inline">\(128\times128\)</span> 个像素，可以处理为 <span class="math inline">\((128\times128,3)\)</span> 的二维数组，每一行就是一个像素，包含 <span class="math inline">\(3\)</span> 个值，即 <span class="math inline">\(\text{RGB}\)</span>. 这就是我们的输入数据。原图如下：</p><p><img src="bird_small.png" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">readin</span>():<br>data = np.array(Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;bird_small.png&#x27;</span>))<br>data = data.reshape((<span class="hljs-number">128</span>*<span class="hljs-number">128</span>, <span class="hljs-number">3</span>))<br><span class="hljs-keyword">return</span> data<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">J</span>(<span class="hljs-params">X, cluster, centroid</span>):<br>res = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(X.shape[<span class="hljs-number">0</span>]):<br>res += np.dot(X[i]-centroid[cluster[i]], X[i]-centroid[cluster[i]])<br><span class="hljs-keyword">return</span> res<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">K_means</span>(<span class="hljs-params">K, X, iteration=<span class="hljs-number">100</span></span>):<br>(m, n) = X.shape<br>bestCluster, bestCentroid, bestJ = np.empty(m), np.empty((K, n)), np.inf<br><span class="hljs-keyword">for</span> <span class="hljs-built_in">iter</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iteration):<br>centroid = X[np.random.randint(<span class="hljs-number">0</span>, m, K)]<br>cluster = np.empty(m, dtype=<span class="hljs-string">&#x27;int&#x27;</span>)<br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>ncentroid = np.zeros((K, n))<br>cnt = np.zeros(K)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>cluster[i] = np.argmin(np.<span class="hljs-built_in">sum</span>((X[i]-centroid)**<span class="hljs-number">2</span>, axis=<span class="hljs-number">1</span>))<br>ncentroid[cluster[i]] += X[i]<br>cnt[cluster[i]] += <span class="hljs-number">1</span><br>ncentroid[cnt!=<span class="hljs-number">0</span>] /= cnt[cnt!=<span class="hljs-number">0</span>][:, np.newaxis]<br>ncentroid[cnt==<span class="hljs-number">0</span>] = X[np.random.randint(<span class="hljs-number">0</span>, m, <span class="hljs-built_in">len</span>(cnt[cnt==<span class="hljs-number">0</span>]))]<br><span class="hljs-keyword">if</span> (centroid == ncentroid).<span class="hljs-built_in">all</span>():<br><span class="hljs-keyword">break</span><br>centroid = ncentroid.copy()<br>cost = J(X, cluster, centroid)<br><span class="hljs-keyword">if</span> cost &lt; bestJ:<br>bestCluster, bestCentroid, bestJ = cluster.copy(), centroid.copy(), cost.copy()<br><span class="hljs-keyword">return</span> bestCluster, bestCentroid<br><br>X = readin()<br>cl, ce = K_means(<span class="hljs-number">16</span>, X, iteration=<span class="hljs-number">20</span>)<br><br>comImg = np.empty((<span class="hljs-number">128</span>*<span class="hljs-number">128</span>, <span class="hljs-number">3</span>), dtype=<span class="hljs-string">&#x27;uint8&#x27;</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">128</span>*<span class="hljs-number">128</span>):<br>comImg[i] = np.floor(ce[cl[i]])<br>comImg = comImg.reshape((<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>))<br>im = Image.fromarray(comImg)<br>im.save(<span class="hljs-string">&#x27;bird_compression.png&#x27;</span>)<br><br></code></pre></td></tr></table></figure><p>得到的压缩结果如下：</p><p><img src="bird_compression.png" /></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Andrew Ng</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[吴恩达机器学习]10·支持向量机</title>
    <link href="/blog-main/2021/01/17/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-10%C2%B7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <url>/blog-main/2021/01/17/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-10%C2%B7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<p>吴恩达机器学习系列课程：<a href="https://www.bilibili.com/video/BV164411b7dx" class="uri">https://www.bilibili.com/video/BV164411b7dx</a></p><span id="more"></span><h2 id="优化目标">优化目标</h2><p>首先回忆逻辑回归的优化目标： <span class="math display">\[\min_\theta\frac{1}{m}\left[\sum_{i=1}^my^{(i)}\left(-\ln h_\theta(x^{(i)})\right)+(1-y^{(i)})\left(-\ln (1-h_\theta(x^{(i)}))\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2\]</span> 记上式中 <span class="math inline">\(-\ln h_\theta(x^{(i)})\)</span> 为 <span class="math inline">\(\text{cost}_1(\theta^T x^{(i)})\)</span>，即分类为 <span class="math inline">\(1\)</span> 时采用的代价，<span class="math inline">\(-\ln(1-h_\theta(x^{(i)}))\)</span> 为 <span class="math inline">\(\text{cost}_0(\theta^T x^{(i)})\)</span>，即分类为 <span class="math inline">\(0\)</span> 时采用的代价，则支持向量机中，我们不再采用 <span class="math inline">\(\textbf{sigmoid}\)</span> 函数，而是： <span class="math display">\[\text{cost}_1(z)=\begin{cases}0&amp;z\geqslant1\\k_1(z-1)&amp;z&lt;1\end{cases}\quad\quad\text{cost}_0(z)=\begin{cases}0&amp;z\leqslant-1\\k_0(z+1)&amp;z&gt;-1\end{cases}\]</span> <img src="cost.png" /></p><p>另外，我们习惯于不除以样本大小 <span class="math inline">\(m\)</span>，并将正则化参数放在第一项而非第二项，即支持向量机的优化目标为： <span class="math display">\[\boxed{\min_\theta C\sum_{i=1}^m\left[y^{(i)}\text{cost}_1(\theta^Tx^{(i)})+(1-y^{(i)})\text{cost}_0(\theta^Tx^{(i)})\right]+\frac{1}{2}\sum_{j=1}^n\theta_j^2}\]</span> 其中，<span class="math inline">\(C\)</span> 是正则化参数，类比逻辑回归中 <span class="math inline">\(\frac{1}{\lambda}\)</span> 的作用。</p><blockquote><p>对该代价函数的理解：</p><p>首先看看 <span class="math inline">\(y\text{cost}_1(\theta^Tx)+(1-y)\text{cost}_0(\theta^Tx)\)</span> 这一项，欲使之最小，最理想的情况就是 <span class="math inline">\(\text{cost}_y(\theta^T x)=0\)</span>，这对应着 <span class="math inline">\(\begin{cases}\theta^Tx\geqslant 1&amp;\text{if }y=1\\\theta^Tx\leqslant -1&amp;\text{if }y=0\end{cases}\)</span>；而在逻辑回归中，我们只需要对比 <span class="math inline">\(\theta^Tx\)</span> 和 <span class="math inline">\(0\)</span> 的大小，这里相当于将条件变得更苛刻。</p><p>注意 <span class="math inline">\(\theta^Tx\)</span> 其实是参数 <span class="math inline">\(\theta\)</span> 和 <span class="math inline">\(x\)</span> 两个向量的点积，可以视作 <span class="math inline">\(x\)</span> 向 <span class="math inline">\(\theta\)</span> 的投影乘上 <span class="math inline">\(\theta\)</span> 的长度 <span class="math inline">\(||\theta||\)</span>；而 <span class="math inline">\(\sum\limits_{j=1}^n\theta_j^2\)</span> 就是 <span class="math inline">\(||\theta||\)</span>。从这个几何角度看，如果 <span class="math inline">\(C\)</span> 取值较大，那么我们很关心能否把原数据集完美地线性分开，因为这样，只要 <span class="math inline">\(||\theta||\)</span> 充分大，那么第一项就能取到 <span class="math inline">\(0\)</span>；相反地，如果 <span class="math inline">\(C\)</span> 取值较小，那么即便有些数据点没有被正确地线性分类（第一项不为 <span class="math inline">\(0\)</span>），由于我们需要的 <span class="math inline">\(||\theta||\)</span> 不必太大，所以总代价依然较小。</p><p><img src="big%20margin.png" /></p></blockquote><h2 id="核函数">核函数</h2><p>对于非线性可分集，我们引入核函数 <span class="math inline">\(\phi:x\mapsto \phi(x)\)</span>，它将原来的 <span class="math inline">\(n\)</span> 维向量映射为更高维的向量，使得这些向量在该更高维空间中线性可分。常用的核函数有：</p><ul><li>线性核：<span class="math inline">\(\phi(x_i)=x_i\)</span>，即不做任何改变；</li><li>高斯核：<span class="math inline">\(\phi(x)=\begin{bmatrix}\exp\left(-\frac{||x-l^{(1)}||^2}{2\sigma^2}\right)\\\exp\left(-\frac{||x-l^{(2)}||^2}{2\sigma^2}\right)\\\vdots\\\exp\left(-\frac{||x-l^{(m)}||^2}{2\sigma^2}\right)\end{bmatrix}\)</span>，其中 <span class="math inline">\(\sigma\)</span> 是参数，<span class="math inline">\(l^{(1)},\cdots,l^{(m)}\)</span> 被称作 landmark，实践中直接采用输入数据 <span class="math inline">\(x^{(1)},\cdots,x^{(m)}\)</span> 作为 landmark。</li></ul><h2 id="scikit-learn-实现"><code>scikit-learn</code> 实现</h2><h3 id="线性可分分类">线性可分分类</h3><p>首先看一下数据集的样纸：</p><p><img src="Figure_1.png" height="50%" width="50%" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br><br>clf = svm.SVC(C=<span class="hljs-number">1</span>, kernel=<span class="hljs-string">&#x27;linear&#x27;</span>)<br>clf.fit(X, y)<br></code></pre></td></tr></table></figure><p>训练结果如下：</p><p><img src="Figure_1_1.png" /></p><h3 id="带高斯核的-textbfsvm">带高斯核的 <span class="math inline">\(\textbf{SVM}\)</span></h3><p>依旧先看数据集的样子：</p><p><img src="Figure_2.png" height="50%" width="50%" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm<br><br>clf = svm.SVC(C=<span class="hljs-number">100</span>, kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>, gamma=<span class="hljs-number">10</span>, probability=<span class="hljs-literal">True</span>)<br>clf.fit(X, y)<br></code></pre></td></tr></table></figure><p>训练结果如下：</p><p><img src="Figure_2_1.png" height="50%" width="50%" /></p><h3 id="调整参数-csigma">调整参数 <span class="math inline">\(C,\sigma\)</span></h3><p>数据集：</p><p><img src="Figure_3.png" height="50%" width="50%" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">maxscore, maxC, maxgamma = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> C <span class="hljs-keyword">in</span> [<span class="hljs-number">0.01</span>, <span class="hljs-number">0.03</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>, <span class="hljs-number">30</span>]:<br><span class="hljs-keyword">for</span> gamma <span class="hljs-keyword">in</span> [<span class="hljs-number">0.01</span>, <span class="hljs-number">0.03</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">10</span>, <span class="hljs-number">30</span>]:<br>clf = svm.SVC(C=C, kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>, gamma=gamma, probability=<span class="hljs-literal">True</span>)<br>clf.fit(X, y)<br>score = clf.score(X, y)<br><span class="hljs-built_in">print</span>(C, gamma, score)<br><span class="hljs-keyword">if</span> score &gt; maxscore:<br>maxscore, maxC, maxgamma = score, C, gamma<br><br>clf = svm.SVC(C=maxC, kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>, gamma=maxgamma, probability=<span class="hljs-literal">True</span>)<br>clf.fit(X, y)<br></code></pre></td></tr></table></figure><p>训练结果如下：</p><p><img src="Figure_3_1.png" height="50%" width="50%" /></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Andrew Ng</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[吴恩达机器学习]9·高偏差与高方差</title>
    <link href="/blog-main/2021/01/05/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-9%C2%B7%E9%AB%98%E5%81%8F%E5%B7%AE%E4%B8%8E%E9%AB%98%E6%96%B9%E5%B7%AE/"/>
    <url>/blog-main/2021/01/05/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-9%C2%B7%E9%AB%98%E5%81%8F%E5%B7%AE%E4%B8%8E%E9%AB%98%E6%96%B9%E5%B7%AE/</url>
    
    <content type="html"><![CDATA[<p>吴恩达机器学习系列课程：<a href="https://www.bilibili.com/video/BV164411b7dx" class="uri">https://www.bilibili.com/video/BV164411b7dx</a></p><span id="more"></span><h2 id="训练集验证集与测试集">训练集、验证集与测试集</h2><p>在以往的实验中，我们把所有数据集拿来训练一个模型，之后用它来测试准确率。这显然不是一个好的做法，因为即便准确率很高，那也可能有过拟合的问题。正确的做法应该是用一个与训练集独立的测试集进行测试，这样才能保证得到的结果公平有效。</p><p>进一步，如果模型中含有超参数，例如正则化的参数 <span class="math inline">\(\lambda\)</span>，这是需要我们人工设置的。不同的超参数得到的结果也不同，我们自然会去选择结果最好的超参数，于是又产生了同样的问题：我们对超参数的选择依赖于模型的结果，而结果又产生自测试集，所以我们依旧没能做到在一个完全独立的测试集上进行测试。所以我们引入验证集，即用验证集而非测试集去调参，最后在测试集上跑结果。测试集自始至终不参与模型的建立。</p><p>值得一提的是，如果我们在训练过程中加入了正则项，那么在计算模型的代价函数（误差）的时候应该去掉正则项。这是因为加入正则项的目的是训练出一个更为合理的参数 <span class="math inline">\(\theta\)</span>，而为了评价这个参数 <span class="math inline">\(\theta\)</span> 的好坏，原本的代价函数才是真正的代价。</p><h2 id="高偏差与高方差">高偏差与高方差</h2><p>在欠拟合的时候，我们称模型是高偏差的；过拟合时，称模型是高方差的。以多项式回归为例，随着多项式系数的增加，我们从欠拟合逐渐过渡到过拟合，训练集上的代价函数 <span class="math inline">\(J_\text{train}(\theta)\)</span> 逐渐减小，但是验证集上的代价函数 <span class="math inline">\(J_\text{valid}(\theta)\)</span> 先减小后增大，形成下图所示情况：</p><p><img src="img.png" width="50%" height="50%" /></p><h2 id="学习曲线">学习曲线</h2><p>误差函数关于训练集大小的曲线，称为学习曲线。作出学习曲线有利于帮助我们分析模型是否过拟合/欠拟合。</p><p>如果模型欠拟合，具有高偏差，当训练集大小很小时，<span class="math inline">\(J_\text{train}(\theta)\)</span> 比较小，而 <span class="math inline">\(J_\text{valid}(\theta)\)</span> 很大；随着训练集大小的增大，<span class="math inline">\(J_\text{train}(\theta)\)</span> 迅速增大，<span class="math inline">\(J_\text{valid}(\theta)\)</span> 减小，但是减小的幅度不大；最后，当训练集大小很大时，二者基本相当且都比较大。</p><p>如果模型过拟合，具有高方差，当训练集大小很小时，<span class="math inline">\(J_\text{train}(\theta)\)</span> 很小，而 <span class="math inline">\(J_\text{valid}(\theta)\)</span> 很大；随着训练集大小的增大，<span class="math inline">\(J_\text{train}(\theta)\)</span> 增大，但是增大的幅度不大，而 <span class="math inline">\(J_\text{valid}(\theta)\)</span> 减小，但是减小的幅度也不大；最后，当训练集大小很大时， <span class="math inline">\(J_\text{train}(\theta)\)</span> 较小，但 <span class="math inline">\(J_\text{valid}(\theta)\)</span> 较大。</p><p><img src="img2.png" /></p><p>从上面的分析以及图像也可以看出，如果模型发生了欠拟合，那么增加训练集的数据量并没有什么帮助；而如果模型发生了过拟合，增加训练集的数据量有一定的帮助。</p><h2 id="实现">实现</h2><h3 id="第一部分正则化线性回归">第一部分·正则化线性回归</h3><p>首先看一下数据集：</p><p><img src="Figure_0.png" /></p><p>回忆正则化线性回归的矩阵形式： <span class="math display">\[\begin{align}J(\theta)&amp;=\frac{1}{2m}\left[\theta^TX^TX\theta-2\theta^TX^Ty+y^Ty+\lambda\hat\theta^T\hat\theta\right]\\\frac{\partial J}{\partial \theta}&amp;=\frac{1}{m}\left[X^TX\theta-X^Ty+\lambda\hat\theta\right]\end{align}\]</span></p><p>其中，<span class="math inline">\(\hat\theta\)</span> 是将 <span class="math inline">\(\theta_0\)</span> 置为 <span class="math inline">\(0\)</span> 后的 <span class="math inline">\(\theta\)</span>（因为不对 <span class="math inline">\(\theta_0\)</span> 做惩罚）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">from</span> scipy.optimize <span class="hljs-keyword">import</span> minimize<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>data = loadmat(<span class="hljs-string">&#x27;ex5data1.mat&#x27;</span>)<br>X, y, Xval, yval, Xtest, ytest = \<br>data[<span class="hljs-string">&#x27;X&#x27;</span>], data[<span class="hljs-string">&#x27;y&#x27;</span>], data[<span class="hljs-string">&#x27;Xval&#x27;</span>], data[<span class="hljs-string">&#x27;yval&#x27;</span>], data[<span class="hljs-string">&#x27;Xtest&#x27;</span>], data[<span class="hljs-string">&#x27;ytest&#x27;</span>]<br>X = np.hstack((np.ones((X.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>)), X))<br>Xval = np.hstack((np.ones((Xval.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>)), Xval))<br>Xtest = np.hstack((np.ones((Xtest.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>)), Xtest))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">unseq</span>(<span class="hljs-params">theta</span>):<br><span class="hljs-keyword">return</span> theta.reshape(theta.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">seq</span>(<span class="hljs-params">theta</span>):<br><span class="hljs-keyword">return</span> theta.flatten()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">J</span>(<span class="hljs-params">theta, X, y, lamb</span>):<br>m = X.shape[<span class="hljs-number">0</span>]<br>thetahat = np.vstack((np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)), theta[<span class="hljs-number">1</span>:, :]))<br><span class="hljs-keyword">return</span> ((theta.T@X.T@X@theta-<span class="hljs-number">2</span>*theta.T@X.T@y+y.T@y+lamb*thetahat.T@thetahat)/m/<span class="hljs-number">2</span>)[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partJ</span>(<span class="hljs-params">theta, X, y, lamb</span>):<br>m = X.shape[<span class="hljs-number">0</span>]<br>thetahat = np.vstack((np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)), theta[<span class="hljs-number">1</span>:, :]))<br><span class="hljs-keyword">return</span> (X.T@X@theta-X.T@y+lamb*thetahat)/m<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">Train</span>(<span class="hljs-params">X, y</span>):<br><span class="hljs-keyword">return</span> minimize(fun = <span class="hljs-keyword">lambda</span> theta, X, y, lamb: J(unseq(theta), X, y, lamb), <br>   x0 = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]), <br>   jac = <span class="hljs-keyword">lambda</span> theta, X, y, lamb: seq(partJ(unseq(theta), X, y, lamb)), <br>   args = (X, y, <span class="hljs-number">1</span>), <br>   method = <span class="hljs-string">&#x27;CG&#x27;</span>)<br><br>res = Train(X, y)<br><span class="hljs-built_in">print</span>(res)<br></code></pre></td></tr></table></figure><p>回归结果为：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache">    <span class="hljs-attribute">fun</span>: <span class="hljs-number">22</span>.<span class="hljs-number">3795418229475</span><br>    <span class="hljs-attribute">jac</span>: array([ <span class="hljs-number">3</span>.<span class="hljs-number">74520898</span>e-<span class="hljs-number">06</span>, -<span class="hljs-number">1</span>.<span class="hljs-number">25949765</span>e-<span class="hljs-number">07</span>])<br><span class="hljs-attribute">message</span>: &#x27;Optimization terminated successfully.&#x27;<br>   <span class="hljs-attribute">nfev</span>: <span class="hljs-number">28</span><br>    <span class="hljs-attribute">nit</span>: <span class="hljs-number">18</span><br>   <span class="hljs-attribute">njev</span>: <span class="hljs-number">28</span><br> <span class="hljs-attribute">status</span>: <span class="hljs-number">0</span><br><span class="hljs-attribute">success</span>: True<br>      <span class="hljs-attribute">x</span>: array([<span class="hljs-number">13</span>.<span class="hljs-number">08771802</span>,  <span class="hljs-number">0</span>.<span class="hljs-number">36774202</span>])<br></code></pre></td></tr></table></figure><p>回归曲线如下：</p><p><img src="Figure_1.png" width="50%" height="50%" /></p><h3 id="第二部分学习曲线的绘制">第二部分·学习曲线的绘制</h3><p>依次增大训练集的大小，计算训练集的误差和测试集的代价函数（注意这时候计算代价应该取 <span class="math inline">\(\lambda=0\)</span>）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">Z_train = []<br>Z_valid = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>+X.shape[<span class="hljs-number">0</span>]):<br>res = Train(X[:i, :], y[:i, :])<br>Z_train.append(J(unseq(res.x), X[:i, :], y[:i, :], <span class="hljs-number">0</span>))<br>Z_valid.append(J(unseq(res.x), Xval, yval, <span class="hljs-number">0</span>))<br><br>ax = plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>ax.set_xlabel(<span class="hljs-string">&#x27;size of training set&#x27;</span>)<br>ax.set_ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)<br>ax.set_title(<span class="hljs-string">&#x27;learning curves&#x27;</span>)<br>ax.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>+<span class="hljs-built_in">len</span>(Z_train)), Z_train, color=<span class="hljs-string">&#x27;darkviolet&#x27;</span>, label=<span class="hljs-string">&#x27;Train&#x27;</span>)<br>ax.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>+<span class="hljs-built_in">len</span>(Z_valid)), Z_valid, color=<span class="hljs-string">&#x27;tomato&#x27;</span>, label=<span class="hljs-string">&#x27;Validation&#x27;</span>)<br>ax.legend()<br>plt.show()<br></code></pre></td></tr></table></figure><p>作图如下：</p><p><img src="Figure_2.png" width="50%" height="50%" /></p><p>可以看见，这是一个典型的欠拟合图像，模型是高偏差的。</p><h3 id="第三部分多项式回归">第三部分·多项式回归</h3><p>欠拟合的原因是我们使用了线性回归，而数据集显然不是线性的。为了更好的拟合之，我们采用多项式回归。</p><p>注意增加高次特征后，特征取值范围可能很大，需要规范化处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python">data = loadmat(<span class="hljs-string">&#x27;ex5data1.mat&#x27;</span>)<br>X, y, Xval, yval, Xtest, ytest = \<br>data[<span class="hljs-string">&#x27;X&#x27;</span>], data[<span class="hljs-string">&#x27;y&#x27;</span>], data[<span class="hljs-string">&#x27;Xval&#x27;</span>], data[<span class="hljs-string">&#x27;yval&#x27;</span>], data[<span class="hljs-string">&#x27;Xtest&#x27;</span>], data[<span class="hljs-string">&#x27;ytest&#x27;</span>]<br><br>dim = <span class="hljs-number">9</span><br>meanX, stdX = [], []<br>meany, stdy = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">featurePrepare</span>(<span class="hljs-params">X, y</span>):<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">feature extension &amp; normalization</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">global</span> meanX, stdX, meany, stdy<br>res = np.empty((X.shape[<span class="hljs-number">0</span>], <span class="hljs-number">0</span>))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(dim):<br>tmpX = X ** i<br>meanX.append(np.mean(tmpX, axis=<span class="hljs-number">0</span>))<br>stdX.append(np.std(tmpX, axis=<span class="hljs-number">0</span>))<br><span class="hljs-keyword">if</span> i:<br>tmpX = (tmpX - meanX[i]) / stdX[i] <span class="hljs-keyword">if</span> stdX[i] <span class="hljs-keyword">else</span> tmpX - meamX[i]<br>res = np.hstack((res, tmpX))<br>meany = np.mean(y, axis=<span class="hljs-number">0</span>)<br>stdy = np.std(y, axis=<span class="hljs-number">0</span>)<br>y = (y - meany) / stdy <span class="hljs-keyword">if</span> stdy <span class="hljs-keyword">else</span> y - meany<br><span class="hljs-keyword">return</span> res, y<br></code></pre></td></tr></table></figure><p>首先，使用最高次为 <span class="math inline">\(8\)</span> 的多项式，且正则化项 <span class="math inline">\(\lambda=0\)</span>，得到拟合效果和学习曲线如下：</p><p><img src="Figure_4.png" /></p><p>过拟合。</p><p>取 <span class="math inline">\(\lambda=1\)</span>，得到拟合效果和学习曲线如下：</p><p><img src="Figure_5.png" /></p><p>取 <span class="math inline">\(\lambda=50\)</span>，得到拟合效果和学习曲线如下：</p><p><img src="Figure_6.png" /></p><p>欠拟合。</p><p><br></p><p>接下来我们依次计算在若干 <span class="math inline">\(\lambda\)</span> 下的代价，并作图如下：</p><p><img src="Figure_7.png" width="50%" height="50%" /></p><p>可以看出，在 <span class="math inline">\(\lambda=3\)</span> 的时候验证集的代价最小，所以我们最终可以选定取 <span class="math inline">\(\lambda=3\)</span>.</p><p>此时，测试集的代价为：<span class="math inline">\(J_\text{test}(\theta)=0.01393303991254464\)</span>.</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Andrew Ng</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[吴恩达机器学习]8·神经网络之反向传播</title>
    <link href="/blog-main/2020/12/30/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-8%C2%B7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <url>/blog-main/2020/12/30/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-8%C2%B7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
    
    <content type="html"><![CDATA[<p>吴恩达机器学习系列课程：<a href="https://www.bilibili.com/video/BV164411b7dx" class="uri">https://www.bilibili.com/video/BV164411b7dx</a></p><span id="more"></span><h2 id="代价函数">代价函数</h2><p>回顾正则化逻辑回归的代价函数： <span class="math display">\[J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^my^{(i)}\ln(h_\theta(x^{(i)}))+(1-y^{(i)})\ln(1-h_\theta(x^{(i)}))\right]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2\]</span> 我们在第四篇中通过极大似然法解释了它的来历，这个代价函数被称作「交叉熵代价函数」，与我们在线性回归中使用的「二次代价函数」形成对比。</p><p>在神经网络中，我们沿用交叉熵代价函数。具体地，对于一个 <span class="math inline">\(K\)</span> 分类的神经网络而言，它可以看作是从输入到 <span class="math inline">\(K\)</span> 个二分类输出的过程：<span class="math inline">\(x\mapsto h_\Theta(x)\)</span>，我们设其代价函数为每一个二分类的<strong>交叉熵代价函数之和</strong>： <span class="math display">\[J(\Theta)=-\frac{1}{m}\left[\sum_{i=1}^m\sum_{k=1}^Ky^{(i)}_k\ln\left(h_\Theta(x^{(i)})_k\right)+(1-y_k^{(i)})\ln\left(1-h_\Theta(x^{(i)})_k\right)\right]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}\left(\Theta_{ji}^{(l)}\right)^2\]</span></p><p>其中：<span class="math inline">\(h_\Theta(x^{(i)})\in\mathbb R^K\)</span>，表示从第 <span class="math inline">\(i\)</span> 个输入经由神经网络得到的输出结果，<span class="math inline">\(h_\Theta(x^{(i)})_k\)</span> 表示其第 <span class="math inline">\(k\)</span> 维.</p><blockquote><p>注意这里有一个符号混用的地方：上标 <span class="math inline">\(^{(i)}\)</span> 在 <span class="math inline">\(x^{(i)}\)</span> 和 <span class="math inline">\(y^{(i)}\)</span> 中表示第 <span class="math inline">\(i\)</span> 个数据的输入（<span class="math inline">\(\in\mathbb R^{n+1}\)</span>）和输出（<span class="math inline">\(\in\mathbb R^{K}\)</span>），但是在 <span class="math inline">\(\Theta^{(l)}\)</span> 中表示第 <span class="math inline">\(l\)</span> 层的 <span class="math inline">\(\Theta\)</span>（<span class="math inline">\(\in \mathbb R^{s_{l+1}\times (s_l+1)}\)</span>）.</p></blockquote><p>现在我们的目标即是最小化代价函数 <span class="math inline">\(J(\Theta)\)</span>，并找到最小时的参数 <span class="math inline">\(\Theta\)</span>.</p><p>我们欲使用梯度下降法解决这个问题，所以我们需要计算 <span class="math inline">\(\frac{\partial J}{\partial \Theta_{ji}^{(l)}}\)</span>，计算方法就是反向传播算法。</p><h2 id="反向传播算法">反向传播算法</h2><h3 id="推导">推导</h3><p>为了推导方便，先假设只有一组数据 <span class="math inline">\((x,y)\)</span>，这样可以省去上标和求和的麻烦。我们对第 <span class="math inline">\(l+1\)</span> 层的第 <span class="math inline">\(j\)</span> 个神经元定义一个误差： <span class="math display">\[\delta_j^{(l+1)}=\frac{\partial J}{\partial z_j^{(l+1)}}\]</span> 其中 <span class="math inline">\(z_j^{(l+1)}=\sum\limits_{k=0}^{s_{l}}\Theta^{(l)}_{jk}a^{(l)}_{k}\)</span>，其实就是把对 <span class="math inline">\(a_j^{(l+1)}\)</span> 的逻辑回归单独拎出来看，取 <span class="math inline">\(\text{sigmoid}\)</span> 前的那个求和值。换句话说，<span class="math inline">\(a^{(l+1)}_j=g\left(z_j^{(l+1)}\right)\)</span>.</p><p>于是乎，我们有： <span class="math display">\[\frac{\partial J}{\partial \Theta^{(l)}_{ji}}=\frac{\partial J}{\partial z_j^{(l+1)}}\cdot\frac{\partial z_j^{(l+1)}}{\partial\Theta_{ji}^{(l)}}=a_i^{(l)}\delta_j^{(l+1)}\]</span> 写作矩阵形式： <span class="math display">\[\color{purple}{\boxed{\Delta^{(l)}=\delta^{(l+1)}\cdot(a^{(l)})^T}}\]</span> 所以现在问题转化为求解 <span class="math inline">\(\delta^{(l+1)}\)</span>.</p><p><br></p><p>采用递推的思想。首先来算输出层，即 <span class="math inline">\(\delta_j^{(L)}\)</span>： <span class="math display">\[\begin{align}\delta_j^{(L)}&amp;=\frac{\partial J}{\partial z_j^{(L)}}=\frac{\partial J}{\partial a_j^{(L)}}\cdot\frac{\partial a_j^{(L)}}{\partial z_j^{(L)}}\\&amp;=\frac{\partial J}{\partial a_j^{(L)}}\cdot g&#39;\left(z_j^{(L)}\right)\\&amp;=-\left(\frac{y_j}{a_j^{(L)}}-\frac{1-y_j}{1-a_j^{(L)}}\right)\cdot\left(a_j^{(L)}(1-a_j^{(L)})\right)\\&amp;=a_j^{(L)}-y_j\end{align}\]</span></p><p>所以： <span class="math display">\[\color{purple}{\boxed{\delta^{(L)}=a^{(L)}-y}}\]</span></p><blockquote><p>上面推导过程的一些注释：</p><p>关于第二行，请注意：<span class="math inline">\(a^{(L)}=g(z^{(L)})\)</span>；</p><p>关于第三行，请注意 <span class="math inline">\(\text{sigmoid}\)</span> 函数的一个很好的性质：<span class="math inline">\(g&#39;(z)=g(z)(1-g(z))\)</span>；</p><p>以及，关于偏导项的计算，请注意 <span class="math inline">\(a^{(L)}=h_\Theta(x)\)</span>，所以 <span class="math inline">\(J(\Theta)\)</span> 在现在的假设条件下可以写作： <span class="math display">\[J(\Theta)=-\left[\sum_{k=1}^Ky_k\ln(a^{(L)}_k)+(1-y_k)\ln(1-a_k^{(L)})\right]+\frac{\lambda}{2}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}\left(\Theta_{ji}^{(l)}\right)^2\]</span></p></blockquote><p><br></p><p>现在计算第 <span class="math inline">\(l\)</span> 层（<span class="math inline">\(2\leqslant l&lt;L\)</span>）的 <span class="math inline">\(\delta^{(l)}\)</span>： <span class="math display">\[\begin{align}\delta_j^{(l)}&amp;=\frac{\partial J}{\partial z_j^{(l)}}\\&amp;=\frac{\partial J}{\partial z^{(l+1)}}\cdot\frac{\partial z^{(l+1)}}{\partial a_j^{(l)}}\cdot\frac{\partial a_j^{(l)}}{\partial z_j^{(l)}}\\&amp;={\delta^{(l+1)}}^T\Theta^{(l)}_{\bullet,j}\cdot g&#39;(z_j^{(l)})\\&amp;={\delta^{(l+1)}}^T\Theta^{(l)}_{\bullet,j}\cdot \left(a_j^{(l)}*(1-a_j^{(l)})\right)\end{align}\]</span> 所以： <span class="math display">\[\color{purple}{\boxed{\delta^{(l)}=\left({\Theta^{(l)}}^T\delta^{(l+1)}\right)*\left(a^{(l)}*(1-a^{(l)})\right)}}\]</span> 其中 <span class="math inline">\(*\)</span> 表示两个向量对应位置相乘。</p><h3 id="步骤总结">步骤总结</h3><p>以上是对一组数据的推导，我们得到了三个重要的结果： <span class="math display">\[\boxed{\begin{align}&amp;\Delta^{(l)}=\delta^{(l+1)}\cdot(a^{(l)})^T&amp;&amp;1\leqslant l&lt;L\\&amp;\delta^{(L)}=a^{(L)}-y\\&amp;\delta^{(l)}=\left({\Theta^{(l)}}^T\delta^{(l+1)}\right)*\left(a^{(l)}*(1-a^{(l)})\right)&amp;&amp;2\leqslant l&lt;L\end{align}}\]</span> 而 <span class="math inline">\(m\)</span> 组数据只需要在一些地方进行累加即可，具体如下：</p><p>设数据集为：<span class="math inline">\(\left\{(x^{(i)},y^{(i)})\mid 1\leqslant i\leqslant m\right\}\)</span>，则反向传播算法的步骤为： 1. 所有 <span class="math inline">\(\Delta^{(l)}\)</span> 置零；</p><ol start="2" type="1"><li><p>遍历数据集，设当前数据为 <span class="math inline">\((x^{(i)},y^{(i)})\)</span>：</p><ol type="1"><li>以 <span class="math inline">\(x^{(i)}\)</span> 为输入做前向传播，得到输出 <span class="math inline">\(a^{(L)}\)</span>；</li><li>置 <span class="math inline">\(\delta^{(L)}=a^{(L)}-y^{(i)}\)</span>，进行反向传播：<span class="math inline">\(\delta^{(l)}={\Theta^{(l)}}^T\delta^{(l+1)}*\left[a^{(l)}(1-a^{(l)})\right],\;2\leqslant l&lt;L\)</span>；</li><li>更新 <span class="math inline">\(\Delta\)</span> 矩阵：<span class="math inline">\(\Delta^{(l)}:=\Delta^{(l)}+\delta^{(l+1)}\cdot(a^{(l)})^T,\;1\leqslant l&lt;L\)</span>；</li></ol></li><li><p>计算 <span class="math inline">\(D\)</span> 矩阵： <span class="math display">\[D_{ij}^{(l)}:=\begin{cases}\frac{1}{m}\left(\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}\right)&amp;\text{if }j\neq0\\\frac{1}{m}\Delta_{ij}^{(l)}&amp;\text{if }j=0\end{cases}\]</span> 这就是我们的偏导数矩阵：<span class="math inline">\(\frac{\partial J}{\partial \Theta_{ij}^{(l)}}=D^{(l)}_{ij}\)</span>.</p></li></ol><p><br></p><p>现在，我们可以用 <span class="math inline">\(D_{ij}^{(l)}\)</span> 做梯度下降了。注意一点，我们的参数 <span class="math inline">\(\Theta^{(l)}\)</span> 应该初始化为 <span class="math inline">\([-\epsilon,\epsilon]\)</span> 中的随机值。</p><h2 id="梯度检验">梯度检验</h2><p>写神经网络反向传播的代码时很容易写 bug，包括一些从结果并不能发现的 bug。梯度检验可以帮助发现这些 bug。</p><p>我们可以数值近似 <span class="math inline">\(J(\theta)\)</span> 的各偏导： <span class="math display">\[\frac{\partial}{\partial \theta_k}J(\theta)\approx\frac{J(\theta_1,\cdots,\theta_k+\epsilon,\cdots,\theta_n)-J(\theta_1,\cdots,\theta_k-\epsilon,\cdots,\theta_n)}{2\epsilon}\]</span></p><p>现在我们可以比较这些偏导估计值与 <span class="math inline">\(D_{ij}^{(l)}\)</span> 对应位置的值，它们应该非常接近。</p><p>注意一点：梯度检验耗时巨大，一旦验证了神经网络反向传播的代码正确后，不应进行梯度检验（删掉/注释掉）。</p><h2 id="实现">实现</h2><p>这次用的数据集依旧是手写数字识别的数据集，共 <span class="math inline">\(5000\)</span> 组数据，每组数据输入是一个由 <span class="math inline">\(20\times20\)</span> 灰度矩阵压缩而来的 <span class="math inline">\(400\)</span> 维向量，输出是 <span class="math inline">\(0\)</span> 到 <span class="math inline">\(9\)</span> 之间的整数。</p><h3 id="第一部分代价函数与前向传播">第一部分·代价函数与前向传播</h3><p>前向传播与上一节的代码没有什么本质的区别，不过这里将其一般化，可适用于 <span class="math inline">\(L\)</span> 层的神经网络且 <span class="math inline">\(m\)</span> 个数据同时计算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forwardPropagation</span>():<br>a = [<span class="hljs-literal">None</span>] * (L+<span class="hljs-number">1</span>)<br>a[<span class="hljs-number">1</span>] = np.hstack((np.ones((m, <span class="hljs-number">1</span>)), X))<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, L):<br>a[l] = sigmoid(np.matmul(a[l-<span class="hljs-number">1</span>], Theta[l-<span class="hljs-number">1</span>].T))<br>a[l] = np.hstack((np.ones((m, <span class="hljs-number">1</span>)), a[l]))<br>a[L] = sigmoid(np.matmul(a[L-<span class="hljs-number">1</span>], Theta[L-<span class="hljs-number">1</span>].T))<br><span class="hljs-keyword">return</span> a<br></code></pre></td></tr></table></figure><p>当然只计算一个数据也是可以的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forwardPropagation</span>(<span class="hljs-params">x</span>):<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">x: (n, 1)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>a = [<span class="hljs-literal">None</span>] * (L+<span class="hljs-number">1</span>)<br>a[<span class="hljs-number">1</span>] = np.vstack((np.ones((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)), x))<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, L):<br>a[l] = sigmoid(np.matmul(Theta[l-<span class="hljs-number">1</span>], a[l-<span class="hljs-number">1</span>]))<br>a[l] = np.vstack((np.ones((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)), a[l]))<br>a[L] = sigmoid(np.matmul(Theta[L-<span class="hljs-number">1</span>], a[L-<span class="hljs-number">1</span>]))<br><span class="hljs-keyword">return</span> a<br></code></pre></td></tr></table></figure><p>代价函数及其正则化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">J</span>(<span class="hljs-params">lamb</span>):<br>res = <span class="hljs-number">0</span><br>FP = forwardPropagation()<br>res -= np.<span class="hljs-built_in">sum</span>(Y * np.log(FP[L]) + (<span class="hljs-number">1</span>-Y) * np.log(<span class="hljs-number">1</span>-FP[L]))<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>res += lamb / <span class="hljs-number">2</span> * np.<span class="hljs-built_in">sum</span>(np.power(Theta[l][:, <span class="hljs-number">1</span>:], <span class="hljs-number">2</span>))<br>res /= m<br><span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure><p>使用 <code>ex4weights.mat</code> 中给定的参数 <span class="math inline">\(\Theta\)</span>，在无正则化时，准确率为 <span class="math inline">\(97.52\%\)</span>，代价为 <span class="math inline">\(0.28762916516131865\)</span>；</p><p>在正则化时（<span class="math inline">\(\lambda=1\)</span>），准确率为 <span class="math inline">\(97.52\%\)</span>，代价为 <span class="math inline">\(0.3844877962428937\)</span>。</p><h3 id="第二部分反向传播">第二部分·反向传播</h3><p>反向传播算法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">backPropagation</span>(<span class="hljs-params">lamb</span>):<br>Delta = [<span class="hljs-literal">None</span>] * L <span class="hljs-comment"># (s_&#123;l+1&#125;, s_l+1)</span><br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>Delta[l] = np.zeros((s[l+<span class="hljs-number">1</span>], s[l]+<span class="hljs-number">1</span>))<br>delta = [<span class="hljs-literal">None</span>] * (L+<span class="hljs-number">1</span>) <span class="hljs-comment"># (s_l, )</span><br>a = forwardPropagation()<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>delta[L] = a[L][i:i+<span class="hljs-number">1</span>, :].T - Y[i:i+<span class="hljs-number">1</span>, :].T<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(L-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):<br>delta[l] = (np.matmul(Theta[l].T, delta[l+<span class="hljs-number">1</span>]) * (a[l][i:i+<span class="hljs-number">1</span>, :].T * (<span class="hljs-number">1</span> - a[l][i:i+<span class="hljs-number">1</span>, :].T)))[<span class="hljs-number">1</span>:, :]<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>Delta[l] += np.matmul(delta[l+<span class="hljs-number">1</span>], a[l][i:i+<span class="hljs-number">1</span>, :])<br>D = [<span class="hljs-literal">None</span>] * L <span class="hljs-comment"># (s_&#123;l+1&#125;, s_l+1)</span><br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>D[l] = (Delta[l] + lamb * np.hstack((np.zeros((s[l+<span class="hljs-number">1</span>], <span class="hljs-number">1</span>)), Theta[l][:, <span class="hljs-number">1</span>:]))) / m<br><span class="hljs-keyword">return</span> D<br></code></pre></td></tr></table></figure><p>梯度下降：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">GradientDescent</span>(<span class="hljs-params">alpha, iteration</span>):<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>Theta[l] = np.random.random((s[l+<span class="hljs-number">1</span>], s[l]+<span class="hljs-number">1</span>))<br>Theta[l] = (Theta[l] - <span class="hljs-number">0.5</span>) / <span class="hljs-number">4</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iteration):<br>D = backPropagation(lamb=<span class="hljs-number">1</span>)<br><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>Theta[l] -= alpha * D[l]<br>Z.append(J(lamb=<span class="hljs-number">1</span>))<br><span class="hljs-keyword">return</span> Theta<br></code></pre></td></tr></table></figure><p>梯度检验（把这段代码插入到梯度下降计算出 <span class="math inline">\(D\)</span> 矩阵之后的地方即可）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(s[l+<span class="hljs-number">1</span>]):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(s[l]+<span class="hljs-number">1</span>):<br>            Theta[l][i, j] -= <span class="hljs-number">0.001</span><br>            J1 = J(lamb=<span class="hljs-number">1</span>)<br>            Theta[l][i, j] += <span class="hljs-number">0.002</span><br>            J2 = J(lamb=<span class="hljs-number">1</span>)<br>            Theta[l][i, j] -= <span class="hljs-number">0.001</span><br>            <span class="hljs-built_in">print</span>(D[l][i, j], (J2 - J1) / <span class="hljs-number">0.002</span>)<br></code></pre></td></tr></table></figure><p>现在我们可以开始训练了！代价随迭代次数增加的变化如下：</p><p><img src="Figure_1.png" width="50%" height="50%" /></p><p>准确率如下：</p><table><thead><tr class="header"><th style="text-align: center;">数字</th><th style="text-align: center;">准确率</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">0</td><td style="text-align: center;">98.6%</td></tr><tr class="even"><td style="text-align: center;">1</td><td style="text-align: center;">97.8%</td></tr><tr class="odd"><td style="text-align: center;">2</td><td style="text-align: center;">93.4%</td></tr><tr class="even"><td style="text-align: center;">3</td><td style="text-align: center;">93.4%</td></tr><tr class="odd"><td style="text-align: center;">4</td><td style="text-align: center;">97.0%</td></tr><tr class="even"><td style="text-align: center;">5</td><td style="text-align: center;">93.8%</td></tr><tr class="odd"><td style="text-align: center;">6</td><td style="text-align: center;">97.8%</td></tr><tr class="even"><td style="text-align: center;">7</td><td style="text-align: center;">95.6%</td></tr><tr class="odd"><td style="text-align: center;">8</td><td style="text-align: center;">95.8%</td></tr><tr class="even"><td style="text-align: center;">9</td><td style="text-align: center;">94.0%</td></tr><tr class="odd"><td style="text-align: center;">Total</td><td style="text-align: center;">95.72%</td></tr></tbody></table><blockquote><p>自己写的梯度下降跑得很慢，以后尝试使用 <code>scipy.optimize.minimize</code>.</p></blockquote><hr /><p><span class="math inline">\(\textbf{Update 2021.01.04}\)</span>：使用了 <code>scipy.optimize.minimize</code> 来最小化 <span class="math inline">\(J(\Theta)\)</span>，method 选用 <span class="math inline">\(\text{CG}\)</span>，结果如下：</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs subunit">    fun: 0.3139915007113895<br>    jac: array([<span class="hljs-string">-2</span>.06059133e<span class="hljs-string">-05</span>, <span class="hljs-string">-3</span>.07615047e<span class="hljs-string">-11</span>,  7.43614742e<span class="hljs-string">-10</span>, ...,<br>      <span class="hljs-string">-2</span>.86333807e<span class="hljs-string">-05</span>,  4.03018555e<span class="hljs-string">-06</span>,  5.10265929e<span class="hljs-string">-05</span>])<br>message: &#x27;Optimization terminated successfully.&#x27;<br>   nfev: 1074<br>    nit: 461<br>   njev: 1074<br> status: 0<br><span class="hljs-keyword">success: </span>True<br>      x: array([<span class="hljs-string">-9</span>.31407860e<span class="hljs-string">-01</span>, <span class="hljs-string">-1</span>.53807524e<span class="hljs-string">-07</span>,  3.71577070e<span class="hljs-string">-06</span>, ...,<br>      <span class="hljs-string">-1</span>.27597236e<span class="hljs-string">-01</span>, <span class="hljs-string">-1</span>.69568829e<span class="hljs-string">-01</span>,  3.47560466e<span class="hljs-string">-01</span>])<br></code></pre></td></tr></table></figure><p><img src="Figure_CG.png" width="50%" height="50%" /></p><table><thead><tr class="header"><th style="text-align: center;">数字</th><th style="text-align: center;">准确率</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">0</td><td style="text-align: center;">99.8%</td></tr><tr class="even"><td style="text-align: center;">1</td><td style="text-align: center;">100.0%</td></tr><tr class="odd"><td style="text-align: center;">2</td><td style="text-align: center;">99.4%</td></tr><tr class="even"><td style="text-align: center;">3</td><td style="text-align: center;">99.4%</td></tr><tr class="odd"><td style="text-align: center;">4</td><td style="text-align: center;">99.2%</td></tr><tr class="even"><td style="text-align: center;">5</td><td style="text-align: center;">99.8%</td></tr><tr class="odd"><td style="text-align: center;">6</td><td style="text-align: center;">99.6%</td></tr><tr class="even"><td style="text-align: center;">7</td><td style="text-align: center;">99.6%</td></tr><tr class="odd"><td style="text-align: center;">8</td><td style="text-align: center;">100.0%</td></tr><tr class="even"><td style="text-align: center;">9</td><td style="text-align: center;">99.0%</td></tr><tr class="odd"><td style="text-align: center;">Total</td><td style="text-align: center;">99.58%</td></tr></tbody></table><p>当然，再次强调，由于没有划分训练集、验证集、测试集，这个准确率并不能说明问题。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Andrew Ng</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[吴恩达机器学习]7·初识神经网络</title>
    <link href="/blog-main/2020/12/29/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-7%C2%B7%E5%88%9D%E8%AF%86%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/blog-main/2020/12/29/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-7%C2%B7%E5%88%9D%E8%AF%86%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<p>吴恩达机器学习系列课程：<a href="https://www.bilibili.com/video/BV164411b7dx" class="uri">https://www.bilibili.com/video/BV164411b7dx</a></p><span id="more"></span><h2 id="神经元模型">神经元模型</h2><p><img src="neuron.png" width="50%" height="50%" /></p><p>一个神经元就是一个函数，根据输入节点信息以及其权重，得到一个输出信息，即： <span class="math display">\[h_\theta(x)=f(\theta, x)\]</span> 这里 <span class="math inline">\(\theta=\begin{bmatrix}\theta_0\\\theta_1\\\vdots\\\theta_n\end{bmatrix},\,x=\begin{bmatrix}x_0\\x_1\\\vdots\\x_n\end{bmatrix}\)</span>.</p><blockquote><p>注意：上图省略了 <span class="math inline">\(x_0\equiv 1\)</span> 这一偏置项。</p></blockquote><p><span class="math inline">\(h_\theta(x)\)</span> 也称作激活函数，一般可以采取 <span class="math inline">\(\text{sigmoid}\)</span> 函数 <span class="math inline">\(h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}\)</span>.</p><h2 id="神经网络">神经网络</h2><p><img src="neuron network.png" width="50%" height="50%" /></p><p>神经网络是好几层的神经元连接在一起的集合。第一层被称作输入层，最后一层被称作输出层，中间其他层被称作隐藏层</p><blockquote><p>注意：上图依旧省略了 <span class="math inline">\(x_0(a_0^{(1)})\equiv a_0^{(2)}\equiv1\)</span> 偏置项。</p></blockquote><p>我们把第 <span class="math inline">\(i\)</span> 层到第 <span class="math inline">\(i+1\)</span> 层的传导单独拿出来分析：设 <span class="math inline">\(s_i\)</span> 表示第 <span class="math inline">\(i\)</span> 层的神经元数量（<strong>不包含偏置项</strong>），那么对于第 <span class="math inline">\(i+1\)</span> 层的第 <span class="math inline">\(k\)</span> 个神经元，有一个 <span class="math inline">\(\theta^{(i)}_k\)</span> 与之对应，使得：<span class="math inline">\(a^{(i+1)}_k=h_{\theta^{(i)}_k}(\hat a^{(i)})=g\left({\theta_k^{(i)}}^T\hat a^{(i)}\right)\)</span>. 这里我用 <span class="math inline">\(\hat a^{(i)}\in\mathbb R^{s_i+1}\)</span> 表示加上偏置项后的 <span class="math inline">\(a^{(i)}\)</span>. 我们可以把这写作矩阵形式： <span class="math display">\[a^{(i+1)}=g\left(\Theta^{(i)}\hat a^{(i)}\right)\]</span> 这里的 <span class="math inline">\(\Theta^{(i)}\in\mathbb R^{s_{i+1}\times(s_i+1)}\)</span>.</p><blockquote><p>其他很多地方会把偏置项对应的参数向量 <span class="math inline">\(b\)</span> 单独拿出来，把上面的式子写作类似于： <span class="math display">\[a^{(i+1)}=g\left(\Theta^{(i)}a^{(i)}+b^{(i)}\right)\]</span> 的形式。这里 <span class="math inline">\(\Theta^{(i)}\in\mathbb R^{s_{i+1}\times s_i},\,b\in\mathbb R^{s_{i+1}}\)</span>. 我这里依旧沿用吴恩达教授的写法习惯。</p></blockquote><h3 id="神经网络实现门电路">神经网络实现门电路</h3><p>通过这个例子能够直观地理解神经网络中参数的作用。</p><p>考虑一个输入层有 <span class="math inline">\(2\)</span> 个神经元（不包括偏置项）且取值 <span class="math inline">\(\in\{0,1\}\)</span>、输出层有 <span class="math inline">\(1\)</span> 个神经元且取值 <span class="math inline">\(\in\{0,1\}\)</span> 的神经网络。如果我们将其参数设置为：<span class="math inline">\(\Theta^{(1)}=\begin{bmatrix}-30\\20\\20\end{bmatrix}\)</span>，那么我们有：</p><table><thead><tr class="header"><th style="text-align: center;">输入1</th><th style="text-align: center;">输入 2</th><th style="text-align: center;">输出</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">0</td><td style="text-align: center;">0</td><td style="text-align: center;"><span class="math inline">\(g(-30)\approx 0\)</span></td></tr><tr class="even"><td style="text-align: center;">0</td><td style="text-align: center;">1</td><td style="text-align: center;"><span class="math inline">\(g(-10)\approx 0\)</span></td></tr><tr class="odd"><td style="text-align: center;">1</td><td style="text-align: center;">0</td><td style="text-align: center;"><span class="math inline">\(g(-10)\approx 0\)</span></td></tr><tr class="even"><td style="text-align: center;">1</td><td style="text-align: center;">1</td><td style="text-align: center;"><span class="math inline">\(g(10)\approx 1\)</span></td></tr></tbody></table><p>于是我们实现了一个与门的功能！</p><p>类似的，我们可以实现或门、非门，然后组合使用与、或、非门，可以用多层神经网络实现异或、同或门等其他电路功能。</p><h2 id="实现">实现</h2><p>这一节的作业依旧是对手写数字进行识别，但是不要求我们自己寻找参数 <span class="math inline">\(\Theta\)</span>，而是在 <code>ex3weights.mat</code> 中给出了。我们只需要根据神经网络的原理进行前向传播。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib<br><br>m = <span class="hljs-number">5000</span><br>n = <span class="hljs-number">401</span><br><br>data = loadmat(<span class="hljs-string">&#x27;ex3data1.mat&#x27;</span>)<br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>] <span class="hljs-comment"># (5000, 400)</span><br>X = np.hstack((np.ones((m, <span class="hljs-number">1</span>)), X)) <span class="hljs-comment"># (5000, 401)</span><br>y = data[<span class="hljs-string">&#x27;y&#x27;</span>] <span class="hljs-comment"># (5000, 1)</span><br><br>data = loadmat(<span class="hljs-string">&#x27;ex3weights.mat&#x27;</span>)<br>Theta1 = data[<span class="hljs-string">&#x27;Theta1&#x27;</span>] <span class="hljs-comment"># (25, 401)</span><br>Theta2 = data[<span class="hljs-string">&#x27;Theta2&#x27;</span>] <span class="hljs-comment"># (10, 26)</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">z</span>):<br><span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-z))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">x</span>):<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">x: (401, )</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>a1 = x.reshape((<span class="hljs-number">401</span>, <span class="hljs-number">1</span>))<br>a2 = np.matmul(Theta1, a1)<br>a2 = sigmoid(a2) <span class="hljs-comment"># (25, 1)</span><br>a2 = np.vstack((np.ones((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)), a2)) <span class="hljs-comment"># (26, 1)</span><br>a3 = np.matmul(Theta2, a2)<br>a3 = sigmoid(a3)<br><span class="hljs-keyword">return</span> a3.argmax(axis=<span class="hljs-number">0</span>)[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span><br><br>Sum = np.zeros(<span class="hljs-number">11</span>)<br>Hit = np.zeros(<span class="hljs-number">11</span>)<br>Accuracy = np.zeros(<span class="hljs-number">11</span>)<br>hit = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5000</span>):<br>Sum[y[<span class="hljs-built_in">id</span>][<span class="hljs-number">0</span>]] += <span class="hljs-number">1</span><br><span class="hljs-keyword">if</span> predict(X[<span class="hljs-built_in">id</span>, :].T) == y[<span class="hljs-built_in">id</span>][<span class="hljs-number">0</span>]:<br>Hit[y[<span class="hljs-built_in">id</span>][<span class="hljs-number">0</span>]] += <span class="hljs-number">1</span><br>hit += <span class="hljs-number">1</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>):<br>Accuracy[i] = Hit[i] / Sum[i]<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">str</span>(i)+<span class="hljs-string">&quot;:&quot;</span>, <span class="hljs-built_in">str</span>(Accuracy[i] * <span class="hljs-number">100</span>)+<span class="hljs-string">&quot;%&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;tot:&quot;</span>, <span class="hljs-built_in">str</span>(hit / <span class="hljs-number">5000</span> * <span class="hljs-number">100</span>)+<span class="hljs-string">&quot;%&quot;</span>)<br></code></pre></td></tr></table></figure><p>各数字准确率和总准确率如下：</p><table><thead><tr class="header"><th style="text-align: center;">数字</th><th style="text-align: center;">准确率</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">1</td><td style="text-align: center;">98.2%</td></tr><tr class="even"><td style="text-align: center;">2</td><td style="text-align: center;">97.0%</td></tr><tr class="odd"><td style="text-align: center;">3</td><td style="text-align: center;">96.0%</td></tr><tr class="even"><td style="text-align: center;">4</td><td style="text-align: center;">96.8%</td></tr><tr class="odd"><td style="text-align: center;">5</td><td style="text-align: center;">98.4%</td></tr><tr class="even"><td style="text-align: center;">6</td><td style="text-align: center;">98.6%</td></tr><tr class="odd"><td style="text-align: center;">7</td><td style="text-align: center;">97.0%</td></tr><tr class="even"><td style="text-align: center;">8</td><td style="text-align: center;">98.2%</td></tr><tr class="odd"><td style="text-align: center;">9</td><td style="text-align: center;">95.8%</td></tr><tr class="even"><td style="text-align: center;">0</td><td style="text-align: center;">99.2%</td></tr><tr class="odd"><td style="text-align: center;">Total</td><td style="text-align: center;">97.52%</td></tr></tbody></table><p>注：直接采用训练集进行测试其实是不严谨的做法。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Andrew Ng</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[吴恩达机器学习]6·逻辑回归之多分类（数字识别）</title>
    <link href="/blog-main/2020/12/28/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-6%C2%B7%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B9%8B%E5%A4%9A%E5%88%86%E7%B1%BB%EF%BC%88%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%EF%BC%89/"/>
    <url>/blog-main/2020/12/28/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-6%C2%B7%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B9%8B%E5%A4%9A%E5%88%86%E7%B1%BB%EF%BC%88%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>吴恩达机器学习系列课程：<a href="https://www.bilibili.com/video/BV164411b7dx" class="uri">https://www.bilibili.com/video/BV164411b7dx</a></p><span id="more"></span><h2 id="数据读入与显示">数据读入与显示</h2><p>给定的数据是 <code>.mat</code> 格式的，使用 <code>scipy</code> 的 <code>loadmat</code> 方法可以读入数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><br>data = loadmat(<span class="hljs-string">&#x27;ex3data1.mat&#x27;</span>)<br><span class="hljs-built_in">print</span>(data)<br><span class="hljs-built_in">print</span>(data[<span class="hljs-string">&#x27;X&#x27;</span>].shape)<br><span class="hljs-built_in">print</span>(data[<span class="hljs-string">&#x27;y&#x27;</span>].shape)<br></code></pre></td></tr></table></figure><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs prolog">&#123;<span class="hljs-string">&#x27;__header__&#x27;</span>: b<span class="hljs-string">&#x27;MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011&#x27;</span>, <span class="hljs-string">&#x27;__version__&#x27;</span>: <span class="hljs-string">&#x27;1.0&#x27;</span>, <span class="hljs-string">&#x27;__globals__&#x27;</span>: [], <span class="hljs-string">&#x27;X&#x27;</span>: array([[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, ..., <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>       [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, ..., <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>       [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, ..., <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>       ...,<br>       [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, ..., <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>       [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, ..., <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>       [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, ..., <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]]), <span class="hljs-string">&#x27;y&#x27;</span>: array([[<span class="hljs-number">10</span>],<br>       [<span class="hljs-number">10</span>],<br>       [<span class="hljs-number">10</span>],<br>       ...,<br>       [ <span class="hljs-number">9</span>],<br>       [ <span class="hljs-number">9</span>],<br>       [ <span class="hljs-number">9</span>]], dtype=uint8)&#125;<br>(<span class="hljs-number">5000</span>, <span class="hljs-number">400</span>)<br>(<span class="hljs-number">5000</span>, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>文件中一共有 <span class="math inline">\(5000\)</span> 个数据，每个数据的输入是一个长为 <span class="math inline">\(400\)</span> 的向量，由 <span class="math inline">\(20\times 20\)</span> 的灰度矩阵压缩而来；输出是一个数字，表示这个数字是多少。</p><blockquote><p>注意：由于 <code>MATLAB</code> 或者 <code>Octave</code> 的一些原因，<span class="math inline">\(0\)</span> 被标记为了 <span class="math inline">\(10\)</span>，我们用 <code>python</code> 时可以把 <span class="math inline">\(10\)</span> 换回成 <span class="math inline">\(0\)</span>；另外，数据是按列压缩的，还原回 <span class="math inline">\(20\times20\)</span> 的矩阵后其实转置了一下，这里提前转置回去方便后续编码。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">data = loadmat(<span class="hljs-string">&#x27;ex3data1.mat&#x27;</span>)<br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>]<br>X = np.transpose(X.reshape((<span class="hljs-number">5000</span>, <span class="hljs-number">20</span>, <span class="hljs-number">20</span>)), [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]).reshape(<span class="hljs-number">5000</span>, <span class="hljs-number">400</span>)<br>y = data[<span class="hljs-string">&#x27;y&#x27;</span>]<br>y[y==<span class="hljs-number">10</span>] = <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>现在我们随机挑选 <span class="math inline">\(100\)</span> 个图像显示出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">show_a_number</span>(<span class="hljs-params">num, ax</span>):<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">num.shape: (400, )</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>ax.matshow(num.reshape((<span class="hljs-number">20</span>, <span class="hljs-number">20</span>)), cmap=matplotlib.cm.binary)<br>ax.axis(<span class="hljs-string">&#x27;off&#x27;</span>)<br><br>fig, ax = plt.subplots(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br><span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br><span class="hljs-built_in">id</span> = np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">5000</span>)<br>show_a_number(X[<span class="hljs-built_in">id</span>, :], ax[i][j])<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="Figure_0.png" /></p><h2 id="多分类">多分类</h2><p>这里采用「一对多」方式完成多分类，也就是说对每一类单独训练，最后挑选概率最大的那一类视为结果。</p><p>沿用上一节写好的矩阵形式的正则化逻辑回归代码，进行 <span class="math inline">\(10\)</span> 次分类即可。</p><p>记得给 <span class="math inline">\(X\)</span> 每一行前加上一个 <span class="math inline">\(1\)</span>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.io <span class="hljs-keyword">import</span> loadmat<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib<br><br>lamb = <span class="hljs-number">1</span><br>alpha = <span class="hljs-number">0.1</span><br>iteration = <span class="hljs-number">10000</span><br>m = <span class="hljs-number">5000</span><br>n = <span class="hljs-number">401</span><br><br>data = loadmat(<span class="hljs-string">&#x27;ex3data1.mat&#x27;</span>)<br>X = data[<span class="hljs-string">&#x27;X&#x27;</span>]<br>X = np.transpose(X.reshape((m, <span class="hljs-number">20</span>, <span class="hljs-number">20</span>)), [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]).reshape(m, <span class="hljs-number">400</span>)<br>X = np.hstack((np.ones((m, <span class="hljs-number">1</span>)), X))<br>y = data[<span class="hljs-string">&#x27;y&#x27;</span>]<br>y[y==<span class="hljs-number">10</span>] = <span class="hljs-number">0</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">h</span>(<span class="hljs-params">Theta, x</span>):<br><span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-np.matmul(Theta.T, x)[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">h_</span>(<span class="hljs-params">Theta, X</span>):<br><span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-np.matmul(X, Theta)))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">J_reg</span>(<span class="hljs-params">Theta, X, y</span>):<br>tmp = h_(Theta, X)<br><span class="hljs-keyword">return</span> (-(np.matmul(y.T, np.log(tmp)) + np.matmul(<span class="hljs-number">1</span> - y.T, np.log(<span class="hljs-number">1</span> - tmp))) / m\<br> + lamb / <span class="hljs-number">2</span> / m * np.matmul(Theta.T, Theta))[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partJ_reg</span>(<span class="hljs-params">Theta, X, y</span>):<br><span class="hljs-keyword">return</span> (np.matmul(X.T, h_(Theta, X) - y) + lamb * np.vstack((np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)), Theta[<span class="hljs-number">1</span>:, :]))) / m<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">GradientDescent</span>(<span class="hljs-params">X, y</span>):<br>Theta = np.zeros((n, <span class="hljs-number">1</span>))<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iteration):<br>Theta = Theta - alpha * partJ_reg(Theta, X, y)<br><span class="hljs-keyword">return</span> Theta<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">classify</span>(<span class="hljs-params">k</span>):<br>y_ = y.copy()<br>y_[y==k] = <span class="hljs-number">1</span><br>y_[y!=k] = <span class="hljs-number">0</span><br>Theta = GradientDescent(X, y_)<br><span class="hljs-keyword">return</span> Theta<br><br>Theta = np.zeros((<span class="hljs-number">10</span>, n))<br><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>Theta[k, :] = classify(k).flatten()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Trained for:&quot;</span>, k)<br><br>np.savetxt(<span class="hljs-string">&#x27;theta.txt&#x27;</span>, Theta, delimiter=<span class="hljs-string">&#x27;,&#x27;</span>)<br></code></pre></td></tr></table></figure><p>现在 <code>theta.txt</code> 中装入了我们训练好的 Theta，接下来我们就可以用它来进行预测了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">Theta, x</span>):<br><span class="hljs-keyword">return</span> np.matmul(Theta, x).argmax(axis=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>直接计算每个数字的准确率以及总准确率（其实这样不严谨，详见第九篇），得到结果：</p><table><thead><tr class="header"><th style="text-align: center;">数字</th><th style="text-align: center;">准确率</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">0</td><td style="text-align: center;">99.0%</td></tr><tr class="even"><td style="text-align: center;">1</td><td style="text-align: center;">97.8%</td></tr><tr class="odd"><td style="text-align: center;">2</td><td style="text-align: center;">88.2%</td></tr><tr class="even"><td style="text-align: center;">3</td><td style="text-align: center;">90.0%</td></tr><tr class="odd"><td style="text-align: center;">4</td><td style="text-align: center;">93.4%</td></tr><tr class="even"><td style="text-align: center;">5</td><td style="text-align: center;">89.4%</td></tr><tr class="odd"><td style="text-align: center;">6</td><td style="text-align: center;">97.0%</td></tr><tr class="even"><td style="text-align: center;">7</td><td style="text-align: center;">93.8%</td></tr><tr class="odd"><td style="text-align: center;">8</td><td style="text-align: center;">91.4%</td></tr><tr class="even"><td style="text-align: center;">9</td><td style="text-align: center;">91.2%</td></tr><tr class="odd"><td style="text-align: center;">Total</td><td style="text-align: center;">93.12%</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Andrew Ng</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[吴恩达机器学习]5·正则化</title>
    <link href="/blog-main/2020/12/24/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-5%C2%B7%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <url>/blog-main/2020/12/24/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-5%C2%B7%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<p>吴恩达机器学习系列课程：<a href="https://www.bilibili.com/video/BV164411b7dx" class="uri">https://www.bilibili.com/video/BV164411b7dx</a></p><span id="more"></span><h2 id="过拟合">过拟合</h2><p>以多项式回归为例。多项式回归是指在线性回归的基础上，人为的引入高阶项并将其视为新的一维特征，然后进行线性回归，从而达到拟合非线性数据的目的。举例而言，设线性回归的一个数据是 <span class="math inline">\(\left(x^{(i)},y^{(i)}\right)\)</span>，我们在多项式回归时可以将其视为 <span class="math inline">\(\left(x^{(i)},{x^{(i)}}^2,y^{(i)}\right)\)</span>，这样就人为引入了一个二阶项，我们的回归方程（假设函数）就变成了：<span class="math inline">\(y=\theta_0+\theta_1x+\theta_2x^2\)</span>，如此就用二次函数去拟合数据了。</p><p>我们知道，一个函数可以泰勒展开成幂级数，越展开幂级数越接近原函数，那在回归中是不是也是次数越高越好呢？答案是否定的，因为我们只有有限个数据。如果特征维数 <span class="math inline">\(n\)</span> 过于接近数据量 <span class="math inline">\(m\)</span>，则它会很好地拟合上我们的数据集，然而对于不在数据集中的数据，这可能并不是一个好的拟合结果。譬如，<span class="math inline">\(n=m\)</span> 时，由插值的原理容易知道，我们可以找到一个唯一的 <span class="math inline">\(n-1\)</span> 次多项式完美地经过所有数据，即每个数据集中的数据偏差都是 <span class="math inline">\(0\)</span>，但是这显然不是一个好的拟合，如下图：</p><p><img src="img1.png" width="50%" height="50%" /></p><p>同样地，逻辑回归也可能产生过拟合：</p><p><img src="img2.png" width="50%" height="50%" /></p><p>解决过拟合的方法有很多，这里学习正则化方法（Regularization）。</p><h2 id="正则化">正则化</h2><p>仍然以多项式回归为例，如果我们的假设函数为：<span class="math inline">\(h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4\)</span>，而其中的高阶项 <span class="math inline">\(\theta_3,\theta_4\)</span> 导致了过拟合问题，那么我们自然希望 <span class="math inline">\(\theta_3\)</span> 和 <span class="math inline">\(\theta_4\)</span> 越小越好。此时，我们只需要对代价函数做出一点修改：<span class="math inline">\(J(\theta)=\frac{1}{2m}\left[\sum\limits_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2+1000\theta_3^2+1000\theta_4^2\right]\)</span>，这样当 <span class="math inline">\(J(\theta)\)</span> 取最小时，<span class="math inline">\(\theta_3\)</span> 和 <span class="math inline">\(\theta_4\)</span> 都接近于 <span class="math inline">\(0\)</span>，我们也就达到了目的。</p><p>一般地，我们并不知道究竟应该对哪些参数做出“惩罚”，所以我们设代价函数为： <span class="math display">\[J(\theta)=\frac{1}{m}\sum_{i=1}^m\text{Cost}\left(h_\theta(x^{(i)})-y^{(i)}\right)+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2\]</span> 其中，<span class="math inline">\(\lambda\)</span> 是正则化参数，<span class="math inline">\(\lambda\sum\limits_{j=1}^n\theta_j^2\)</span> 是正则化项。即我们对<strong>除了 <span class="math inline">\(\theta_0\)</span> 以外</strong>的参数都做“惩罚”。</p><p>如何理解这个代价函数呢？使前一项 <span class="math inline">\(\frac{1}{m}\sum\limits_{i=1}^m\text{Cost}\left(h_\theta(x^{(i)})-y^{(i)}\right)\)</span> 尽可能小是为了去拟合数据集中的数据，使正则化项 <span class="math inline">\(\frac{\lambda}{2m}\sum\limits_{j=1}^n\theta_j^2\)</span> 尽可能小是为了减小各 <span class="math inline">\(\theta_j\)</span> 以避免过拟合，而 <span class="math inline">\(\lambda\)</span> 就是在二者之间权衡的参数。如果 <span class="math inline">\(\lambda\)</span> 很大，意味着正则化项占主要地位，有可能导致所有的 <span class="math inline">\(\theta_j\)</span> 都太小了而无法拟合好数据，即欠拟合；如果 <span class="math inline">\(\lambda\)</span> 很小，意味着拟合数据占主要地位，就有可能过拟合。所以一个合适的正则化参数 <span class="math inline">\(\lambda\)</span> 非常重要。</p><h3 id="线性回归的正则化">线性回归的正则化</h3><p>线性回归的含有正则化项的代价函数为： <span class="math display">\[\begin{align}J(\theta)&amp;=\frac{1}{2m}\left[\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2+\lambda\sum_{j=1}^n\theta_j^2\right]\\&amp;=\frac{1}{2m}\left[\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)^2+\lambda\sum_{j=1}^n\theta_j^2\right]\end{align}\]</span> 对其求导： <span class="math display">\[\frac{\partial J}{\partial \theta_j}=\frac{1}{m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}_j+[j\neq 0]\frac{\lambda}{m}\theta_j,\quad j=0,1,\cdots,n\]</span> 所以梯度下降时，我们的迭代过程为： <span class="math display">\[\begin{align}\theta_j&amp;:=\theta_j-\alpha \frac{\partial J}{\partial \theta_j}\\&amp;=\theta_j-\alpha \left[\frac{1}{m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}_j+\frac{\lambda}{m}\theta_j\right]\\&amp;=\theta_j\left(1-\alpha\frac{\lambda}{m}\right)-\alpha\frac{1}{m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}_j,\quad j=1,\cdots,n\end{align}\]</span> <br></p><p>若不用梯度下降，而是直接用正规方程，即在数学上解它，那么：</p><p>为了记号的方便，我们先假定对 <span class="math inline">\(\theta_0\)</span> 也进行“惩罚”。首先将 <span class="math inline">\(J(\theta)\)</span> 写作矩阵形式： <span class="math display">\[\begin{align}J(\theta)&amp;=\frac{1}{2m}\left[\sum_{i=1}^m\left(\theta^Tx^{(i)}\right)^2-2\sum_{i=1}^m\theta^Tx^{(i)}y^{(i)}+\sum_{i=1}^m\left(y^{(i)}\right)^2+\lambda\theta^T\theta\right]\\&amp;=\frac{1}{2m}\left[\left(\theta^TX^T\right)\left(\theta^TX^T\right)^T-2\theta^TX^Ty+y^Ty+\lambda\theta^T\theta\right]\\&amp;=\frac{1}{2m}\left[\theta^TX^TX\theta-2\theta^TX^Ty+y^Ty+\lambda\theta^T\theta\right]\end{align}\]</span> 然后令 <span class="math display">\[\frac{\partial J}{\partial \theta}=\frac{1}{m}\left[X^TX\theta-X^Ty+\lambda\theta\right]=0\]</span> 则： <span class="math display">\[(X^TX+\lambda)\theta=X^Ty\]</span> 解得： <span class="math display">\[\theta=(X^TX+\lambda)^{-1}X^Ty\]</span> 现在把 <span class="math inline">\(j=0\)</span> 的特殊情况考虑进去，那么最后的结果就是： <span class="math display">\[\theta=\left(X^TX+\lambda\begin{bmatrix}0&amp;&amp;&amp;\\&amp;1&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;1\end{bmatrix}\right)^{-1}X^Ty\]</span></p><h3 id="逻辑回归的正则化">逻辑回归的正则化</h3><p>逻辑回归的含有正则化项的代价函数为： <span class="math display">\[\begin{align}J(\theta)&amp;=-\frac{1}{m}\sum_{i=1}^m\left[y^{(i)}\ln(h_\theta(x^{(i)}))+(1-y^{(i)})\ln(1-h_\theta(x^{(i)}))\right]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2\\&amp;=\frac{1}{m}\sum_{i=1}^m\left[y^{(i)}\ln\left(1+e^{-\theta^Tx^{(i)}}\right)+\left(1-y^{(i)}\right)\ln\left(1+e^{\theta^Tx^{(i)}}\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2\end{align}\]</span> 对其求导： <span class="math display">\[\frac{\partial J}{\partial \theta_j}=\frac{1}{m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x^{(i)}_j+[j\neq 0]\frac{\lambda}{m}\theta_j,\quad j=0,1,\cdots,n\]</span> 所以梯度下降时，我们的迭代过程为： <span class="math display">\[\begin{align}\theta_0&amp;:=\theta_0-\alpha\frac{\partial J}{\partial\theta_j}\\&amp;=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x_0^{(i)}\\\theta_j&amp;:=\theta_j-\alpha \frac{\partial J}{\partial \theta_j}\\&amp;=\theta_j-\alpha \left[\frac{1}{m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x^{(i)}_j+\frac{\lambda}{m}\theta_j\right]\\&amp;=\theta_j\left(1-\alpha\frac{\lambda}{m}\right)-\alpha\frac{1}{m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x^{(i)}_j,\quad j=1,\cdots,n\end{align}\]</span></p><p><br></p><p>和线性回归一样，我们更希望将上面的式子写作矩阵形式。</p><blockquote><p>注意：以下表达式与 <code>numpy</code> 的矩阵形式相对应，即存在 "broadcast" 的表示方式，在数学上不一定严谨。</p></blockquote><h4 id="代价函数的矩阵形式">代价函数的矩阵形式</h4><p>回顾逻辑回归的代价函数： <span class="math display">\[J(\theta)=\frac{1}{m}\sum_{i=1}^m\left[-y^{(i)}\ln(h_\theta(x^{(i)}))-(1-y^{(i)})\ln(1-h_\theta(x^{(i)}))\right]\]</span> 其中， <span class="math display">\[h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}\]</span> 设： <span class="math display">\[X=\begin{bmatrix}(x^{(1)})^T\\(x^{(2)})^T\\\vdots\\(x^{(m)})^T\end{bmatrix}\quad \theta=\begin{bmatrix}\theta_0\\\theta_1\\\vdots\\\theta_n\end{bmatrix}\quad y=\begin{bmatrix}y^{(0)}\\y^{(1)}\\\vdots\\y^{(m)}\end{bmatrix}\]</span> 于是： <span class="math display">\[X\theta=\begin{bmatrix}(x^{(1)})^T\theta\\(x^{(2)})^T\theta\\\vdots\\(x^{(m)})^T\theta\end{bmatrix}=\begin{bmatrix}\theta^T(x^{(1)})\\\theta^T(x^{(2)})\\\vdots\\\theta^T(x^{(m)})\end{bmatrix}\]</span> 所以，设： <span class="math display">\[h&#39;_\theta(X)=1/(1+e^{-X\theta})\in\mathbb R^{m}\]</span> 于是： <span class="math display">\[J(\theta)=-\frac{1}{m}\left[y^T\ln(h&#39;_\theta(X))+(1-y^T)\ln(1-h&#39;_\theta(X))\right]\]</span></p><h4 id="代价函数偏导的矩阵形式">代价函数偏导的矩阵形式</h4><p>逻辑回归的代价函数的偏导函数向量： <span class="math display">\[\frac{\partial J}{\partial \theta}=\frac{1}{m}\sum_{i=1}^m\left[\left(h_\theta(x^{(i)})-y^{(i)}\right)x^{(i)}\right]=\frac{1}{m}X^T(h&#39;_\theta(X)-y)\]</span></p><h4 id="正则化后的矩阵形式">正则化后的矩阵形式</h4><p>回顾正则化后的逻辑回归代价函数及其偏导： <span class="math display">\[\begin{align}J(\theta)&amp;=\frac{1}{m}\sum_{i=1}^m\left[-y^{(i)}\ln(h_\theta(x^{(i)}))-(1-y^{(i)})\ln(1-h_\theta(x^{(i)}))\right]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2\\\frac{\partial J}{\partial \theta}&amp;=\frac{1}{m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x^{(i)}+\frac{\lambda}{m}\hat\theta\end{align}\]</span> 注意，这里 <span class="math inline">\(\hat\theta=\begin{bmatrix}0\\\theta_1\\\vdots\\\theta_n\end{bmatrix}\)</span>，即在 <span class="math inline">\(\theta\)</span> 中将 <span class="math inline">\(\theta_0\)</span> 换成 <span class="math inline">\(0\)</span>.</p><p>有了之前的推导，就容易写出正则化后的矩阵形式了： <span class="math display">\[\boxed{\begin{align}J(\theta)&amp;=-\frac{1}{m}\left[y^T\ln(h&#39;_\theta(X))+(1-y^T)\ln(1-h&#39;_\theta(X))\right]+\frac{\lambda}{2m}\hat\theta^T\hat\theta\\\frac{\partial J}{\partial \theta}&amp;=\frac{1}{m}X^T(h&#39;_\theta(X)-y)+\frac{\lambda}{m}\hat\theta\end{align}}\]</span> 这也是我们使用 <code>numpy</code> 实现正则化逻辑回归时的表达式。</p><h2 id="实现">实现</h2><p>首先还是来看一下数据集：</p><p><img src="Figure_0.png" width="50%" height="50%" /></p><p>显然，我们需要用一个多项式去进行逻辑回归，这里将数据的两维都扩充为 <span class="math inline">\(3\)</span> 次，形成有 <span class="math inline">\(16\)</span> 维特征的数据，然后进行逻辑回归。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>alpha = <span class="hljs-number">0.01</span><br>lamb = <span class="hljs-number">1</span><br>iteration = <span class="hljs-number">1000000</span><br>Z = []<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">h</span>(<span class="hljs-params">Theta, x</span>):<br><span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-np.matmul(Theta.T, x)[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">h_</span>(<span class="hljs-params">Theta, X</span>):<br><span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-np.matmul(X, Theta)))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">J_reg</span>(<span class="hljs-params">Theta, X, y</span>):<br>tmp = h_(Theta, X)<br><span class="hljs-keyword">return</span> (-(np.matmul(y.T, np.log(tmp)) + np.matmul(<span class="hljs-number">1</span> - y.T, np.log(<span class="hljs-number">1</span> - tmp))) / m\<br> + lamb / <span class="hljs-number">2</span> / m * np.matmul(Theta.T, Theta))[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partJ_reg</span>(<span class="hljs-params">Theta, X, y</span>):<br><span class="hljs-keyword">return</span> (np.matmul(X.T, h_(Theta, X) - y) + lamb * np.vstack((np.zeros((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)), Theta[<span class="hljs-number">1</span>:, :]))) / m<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">GradientDescent</span>(<span class="hljs-params">X, Y</span>):<br>T = np.zeros((n, <span class="hljs-number">1</span>))<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iteration):<br>T = T - alpha * partJ_reg(T, X, Y)<br>Z.append(J_reg(T, X, Y))<br><span class="hljs-keyword">return</span> T<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">extend</span>(<span class="hljs-params">x1, x2</span>):<br>X = []<br><span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>X.append(np.power(x1, j) * np.power(x2, k))<br><span class="hljs-keyword">return</span> X<br><br>data = np.genfromtxt(<span class="hljs-string">&quot;ex2data2.txt&quot;</span>, delimiter = <span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>Y = data[:, -<span class="hljs-number">1</span>:]<br>n = <span class="hljs-number">16</span><br>X = np.zeros((m, n))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>X[i] = extend(data[i][<span class="hljs-number">0</span>], data[i][<span class="hljs-number">1</span>])<br><br>T = GradientDescent(X, Y)<br><span class="hljs-built_in">print</span>(T.T)<br><span class="hljs-built_in">print</span>(J_reg(T, X, Y))<br><br><span class="hljs-comment">#============================== draw the picture ==============================#</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calc</span>(<span class="hljs-params">x1, x2</span>):<br>res = np.zeros((x1.shape[<span class="hljs-number">0</span>], x1.shape[<span class="hljs-number">1</span>]))<br><span class="hljs-keyword">for</span> ix <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(x1.shape[<span class="hljs-number">0</span>]):<br><span class="hljs-keyword">for</span> iy <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(x1.shape[<span class="hljs-number">1</span>]):<br>tmp = np.array(extend(x1[ix][iy], x2[ix][iy]), ndmin = <span class="hljs-number">2</span>)<br>res[ix][iy] = h(T, tmp.T)<br><span class="hljs-keyword">return</span> res<br><br>minx1, maxx1 = data[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">min</span>(), data[:, <span class="hljs-number">0</span>].<span class="hljs-built_in">max</span>()+<span class="hljs-number">0.1</span><br>minx2, maxx2 = data[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">min</span>(), data[:, <span class="hljs-number">1</span>].<span class="hljs-built_in">max</span>()+<span class="hljs-number">0.1</span><br>delta = <span class="hljs-number">0.01</span><br>x1, x2 = np.meshgrid(np.arange(minx1, maxx1, delta), np.arange(minx2, maxx2, delta))<br><br>p1 = plt.subplot(<span class="hljs-number">121</span>)<br>plt.contour(x1, x2, calc(x1, x2), [<span class="hljs-number">0.5</span>], colors = <span class="hljs-string">&#x27;magenta&#x27;</span>)<br><br>X0 = data[data[:, -<span class="hljs-number">1</span>] == <span class="hljs-number">0</span>, :]<br>X1 = data[data[:, -<span class="hljs-number">1</span>] == <span class="hljs-number">1</span>, :]<br>p1.set_title(<span class="hljs-string">&quot;lambda = &quot;</span> + <span class="hljs-built_in">str</span>(lamb))<br>p1.set_xlabel(<span class="hljs-string">&quot;Microchip test 1&quot;</span>)<br>p1.set_ylabel(<span class="hljs-string">&quot;Microchip test 2&quot;</span>)<br>p1.scatter(X0[:, <span class="hljs-number">0</span>:<span class="hljs-number">1</span>], X0[:, <span class="hljs-number">1</span>:<span class="hljs-number">2</span>], marker = <span class="hljs-string">&#x27;x&#x27;</span>, c = <span class="hljs-string">&#x27;r&#x27;</span>)<br>p1.scatter(X1[:, <span class="hljs-number">0</span>:<span class="hljs-number">1</span>], X1[:, <span class="hljs-number">1</span>:<span class="hljs-number">2</span>], marker = <span class="hljs-string">&#x27;o&#x27;</span>)<br><br>p2 = plt.subplot(<span class="hljs-number">122</span>)<br>p2.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, iteration+<span class="hljs-number">1</span>), Z)<br>p2.set_title(<span class="hljs-string">&quot;lambda = &quot;</span> + <span class="hljs-built_in">str</span>(lamb))<br>p2.set_xlabel(<span class="hljs-string">&quot;Iteration&quot;</span>)<br>p2.set_ylabel(<span class="hljs-string">&quot;Cost&quot;</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure><p>以下是 <span class="math inline">\(\lambda=0,1,5\)</span> 的回归结果和收敛情况：</p><p><img src="Figure_11.png" width="100%" height="100%" /></p><p><img src="Figure_12.png" width="100%" height="100%" /></p><p><img src="Figure_13.png" width="100%" height="100%" /></p><hr /><p>接下来我换了一个生成特征的方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">extend</span>(<span class="hljs-params">x1, x2</span>):<br>X = []<br><span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">6</span> + <span class="hljs-number">1</span>):<br><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(j + <span class="hljs-number">1</span>):<br>X.append(np.power(x1, k) * np.power(x2, j-k))<br><span class="hljs-keyword">return</span> X<br></code></pre></td></tr></table></figure><p>结果如下：</p><p><img src="Figure_21.png" width="100%" height="100%" /></p><p><img src="Figure_22.png" width="100%" height="100%" /></p><p><img src="Figure_23.png" width="100%" height="100%" /></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Andrew Ng</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[吴恩达机器学习]4·逻辑回归之二分类</title>
    <link href="/blog-main/2020/12/22/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-4%C2%B7%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B9%8B%E4%BA%8C%E5%88%86%E7%B1%BB/"/>
    <url>/blog-main/2020/12/22/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-4%C2%B7%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B9%8B%E4%BA%8C%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<p>吴恩达机器学习系列课程：<a href="https://www.bilibili.com/video/BV164411b7dx" class="uri">https://www.bilibili.com/video/BV164411b7dx</a></p><span id="more"></span><h2 id="二分类问题与逻辑回归的引入">二分类问题与逻辑回归的引入</h2><p>给定数据集： <span class="math display">\[\left\{\left(x^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}\]</span> 其中 <span class="math inline">\(x^{(i)}\)</span> 是一个 <span class="math inline">\(n\)</span> 维向量 <span class="math inline">\(\left(x_0^{(i)},\cdots,x_n^{(i)}\right)^T\)</span>，且 <span class="math inline">\(y^{(i)}\in\{0,1\}\)</span>，也即是对于输入 <span class="math inline">\(x^{(i)}\)</span>，我们将其分类为 <span class="math inline">\(0\)</span> 或 <span class="math inline">\(1\)</span> 两类。试用一模型拟合该分类结果。</p><p><br></p><p>鉴于 <span class="math inline">\(y\)</span> 取值的离散性，线性回归在这里不好使了，我们引入逻辑回归的概念。</p><p>回忆线性回归的假设函数：<span class="math inline">\(h_\theta(x)=\theta^Tx\)</span>，我们在其外套上 <span class="math inline">\(\text{sigmoid}\)</span> 函数，构造逻辑回归的假设函数为： <span class="math display">\[h_\theta(x)=g\left(\theta^Tx\right)=\frac{1}{1+e^{-\theta^T x}}\]</span></p><blockquote><p><span class="math inline">\(\text{sigmoid}\)</span> 函数： <span class="math display">\[g(z)=\frac{1}{1+e^{-z}}\]</span> 是一个介于 <span class="math inline">\((0,1)\)</span> 之间的单增 <span class="math inline">\(S\)</span> 形函数。</p></blockquote><p>也就是说，对于一个参数为 <span class="math inline">\(\theta\)</span> 的逻辑回归模型，输入 <span class="math inline">\(x\)</span>，得到 <span class="math inline">\(h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}\)</span> 的预测。我们可以把这个值视为 <span class="math inline">\(x\)</span> 这组数据对应的 <span class="math inline">\(y\)</span> 等于 <span class="math inline">\(1\)</span> 的概率，如果它 <span class="math inline">\(\geqslant 0.5\)</span>，则分类为 <span class="math inline">\(1\)</span>，否则分类为 <span class="math inline">\(0\)</span>.</p><p>又根据 <span class="math inline">\(\text{sigmoid}\)</span> 函数的性质，<span class="math inline">\(h_\theta(x)\geqslant 0.5\iff \theta^Tx\geqslant0\)</span>. 所以只要 <span class="math inline">\(\theta^Tx\geqslant0\)</span>，就分类为 <span class="math inline">\(1\)</span>，否则分类为 <span class="math inline">\(0\)</span>；于是乎，<span class="math inline">\(\theta^Tx=0\)</span> 这条线（超平面）被称作决策边界，它将整个空间划分成两块，各自属于一个分类。</p><h2 id="代价函数">代价函数</h2><p>现在，我们的任务就是从数据集中求解逻辑回归的参数 <span class="math inline">\(\theta\)</span>. 仍然采用代价函数的思想——找到使代价最小的参数即可。</p><p>广义上来讲，代价函数是这样的一个函数： <span class="math display">\[J(\theta)=\frac{1}{m}\sum_{i=1}^m\text{Cost}\left(h_\theta(x^{(i)}),y^{(i)}\right)\]</span> 也就是说用每个数据的估计值 <span class="math inline">\(h_\theta(x^{(i)})\)</span> 和真实值 <span class="math inline">\(y^{(i)}\)</span> 计算一个代价 <span class="math inline">\(\text{Cost}\left(h_\theta(x^{(i)}),y^{(i)}\right)\)</span>，比如线性回归中这个代价就是二者差值的平方。</p><p><br></p><p>具体到逻辑回归里，这个代价定义为： <span class="math display">\[\begin{align}\text{Cost}\left(h_\theta(x),y\right)&amp;=\begin{cases}-\ln(h_\theta(x))&amp;y=1\\-\ln(1-h_\theta(x))&amp;y=0\end{cases}\\&amp;=-y\ln(h_\theta(x))-(1-y)\ln(1-h_\theta(x))\\&amp;=y\ln\left(1+e^{-\theta^Tx}\right)+(1-y)\ln\left(1+e^{\theta^Tx}\right)\end{align}\]</span></p><blockquote><p>上式的来源：</p><p>前文已经提到，<span class="math inline">\(h_\theta(x)=\mathbb P(y=1)\)</span>，于是 <span class="math inline">\(1-h_\theta(x)=\mathbb P(y=0)\)</span>，故： <span class="math display">\[\mathbb P(y=k)=[h_\theta(x)]^k[1-h_\theta(x)]^{1-k},\quad k\in\{0,1\}\]</span> 考虑<strong>极大似然法</strong>，在数据集 <span class="math inline">\(\left\{\left(x^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}\)</span> 下，似然函数为： <span class="math display">\[L(\theta)=\prod_{i=1}^m\mathbb P\left(y=y^{(i)}\right)=\prod_{i=1}^m\left[h_\theta(x^{(i)})\right]^{y^{(i)}}\left[1-h_\theta(x^{(i)})\right]^{1-y^{(i)}}\]</span> 取对数得到： <span class="math display">\[\ln L(\theta)=\sum_{i=1}^my^{(i)}\ln(h_\theta(x^{(i)}))+(1-y^{(i)})\ln(1-h_\theta(x^{(i)}))\]</span> 注意到，极大似然法的目标是找到 <span class="math inline">\(L(\theta)\)</span> 或 <span class="math inline">\(\ln L(\theta)\)</span> 的极大值，而逻辑回归的目标是找到 <span class="math inline">\(J(\theta)\)</span> 的极小值，所以自然的，我们直接使用 <span class="math inline">\(\ln L(\theta)\)</span> 来定义 <span class="math inline">\(J(\theta)\)</span>： <span class="math display">\[J(\theta)=-\frac{1}{m}\ln L(\theta)\]</span> 这个 <span class="math inline">\(\frac{1}{m}\)</span> 对极大值/极小值没有影响，仅是取一下平均罢了。</p><p>换句话说，<strong>逻辑回归的本质是拟合 <span class="math inline">\(y\)</span> 取值的概率，并以极大似然法解之。</strong></p></blockquote><h2 id="梯度下降解逻辑回归">梯度下降解逻辑回归</h2><p>我们对逻辑回归定义的代价函数是非常好的：它是一个凸函数。这有助于我们进行梯度下降求解。</p><p>为了求偏导，我们先计算： <span class="math display">\[\begin{align}\frac{\partial}{\partial\theta}\text{Cost}(h_\theta(x), y)&amp;=\frac{\partial}{\partial \theta}\left[y\ln\left(1+e^{-\theta^Tx}\right)+(1-y)\ln\left(1+e^{\theta^Tx}\right)\right]\\&amp;=\frac{-yxe^{-\theta^Tx}}{1+e^{-\theta^Tx}}+\frac{(1-y)xe^{\theta^T x}}{1+e^{\theta^T x}}\\&amp;=\frac{-yx+(1-y)xe^{\theta^Tx}}{1+e^{\theta^Tx}}\\&amp;=\left(-y+\frac{1}{1+e^{-\theta^Tx}}\right)x\\&amp;=(h_\theta(x)-y)x\end{align}\]</span> 于是乎， <span class="math display">\[\frac{\partial J}{\partial\theta}=\frac{1}{m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)x^{(i)}\]</span> 没错，这个偏导的形式和线性回归的偏导形式完全相同！不同的只是 <span class="math inline">\(h_\theta(x)\)</span> 的定义——逻辑回归的假设函数在线性回归的假设函数外套了一层 <span class="math inline">\(\text{sigmoid}\)</span> 函数，也正是这一层 <span class="math inline">\(\text{sigmoid}\)</span> 函数，让我们不能像线性回归那样直接给出解析解，而必须使用梯度下降等方法。</p><p>现在我们对其使用梯度下降即可。</p><h2 id="实现">实现</h2><p>首先看一下数据的散点图：</p><p><img src="Figure_1.png" width="50%" height="50%" /></p><p>python 实现逻辑回归如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> math<br><br>alpha = <span class="hljs-number">0.01</span><br>iteration = <span class="hljs-number">10000</span><br>Z = []<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">Normalization</span>(<span class="hljs-params">data</span>):<br><span class="hljs-keyword">return</span> (data - data.mean(axis = <span class="hljs-number">0</span>)) / data.std(axis = <span class="hljs-number">0</span>, ddof = <span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">h</span>(<span class="hljs-params">T, x</span>):<br><span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.e ** (-np.matmul(T.T, x)[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">J</span>(<span class="hljs-params">T, X, Y</span>):<br>res = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>res -= Y[i] * math.log(h(T, X[i:i+<span class="hljs-number">1</span>, :].T)) + \<br>(<span class="hljs-number">1</span> - Y[i]) * math.log(<span class="hljs-number">1</span> - h(T, X[i:i+<span class="hljs-number">1</span>, :].T))<br>res /= m<br><span class="hljs-keyword">return</span> res<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partJ</span>(<span class="hljs-params">T, X, Y</span>):<br>res = np.zeros((n, <span class="hljs-number">1</span>))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>res += (h(T, X[i:i+<span class="hljs-number">1</span>, :].T) - Y[i]) * X[i:i+<span class="hljs-number">1</span>, :].T<br>res /= m<br><span class="hljs-keyword">return</span> res<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">GradientDescent</span>(<span class="hljs-params">X, Y</span>):<br>T = np.zeros((n, <span class="hljs-number">1</span>))<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iteration):<br>T = T - alpha * partJ(T, X, Y)<br>Z.append(J(T, X, Y))<br><span class="hljs-keyword">return</span> T<br><br>data = np.genfromtxt(<span class="hljs-string">&quot;ex2data1.txt&quot;</span>, delimiter = <span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>data[:, :-<span class="hljs-number">1</span>] = Normalization(data[:, :-<span class="hljs-number">1</span>])<br>X = np.column_stack((np.ones((m, <span class="hljs-number">1</span>)), data[:, :-<span class="hljs-number">1</span>]))<br>Y = data[:, -<span class="hljs-number">1</span>]<br>T = GradientDescent(X, Y)<br><span class="hljs-built_in">print</span>(T)<br><span class="hljs-built_in">print</span>(J(T, X, Y))<br><br>p1 = plt.subplot(<span class="hljs-number">111</span>)<br>p1.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, iteration+<span class="hljs-number">1</span>), Z)<br>p1.set_xlabel(<span class="hljs-string">&quot;Iteration&quot;</span>)<br>p1.set_ylabel(<span class="hljs-string">&quot;Cost&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>在学习率为 <span class="math inline">\(0.01\)</span> 下迭代了 <span class="math inline">\(10000\)</span> 次，结果为：<span class="math inline">\(\theta=(1.2677702,\,3.05550587,\,2.81891901)^T\)</span>，此时 <span class="math inline">\(J(\theta)=0.21065763610049573\)</span>. 决策边界的图像为：</p><p><img src="Figure_3.png" width="50%" height="50%" /></p><p><span class="math inline">\(J(\theta)\)</span> 随迭代次数收敛情况如下：</p><p><img src="Figure_2.png" width="50%" height="50%" /></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Andrew Ng</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[吴恩达机器学习]3·正规方程解多元线性回归</title>
    <link href="/blog-main/2020/12/22/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3%C2%B7%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E8%A7%A3%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <url>/blog-main/2020/12/22/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3%C2%B7%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E8%A7%A3%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<p>吴恩达机器学习系列课程：<a href="https://www.bilibili.com/video/BV164411b7dx" class="uri">https://www.bilibili.com/video/BV164411b7dx</a></p><span id="more"></span><h2 id="正规方程">正规方程</h2><p>说白了，这就是用我们在微积分中学习的多元微分学知识直接解出答案。</p><p>对于代价函数： <span class="math display">\[J(\theta)=J(\theta_0, \theta_1,\cdots, \theta_n)\]</span> 如果它是连续的，那么要求出它的最小值，只需要令各偏导为零，就能得到 <span class="math inline">\(\theta\)</span> 的值： <span class="math display">\[\frac{\partial J}{\partial \theta_j}=0,\quad j=0,1,\cdots,n\]</span> 或写作向量形式： <span class="math display">\[\frac{\partial J}{\partial \theta}=\vec 0\]</span> 下面我们就来对多元线性回归的代价函数解一解。</p><p><br></p><p>多元线性回归的代价函数为： <span class="math display">\[J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)^2\]</span> 于是其偏导函数为： <span class="math display">\[\frac{\partial J}{\partial \theta}=\frac{1}{m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}\]</span> 要使之为零向量，只能是： <span class="math display">\[\theta^Tx^{(i)}=y^{(i)},\quad i=1,2,\cdots,m\]</span> 恒成立。写作矩阵为： <span class="math display">\[X\theta=y\]</span> 其中， <span class="math display">\[X=\begin{bmatrix}x_0^{(1)}&amp;x_1^{(1)}&amp;\cdots&amp; x_n^{(1)}\\x_0^{(2)}&amp;x_1^{(2)}&amp;\cdots&amp; x_n^{(2)}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\x_0^{(m)}&amp;x_1^{(m)}&amp;\cdots&amp; x_n^{(m)}\end{bmatrix}=\begin{bmatrix}{x^{(1)}}^T\\{x^{(2)}}^T\\\vdots\\{x^{(m)}}^T\\\end{bmatrix},\quad y=\begin{bmatrix}y^{(1)}\\y^{(2)}\\\vdots\\y^{(m)}\end{bmatrix}\]</span> 两边同时乘以 <span class="math inline">\(X^T\)</span>，假设 <span class="math inline">\(X^TX\)</span> 可逆，解得： <span class="math display">\[\theta=(X^TX)^{-1}X^Ty\]</span> 这就是数学上多元线性回归方程的精确解。</p><p><br></p><p>这里，<span class="math inline">\(X^TX\)</span> 是一个 <span class="math inline">\((n+1)\times(n+1)\)</span> 的矩阵，因此直接计算 <span class="math inline">\(\theta\)</span> 的复杂度是 <span class="math inline">\(O(n^3)\)</span> 的，如果 <span class="math inline">\(n\)</span> 不是很大，这是有效的，但是如果 <span class="math inline">\(n\)</span> 达到了 <span class="math inline">\(10^4,10^5\)</span> 或更高级别，就需要使用梯度下降了。</p><h3 id="实现">实现</h3><p>仍然对第二篇中的多元线性回归数据进行求解。</p><p>代码很简洁：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">J</span>(<span class="hljs-params">T, X, Y</span>):<br>res = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>res += (np.matmul(T.T, X[i:i+<span class="hljs-number">1</span>, :].T) - Y[i:i+<span class="hljs-number">1</span>, :]) ** <span class="hljs-number">2</span><br>res /= <span class="hljs-number">2</span> * m;<br><span class="hljs-keyword">return</span> res<br><br>data = np.genfromtxt(<span class="hljs-string">&quot;ex1data2.txt&quot;</span>, delimiter = <span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>X = np.column_stack((np.ones((m, <span class="hljs-number">1</span>)), data[:, :-<span class="hljs-number">1</span>]))<br>Y = data[:, -<span class="hljs-number">1</span>:]<br>T = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T), Y)<br><span class="hljs-built_in">print</span>(T)<br><span class="hljs-built_in">print</span>(J(T, X, Y))<br></code></pre></td></tr></table></figure><p>很快给出了结果：<span class="math inline">\(\theta=(89597.9095428,\,139.21067402,\,-8738.01911233)^T\)</span>.</p><h2 id="不可逆情形">不可逆情形</h2><p>前一节的推导基于 <span class="math inline">\(X^TX\)</span> 可逆的假设，如若不可逆，我们只需将代码中的 <code>inv()</code> 换成 <code>pinv()</code> 求出<strong>伪逆矩阵</strong>即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">J</span>(<span class="hljs-params">T, X, Y</span>):<br>res = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>res += (np.matmul(T.T, X[i:i+<span class="hljs-number">1</span>, :].T) - Y[i:i+<span class="hljs-number">1</span>, :]) ** <span class="hljs-number">2</span><br>res /= <span class="hljs-number">2</span> * m;<br><span class="hljs-keyword">return</span> res<br><br>data = np.genfromtxt(<span class="hljs-string">&quot;ex1data2.txt&quot;</span>, delimiter = <span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>X = np.column_stack((np.ones((m, <span class="hljs-number">1</span>)), data[:, :-<span class="hljs-number">1</span>]))<br>Y = data[:, -<span class="hljs-number">1</span>:]<br>T = np.matmul(np.matmul(np.linalg.pinv(np.matmul(X.T, X)), X.T), Y)<br><span class="hljs-built_in">print</span>(T)<br><span class="hljs-built_in">print</span>(J(T, X, Y))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Andrew Ng</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[吴恩达机器学习]2·梯度下降解多元线性回归</title>
    <link href="/blog-main/2020/12/21/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2%C2%B7%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%A7%A3%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <url>/blog-main/2020/12/21/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2%C2%B7%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%A7%A3%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<p>吴恩达机器学习系列课程：<a href="https://www.bilibili.com/video/BV164411b7dx" class="uri">https://www.bilibili.com/video/BV164411b7dx</a></p><span id="more"></span><h2 id="多元线性回归">多元线性回归</h2><p>类似于一元的线性回归，不过我们现在有多个自变量 <span class="math inline">\(x_1,x_2,\cdots,x_n\)</span>，即给定的数据集为： <span class="math display">\[\left\{\left(x_1^{(i)},x_2^{(i)},\cdots,x_n^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}\]</span> 相应地，回归方程也具有多个参数 <span class="math inline">\(\theta_0,\theta_1,\cdots,\theta_n\)</span>： <span class="math display">\[h_\theta(x)=\theta^Tx=\theta_0x_0+\cdots+\theta_nx_n\]</span> 这里我们假定 <span class="math inline">\(x_0\)</span> 恒等于 <span class="math inline">\(1\)</span>，并以向量表示自变量和参数：<span class="math inline">\(\theta=(\theta_0,\cdots,\theta_n)^T,\;x=(x_0,\cdots,x_n)^T\)</span>.</p><h2 id="梯度下降解多元线性回归">梯度下降解多元线性回归</h2><p>类似的，我们定义代价函数： <span class="math display">\[J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)^2\]</span> 于是， <span class="math display">\[\frac{\partial J}{\partial \theta}=\frac{1}{m}\sum_{i=1}^m\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}\]</span> 梯度下降时，不断作迭代： <span class="math display">\[\theta:=\theta-\alpha\cdot\frac{\partial J}{\partial \theta}\]</span> 即可。</p><h3 id="特征缩放与标准化">特征缩放与标准化</h3><p>当我们的不同自变量取值范围相差较大时，梯度下降可能会很慢，这时，我们需要把所有自变量进行缩放、标准化。具体的，只要我们置： <span class="math display">\[x_i^{(j)}:=\frac{x_i^{(j)}-\mu_i}{\sigma_i}\]</span> 其中，<span class="math inline">\(\mu_i=\frac{1}{m}\sum\limits_{j=1}^m x_i^{(j)}\)</span> 是样本均值，<span class="math inline">\(\sigma_i=\sqrt{\frac{\sum\limits_{j=1}^m\left(x_i^{(j)}-\mu_i\right)^2}{m-1}}\)</span> 是样本标准差，就完成了归一化。</p><p>归一化后样本均值为 <span class="math inline">\(0\)</span>，方差为 <span class="math inline">\(1\)</span>.</p><h3 id="实现">实现</h3><p><code>Normalization</code> 函数将数据集标准化，<code>J</code> 函数即计算 <span class="math inline">\(J(\theta)\)</span>，<code>partJ</code> 函数计算 <span class="math inline">\(\frac{\partial J}{\partial\theta}\)</span>，<code>GradientDescent</code> 进行梯度下降。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>alpha = <span class="hljs-number">0.01</span><br>iteration = <span class="hljs-number">10000</span><br>Z = []<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">Normalization</span>(<span class="hljs-params">data</span>):<br><span class="hljs-keyword">return</span> (data - data.mean(axis = <span class="hljs-number">0</span>)) / data.std(axis = <span class="hljs-number">0</span>, ddof = <span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">J</span>(<span class="hljs-params">T, X, Y</span>):<br>res = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>res += (np.matmul(T.T, X[i:i+<span class="hljs-number">1</span>, :].T) - Y[i:i+<span class="hljs-number">1</span>, :]) ** <span class="hljs-number">2</span><br>res /= <span class="hljs-number">2</span> * m;<br><span class="hljs-keyword">return</span> res<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">partJ</span>(<span class="hljs-params">T, X, Y</span>):<br>res = np.zeros((n, <span class="hljs-number">1</span>))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(m):<br>res += (np.matmul(T.T, X[i:i+<span class="hljs-number">1</span>, :].T) - Y[i:i+<span class="hljs-number">1</span>, :]) * X[i:i+<span class="hljs-number">1</span>, :].T<br>res /= m<br><span class="hljs-keyword">return</span> res<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">GradientDescent</span>(<span class="hljs-params">X, Y</span>):<br>T = np.zeros((n, <span class="hljs-number">1</span>))<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iteration):<br>T = T - alpha * partJ(T, X, Y)<br>Z.append(J(T, X, Y)[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>])<br><span class="hljs-keyword">return</span> T<br><br>data = np.genfromtxt(<span class="hljs-string">&quot;ex1data2.txt&quot;</span>, delimiter = <span class="hljs-string">&#x27;,&#x27;</span>)<br>(m, n) = data.shape<br>data = Normalization(data)<br>X = np.column_stack((np.ones((m, <span class="hljs-number">1</span>)), data[:, :-<span class="hljs-number">1</span>]))<br>Y = data[:, -<span class="hljs-number">1</span>:]<br>T = GradientDescent(X, Y)<br><span class="hljs-built_in">print</span>(T)<br><br>p1 = plt.subplot(<span class="hljs-number">111</span>)<br>p1.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, iteration+<span class="hljs-number">1</span>), Z)<br>p1.set_xlabel(<span class="hljs-string">&#x27;Iteration&#x27;</span>)<br>p1.set_ylabel(<span class="hljs-string">&#x27;Cost&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>最后得到的结果：<span class="math inline">\(\theta=(-1.11051830\times10^{-16},8.84765988\times10^{-1},-5.31788197\times10^{-2})^T\)</span>.</p><p>学习率取为 <span class="math inline">\(0.01\)</span> 时，代价函数值随迭代次数的变化：</p><p><img src="Figure_1.png" width="50%" height="50%" /></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Andrew Ng</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[吴恩达机器学习]1·梯度下降解一元线性回归</title>
    <link href="/blog-main/2020/12/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1%C2%B7%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%A7%A3%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <url>/blog-main/2020/12/20/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1%C2%B7%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%A7%A3%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<p>吴恩达机器学习系列课程：<a href="https://www.bilibili.com/video/BV164411b7dx" class="uri">https://www.bilibili.com/video/BV164411b7dx</a></p><span id="more"></span><h2 id="一元线性回归">一元线性回归</h2><p>给定数据集 <span class="math display">\[\left\{\left(x^{(i)},y^{(i)}\right),\;i=1,2,\cdots,m\right\}\]</span> 试用一线性函数 <span class="math inline">\(y=\theta_1x+\theta_0\)</span> 拟合之。当然，在数学上，我们熟知最小二乘法可以解决这个问题。</p><h2 id="梯度下降法">梯度下降法</h2><p>这是我自学机器学习中遇到的第一个算法，其基本思想很简单。对于代价函数 <span class="math inline">\(J(\theta_0,\theta_1)\)</span>，我们想得到它的一个极小值，只需要从任意点开始，选择函数 <span class="math inline">\(J\)</span> 的梯度方向的逆方向，也即方向导数最大的方向移动一点点，然后不断重复这个过程。</p><p>我们知道梯度定义为： <span class="math display">\[\text{grad }J=\left\{\frac{\partial J}{\partial \theta_0},\frac{\partial J}{\partial \theta_1}\right\}\]</span> 所以不断迭代进行： <span class="math display">\[\theta_j:=\theta_j-\alpha\cdot\frac{\partial J}{\partial\theta_j}\]</span> 即可。其中，<span class="math inline">\(\alpha\)</span> 就是这一步的长度，也称为学习率。</p><p>学习率的选取很重要，过小则梯度下降很慢，过大则有可能不收敛。</p><h2 id="梯度下降解一元线性回归">梯度下降解一元线性回归</h2><p>应用在解一元线性回归问题上，我们定义代价函数为： <span class="math display">\[J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^m\left(\theta_1x^{(i)}+\theta_0-y^{(i)}\right)^2\]</span> 也即是每个数据纵坐标的真实值与估计值的差的平方和的平均，除以 <span class="math inline">\(2\)</span> 仅是为了后续求导方便，没有什么本质的影响。</p><p>由于： <span class="math display">\[\text{grad }J=\left\{\frac{\partial J}{\partial \theta_0},\frac{\partial J}{\partial \theta_1}\right\}=\left\{\frac{1}{m}\sum_{i=1}^m\left(\theta_1x^{(i)}+\theta_0-y^{(i)}\right),\frac{1}{m}\sum_{i=1}^mx_i\left(\theta_1x^{(i)}+\theta_0-y^{(i)}\right)\right\}\]</span> 于是每一次按照该方向的逆方向走一小步即可。</p><h3 id="c-实现">C++ 实现</h3><p>对于这个问题，我们可以通过预处理达到每次迭代 <span class="math inline">\(O(1)\)</span> 地更新。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;bits/stdc++.h&gt;</span></span><br><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-type">const</span> <span class="hljs-type">double</span> eps = <span class="hljs-number">1e-8</span>;<br><span class="hljs-type">const</span> <span class="hljs-type">double</span> alpha = <span class="hljs-number">0.01</span>;<br><br><span class="hljs-type">int</span> m;<br><span class="hljs-type">double</span> sumx, sumy, sumxy, sumx2;<br><span class="hljs-type">double</span> th0, th1;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br><span class="hljs-built_in">freopen</span>(<span class="hljs-string">&quot;ex1data1.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, stdin);<br><span class="hljs-type">double</span> x, y;<br><span class="hljs-keyword">while</span>(<span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%lf,%lf&quot;</span>, &amp;x, &amp;y) != EOF)<br>m++, sumx += x, sumy += y, sumx2 += x * x, sumxy += x * y;<br>th0 = th1 = <span class="hljs-number">0</span>;<br><span class="hljs-keyword">while</span>(<span class="hljs-number">1</span>)&#123;<br><span class="hljs-type">double</span> nth0, nth1;<br>nth0 = th0 - alpha * <span class="hljs-number">1.0</span> / m * (th1 * sumx - sumy + m * th0);<br>nth1 = th1 - alpha * <span class="hljs-number">1.0</span> / m * (th1 * sumx2 - sumxy + sumx * th0);<br><span class="hljs-keyword">if</span>(<span class="hljs-built_in">fabs</span>(th0 - nth0) &lt; eps &amp;&amp; <span class="hljs-built_in">fabs</span>(th1 - nth1) &lt; eps)<span class="hljs-keyword">break</span>;<br>th0 = nth0, th1 = nth1;<br>&#125;<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%f %f\n&quot;</span>, th0, th1);<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>得到的结果是：<span class="math inline">\(\theta_0=-3.895775,\,\theta_1=1.193033\)</span>.</p><p><br></p><p>如果把 <span class="math inline">\(\alpha\)</span> 设为 <span class="math inline">\(0.1\)</span>，上面的程序无法得到结果，这是因为步长太大以至于无法收敛，除了人为调小 <span class="math inline">\(\alpha\)</span> 以外，我尝试了让程序自适应地调整 <span class="math inline">\(\alpha\)</span>. 具体地，如果 <span class="math inline">\(\alpha\)</span> 设置合理，我们的代价函数应该是一个随着迭代次数单调递减的函数，所以倘若代价函数出现了增加，我们就需要调小 <span class="math inline">\(\alpha\)</span>，基于这个思想，我写下了如下的程序：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;bits/stdc++.h&gt;</span></span><br><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">105</span>;<br><span class="hljs-type">const</span> <span class="hljs-type">double</span> eps = <span class="hljs-number">1e-8</span>;<br><span class="hljs-type">double</span> alpha = <span class="hljs-number">100</span>;<br><br><span class="hljs-type">int</span> m;<br><span class="hljs-type">double</span> x[N], y[N];<br><span class="hljs-type">double</span> th0, th1;<br><br><span class="hljs-function"><span class="hljs-keyword">inline</span> <span class="hljs-type">double</span> <span class="hljs-title">gradJ</span><span class="hljs-params">(<span class="hljs-type">int</span> k)</span></span>&#123;<br><span class="hljs-type">double</span> res = <span class="hljs-number">0</span>;<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt;= m; i++)<br>res += (th1 * x[i] + th0 - y[i]) * (k == <span class="hljs-number">1</span> ? x[i] : <span class="hljs-number">1</span>);<br><span class="hljs-keyword">return</span> res / m;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">inline</span> <span class="hljs-type">double</span> <span class="hljs-title">calc</span><span class="hljs-params">()</span></span>&#123;<br><span class="hljs-type">double</span> res = <span class="hljs-number">0</span>;<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt;= m; i++)<br>res += (th1 * x[i] + th0 - y[i]) * (th1 * x[i] + th0 - y[i]);<br><span class="hljs-keyword">return</span> res / m / <span class="hljs-number">2</span>;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br><span class="hljs-built_in">freopen</span>(<span class="hljs-string">&quot;ex1data1.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, stdin);<br><span class="hljs-keyword">while</span>(<span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%lf,%lf&quot;</span>, &amp;x[<span class="hljs-number">0</span>], &amp;y[<span class="hljs-number">0</span>]) != EOF)<br>m++, x[m] = x[<span class="hljs-number">0</span>], y[m] = y[<span class="hljs-number">0</span>];<br>th0 = th1 = <span class="hljs-number">0</span>;<br><span class="hljs-type">double</span> preJ = <span class="hljs-number">1e9</span>, J = <span class="hljs-number">0</span>;<br><span class="hljs-keyword">while</span>(<span class="hljs-number">1</span>)&#123;<br><span class="hljs-type">double</span> nth0, nth1;<br>nth0 = th0 - alpha * <span class="hljs-built_in">gradJ</span>(<span class="hljs-number">0</span>);<br>nth1 = th1 - alpha * <span class="hljs-built_in">gradJ</span>(<span class="hljs-number">1</span>);<br><span class="hljs-keyword">if</span>(<span class="hljs-built_in">fabs</span>(th0 - nth0) &lt; eps &amp;&amp; <span class="hljs-built_in">fabs</span>(th1 - nth1) &lt; eps)<span class="hljs-keyword">break</span>;<br>th0 = nth0, th1 = nth1;<br>J = <span class="hljs-built_in">calc</span>();<br><span class="hljs-keyword">if</span>(J &gt; preJ)alpha /= <span class="hljs-number">5</span>;<br>preJ = J;<br>&#125;<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%f\n%f %f\n&quot;</span>, alpha, th0, th1);<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>最终得到的结果：<span class="math inline">\(\alpha=0.0064,\,\theta_0=-3.895772,\,\theta_1=1.193033\)</span>.</p><h3 id="python-实现">Python 实现</h3><p>Python可以更方便地作图，也是现在机器学习中最热门的语言。</p><p>首先看一下原数据的散点图：</p><p><img src="Figure_1.png" width="50%" height="50%" /></p><p>如下是学习率设置为 <span class="math inline">\(0.01\)</span> 的 python 代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>X = []<br>Y = []<br>m = <span class="hljs-number">0</span><br>alpha = <span class="hljs-number">0.01</span><br>eps = <span class="hljs-number">1e-8</span><br>th0, th1 = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradJ</span>(<span class="hljs-params">k</span>):<br>res = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(X, Y):<br>res += (th1 * x + th0 - y) * (x <span class="hljs-keyword">if</span> k == <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1</span>)<br><span class="hljs-keyword">return</span> res / m<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calc</span>():<br>res = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(X, Y):<br>res += (th1 * x + th0 - y) * (th1 * x + th0 - y)<br><span class="hljs-keyword">return</span> res / <span class="hljs-number">2</span> / m<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;ex1data1.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> infile:<br>data = infile.readlines()<br><span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> data:<br>x, y = line.split(<span class="hljs-string">&#x27;,&#x27;</span>)<br>X.append(<span class="hljs-built_in">float</span>(x))<br>Y.append(<span class="hljs-built_in">float</span>(y))<br>m += <span class="hljs-number">1</span><br><br><br><span class="hljs-keyword">while</span> <span class="hljs-number">1</span>:<br>nth0 = th0 - alpha * gradJ(<span class="hljs-number">0</span>)<br>nth1 = th1 - alpha * gradJ(<span class="hljs-number">1</span>)<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">abs</span>(th0 - nth0) &lt; eps <span class="hljs-keyword">and</span> <span class="hljs-built_in">abs</span>(th1 - nth1) &lt; eps:<br><span class="hljs-keyword">break</span><br>th0 = nth0<br>th1 = nth1<br><span class="hljs-built_in">print</span>(th0, th1)<br>plt.scatter(X, Y)<br>plt.plot([<span class="hljs-built_in">min</span>(X), <span class="hljs-built_in">max</span>(X)], [th0+<span class="hljs-built_in">min</span>(X)*th1, th0+<span class="hljs-built_in">max</span>(X)*th1], c=<span class="hljs-string">&quot;magenta&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>得到的结果是：<span class="math inline">\(\theta_0=-3.895775334348606,\,\theta_1=1.193033087238351\)</span>，作图如下：</p><p><img src="Figure_2.png" width="50%" height="50%" /></p><p>洋红色的线条就是我们拟合的回归曲线。</p><p><br></p><p>之前说过，学习率过大会导致答案不收敛，也就是 <span class="math inline">\(J(\theta_0,\theta_1)\)</span> 可能会随着迭代次数的增加而增大，我们可以作图验证：</p><p><img src="Figure_3.png" width="100%" height="100%" /></p><p>可以看见，学习率在 <span class="math inline">\(0.025\)</span> 时 <span class="math inline">\(J\)</span> 发散，而 <span class="math inline">\(0.02\)</span> 和 <span class="math inline">\(0.01\)</span> 下是收敛的。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>Andrew Ng</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[具体数学]第六章·特殊的数（第二部分）</title>
    <link href="/blog-main/2020/08/29/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E5%85%AD%E7%AB%A0%C2%B7%E7%89%B9%E6%AE%8A%E7%9A%84%E6%95%B0%EF%BC%88%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%89/"/>
    <url>/blog-main/2020/08/29/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E5%85%AD%E7%AB%A0%C2%B7%E7%89%B9%E6%AE%8A%E7%9A%84%E6%95%B0%EF%BC%88%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<blockquote><p>第六章·特殊的数分7节，包括斯特林数、欧拉数、调和数、调和求和法、伯努利数、斐波那契数和连项式的内容。这是笔记第二部分，包括调和数、调和求和法、伯努利数。</p></blockquote><h2 id="调和数">调和数</h2><p><span class="math display">\[\color{purple}{H_n=1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}=\sum_{k=1}^n\frac{1}{k}}\]</span></p><h3 id="从两个例子引入">从两个例子引入</h3><h4 id="例一">例一</h4><p>将 <span class="math inline">\(n\)</span> 张长度为 <span class="math inline">\(2\)</span> 纸牌摞在桌边，尽可能地向外伸出，求伸出长度。</p><p>我们放置牌的方法应是：上面 <span class="math inline">\(k\)</span> 张牌的重心恰好在第 <span class="math inline">\(k+1\)</span> 张牌的边缘处。设从上往下数第 <span class="math inline">\(k\)</span> 张牌的边缘到第 <span class="math inline">\(1\)</span> 张牌的边缘距离为 <span class="math inline">\(d_k\)</span>（<span class="math inline">\(d_1=0\)</span>），则： <span class="math display">\[d_{k+1}=\frac{(d_1+1)+(d_2+1)+\cdots+(d_k+1)}{k}\]</span> 这是一个 <span class="math inline">\(k\)</span> 阶递推式，用经典的<strong>错位相减</strong>可以将其变成 <span class="math inline">\(1\)</span> 阶递推式： <span class="math display">\[\begin{align}&amp;kd_{k+1}-(k-1)d_k=\sum_{i=1}^k(d_k+1)-\sum_{i=1}^{k-1}(d_k+1)=d_k+1\\\implies &amp;d_{k+1}=d_k+\frac{1}{k}\\\implies&amp;d_{k+1}=\frac{1}{k}+\frac{1}{k-1}+\cdots+1=H_k\end{align}\]</span></p><h4 id="例二">例二</h4><p>一只蠕虫从一根 <span class="math inline">\(1\text{m}\)</span> 长的橡皮筋的一段开始爬行，每分钟爬行 <span class="math inline">\(1\text{cm}\)</span>，同时橡皮筋被拉长 <span class="math inline">\(1\text{m}\)</span>（拉长时，蠕虫在橡皮筋上到起点和终点的<strong>比例</strong>不变）。蠕虫是否能到达终点？</p><p>我们这样考虑：第 <span class="math inline">\(1\)</span> 分钟后，蠕虫在橡皮筋的 <span class="math inline">\(\frac{1}{100}\)</span> 处，第 <span class="math inline">\(2\)</span> 分钟后，蠕虫在橡皮筋的 <span class="math inline">\(\frac{1}{100}+\frac{1}{200}\)</span> 处……第 <span class="math inline">\(n\)</span> 分钟后，蠕虫在橡皮筋的 <span class="math inline">\(\frac{1}{100}H_n\)</span> 处。由于 <span class="math inline">\(H_n\)</span> 是发散的，蠕虫终会达到终点。</p><h3 id="调和数与斯特林数的联系">调和数与斯特林数的联系</h3><p><span class="math display">\[\newcommand{\stra}[2]{\begin{bmatrix}#1\\#2\end{bmatrix}}\newcommand{\strb}[2]{\begin{Bmatrix}#1\\#2\end{Bmatrix}}\]</span></p><p>回忆<strong>第一类斯特林数</strong>：<span class="math inline">\(\stra{n}{k}\)</span> 表示 <span class="math inline">\(n\)</span> 个物品的 <span class="math inline">\(k\)</span> 轮换数，满足递归式：<span class="math inline">\(\stra{n}{k}=(n-1)\stra{n-1}{k}+\stra{n-1}{k-1}\)</span>，所以： <span class="math display">\[\stra{n+1}{2}=n\stra{n}{2}+\stra{n}{1}=n\stra{n}{2}+(n-1)!\]</span> 两边同时除以 <span class="math inline">\(n!\)</span>，得到： <span class="math display">\[\frac{1}{n!}\stra{n+1}{2}=\frac{1}{(n-1)!}\stra{n}{2}+\frac{1}{n}\]</span> 于是得到： <span class="math display">\[\color{purple}{\stra{n+1}{2}=n!H_n}\]</span></p><h3 id="调和数的估计黎曼-zeta-函数欧拉常数">调和数的估计，黎曼 <span class="math inline">\(\zeta\)</span> 函数，欧拉常数</h3><p>调和数是发散的，一个广为人知的证明方式是按照 <span class="math inline">\(2\)</span> 的幂次分组，这里不再赘述。</p><p>我们可以用积分去近似调和数： <span class="math display">\[\int_1^{n+1}\frac{\mathrm dx}{x}&lt;H_n&lt;1+\int_1^n\frac{\mathrm dx}{x}\]</span> 即： <span class="math display">\[\ln(n+1)&lt;H_n&lt;\ln n+1\]</span></p><p><br></p><p>对调和数进行推广，我们可以得到 <span class="math inline">\(r\)</span> 次调和数： <span class="math display">\[\color{purple}{H^{(r)}_n=\sum_{k=1}^n\frac{1}{k^r}}\]</span> 如果令 <span class="math inline">\(n\to\infty\)</span>，那么我们得到黎曼 <span class="math inline">\(\zeta\)</span> 函数： <span class="math display">\[\color{purple}{\zeta(r)=H^{(r)}_\infty=\sum_{k=1}^\infty\frac{1}{k^r}}\]</span></p><p><br></p><p>欧拉曾发现一个简洁的方法，利用广义调和数 <span class="math inline">\(H^{(r)}_n\)</span> 来近似调和数 <span class="math inline">\(H_n\)</span>. 首先考虑无穷级数： <span class="math display">\[\ln\left(\frac{k}{k-1}\right)=\frac{1}{k}+\frac{1}{2k^2}+\frac{1}{3k^3}+\cdots\]</span></p><blockquote><p>附注：令 <span class="math inline">\(S(k)=\sum\limits_{i=1}^\infty\frac{1}{ik^i}\)</span>，则： <span class="math display">\[S&#39;(k)=\sum_{i=1}^\infty\frac{-1}{k^{i+1}}=-\frac{1}{k^2}\frac{1}{1-\frac{1}{k}}=-\frac{1}{k(k-1)}=\frac{1}{k}-\frac{1}{k-1}\]</span> 于是， <span class="math display">\[S(k)=\int S&#39;(k)\mathrm dk=\ln\left(\frac{k}{k-1}\right)\]</span> 故上述无穷级数成立。</p></blockquote><p>求和有： <span class="math display">\[\begin{align}\sum_{k=2}^n\ln\left(\frac{k}{k-1}\right)=\ln k=\sum_{i=1}^\infty\frac{H^{(i)}_n-1}{i}\end{align}\]</span> 于是我们就对 <span class="math inline">\(H_n\)</span> 和 <span class="math inline">\(\ln n\)</span> 的差有一个表达式： <span class="math display">\[H_n-\ln n=1-\frac{1}{2}(H^{(2)}_n-1)-\frac{1}{3}(H^{(3)}_n-1)-\frac{1}{4}(H^{(4)}_n-1)-\cdots\]</span> 当 <span class="math inline">\(n\to\infty\)</span> 时，有： <span class="math display">\[\lim_{n\to\infty}(H_n-\ln n)=1-\frac{1}{2}(\zeta(2)-1)-\frac{1}{3}(\zeta(3)-1)-\frac{1}{4}(\zeta(4)-1)-\cdots\]</span> 称之为欧拉常数 <span class="math inline">\(\gamma\)</span>，即： <span class="math display">\[\color{purple}{\gamma=\lim_{n\to\infty}(H_n-\ln n)=1-\frac{1}{2}(\zeta(2)-1)-\frac{1}{3}(\zeta(3)-1)-\frac{1}{4}(\zeta(4)-1)-\cdots}\]</span></p><h2 id="调和求和法">调和求和法</h2><h3 id="例一-1">例一</h3><p>求： <span class="math display">\[\sum_{0\leqslant k&lt;n}\binom{k}{m}H_k\]</span> 回忆<strong>分部求和法</strong>： <span class="math display">\[\sum\nolimits_a^bu(x)\Delta v(x)\delta x=u(x)v(x)\Big|_a^b-\sum\nolimits_a^bEv(x)\Delta u(x)\delta x\]</span> 由于 <span class="math display">\[\Delta H_k=H_{k+1}-H_k=\frac{1}{k+1}\quad,\quad\Delta \binom{k}{m+1}=\binom{k+1}{m+1}-\binom{k}{m+1}=\binom{k}{m}\]</span> 应用分部求和法有： <span class="math display">\[\begin{align}\sum_{0\leqslant k&lt;n}\binom{k}{m}H_k&amp;=\sum\nolimits_0^nH_k\Delta\binom{k}{m+1}\delta k\\&amp;=\binom{k}{m+1}H_k\Big|_0^n-\sum\nolimits_0^n\binom{k+1}{m+1}\frac{1}{k+1}\delta k\\&amp;=\binom{n}{m+1}H_n-\frac{1}{m+1}\sum\nolimits_0^n\binom{k}{m}\delta k\\&amp;=\binom{n}{m+1}H_n-\frac{1}{m+1}\binom{n}{m+1}\\&amp;=\binom{n}{m+1}\left(H_n-\frac{1}{m+1}\right)\end{align}\]</span></p><h3 id="例二-1">例二</h3><p>求： <span class="math display">\[S_n=\sum_{k=1}^n\frac{H_k}{k}\]</span> <strong>方法1</strong>： <span class="math display">\[\begin{align}S_n&amp;=\sum_{k=1}^n\frac{H_k}{k}\\&amp;=\sum_{k=1}^n\sum_{j=1}^k\frac{1}{jk}\\&amp;=\frac{1}{2}\left(\sum_{k=1}^n\sum_{j=1}^n\frac{1}{jk}+\sum_{k=1}^n\frac{1}{k^2}\right)\\&amp;=\frac{1}{2}\left(H_n^2+H_n^{(2)}\right)\end{align}\]</span> <strong>方法2</strong>： <span class="math display">\[\begin{align}S_n-H^{(2)}_n&amp;=\sum_{k=1}^n\left(\frac{H_k}{k}-\frac{1}{k^2}\right)\\&amp;=\sum_{k=1}^n\frac{kH_k-1}{k^2}\\&amp;=\sum_{k=1}^n\frac{H_{k-1}}{k}\\&amp;=\sum\nolimits_1^{n+1}H_{k-1}\Delta H_{k-1}\delta k\\&amp;=H^2_{k-1}\big|_1^{n+1}-\sum\nolimits_1^{n+1}H_k\frac{1}{k}\delta k\\&amp;=H^2_n-S_n\end{align}\]</span> 于是 <span class="math display">\[S_n=\frac{1}{2}\left(H^2_n+H_n^{(2)}\right)\]</span></p><h3 id="例三">例三</h3><p>求： <span class="math display">\[U_n=\sum_{k\geqslant 1}\binom{n}{k}\frac{(-1)^{k-1}}{k}(n-k)^n,\quad整数\;n\geqslant 1.\]</span></p><p>将 <span class="math inline">\((n-k)^n\)</span> 二项展开得： <span class="math display">\[U_n=\sum_{k\geqslant 1}\binom{n}{k}\frac{(-1)^{k-1}}{k}\sum_j\binom{n}{j}(-k)^jn^{n-j}=\sum_{j}\binom{n}{j}(-1)^{j-1}n^{n-j}\sum_{k\geqslant1}\binom{n}{k}(-1)^kk^{j-1}\]</span> 回忆<strong>高阶差分</strong>公式： <span class="math display">\[\Delta^nf(x)=\sum_{k}\binom{n}{k}(-1)^{n-k}f(x+k)\]</span> 所以有： <span class="math display">\[\begin{align}U_n=&amp;\binom{n}{0}(-1)n^n\sum_{k\geqslant1}\binom{n}{k}(-1)^kk^{-1}&amp;&amp;单独处理\;j=0\\&amp;+\sum_{j\geqslant 1}\binom{n}{j}(-1)^{j-1}n^{n-j}\sum_{k}\binom{n}{k}(-1)^kk^{j-1}&amp;&amp;补上\;k=0\\&amp;-\sum_{j\geqslant 1}\binom{n}{j}(-1)^{j-1}n^{n-j}\binom{n}{0}0^{j-1}&amp;&amp;再减去\;k=0\\=&amp;-n^n\sum_{k\geqslant1}\binom{n}{k}\frac{(-1)^k}{k}\\&amp;+(-1)^n\sum_{j\geqslant 1}\binom{n}{j}(-1)^{j-1}n^{n-j}\sum_k\binom{n}{k}(-1)^{n-k}k^{j-1}&amp;&amp;凑出高阶差分\\&amp;-n^n&amp;&amp;只有\;j=1\;时不为零\\=&amp;n^n\sum_{k\geqslant1}\binom{n}{k}\frac{(-1)^{k-1}}{k}\\&amp;+0&amp;&amp;次数小于\;n\;的\;n\;阶差分为零\\&amp;-n^n\\=&amp;n^n(T_n-1)\end{align}\]</span></p><p>现在，我们只需要求解： <span class="math display">\[T_n=\sum_{k\geqslant 1}\binom{n}{k}\frac{(-1)^{k-1}}{k}\]</span></p><p>将 <span class="math inline">\(\binom{n}{k}\)</span> 拆开为 <span class="math inline">\(\binom{n-1}{k-1}+\binom{n-1}{k}\)</span>，则可以用 <span class="math inline">\(T_{n-1}\)</span> 表示 <span class="math inline">\(T_n\)</span>： <span class="math display">\[\begin{align}T_n&amp;=\sum_{k\geqslant 1}\left(\binom{n-1}{k-1}+\binom{n-1}{k}\right)\frac{(-1)^{k-1}}{k}\\&amp;=\sum_{k\geqslant 1}\binom{n}{k}\frac{(-1)^{k-1}}{n}+\sum_{k\geqslant 1}\binom{n-1}{k}\frac{(-1)^{k-1}}{k}\\&amp;=-\frac{1}{n}\sum_{k\geqslant 1}\binom{n}{k}(-1)^{k}+T_{n-1}\\&amp;=\frac{1}{n}+T_{n-1}\end{align}\]</span> 又 <span class="math inline">\(T_1=1\)</span>，故： <span class="math display">\[T_n=H_n\]</span> 所以，我们最终解得： <span class="math display">\[U_n=n^n(H_n-1)\]</span></p><h2 id="伯努利数">伯努利数</h2><h3 id="定义与自然数幂和">定义与自然数幂和</h3><p><span class="math inline">\(\textbf{Jacob Bernoulli}\)</span> 在研究 <span class="math inline">\(m\)</span> 次幂和时发现了： <span class="math display">\[S_m(n):=\sum_{k=0}^{n-1} k^m=0^m+1^m+\cdots+(n-1)^m=\frac{1}{m+1}\sum_{k=0}^m\binom{m+1}{k}B_kn^{m+1-k}\]</span> 其中伯努利数递归地定义为： <span class="math display">\[\sum_{j=0}^m\binom{m+1}{j}B_j=[m=0],\quad \forall m\geqslant 0\]</span> 前几个值为：</p><table><thead><tr class="header"><th><span class="math inline">\(n\)</span></th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(B_n\)</span></td><td><span class="math inline">\(1\)</span></td><td><span class="math inline">\(-\frac{1}{2}\)</span></td><td><span class="math inline">\(\frac{1}{6}\)</span></td><td><span class="math inline">\(0\)</span></td><td><span class="math inline">\(-\frac{1}{30}\)</span></td><td><span class="math inline">\(0\)</span></td><td><span class="math inline">\(\frac{1}{42}\)</span></td><td><span class="math inline">\(0\)</span></td><td><span class="math inline">\(-\frac{1}{30}\)</span></td><td><span class="math inline">\(0\)</span></td><td><span class="math inline">\(\frac{5}{66}\)</span></td><td><span class="math inline">\(0\)</span></td><td><span class="math inline">\(-\frac{691}{2730}\)</span></td></tr></tbody></table><p>证明：采用扰动法对 <span class="math inline">\(m\)</span> 进行归纳： <span class="math display">\[\begin{align}S_{m+1}(n+1)=S_{m+1}(n)+n^{m+1}&amp;=\sum_{k=0}^{n-1}(k+1)^{m+1}\\&amp;=\sum_{k=0}^{n-1}\sum_{j=0}^{m+1}\binom{m+1}{j}k^j\\&amp;=\sum_{j=0}^{m+1}\binom{m+1}{j}S_j(n)\end{align}\]</span> 于是： <span class="math display">\[n^{m+1}=\sum_{j=0}^m\binom{m+1}{j}S_j(n)\]</span> 假设伯努利的自然数幂和公式对 <span class="math inline">\(0\leqslant j&lt;m\)</span> 均成立，记 <span class="math inline">\(\hat S_m(n)=\frac{1}{m+1}\sum\limits_{k=0}^m\binom{m+1}{k}B_kn^{m+1-k}\)</span>，那么： <span class="math display">\[\begin{align}n^{m+1}&amp;=\sum_{j=0}^m\binom{m+1}{j}\frac{1}{j+1}\sum_{k=0}^j\binom{j+1}{k}B_kn^{j+1-k}+\binom{m+1}{m}\left(S_m(n)-\hat S_m(n)\right)\\&amp;=\sum_{k=0}^m\sum_{j=k}^m\binom{m+1}{j}\frac{1}{j+1}\binom{j+1}{k}B_kn^{j+1-k}+(m+1)\left(S_m(n)-\hat S_m(n)\right)\\&amp;=\sum_{k=0}^m\sum_{j=k}^m\binom{m+1}{j}\binom{j+1}{j-k}\frac{B_{j-k}n^{k+1}}{j+1}+(m+1)\left(S_m(n)-\hat S_m(n)\right)\\&amp;=\sum_{k=0}^m\sum_{j=k}^m\binom{m+1}{j}\binom{j+1}{k+1}\frac{B_{j-k}n^{k+1}}{j+1}+(m+1)\left(S_m(n)-\hat S_m(n)\right)\\&amp;=\sum_{k=0}^m\frac{n^{k+1}}{k+1}\sum_{j=k}^m\binom{m+1}{j}\binom{j}{k}B_{j-k}n^{k+1}+(m+1)\left(S_m(n)-\hat S_m(n)\right)\\&amp;=\sum_{k=0}^m\frac{n^{k+1}}{k+1}\sum_{j=k}^m\binom{m+1}{k}\binom{m+1-k}{j-k}B_{j-k}n^{k+1}+(m+1)\left(S_m(n)-\hat S_m(n)\right)\\&amp;=\sum_{k=0}^m\frac{n^{k+1}}{k+1}\binom{m+1}{k}\sum_{j=k}^m\binom{m+1-k}{j-k}B_{j-k}n^{k+1}+(m+1)\left(S_m(n)-\hat S_m(n)\right)\\&amp;=\sum_{k=0}^m\frac{n^{k+1}}{k+1}\binom{m+1}{k}\sum_{j=0}^{m-k}\binom{m+1-k}{j}B_{j}n^{k+1}+(m+1)\left(S_m(n)-\hat S_m(n)\right)\\&amp;=\sum_{k=0}^m\frac{n^{k+1}}{k+1}\binom{m+1}{k}[m=k]+(m+1)\left(S_m(n)-\hat S_m(n)\right)\\&amp;=\frac{n^{m+1}}{m+1}(m+1)+(m+1)\left(S_m(n)-\hat S_m(n)\right)\\&amp;=n^{m+1}+(m+1)\left(S_m(n)-\hat S_m(n)\right)\end{align}\]</span> 故： <span class="math display">\[S_m(n)=\hat S_m(n)\]</span> 归纳完毕。</p><h3 id="从一个幂级数推导的推论">从一个幂级数推导的推论</h3>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>具体数学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[具体数学]第六章·特殊的数（第一部分）</title>
    <link href="/blog-main/2020/08/26/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E5%85%AD%E7%AB%A0%C2%B7%E7%89%B9%E6%AE%8A%E7%9A%84%E6%95%B0%EF%BC%88%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%89/"/>
    <url>/blog-main/2020/08/26/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E5%85%AD%E7%AB%A0%C2%B7%E7%89%B9%E6%AE%8A%E7%9A%84%E6%95%B0%EF%BC%88%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<blockquote><p>第六章·特殊的数分7节，包括斯特林数、欧拉数、调和数、调和求和法、伯努利数、斐波那契数和连项式的内容。这是笔记第一部分，包括斯特林数、欧拉数，它们可以构成类似杨辉三角的三角形。</p></blockquote><p><span class="math display">\[\newcommand{\stra}[2]{\begin{bmatrix}#1\\#2\end{bmatrix}}\newcommand{\strb}[2]{\begin{Bmatrix}#1\\#2\end{Bmatrix}}\newcommand{\elr}[2]{\left\langle\begin{matrix}#1\\#2\end{matrix}\right\rangle}\newcommand{\elrdb}[2]{\left\langle\left\langle\begin{matrix}#1\\#2\end{matrix}\right\rangle\right\rangle}\]</span></p><h2 id="斯特林数">斯特林数</h2><p>符号表示：</p><p><strong>第一类斯特林数（斯特林轮换数）</strong>：<span class="math inline">\(\stra{n}{k}\)</span>；</p><p><strong>第二类斯特林数（斯特林子集数）</strong>：<span class="math inline">\(\strb{n}{k}\)</span>.</p><blockquote><p>注：自定义 <span class="math inline">\(\LaTeX\)</span> 运算符号，以上述两个斯特林数为例：</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\stra</span>&#125;[2]&#123;<span class="hljs-keyword">\begin</span>&#123;bmatrix&#125;<span class="hljs-params">#1</span><span class="hljs-keyword">\\</span><span class="hljs-params">#2</span><span class="hljs-keyword">\end</span>&#123;bmatrix&#125;&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\strb</span>&#125;[2]&#123;<span class="hljs-keyword">\begin</span>&#123;Bmatrix&#125;<span class="hljs-params">#1</span><span class="hljs-keyword">\\</span><span class="hljs-params">#2</span><span class="hljs-keyword">\end</span>&#123;Bmatrix&#125;&#125;<br></code></pre></td></tr></table></figure><p><code>\newcommand&#123;运算符&#125;[参数数量]&#123;运算符长什么样&#125;</code>，其中 <code>#1</code>,<code>#2</code> 标注参数位置。</p></blockquote><h3 id="第二类斯特林数">第二类斯特林数</h3><p>我们先来考虑第二类斯特林数。<span class="math inline">\(\strb{n}{k}\)</span> 表示将一个有 <span class="math inline">\(n\)</span> 件物品的集合划分成 <span class="math inline">\(k\)</span> 个非空子集的方案数。</p><blockquote><p>花括号也用来表示集合，这一雷同有助于让我们记住 <span class="math inline">\(\strb{n}{k}\)</span> 的意义。</p></blockquote><p>我们可以推导 <span class="math inline">\(\strb{n}{k}\)</span> 的递归式：把 <span class="math inline">\(n\)</span> 件物品分成 <span class="math inline">\(k\)</span> 个部分，要么是最后一件物品单独一类，这一共有 <span class="math inline">\(\strb{n-1}{k-1}\)</span> 种方案；要么是最后一件物品加入前 <span class="math inline">\(n-1\)</span> 件物品划分出的 <span class="math inline">\(k\)</span> 个部分之一去，共有 <span class="math inline">\(k\strb{n-1}{k}\)</span> 种方案。所以我们有： <span class="math display">\[\color{purple}{\strb{n}{k}=k\strb{n-1}{k}+\strb{n-1}{k-1}}\]</span> （如果系数 <span class="math inline">\(k\)</span> 是 <span class="math inline">\(1\)</span> 就是二项式系数了）</p><p>基于此，我们可以生成第二类斯特林数的三角形：</p><table style="width:100%;"><thead><tr class="header"><th style="text-align: center;"><span class="math inline">\(n\)</span></th><th style="text-align: center;"><span class="math inline">\(\strb{n}{0}\)</span></th><th style="text-align: center;"><span class="math inline">\(\strb{n}{1}\)</span></th><th style="text-align: center;"><span class="math inline">\(\strb{n}{2}\)</span></th><th style="text-align: center;"><span class="math inline">\(\strb{n}{3}\)</span></th><th style="text-align: center;"><span class="math inline">\(\strb{n}{4}\)</span></th><th style="text-align: center;"><span class="math inline">\(\strb{n}{5}\)</span></th><th style="text-align: center;"><span class="math inline">\(\strb{n}{6}\)</span></th><th style="text-align: center;"><span class="math inline">\(\strb{n}{7}\)</span></th><th style="text-align: center;"><span class="math inline">\(\strb{n}{8}\)</span></th><th style="text-align: center;"><span class="math inline">\(\strb{n}{9}\)</span></th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(2\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(3\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(3\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(4\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(7\)</span></td><td style="text-align: center;"><span class="math inline">\(6\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(5\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(15\)</span></td><td style="text-align: center;"><span class="math inline">\(25\)</span></td><td style="text-align: center;"><span class="math inline">\(10\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(6\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(31\)</span></td><td style="text-align: center;"><span class="math inline">\(90\)</span></td><td style="text-align: center;"><span class="math inline">\(65\)</span></td><td style="text-align: center;"><span class="math inline">\(15\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(7\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(63\)</span></td><td style="text-align: center;"><span class="math inline">\(301\)</span></td><td style="text-align: center;"><span class="math inline">\(350\)</span></td><td style="text-align: center;"><span class="math inline">\(140\)</span></td><td style="text-align: center;"><span class="math inline">\(21\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(8\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(127\)</span></td><td style="text-align: center;"><span class="math inline">\(966\)</span></td><td style="text-align: center;"><span class="math inline">\(1701\)</span></td><td style="text-align: center;"><span class="math inline">\(1050\)</span></td><td style="text-align: center;"><span class="math inline">\(266\)</span></td><td style="text-align: center;"><span class="math inline">\(28\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(9\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(255\)</span></td><td style="text-align: center;"><span class="math inline">\(3025\)</span></td><td style="text-align: center;"><span class="math inline">\(7770\)</span></td><td style="text-align: center;"><span class="math inline">\(6951\)</span></td><td style="text-align: center;"><span class="math inline">\(2646\)</span></td><td style="text-align: center;"><span class="math inline">\(462\)</span></td><td style="text-align: center;"><span class="math inline">\(36\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td></tr></tbody></table><h3 id="第一类斯特林数">第一类斯特林数</h3><p>现在讨论第一类斯特林数。<span class="math inline">\(\stra{n}{k}\)</span> 表示将 <span class="math inline">\(n\)</span> 个元素排成 <span class="math inline">\(k\)</span> 个<strong>轮换</strong>的方案数。这里，<strong>轮换</strong>是指环形排列，可以转动而相等，例如 <span class="math inline">\([A,B,C,D]=[B,C,D,A]=[C,D,A,B]=[D,A,B,C]\)</span>，但是 <span class="math inline">\([A,B,C,D]\neq[A,B,D,C]\)</span>.</p><p>先看几个特殊的值，<span class="math inline">\(n\)</span> 个元素的 <span class="math inline">\(n\)</span> 轮换有 <span class="math inline">\(\frac{n!}{n}\)</span> 种，所以 <span class="math inline">\(\stra{n}{1}=(n-1)!\)</span>；当所有轮换都是单元素或者双元素时，轮换和子集没有差别，所以 <span class="math inline">\(\stra{n}{n}=\strb{n}{n}=1,\,\stra{n}{n-1}=\strb{n}{n-1}=\binom{n}{2}\)</span>.</p><p>我们也可以推导 <span class="math inline">\(\stra{n}{k}\)</span> 的递归式：考虑最后一件物品，要么单独放在自己的轮换里，有 <span class="math inline">\(\stra{n-1}{k-1}\)</span> 种方式；要么加入前 <span class="math inline">\(n-1\)</span> 件物品分成的 <span class="math inline">\(\stra{n-1}{k}\)</span> 个轮换中的一个，而方法有 <span class="math inline">\(n-1\)</span> 种（插入任何一个轮换的任何一个位置都行）。所以我们有： <span class="math display">\[\color{purple}{\stra{n}{k}=(n-1)\stra{n-1}{k}+\stra{n-1}{k-1}}\]</span></p><blockquote><p>记忆是很方便的，第一类斯特林数是在二项式系数的递归式中乘上了<strong>上指标</strong> <span class="math inline">\(n-1\)</span> 作为系数，第二类斯特林数是乘上了<strong>下指标</strong> <span class="math inline">\(k\)</span> 作为系数。</p></blockquote><p>基于此，我们也可以生成第一类斯特林数的三角形：</p><table style="width:100%;"><thead><tr class="header"><th style="text-align: center;"><span class="math inline">\(n\)</span></th><th style="text-align: center;"><span class="math inline">\(\stra{n}{0}\)</span></th><th style="text-align: center;"><span class="math inline">\(\stra{n}{1}\)</span></th><th style="text-align: center;"><span class="math inline">\(\stra{n}{2}\)</span></th><th style="text-align: center;"><span class="math inline">\(\stra{n}{3}\)</span></th><th style="text-align: center;"><span class="math inline">\(\stra{n}{4}\)</span></th><th style="text-align: center;"><span class="math inline">\(\stra{n}{5}\)</span></th><th style="text-align: center;"><span class="math inline">\(\stra{n}{6}\)</span></th><th style="text-align: center;"><span class="math inline">\(\stra{n}{7}\)</span></th><th style="text-align: center;"><span class="math inline">\(\stra{n}{8}\)</span></th><th style="text-align: center;"><span class="math inline">\(\stra{n}{9}\)</span></th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(2\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(3\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(2\)</span></td><td style="text-align: center;"><span class="math inline">\(3\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(4\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(6\)</span></td><td style="text-align: center;"><span class="math inline">\(11\)</span></td><td style="text-align: center;"><span class="math inline">\(6\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(5\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(24\)</span></td><td style="text-align: center;"><span class="math inline">\(50\)</span></td><td style="text-align: center;"><span class="math inline">\(35\)</span></td><td style="text-align: center;"><span class="math inline">\(10\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(6\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(120\)</span></td><td style="text-align: center;"><span class="math inline">\(274\)</span></td><td style="text-align: center;"><span class="math inline">\(225\)</span></td><td style="text-align: center;"><span class="math inline">\(85\)</span></td><td style="text-align: center;"><span class="math inline">\(15\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(7\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(720\)</span></td><td style="text-align: center;"><span class="math inline">\(1764\)</span></td><td style="text-align: center;"><span class="math inline">\(1624\)</span></td><td style="text-align: center;"><span class="math inline">\(735\)</span></td><td style="text-align: center;"><span class="math inline">\(175\)</span></td><td style="text-align: center;"><span class="math inline">\(21\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(8\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(5040\)</span></td><td style="text-align: center;"><span class="math inline">\(13068\)</span></td><td style="text-align: center;"><span class="math inline">\(13132\)</span></td><td style="text-align: center;"><span class="math inline">\(6769\)</span></td><td style="text-align: center;"><span class="math inline">\(1960\)</span></td><td style="text-align: center;"><span class="math inline">\(322\)</span></td><td style="text-align: center;"><span class="math inline">\(28\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(9\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(40320\)</span></td><td style="text-align: center;"><span class="math inline">\(\small{109584}\)</span></td><td style="text-align: center;"><span class="math inline">\(\small{118124}\)</span></td><td style="text-align: center;"><span class="math inline">\(67284\)</span></td><td style="text-align: center;"><span class="math inline">\(22449\)</span></td><td style="text-align: center;"><span class="math inline">\(4536\)</span></td><td style="text-align: center;"><span class="math inline">\(546\)</span></td><td style="text-align: center;"><span class="math inline">\(36\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td></tr></tbody></table><h3 id="一些恒等式">一些恒等式</h3><p>我们知道，每一种排列都和一个轮换等价——例如 <span class="math inline">\(15234\)</span>，从下标向数值连边，就构成了若干个环 <span class="math inline">\([1][2543]\)</span>，也即形成了一个轮换（这一点在竞赛中也经常使用）。所以，所有轮换方案加起来就是所有排列数，即： <span class="math display">\[\color{blue}{\sum_{k=0}^n\stra{n}{k}=n!\quad,整数\;n\geqslant 0}\]</span> 之前说过，普通的幂次和下降幂之间可以通过斯特林数建立起联系，我们观察前几个关系： <span class="math display">\[\begin{align}&amp;x^0=x^{\underline 0}\\&amp;x^1=x^{\underline 1}\\&amp;x^2=x^{\underline 2}+x^{\underline 1}\\&amp;x^3=x^{\underline 3}+3x^{\underline 2}+x^{\underline 1}\\&amp;x^4=x^{\underline 4}+6x^{\underline 3}+7x^{\underline 2}+x^{\underline 1}\end{align}\]</span> 会发现这系数不就是第二类斯特林数吗？也就是说，我们可以猜想： <span class="math display">\[\color{blue}{x^n=\sum_k\strb{n}{k}x^{\underline k}}\]</span> 数学归纳法证明： <span class="math display">\[\begin{align}x^n=x\cdot x^{n-1}&amp;=x\sum_k\strb{n-1}{k}x^{\underline k}\\&amp;=\sum_k\strb{n-1}{k}x^{\underline {k+1}}+\sum_k\strb{n-1}{k}kx^{\underline k}&amp;&amp;x\cdot x^{\underline k}=x^{\underline{k+1}}+kx^{\underline k}\\&amp;=\sum_k\left(\strb{n-1}{k-1}+k\strb{n-1}{k}\right)x^{\underline k}\\&amp;=\sum_k\strb{n}{k}x^{\underline k}\end{align}\]</span> 证毕。</p><p>同样的，上升阶乘幂与普通的幂次之间也有类似关系： <span class="math display">\[\color{blue}{x^{\overline n}=\sum_k\stra{n}{k}x^k}\]</span> 数学归纳法证明： <span class="math display">\[\begin{align}x^{\overline n}=(x+n-1)x^{\overline {n-1}}&amp;=(x+n-1)\sum_k\stra{n-1}{k}x^k\\&amp;=\sum_k\stra{n-1}{k}x^{k+1}+\sum_k\stra{n-1}{k}(n-1)x^k&amp;&amp;(x+n-1)x^k=x^{k+1}+(n-1)x^k\\&amp;=\sum_k\left(\stra{n-1}{k-1}+(n-1)\stra{n-1}{k}\right)x^k\\&amp;=\sum_k\stra{n}{k}x^k\end{align}\]</span> 证毕。</p><p>上面是用下降幂表示了普通幂，普通幂表示了上升幂。但如果我们想用普通幂表示下降幂，上升幂表示普通幂呢？只需要应用 <span class="math inline">\(x^{\underline n}=(-1)^n(-x)^{\overline n}\)</span>，把 <span class="math inline">\(x\)</span> 取相反值就得到： <span class="math display">\[\color{blue}{\begin{align}&amp;x^{n}=\sum_k\strb{n}{k}(-1)^{n-k}x^{\overline k}\\&amp;x^{\underline n}=\sum_k\stra{n}{k}(-1)^{n-k}x^k\end{align}}\]</span> 如果我们把普通幂表示上升幂的式子代入上升幂表示普通幂的式子，那么得到： <span class="math display">\[x^n=\sum_k\strb{n}{k}(-1)^{n-k}\sum_m\stra{k}{m}x^m=\sum_{k,m}(-1)^{n-k}\strb{n}{k}\stra{k}{m}x^m\]</span> 由于这是一个恒等式，所以 <span class="math inline">\(x\)</span> 对应系数相等，于是我们得到： <span class="math display">\[\color{blue}{\sum_{k,m}(-1)^{n-k}\strb{n}{k}\stra{k}{m}=[m=n]}\]</span> <br></p><p>更多的实践总结出了一大堆斯特林数恒等式： <span class="math display">\[\begin{align}&amp;\stra{n}{k}=(n-1)\stra{n-1}{k}+\stra{n-1}{k-1}&amp;&amp;递归式\\&amp;\strb{n}{k}=k\strb{n-1}{k}+\strb{n-1}{k-1}&amp;&amp;递归式\\&amp;x^{n}=\sum_k\strb{n}{k}x^{\underline k}=\sum_k\strb{n}{k}(-1)^{n-k}x^{\overline k}&amp;&amp;在幂之间转换\\&amp;x^{\overline n}=\sum_k\stra{n}{k}x^k&amp;&amp;在幂之间转换\\&amp;x^{\underline n}=\sum_k\stra{n}{k}(-1)^{n-k}x^k&amp;&amp;在幂之间转换\\&amp;\sum_k\stra{n}{k}\strb{k}{m}(-1)^{n-k}=[m=n]&amp;&amp;反转公式\\&amp;\sum_{k,m}\strb{n}{k}\stra{k}{m}(-1)^{n-k}=[m=n]&amp;&amp;反转公式\\&amp;\stra{n}{k}=\strb{-k}{-n}&amp;&amp;对偶性\\\\&amp;\strb{n+1}{m+1}=\sum_k\binom{n}{k}\strb{k}{m}\\&amp;\stra{n+1}{m+1}=\sum_k\stra{n}{k}\binom{k}{m}\\&amp;\strb{n}{m}=\sum_k\binom{n}{k}\strb{k+1}{m+1}(-1)^{n-k}\\&amp;\stra{n}{m}=\sum_k\stra{n+1}{k+1}\binom{k}{m}(-1)^{m-k}\\&amp;m!\strb{n}{m}=\sum_k\binom{m}{k}k^n(-1)^{m-k}\\&amp;\strb{n+1}{m+1}=\sum_{k=0}^{n}\strb{k}{m}(m+1)^{n-k}\\&amp;\stra{n+1}{m+1}=\sum_{k=0}^n\stra{k}{m}n^{\underline{n-k}}=n!\sum_{k=0}^n\stra{k}{m}/k!\\&amp;\strb{m+n+1}{m}=\sum_{k=0}^mk\strb{n+k}{k}\\&amp;\stra{m+n+1}{m}=\sum_{k=0}^m(n+k)\stra{n+k}{k}\\&amp;\binom{n}{m}=\sum_k\strb{n+1}{k+1}\stra{k}{m}(-1)^{m-k}\\&amp;n^{\underline{n-m}}[n\geqslant m]=\sum_k\stra{n+1}{k+1}\strb{k}{m}(-1)^{m-k}\\&amp;\strb{n}{n-m}=\sum_k\binom{m-n}{m+k}\binom{m+n}{n+k}\stra{m+k}{k}\\&amp;\stra{n}{n-m}=\sum_k\binom{m-n}{m+k}\binom{m+n}{n+k}\strb{m+k}{k}\\&amp;\strb{n}{l+m}\binom{l+m}{l}=\sum_k\binom{k}{l}\strb{n-k}{m}\binom{n}{k}\\&amp;\stra{n}{l+m}\binom{l+m}{l}=\sum_k\stra{k}{l}\stra{n-k}{m}\binom{n}{k}\end{align}\]</span></p><h2 id="欧拉数">欧拉数</h2><h3 id="定义和递归式">定义和递归式</h3><p><span class="math inline">\(\elr{n}{k}\)</span> 表示 <span class="math inline">\(\{1,2,\cdots,n\}\)</span> 的有 <span class="math inline">\(k\)</span> 个<strong>升高</strong>的排列 <span class="math inline">\(\pi_1\pi_2\cdots\pi_n\)</span> 的个数。也即，有 <span class="math inline">\(k\)</span> 个地方 <span class="math inline">\(\pi_j&lt;\pi_{j+1}\)</span>.</p><p>推导欧拉数的递归式：尝试插入数 <span class="math inline">\(n\)</span>，如果它插在某个上升段的最后，那么升高数 <span class="math inline">\(+1\)</span>，这样的位置有 <span class="math inline">\(n-k\)</span> 个；如果它插在某个上升段的中间，升高数不变，这样的位置有 <span class="math inline">\(k+1\)</span> 个。所以我们有： <span class="math display">\[\color{purple}{\elr{n}{k}=(k+1)\elr{n-1}{k}+(n-k)\elr{n-1}{k-1}}\]</span> 初始条件：<span class="math inline">\(\elr{0}{k}=[k=0]\)</span>，并假定 <span class="math inline">\(k&lt;0\)</span> 时 <span class="math inline">\(\elr{n}{k}=0\)</span>.</p><p>由此我们可以得到欧拉数的三角形：</p><table><thead><tr class="header"><th style="text-align: center;"><span class="math inline">\(n\)</span></th><th style="text-align: center;"><span class="math inline">\(\elr{n}{0}\)</span></th><th style="text-align: center;"><span class="math inline">\(\elr{n}{1}\)</span></th><th style="text-align: center;"><span class="math inline">\(\elr{n}{2}\)</span></th><th style="text-align: center;"><span class="math inline">\(\elr{n}{3}\)</span></th><th style="text-align: center;"><span class="math inline">\(\elr{n}{4}\)</span></th><th style="text-align: center;"><span class="math inline">\(\elr{n}{5}\)</span></th><th style="text-align: center;"><span class="math inline">\(\elr{n}{6}\)</span></th><th style="text-align: center;"><span class="math inline">\(\elr{n}{7}\)</span></th><th style="text-align: center;"><span class="math inline">\(\elr{n}{8}\)</span></th><th style="text-align: center;"><span class="math inline">\(\elr{n}{9}\)</span></th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(2\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(3\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(4\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(4\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(11\)</span></td><td style="text-align: center;"><span class="math inline">\(11\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(5\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(26\)</span></td><td style="text-align: center;"><span class="math inline">\(66\)</span></td><td style="text-align: center;"><span class="math inline">\(26\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(6\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(57\)</span></td><td style="text-align: center;"><span class="math inline">\(302\)</span></td><td style="text-align: center;"><span class="math inline">\(302\)</span></td><td style="text-align: center;"><span class="math inline">\(57\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(7\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(120\)</span></td><td style="text-align: center;"><span class="math inline">\(1191\)</span></td><td style="text-align: center;"><span class="math inline">\(2416\)</span></td><td style="text-align: center;"><span class="math inline">\(1191\)</span></td><td style="text-align: center;"><span class="math inline">\(120\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(8\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(247\)</span></td><td style="text-align: center;"><span class="math inline">\(4293\)</span></td><td style="text-align: center;"><span class="math inline">\(15619\)</span></td><td style="text-align: center;"><span class="math inline">\(15619\)</span></td><td style="text-align: center;"><span class="math inline">\(4293\)</span></td><td style="text-align: center;"><span class="math inline">\(247\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(9\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(502\)</span></td><td style="text-align: center;"><span class="math inline">\(14608\)</span></td><td style="text-align: center;"><span class="math inline">\(88234\)</span></td><td style="text-align: center;"><span class="math inline">\(\small{156190}\)</span></td><td style="text-align: center;"><span class="math inline">\(88234\)</span></td><td style="text-align: center;"><span class="math inline">\(14608\)</span></td><td style="text-align: center;"><span class="math inline">\(502\)</span></td><td style="text-align: center;"><span class="math inline">\(1\)</span></td><td style="text-align: center;"><span class="math inline">\(0\)</span></td></tr></tbody></table><h3 id="性质和恒等式">性质和恒等式</h3><p>从表中可以看出，欧拉数具有对称性： <span class="math display">\[\color{blue}{\elr{n}{k}=\elr{n}{n-1-k}}\]</span> 因为 <span class="math inline">\(\pi_1\cdots\pi_n\)</span> 有 <span class="math inline">\(k\)</span> 个升高当且仅当 <span class="math inline">\(\pi_n\cdots\pi_1\)</span> 有 <span class="math inline">\(n-1-k\)</span> 个升高。</p><p><span class="math inline">\(\textbf{Worpitzky}\)</span> 恒等式： <span class="math display">\[\color{blue}{x^n=\sum_k\elr{n}{k}\binom{x+k}{n}}\]</span> 证明：数学归纳法。首先 <span class="math inline">\(n=0\)</span> 时，<span class="math inline">\(1=\sum\limits_k\langle\begin{smallmatrix}0\\k\end{smallmatrix}\rangle\binom{x+k}{0}=1\)</span> 成立；假设 <span class="math inline">\(\leqslant n\)</span> 均成立，则： <span class="math display">\[\begin{align}x^{n+1}&amp;=x\cdot x^n\\&amp;=x\sum\limits _k\elr{n}{k}\binom{x+k}{n}\\&amp;=\sum_k\elr{n}{k}\left[(k+1)\binom{x+k}{n+1}+(n-k)\binom{x+k+1}{n+1}\right]\\&amp;=\sum_k\left(\elr{n+1}{k}-(n+1-k)\elr{n}{k-1}\right)\binom{x+k}{n+1}+\sum_k\elr{n}{k}(n-k)\binom{x+k+1}{n+1}\\&amp;=\sum_k\elr{n+1}{k}\binom{x+k}{n+1}+\sum_k\elr{n}{k}(n-k)\binom{x+k+1}{n+1}-\sum_k(n+1-k)\elr{n}{k-1}\binom{x+k}{n+1}\\&amp;=\sum_k\elr{n+1}{k}\binom{x+k}{n+1}\end{align}\]</span> 证毕。（注：上述推导用了恒等式：<span class="math inline">\(x\binom{x+k}{n}=(k+1)\binom{x+k}{n+1}+(n-k)\binom{x+k+1}{n+1}\)</span>. ）</p><p>还有一些其他性质： <span class="math display">\[\begin{align}&amp;\elr{n}{m}=\sum_{k=0}^m\binom{n+1}{k}(m+1-k)^n(-1)^k\\&amp;m!\strb{n}{m}=\sum_k\elr{n}{k}\binom{k}{n-m}\\&amp;\elr{n}{m}=\sum_k\strb{n}{k}\binom{n-k}{m}(-1)^{n-k-m}k!\end{align}\]</span></p><p>（二阶欧拉数和斯特林多项式目前跳过）</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>具体数学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[具体数学]第五章·二项式系数（第二部分）</title>
    <link href="/blog-main/2020/08/23/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E4%BA%94%E7%AB%A0%C2%B7%E4%BA%8C%E9%A1%B9%E5%BC%8F%E7%B3%BB%E6%95%B0%EF%BC%88%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%89/"/>
    <url>/blog-main/2020/08/23/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E4%BA%94%E7%AB%A0%C2%B7%E4%BA%8C%E9%A1%B9%E5%BC%8F%E7%B3%BB%E6%95%B0%EF%BC%88%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<blockquote><p>第五章·二项式系数分8节，包含基本恒等式、生成函数、超几何函数、机械求和法等内容。这是笔记的第二部分，包括生成函数的内容。</p><p>至于超几何函数那块啊……呃……有点头秃……先放一放……</p></blockquote><h2 id="生成函数">生成函数</h2><p>对于一个无限序列 <span class="math inline">\(\langle a_0,a_1,a_2,\cdots\rangle\)</span>，定义<strong>形式幂级数</strong>： <span class="math display">\[\color{purple}{A(z)=a_0+a_1z+a_2z^2+\cdots=\sum_{n\geqslant 0}a_nz^n}\]</span> 为该序列的<strong>普通生成函数</strong>（<span class="math inline">\(\textbf{Ordinary Generating Function, OGF}\)</span>）。</p><p>用记号 <span class="math inline">\([z^n]A(z)\)</span> 表示 <span class="math inline">\(A(z)\)</span> 中 <span class="math inline">\(z^n\)</span> 的系数，即： <span class="math display">\[[z^n]A(z)=a_n\]</span></p><blockquote><p>这里用 <span class="math inline">\(z\)</span> 而非 <span class="math inline">\(x\)</span> 是因为我们常常把 <span class="math inline">\(z\)</span> 视为复数。</p></blockquote><h3 id="运算">运算</h3><p>设 <span class="math inline">\(\langle a_n\rangle,\langle b_n\rangle\)</span> 的生成函数分别为 <span class="math inline">\(A(z),B(z)\)</span>，则加减法： <span class="math display">\[A(z)\pm B(z)=\sum_{n\geqslant 0} (a_n\pm b_n)z^n\]</span> 乘法： <span class="math display">\[\color{purple}{\begin{align}A(z)B(z)&amp;=a_0b_0+(a_0b_1+a_1b_0)z+(a_0b_2+a_1b_1+a_2b_0)z^2+\cdots\\&amp;=\sum_{n\geqslant 0}z^n\sum_{k=0}^na_kb_{n-k}\end{align}}\]</span> 即乘积的系数 <span class="math inline">\(c_n=\sum\limits_{k=0}^na_kb_{n-k}\)</span>，称序列 <span class="math inline">\(\langle c_n \rangle\)</span> 为序列 <span class="math inline">\(\langle a_n \rangle\)</span> 和 <span class="math inline">\(\langle b_n \rangle\)</span> 的<strong>卷积（convolution）</strong>。</p><h3 id="用生成函数来发现和证明恒等式">用生成函数来发现和证明恒等式</h3><p>二项式定理告诉我们：<strong><span class="math inline">\((1+z)^r\)</span> 是序列 <span class="math inline">\(\left\langle \binom{r}{0},\binom{r}{1},\binom{r}{2},\cdots \right\rangle\)</span> 的生成函数</strong>，即： <span class="math display">\[(1+z)^r=\sum_{n\geqslant 0} \binom{r}{n}z^n\]</span> 如果我们把 <span class="math inline">\((1+z)^r\)</span> 和 <span class="math inline">\((1+z)^s\)</span> 相乘，就得到了一个新的生成函数： <span class="math display">\[(1+z)^{r+s}=\sum_{n\geqslant 0}z^n\sum_{k=0}^n\binom{r}{k}\binom{s}{n-k}\]</span> 对比两边 <span class="math inline">\(z^n\)</span> 的系数，就可以得到范德蒙德卷积式： <span class="math display">\[\binom{r+s}{n}=\sum_{k\geqslant 0}\binom{r}{k}\binom{s}{n-k}\]</span> <br></p><p>考虑 <span class="math inline">\((1-z)^r\)</span>，它是序列 <span class="math inline">\(\left\langle\binom{r}{0},-\binom{r}{1},\binom{r}{2},\cdots\right\rangle\)</span> 的生成函数，我们把 <span class="math inline">\((1-z)^r\)</span> 和 <span class="math inline">\((1+z)^r\)</span> 相乘，就得到了一个新的生成函数： <span class="math display">\[(1-z^2)^r=\sum_{n\geqslant 0}z^n\sum_{k=0}^n(-1)^{k}\binom{r}{k}\binom{r}{n-k}\]</span> 对比两边 <span class="math inline">\(z^n\)</span> 的系数，得到： <span class="math display">\[\color{purple}{\sum_{k=0}^n\binom{r}{k}\binom{r}{n-k}(-1)^k=(-1)^{n/2}\binom{r}{n/2}[n\;是偶数]}\]</span> 我们发现了一个新的恒等式。</p><p><br></p><p>二项式系数也出现在其它一些生成函数中，以下是一个重要的恒等式（特点：下指标不动，上指标变化）： <span class="math display">\[\begin{align}&amp;\color{purple}{\frac{1}{(1-z)^{n+1}}}=(1-z)^{-n-1}=\sum_{k\geqslant0}\binom{-n-1}{k}(-1)^kz^{k}=\sum_{k\geqslant 0}\binom{k+n}{k}z^k=\color{purple}{\sum_{k\geqslant 0}\binom{n+k}{n}z^k}\end{align}\]</span> 即 <strong><span class="math inline">\(\frac{1}{(1-z)^{n+1}}\)</span> 是序列 <span class="math inline">\(\left\langle\binom{n+k}{n}\right\rangle\)</span> (<span class="math inline">\(n\)</span> 是常数，<span class="math inline">\(k\)</span> 是下标）的生成函数。</strong></p><p>取 <span class="math inline">\(n=0\)</span> 得到： <span class="math display">\[\frac{1}{1-z}=1+z+z^2+\cdots=\sum_{k\geqslant 0}z^k\]</span> 即 <strong><span class="math inline">\(\frac{1}{1-z}\)</span> 是 <span class="math inline">\(\langle1,1,\cdots\rangle\)</span> 的生成函数</strong>。</p><p>这特别有用，因为任何其他序列和 <span class="math inline">\(\langle1,1,\cdots\rangle\)</span> 的卷积都是<strong>前缀和</strong>的序列，即 <span class="math inline">\(c_n=\sum\limits_{k=0}^na_k\)</span>；<strong>如此，若 <span class="math inline">\(A(z)\)</span> 是 <span class="math inline">\(\langle a_n\rangle\)</span> 的生成函数，则 <span class="math inline">\(\frac{A(z)}{1-z}\)</span> 是 <span class="math inline">\(\left\langle \sum\limits_{k=0}^na_k \right\rangle\)</span> 的生成函数。</strong></p><blockquote><p>应用：错排问题。我们之前用反演解决了，现在用生成函数来解决。</p><p>我们已知： <span class="math display">\[n!=\sum_{k=0}^n\binom{n}{k}D_{n-k}\]</span> 上次是实施反演，这次两边同除 <span class="math inline">\(n!\)</span>： <span class="math display">\[1=\sum_{k=0}^n\frac{1}{k!}\frac{D_{n-k}}{(n-k)!}\]</span> 注意 <span class="math inline">\(\sum\limits_{k=0}^n\frac{1}{k!}\frac{D_{n-k}}{(n-k)!}\)</span> 是序列 <span class="math inline">\(\left\langle\frac{1}{k!}\right\rangle\)</span> 和 <span class="math inline">\(\left\langle\frac{D_{n-k}}{(n-k)!}\right\rangle\)</span> 的卷积，而 <span class="math inline">\(\left\langle\frac{1}{k!}\right\rangle\)</span> 的生成函数就是 <span class="math inline">\(e^z\)</span>，所以设 <span class="math inline">\(E(z)=\sum\limits_{n=0}^\infty\frac{D_n}{n!}z^n\)</span>，那么 <span class="math inline">\(E(z)\)</span> 和 <span class="math inline">\(e^z\)</span> 的乘积就是 <span class="math inline">\(\frac{1}{1-z}\)</span>，即： <span class="math display">\[\frac{1}{1-z}=e^zE(z)\]</span> 于是： <span class="math display">\[E(z)=\frac{1}{1-z}e^{-z}=(z^0+z^1+z^2+\cdots)\left(\frac{1}{0!}z^0-\frac{1}{1!}z^1+\frac{1}{2!}z^2-\cdots\right)\]</span> 比较系数得到： <span class="math display">\[\frac{D_n}{n!}=\sum_{k=0}^n\frac{(-1)^k}{k!}\]</span> 我们反演也是得到的这个式子。</p><p>启示：应用生成函数解决问题，常常是<strong>不断在封闭形式和展开式之间来回转化</strong>；麦克劳林级数天然就是一个生成函数。</p></blockquote><h3 id="广义二项级数和广义指数级数">广义二项级数和广义指数级数</h3><p><strong>定义</strong>： <span class="math display">\[\begin{align}&amp;\mathcal{B}_t(z)=\sum_{k\geqslant 0}(tk)^{\underline{k-1}}\frac{z^k}{k!}=\sum_{k\geqslant 0}\binom{tk}{k}\frac{z^k}{tk-k+1}\\&amp;\mathcal{E}_t(z)=\sum_{k\geqslant 0}(tk+1)^{k-1}\frac{z^k}{k!}\end{align}\]</span> 将在第7章证明： <span class="math display">\[\begin{align}&amp;\mathcal{B}_t(z)^{1-t}-\mathcal{B}_t(z)^{-t}=z\\&amp;\mathcal{E}_t(z)^{-t}\ln\mathcal{E}_t(z)=z\end{align}\]</span> （注意 <span class="math inline">\(t=0\)</span> 时，<span class="math inline">\(\mathcal{B}_0(z)=1+z,\,\mathcal{E}_0(z)=e^z\)</span>. ）</p><p><span class="math inline">\(\textbf{Johann Heinrich Lambert}\)</span> 注意到了它们满足恒等式： <span class="math display">\[\begin{cases}\mathcal{B}_t(z)^r=\sum\limits_{k\geqslant 0}\binom{tk+r}{k}\frac{r}{tk+r}z^k\\\mathcal{E}_t(z)^r=\sum\limits_{k\geqslant 0}r\frac{(tk+r)^{k-1}}{k!}z^k\end{cases}\quad\begin{cases}\frac{\mathcal{B}_t(z)^r}{1-t+t\mathcal{B}_t(z)^{-1}}=\sum\limits_{k\geqslant 0}\binom{tk+r}{k}z^k\\\frac{\mathcal{E}_t(z)^r}{1-zt\mathcal{E}_t(z)^t}=\sum\limits_{k\geqslant0}\frac{(tk+r)^k}{k!}z^k\end{cases}\]</span> 我们把这两组恒等式乘在一起，就得到一些新的恒等式。例如： <span class="math display">\[\begin{align}\mathcal{B}_t(z)^r\frac{\mathcal{B}_t(z)^s}{1-t+t\mathcal{B}_t(z)^{-1}}&amp;=\sum_{k\geqslant 0}\binom{tk+r}{k}\frac{r}{tk+r}z^k\sum_{j\geqslant 0}\binom{tj+s}{j}z^j\\&amp;=\sum_{n\geqslant 0}z^n\sum_{k\geqslant 0}\binom{tk+r}{k}\frac{r}{tk+r}\binom{tn-tk+s}{n-k}\end{align}\]</span> 同时，它又等于： <span class="math display">\[\frac{\mathcal{B}_t(z)^{r+s}}{1-t+t\mathcal{B}_t(z)^{-1}}=\sum_{n\geqslant 0}\binom{tn+s+r}{n}z^n\]</span> 于是比较系数，我们得到恒等式： <span class="math display">\[\sum_{k\geqslant 0}\binom{tk+r}{k}\binom{tn-tk+s}{n-k}\frac{r}{tk+r}=\binom{tn+s+r}{n}\]</span> 类似的操作可以得到其他恒等式。下表列出<strong>一般的卷积恒等式</strong>（对整数 <span class="math inline">\(n\geqslant 0\)</span> 成立）： <span class="math display">\[\color{purple}{\begin{align}&amp;\sum_{k}\binom{tk+r}{k}\binom{tn-tk+s}{n-k}\frac{r}{tk+r}=\binom{tn+s+r}{n}\\&amp;\sum_{k}\binom{tk+r}{k}\binom{tn-tk+s}{n-k}\frac{r}{tk+r}\cdot\frac{s}{tn-tk+s}=\binom{tn+r+s}{n}\frac{r+s}{tn+r+s}\\&amp;\sum_k\binom{n}{k}(tk+r)^k(tn-tk+s)^{n-k}\frac{r}{tk+r}=(tn+r+s)^n\\&amp;\sum_k\binom{n}{k}(tk+r)^k(tn-tk+s)^{n-k}\frac{r}{tk+r}\cdot\frac{s}{tn-tk+s}=(tn+r+s)^n\frac{r+s}{tn+r+s}\end{align}}\]</span> <br></p><p>我们取 <span class="math inline">\(t\)</span> 为一些特殊值代入，可以得到一些用途广泛、非常重要的级数： <span class="math display">\[\color{purple}{\begin{align}&amp;\mathcal{B}_2(z)=\sum_k\binom{2k}{k}\frac{z^k}{1+k}=\sum_k\binom{2k+1}{k}\frac{z^k}{1+2k}=\frac{1-\sqrt{1-4z}}{2z}\\&amp;\mathcal{B}_{-1}(z)=\sum_k\binom{1-k}{k}\frac{z^k}{1-k}=\sum_k\binom{2k-1}{k}\frac{(-z)^k}{1-2k}=\frac{1+\sqrt{1+4z}}{2}\\&amp;\mathcal{B}_2(z)^r=\sum_k\binom{2k+r}{k}\frac{r}{2k+r}z^k\\&amp;\mathcal{B}_{-1}(z)^r=\sum_k\binom{r-k}{k}\frac{r}{r-k}z^k\\&amp;\frac{\mathcal{B}_2(z)^r}{\sqrt{1-4z}}=\sum_k\binom{2k+r}{k}z^k\\&amp;\frac{\mathcal{B}_{-1}(z)^r}{\sqrt{1+4z}}=\sum_k\binom{r-k}{k}z^k\end{align}}\]</span> 其中，<span class="math inline">\(\mathcal{B}_2(z)\)</span> 的系数 <span class="math inline">\(\binom{2k}{k}\frac{1}{1+k}\)</span> 被称为 <span class="math inline">\(\textbf{Catalan}\)</span> 数 <span class="math inline">\(C_n\)</span>。</p><p>最后，给出一个推论，表明了 <span class="math inline">\(\mathcal{B}_{-1}(z)\)</span> 和 <span class="math inline">\(\mathcal{B}_2(-z)\)</span> 的关系： <span class="math display">\[\frac{\mathcal{B}_{-1}(z)^{n+1}-(-z)^{n+1}\mathcal{B}_2(-z)^{n+1}}{\sqrt{1+4z}}=\sum_{k\leqslant n}\binom{n-k}{k}z^k\]</span> 证：当 <span class="math inline">\(k&gt;n\)</span> 时， <span class="math display">\[\begin{align}[z^k]\frac{(-z)^{n+1}\mathcal{B}_2(-z)^{n+1}}{\sqrt{1+4z}}&amp;=(-1)^{n+1}[z^{k-n-1}]\frac{\mathcal{B}_2(-z)^{n+1}}{\sqrt{1+4z}}\\&amp;=(-1)^{n+1}(-1)^{k-n-1}[z^{k-n-1}]\frac{\mathcal{B}_2(z)^{n+1}}{\sqrt{1-4z}}&amp;&amp;置\;z:=-z\\&amp;=(-1)^k\binom{2k-n-1}{k-n-1}\\&amp;=(-1)^k\binom{2k-n-1}{k}\\&amp;=\binom{n-k}{k}\\&amp;=[z^k]\frac{\mathcal{B}_{-1}(z)^n}{\sqrt{1+4z}}\end{align}\]</span> 二者抵消。</p><p>于是我们利用 <span class="math inline">\(\mathcal{B}_2(z)\)</span> 和 <span class="math inline">\(\mathcal{B}_{-1}(z)\)</span> 的封闭形式可以得到上述推论的封闭形式： <span class="math display">\[\color{purple}{\sum_{k\leqslant n}\binom{n-k}{k}z^k=\frac{1}{\sqrt{1+4z}}\left(\left(\frac{1+\sqrt{1+4z}}{2}\right)^{n+1}-\left(\frac{1-\sqrt{1+4z}}{2}\right)^{n+1}\right)\quad,整数\;n\geqslant 0}\]</span> 类似的，我们有： <span class="math display">\[\color{purple}{\sum_{k&lt;n}\binom{n-k}{k}\frac{n}{n-k}z^k=\left(\frac{1+\sqrt{1+4z}}{2}\right)^n+\left(\frac{1-\sqrt{1+4z}}{2}\right)^n\quad,整数\;n&gt;0}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>具体数学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[具体数学]第五章·二项式系数（第一部分）</title>
    <link href="/blog-main/2020/08/16/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E4%BA%94%E7%AB%A0%C2%B7%E4%BA%8C%E9%A1%B9%E5%BC%8F%E7%B3%BB%E6%95%B0%EF%BC%88%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%89/"/>
    <url>/blog-main/2020/08/16/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E4%BA%94%E7%AB%A0%C2%B7%E4%BA%8C%E9%A1%B9%E5%BC%8F%E7%B3%BB%E6%95%B0%EF%BC%88%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<blockquote><p>第五章·二项式系数分8节，包含基本恒等式、生成函数、超几何函数、机械求和法等内容。这是笔记第一部分，包括基本恒等式、一些练习、处理的技巧，以及牛顿级数、二项式反演。</p></blockquote><h2 id="基本恒等式">基本恒等式</h2><h3 id="二项式系数">二项式系数</h3><p><span class="math display">\[\binom{r}{k}=\begin{cases}\frac{r(r-1)\cdots(r-k+1)}{k(k-1)\cdots 1}=\frac{r^{\underline{k}}}{k!},&amp;k\geqslant 0\\0,&amp;k&lt;0\end{cases}\]</span> 注意上式中，<span class="math inline">\(k\in \mathbb Z\)</span>，而 <span class="math inline">\(r\)</span> 可以是<strong>任意实数</strong>（甚至<strong>复数</strong>）。</p><ul><li>只有当 <span class="math inline">\(r,k\)</span> 取非负整数时，<span class="math inline">\(\binom{r}{k}\)</span> 才有组合解释。但是，鉴于二项式系数还有许多其他用途，所以将范围推广至实数；</li><li>可以把 <span class="math inline">\(\binom{r}{k}\)</span> 视为 <span class="math inline">\(r\)</span> 的 <span class="math inline">\(k\)</span> 次多项式，此观点常常有用；</li><li>下指标 <span class="math inline">\(k\)</span> 是非整数的情形应用很少，故这里不考虑。</li></ul><h3 id="最重要的十个二项式系数恒等式">最重要的十个二项式系数恒等式</h3><p><span class="math display">\[\color{purple}{\begin{align}&amp;\binom{n}{k}=\frac{n!}{k!(n-k)!},\quad 整数\;n\geqslant k\geqslant 0&amp;&amp;阶乘展开式\\&amp;\binom{n}{k}=\binom{n}{n-k},\quad 整数\;n\geqslant0,k\in\mathbb{Z}&amp;&amp;对称恒等式\\&amp;\binom{r}{k}=\frac{r}{k}\binom{r-1}{k-1},\quad 整数\;k\neq 0&amp;&amp;吸收/提取恒等式\\&amp;\binom{r}{k}=\binom{r-1}{k}+\binom{r-1}{k-1},\quad k\in\mathbb{Z}&amp;&amp;加法恒等式\\&amp;\binom{r}{k}=(-1)^k\binom{k-r-1}{k},\quad k\in\mathbb{Z}&amp;&amp;上指标反转\\&amp;\binom{r}{m}\binom{m}{k}=\binom{r}{k}\binom{r-k}{m-k},\quad m,k\in\mathbb{Z}&amp;&amp;三项式版恒等式\\&amp;\sum_{k}\binom{r}{k}x^ky^{r-k}=(x+y)^r，整数\;r\geqslant 0\;或者\; \left|\frac{x}{y}\right|&lt;1&amp;&amp;二项式定理\\&amp;\sum_{k\leqslant n}\binom{r+k}{k}=\binom{r+n+1}{n},\quad n\in\mathbb{Z}&amp;&amp;平行求和法\\&amp;\sum_{0\leqslant k\leqslant n}\binom{k}{m}=\binom{n+1}{m+1},\quad 整数\;m,n\geqslant 0&amp;&amp;上指标求和法\\&amp;\sum_{k}\binom{r}{k}\binom{s}{n-k}=\binom{r+s}{n},\quad n\in\mathbb{Z}&amp;&amp;范德蒙德卷积公式\end{align}}\]</span></p><p>其中，范德蒙德卷积公式有如下推论： <span class="math display">\[\color{blue}{\begin{align}&amp;\sum_k\binom{l}{m+k}\binom{s}{n+k}=\binom{l+s}{l-m+n}&amp;&amp;,整数\;l\geqslant 0,\,m,n\in\mathbb{Z}\\&amp;\sum_k\binom{l}{m+k}\binom{s+k}{n}(-1)^k=(-1)^{l+m}\binom{s-m}{n-l}&amp;&amp;,整数\;l\geqslant 0,\,m,n\in\mathbb{Z}\\&amp;\sum_{k\leqslant l}\binom{l-k}{m}\binom{s}{k-n}(-1)^k=(-1)^{l+m}\binom{s-m-1}{l-m-n}&amp;&amp;,整数\;l,m,n\geqslant 0\\&amp;\sum_{-q\leqslant k\leqslant l}\binom{l-k}{m}\binom{q+k}{n}=\binom{l+q+1}{m+n+1}&amp;&amp;,整数\;m,n\geqslant0,整数\;l+q\geqslant 0\end{align}}\]</span></p><blockquote><p>注意<strong>对称性+上指标反转</strong>可以把上/下指标中的变量给移动到下/上指标去。上述的推论都可以通过<strong>不断进行对称和反转</strong>得到。</p></blockquote><h3 id="上述恒等式的证明">上述恒等式的证明</h3><ul><li><p>吸收/提取恒等式： <span class="math display">\[\binom{r}{k}=\frac{r^{\underline{k}}}{k!}=\frac{r}{k}\frac{(r-1)^{\underline{k-1}}}{(k-1)!}=\frac{r}{k}\binom{r-1}{k-1}\]</span></p><blockquote><p>在杨辉三角中，即 <span class="math inline">\((r,k)\)</span> 的数等于 <span class="math inline">\((r-1,k-1)\)</span> 的数乘上 <span class="math inline">\(r/k\)</span>.</p></blockquote></li><li><p>加法恒等式：代数证明略，组合证明：先设 <span class="math inline">\(r\in\mathbb{Z}_+\)</span>，从 <span class="math inline">\(r\)</span> 个人中选取 <span class="math inline">\(k\)</span> 个人，等价于设一个特殊的人，选他+从 <span class="math inline">\(r-1\)</span> 人中选 <span class="math inline">\(k-1\)</span> 个，不选他+从 <span class="math inline">\(r-1\)</span> 人中选 <span class="math inline">\(k\)</span> 个。推广到实数，由于等号两边都是 <span class="math inline">\(r\)</span> 的 <span class="math inline">\(k\)</span> 次多项式，它们的差也是 <span class="math inline">\(r\)</span> 的 <span class="math inline">\(k\)</span> 次多项式，最多有 <span class="math inline">\(k\)</span> 个零点，或者恒为零；然而易知这个等式在 <span class="math inline">\(\mathbb{Z^*}\)</span> 内恒成立，已经有多于 <span class="math inline">\(k\)</span> 个零点了，所以二者之差恒为零，即等式在 <span class="math inline">\(\mathbb{R}\)</span> 上恒成立。<span class="math inline">\(\square\)</span></p><blockquote><p>上述过程称为<strong>多项式推理法</strong>，非常重要！</p><p>在杨辉三角中，即上两个相加等于下面一个。</p></blockquote></li><li><p>上指标反转： <span class="math display">\[(-1)^k\binom{k-r-1}{k}=(-1)^k\frac{(k-r-1)^{\underline k}}{k!}=\frac{r^{\underline k}}{k!}=\binom{r}{k}\]</span></p></li><li><p>三项式版恒等式： <span class="math display">\[\begin{align}&amp;\binom{r}{m}\binom{m}{k}=\frac{r!}{m!(r-m)!}\frac{m!}{k!(m-k)!}=\frac{r!}{(r-m)!k!(m-k)!}\\&amp;\binom{r}{k}\binom{r-k}{m-k}=\frac{r!}{k!(r-k)!}\frac{(r-k)!}{(m-k)!(r-m)!}=\frac{r!}{k!(m-k)!(r-m)!}\end{align}\]</span> 故二者相等。</p></li><li><p>平行求和法： <span class="math display">\[\begin{align}\sum_{k\leqslant n}\binom{r+k}{k}&amp;=\binom{r}{0}+\binom{r+1}{1}+\cdots+\binom{r+n}{n}\\&amp;=\binom{r}{-1}+\binom{r}{0}+\binom{r+1}{1}+\cdots+\binom{r+n}{n}&amp;&amp;因为\;\binom{r}{-1}=0\\&amp;=\binom{r+1}{0}+\binom{r+1}{1}+\cdots+\binom{r+n}{n}&amp;&amp;前两项加法恒等式合并\\&amp;=\cdots&amp;&amp;不断合并\\&amp;=\binom{r+n+1}{n}\end{align}\]</span></p><blockquote><p>在杨辉三角中，即一条斜 <span class="math inline">\(45^\circ\)</span> 向下的线上的所有值加起来等于末尾那个数的下一行的数。</p></blockquote></li><li><p>上指标求和法： <span class="math display">\[\begin{align}\sum_{0\leqslant k\leqslant n}\binom{k}{m}&amp;=\binom{0}{m}+\binom{1}{m}+\cdots+\binom{n}{m}\\&amp;=\binom{0}{m+1}+\binom{0}{m}+\binom{1}{m}+\cdots+\binom{n}{m}&amp;&amp;因为\;\binom{0}{m+1}=0\\&amp;=\binom{1}{m+1}+\binom{1}{m}+\cdots+\binom{n}{m}&amp;&amp;前两项加法恒等式合并\\&amp;=\cdots&amp;&amp;不断合并\\&amp;=\binom{n+1}{m+1}\end{align}\]</span></p><blockquote><p>在杨辉三角中，即一条竖线上的 <span class="math inline">\(n\)</span> 个值加起来等于末尾那个值的右下角的值。</p></blockquote></li><li><p>范德蒙德卷积公式：组合证明：先设 <span class="math inline">\(r,s\in\mathbb{Z}_+\)</span>，左式是从 <span class="math inline">\(r\)</span> 个男生中选 <span class="math inline">\(k\)</span> 个，加上从 <span class="math inline">\(s\)</span> 个女生中选 <span class="math inline">\(n-k\)</span> 个，枚举这个 <span class="math inline">\(k\)</span>，右式是从 <span class="math inline">\(r+s\)</span> 个人中选 <span class="math inline">\(n\)</span> 个人，二者显然相等。然后再由前面提及的<strong>多项式推理法</strong>推广到 <span class="math inline">\(\mathbb{R}\)</span> 上。</p></li></ul><blockquote><p>我的批注：上面许多恒等式都可以在<strong>杨辉三角</strong>中形象化地记忆，其证明也在杨辉三角中有直观体现。</p><p><img src="形象化.png" /></p></blockquote><p>除此以外，关于二项式系数的恒等式还有茫茫多……</p><h2 id="基本练习">基本练习</h2><h3 id="问题1比值的和式">问题1：比值的和式</h3><p>求 <span class="math display">\[\sum_{k=0}^m\frac{\binom{m}{k}}{\binom{n}{k}},\quad整数\;n\geqslant m\geqslant 0\]</span> 的封闭形式。</p><p>解：由三项式恒等式 <span class="math inline">\(\binom{r}{m}\binom{m}{k}=\binom{r}{k}\binom{r-k}{m-k}\)</span>，可知：<span class="math inline">\(\binom{m}{k}/\binom{n}{k}=\binom{n-k}{m-k}/\binom{n}{m}\)</span>，于是： <span class="math display">\[\begin{align}\sum_{k=0}^m\frac{\binom{m}{k}}{\binom{n}{k}}&amp;=\sum_{k=0}^m\frac{\binom{n-k}{m-k}}{\binom{n}{m}}\\&amp;=\frac{1}{\binom{n}{m}}\sum_{k=0}^m\binom{n-k}{m-k}\\&amp;=\frac{1}{\binom{n}{m}}\sum_{t=0}^m\binom{n-m+t}{t}&amp;&amp;令\;t=m-k\\&amp;=\frac{1}{\binom{n}{m}}\binom{n+1}{m}&amp;&amp;平行求和法\\&amp;=\frac{n+1}{n+1-m}\end{align}\]</span></p><blockquote><p>上述过程怎么想到的呢？首先运用三项式恒等式可以使分母不含 <span class="math inline">\(k\)</span>，得以提出求和号；然后，考虑 <span class="math inline">\(\sum\limits_{k=0}^m\binom{n-k}{m-k}\)</span> 在杨辉三角中的形态，是一个斜 <span class="math inline">\(45^\circ\)</span> 向下的形态，于是转换到平行求和法。</p></blockquote><h3 id="问题2来自排序文献">问题2：来自排序文献</h3><p>求 <span class="math display">\[T=\sum_{k=0}^nk\binom{m-k-1}{m-n-1}/\binom{m}{n}\]</span> 的封闭形式。</p><p>解：</p><p>设 <span class="math inline">\(S=\sum\limits_{k=0}^nk\binom{m-k-1}{m-n-1}\)</span>，<span class="math inline">\(T\)</span> 即是 <span class="math inline">\(S/\binom{m}{n}\)</span>，下面计算 <span class="math inline">\(S\)</span>： <span class="math display">\[\begin{align}S&amp;=\sum_{k=0}^nk\binom{m-k-1}{m-n-1}\\&amp;=\sum_{k=0}^nm\binom{m-k-1}{m-n-1}-\sum_{k=0}^n(m-k)\binom{m-k-1}{m-n-1}\\&amp;=m\sum_{k=0}^{n}\binom{m-k-1}{m-n-1}-(m-n)\sum_{k=0}^n\binom{m-k}{m-n}&amp;&amp; 对后者使用吸收恒等式\\&amp;=mA-(m-n)B\end{align}\]</span> 其中， <span class="math display">\[\begin{align}A&amp;=\sum_{k=0}^n\binom{m-k-1}{m-n-1}\\&amp;=\sum_{t=m-n-1}^{m-1}\binom{t}{m-n-1}&amp;&amp;令\;t=m-k-1\\&amp;=\binom{m}{m-n}&amp;&amp;实施上指标求和法\\\\B&amp;=\sum_{k=0}^n\binom{m-k}{m-n}\\&amp;=\sum_{t=m-n}^m\binom{t}{m-n}&amp;&amp;令\;t=m-k\\&amp;=\binom{m+1}{m-n+1}&amp;&amp;实施上指标求和法\end{align}\]</span></p><p>于是乎： <span class="math display">\[\begin{align}S&amp;=mA-(m-n)B\\&amp;=m\binom{m}{m-n}-(m-n)\binom{m+1}{m-n+1}\\&amp;=\binom{m}{m-n}\left[m-(m-n)\frac{m+1}{m-n+1}\right]&amp;&amp;吸收恒等式\\&amp;=\frac{m!}{(m-n+1)!(n-1)!}=\binom{m}{n-1}\end{align}\]</span></p><p>故 <span class="math display">\[T=S/\binom{m}{n}=\frac{m!}{(m-n+1)!(n-1)!}\cdot\frac{n!(m-n)!}{m!}=\frac{n}{m-n+1}\]</span></p><blockquote><p>上述解答的思考过程：首先，<span class="math inline">\(S\)</span> 中二项式系数外有个 <span class="math inline">\(k\)</span>，自然想到用吸收恒等式把 <span class="math inline">\(k\)</span> 给<strong>吸进去</strong>，为了应用吸收恒等式，把原式拆成了两部分；观察 <span class="math inline">\(A,B\)</span> 的形态，在杨辉三角中，它们都是某一竖条相加，自然转换成上指标求和法求解。</p></blockquote><h3 id="问题3来自以往的考试题">问题3：来自以往的考试题</h3><p>求 <span class="math display">\[Q_n=\sum_{k\leqslant 2^n}\binom{2^n-k}{k}(-1)^k\]</span> 解：似乎没有什么公式可供使用，我们考虑求出它的<strong>递归式</strong>。然而，<span class="math inline">\(n\)</span> 总是以 <span class="math inline">\(2\)</span> 的幂次跳跃，在杨辉三角中相距 <span class="math inline">\(2^k\)</span> 行数的两个数也不好找什么关系（我们知道的关系时加法恒等式，给定相邻两行的关系）。不过这给我们启发，我们不妨考虑一个更普适的问题： <span class="math display">\[R_m=\sum_{k\leqslant m}\binom{m-k}{k}(-1)^k\]</span> 那么 <span class="math inline">\(Q_n=R_{2^n}\)</span>. 若解得 <span class="math inline">\(R_m\)</span>，则自然解得 <span class="math inline">\(Q_n\)</span>. <span class="math display">\[\begin{align}R_m&amp;=\sum_{k\leqslant m}\binom{m-k}{k}(-1)^k\\&amp;=\sum_{k\leqslant m}\binom{m-k-1}{k}(-1)^k+\sum_{k\leqslant m}\binom{m-k-1}{k-1}(-1)^k\\&amp;=\left[\sum_{k\leqslant m-1}\binom{m-k-1}{k}(-1)^k+\binom{-1}{m}(-1)^m\right]+\left[\sum_{k\leqslant m-1}\binom{m-k-2}{k}(-1)^{k+1}\right]\\&amp;=\left[R_{m-1}+{(-1)}^{2m}\right]+\left[-\sum_{k\leqslant m-2}\binom{m-2-k}{k}(-1)^{k}+\binom{-1}{m-1}(-1)^{m}\right]\\&amp;=R_{m-1}+1-R_{m-2}+(-1)^{2m-1}\\&amp;=R_{m-1}-R_{m-2}\end{align}\]</span></p><p>于是 <span class="math inline">\(R_m=R_{m-1}-R_{m-2}=(R_{m-2}-R_{m-3})-R_{m-2}=-R_{m-3}=R_{m-6}\)</span>，以 <span class="math inline">\(6\)</span> 为周期，故： <span class="math display">\[R_m=\begin{cases}1&amp;m\bmod 6=0\\1&amp;m\bmod 6=1\\0&amp;m\bmod 6=2\\-1&amp;m\bmod 6=3\\-1&amp;m\bmod 6=4\\0&amp;m\bmod 6=5\\\end{cases}\]</span> 故： <span class="math display">\[Q_n=R_{2^n}=\begin{cases}1&amp;n=0\\0&amp;n\;奇数\\-1&amp;n\;偶数且\;n&gt;0\end{cases}\]</span></p><blockquote><p>当找不到公式予以化简时，不妨考虑寻找<strong>递归式</strong>。</p></blockquote><h3 id="问题4包含两个二项式系数的和式">问题4：包含两个二项式系数的和式</h3><p>求 <span class="math display">\[\sum_{k=0}^nk\binom{m-k-1}{m-n-1}\quad,整数\;m&gt;n\geqslant 0\]</span> 的封闭形式。</p><p>解：这个就是问题2中的 <span class="math inline">\(S\)</span>，不过我们换一个角度看它。视 <span class="math inline">\(k\)</span> 为 <span class="math inline">\(\binom{k}{1}\)</span>，再根据范德蒙德卷积公式的推论公式：<span class="math inline">\(\sum\limits_{-q\leqslant k\leqslant l}\binom{l-k}{m}\binom{q+k}{n}=\binom{l+q+1}{m+n+1}\)</span>，有： <span class="math display">\[\sum_{k=0}^n\binom{k}{1}\binom{m-k-1}{m-n-1}=\binom{m}{m-n+1}\]</span> &gt; 注意应用推论公式的条件是 <span class="math inline">\(0\leqslant k &lt; m\)</span>，这里只加到了 <span class="math inline">\(n\)</span> 仍是正确的，是因为当 <span class="math inline">\(k&gt;n\)</span> 时，<span class="math inline">\(m-k-1&lt;m-n-1\)</span>，于是 <span class="math inline">\(\binom{m-k-1}{m-n-1}=0\)</span>.</p><h3 id="问题5有三个因子的和式">问题5：有三个因子的和式</h3><p>求 <span class="math display">\[\sum_k\binom{n}{k}\binom{s}{k}k\quad,整数\;n\geqslant 0\]</span> 的封闭形式。</p><p>解： <span class="math display">\[\begin{align}\sum_k\binom{n}{k}\binom{s}{k}k&amp;=\sum_{k}\binom{n}{k}\binom{s-1}{k-1}s&amp;&amp;提取恒等式\\&amp;=s\sum_k\binom{n}{n-k}\binom{s-1}{k-1}&amp;&amp;对称性\\&amp;=s\binom{n+s-1}{n-1}&amp;&amp;范德蒙德卷积\end{align}\]</span></p><blockquote><p>如果把 <span class="math inline">\(k\)</span> 吸进 <span class="math inline">\(\binom{n}{k}\)</span>，得到的 <span class="math inline">\(\binom{n-1}{k-1}\)</span> 中 <span class="math inline">\(n-1\)</span> 有可能是负的，则不能运用对称性继续推导了（一定注意对称性要求上指标是<strong>正整数</strong>）。</p></blockquote><h3 id="问题6一个令人惊悚的和式">问题6：一个令人惊悚的和式</h3><p>求 <span class="math display">\[\sum_{k\geqslant 0}\binom{n+k}{2k}\binom{2k}{k}\frac{(-1)^k}{k+1}\quad,整数\;n\geqslant k\]</span> 的封闭形式。</p><p>解： <span class="math display">\[\begin{align}\sum_{k\geqslant 0}\binom{n+k}{2k}\binom{2k}{k}\frac{(-1)^k}{k+1}&amp;=\sum_{k\geqslant 0}\binom{n+k}{k}\binom{n}{k}\frac{(-1)^k}{k+1}&amp;&amp;三项式版恒等式\\&amp;=\sum_{k\geqslant 0}\binom{n+k}{k}\binom{n+1}{k+1}\frac{1}{n+1}(-1)^k&amp;&amp;吸收恒等式\\&amp;=\frac{1}{n+1}\sum_{k\geqslant0}\binom{-n-1}{k}\binom{n+1}{k+1}&amp;&amp;上指标反转\\&amp;=\frac{1}{n+1}\sum_{k\geqslant 0}\binom{-n-1}{k}\binom{n+1}{n-k}&amp;&amp;对称性\\&amp;=\frac{1}{n+1}\binom{0}{n}=[n=0]\end{align}\]</span></p><blockquote><p>再三强调对称性只能应用在上指标是正整数的情形下，上面推到过程如果对 <span class="math inline">\(\binom{n+k}{k}\)</span> 使用了对称性，会得到错误的结果（恒为 <span class="math inline">\(0\)</span>）。</p></blockquote><h3 id="问题7新的障碍">问题7：新的障碍</h3><p>求 <span class="math display">\[\sum_{k\geqslant 0}\binom{n+k}{m+2k}\binom{2k}{k}\frac{(-1)^k}{k+1}\quad,整数\;m,n&gt;0\]</span> 的封闭形式，</p><p>解： <span class="math display">\[\begin{align}&amp;\sum_{k\geqslant 0}\binom{n+k}{m+2k}\binom{2k}{k}\frac{(-1)^k}{k+1}\\=&amp;\sum_{k\geqslant 0}\sum_{0\leqslant j\leqslant n+k-1}\binom{n+k-1-j}{2k}\binom{j}{m-1}\binom{2k}{k}\frac{(-1)^k}{k+1}&amp;&amp;逆用范德蒙德卷积公式的推论\\=&amp;\sum_{j\geqslant 0}\binom{j}{m-1}\sum_{k\geqslant j-n+1\\k\geqslant 0}\binom{n+k-1-j}{2k}\binom{2k}{k}\frac{(-1)^k}{k+1}&amp;&amp;求和号换序\\=&amp;\sum_{0\leqslant j&lt; n}\binom{j}{m-1}[n=j+1]&amp;&amp;上一题结论\\=&amp;\binom{n-1}{m-1}\end{align}\]</span></p><blockquote><p>解题思路：如果 <span class="math inline">\(m=0\)</span>，就是上一题了，可惜这个 <span class="math inline">\(m\)</span> 使得三项式版恒等式无法使用，后续也就难以进行了。既然这个 <span class="math inline">\(m\)</span> 这么讨厌，那就让它消失，精彩之处在于<strong>逆用</strong>范德蒙德卷积公式的推论达到这个效果。</p></blockquote><h3 id="问题8不同的障碍">问题8：不同的障碍</h3><p>求 <span class="math display">\[\sum_{k\geqslant0}\binom{n+k}{2k}\binom{2k}{k}\frac{(-1)^k}{k+1+m}\quad,整数\;m,n\geqslant 0\]</span> 的封闭形式。</p><p>解：</p><p><span class="math display">\[\begin{align}\sum_{k\geqslant0}\binom{n+k}{2k}\binom{2k}{k}\frac{(-1)^k}{k+1+m}&amp;=\sum_{k\geqslant 0}\binom{n+k}{k}\binom{n}{k}\frac{(-1)^k}{k+1+m}&amp;&amp;三项式版恒等式\end{align}\]</span></p><p>根据<strong>问题1的结论</strong>，<span class="math inline">\(\sum\limits_{k=0}^m{\binom{m}{k}}{\binom{n}{k}}^{-1}=\frac{n+1}{n+1-m},\quad整数\;n\geqslant m\geqslant 0\)</span>，于是 <span class="math inline">\(\frac{1}{k+1+m}=\frac{1}{k+1}\sum\limits_{j=0}^m\binom{m}{j}{\binom{-k-2}{j}}^{-1}\)</span>. 带入上式得： <span class="math display">\[\begin{align}原式&amp;=\sum_{k\geqslant 0}\sum\limits_{j=0}^m\binom{n+k}{k}\binom{n}{k}\frac{(-1)^k}{k+1}\binom{m}{j}{\binom{-k-2}{j}}^{-1}\\&amp;=\sum_{j=0}^m\sum_{k\geqslant 0}\binom{n+k}{k}\binom{n+1}{k+1}\frac{(-1)^k}{n+1}\binom{m}{j}{\binom{-k-2}{j}}^{-1}&amp;&amp;吸收恒等式\end{align}\]</span> 接下来作者就把它展开成阶乘，然后写回二项式系数了……可我实在是没看出来作者怎么做到的，所以这个坑先留着吧……</p><h2 id="处理的技巧">处理的技巧</h2><h3 id="技巧1取一半">技巧1：取一半</h3><p><strong>加倍公式</strong>（duplication formula）： <span class="math display">\[\color{blue}{r^{\underline k}{\left(r-\frac{1}{2}\right)}^{\underline k}=(2r)^{\underline {2k}}/2^{2k}\quad,整数\;k\geqslant 0}\]</span> 证明： <span class="math display">\[\begin{align}r^{\underline k}{\left(r-\frac{1}{2}\right)}^{\underline k}&amp;=r(r-1)\cdots (r-k+1)\cdot \left(r-\frac{1}{2}\right)\left(r-\frac{3}{2}\right)\cdots\left(r-\frac{2k-1}{2}\right)\\&amp;=r\left(r-\frac{1}{2}\right)(r-1)\left(r-\frac{3}{2}\right)\cdots \left(r-\frac{2k-1}{2}\right)(r-k+1)\\&amp;=2r(2r-1)(2r-2)(2r-3)\cdots(2r-2k+1)(2r-2k+2)/2^{2k}\\&amp;={(2r)}^{\underline {2k}}/2^{2k}\end{align}\]</span> 证毕。</p><p>对加倍公式两边除以 <span class="math inline">\({k!}^2\)</span>，得到： <span class="math display">\[\color{purple}{\binom{r}{k}\binom{r-\frac{1}{2}}{k}=\binom{2r}{2k}\binom{2k}{k}/2^{2k}\quad,k\in\mathbb{Z}}\tag{1}\]</span> 如果令 <span class="math inline">\(k=r=n\)</span>，得到： <span class="math display">\[\color{purple}{\binom{n-\frac{1}{2}}{n}=\binom{2n}{n}/{2^{2n}}\quad,n\in\mathbb{Z}}\tag{2}\]</span> 对上式进行<strong>反转上指标</strong>，得到： <span class="math display">\[\color{purple}{(-1)^n\binom{-\frac{1}{2}}{n}=\binom{2n}{n}/2^{2n}}\implies\color{purple}{\binom{-\frac{1}{2}}{n}={\left(-\frac{1}{4}\right)}^n\binom{2n}{n}\quad,n\in\mathbb{Z}}\tag{3}\]</span> 在 <span class="math inline">\((1)\)</span> 式中，设 <span class="math inline">\(r=\frac{n}{2}\)</span>，对所有 <span class="math inline">\(k\in\mathbb{Z}\)</span> 求和，得到： <span class="math display">\[\color{purple}{\begin{align}\sum_k\binom{n}{2k}\binom{2k}{k}2^{-2k}&amp;=\sum_k\binom{\frac{n}{2}}{k}\binom{\frac{n-1}{2}}{k}\\&amp;=\sum_k\binom{\frac{n}{2}}{\frac{n}{2}-k}\binom{\frac{n-1}{2}}{k}\;或\;\sum_k\binom{\frac{n}{2}}{k}\binom{\frac{n-1}{2}}{\frac{n-1}{2}-k}&amp;&amp;选整数那个应用对称性\\&amp;=\binom{n-\frac{1}{2}}{\frac{n}{2}}\;或\;\binom{n-\frac{1}{2}}{\frac{n-1}{2}}&amp;&amp;范德蒙德卷积\\&amp;=\binom{n-\frac{1}{2}}{\left\lfloor\frac{n}{2}\right\rfloor}\end{align}}\tag{4}\]</span> 由范德蒙德卷积我们知道：<span class="math inline">\(\sum\limits_k\binom{-1/2}{k}\binom{-1/2}{n-k}=\binom{-1}{n}=(-1)^n\)</span>，如果把 <span class="math inline">\((3)\)</span> 式结论代入得到： <span class="math display">\[\binom{-1/2}{k}\binom{-1/2}{n-k}=\left(-\frac{1}{4}\right)^n\binom{2k}{k}\binom{2n-2k}{n-k}\]</span> 于是我们有： <span class="math display">\[\color{purple}{\sum_k\binom{2k}{k}\binom{2n-2k}{n-k}=4^n\quad,\;整数\;n\geqslant 0}\tag{5}\]</span></p><blockquote><p>这个技巧常常怎么使用呢？我们对形如 <span class="math inline">\(\binom{2k}{k}\)</span> 的二项式系数特别不爽，因为无论是对称性还是上指标反转都无法将其处理为上下指标只有一边含有 <span class="math inline">\(k\)</span> 的形式。还记得问题6、7、8中，我们是靠着三项式版恒等式把它干掉的。现在我们多了一种工具，即把 <span class="math inline">\(\binom{2k}{k}\)</span> 写成 <span class="math inline">\(\binom{n-1/2}{k}\)</span> 的式子，其中 <span class="math inline">\(n\)</span> 是某个适当的整数（常取 <span class="math inline">\(0,1,k\)</span>）。核心公式： <span class="math display">\[\binom{2k}{k}=2^{2k}\binom{r}{k}\binom{r-1/2}{k}\binom{2r}{2k}^{-1}\]</span></p></blockquote><h3 id="技巧2高阶差分">技巧2：高阶差分</h3><p>差分： <span class="math display">\[\Delta f(x)=f(x+1)-f(x)\]</span> 高阶差分： <span class="math display">\[\begin{align}&amp;\Delta^2f(x)=\Delta f(x+1)-\Delta f(x)=f(x+2)-2f(x+1)+f(x)\\&amp;\Delta^3f(x)=\Delta^2f(x+1)-\Delta^2f(x)=f(x+3)-3f(x+2)+3f(x+1)-f(x)\\&amp;\cdots\\&amp;\color{purple}{\Delta^nf(x)=\sum_{k}\binom{n}{k}(-1)^{n-k}f(x+k)}\end{align}\]</span> 利用<strong>算子</strong>证明：<span class="math inline">\(E\)</span> 移位算子，<span class="math inline">\(1\)</span> 恒等算子，那么差分算子 <span class="math inline">\(\Delta=E-1\)</span>，对算子应用二项式定理： <span class="math display">\[\Delta^n=(E-1)^n=\sum_k\binom{n}{k}E^k(-1)^{n-k}\]</span> 证毕。</p><p><br></p><h4 id="应用1负的下降幂的一个情形">应用1（负的下降幂的一个情形）</h4><p>考虑函数 <span class="math inline">\(f(x)=(x-1)^{\underline {-1}}=\frac{1}{x}\)</span>，则根据 <span class="math inline">\(\Delta x^{\underline n}=nx^{\underline {n-1}}\)</span>，有： <span class="math display">\[\Delta^n\left((x-1)^{\underline{-1}}\right)=(-1)^n\frac{n!}{x(x+1)\cdots(x+n)}\]</span> 代入高阶差分式有： <span class="math display">\[\begin{align}&amp;\sum_k\binom{n}{k}(-1)^{n-k}\frac{1}{x+k}=(-1)^n\frac{n!}{x(x+1)\cdots (x+n)}\\\implies&amp;\color{purple}{\sum_k\binom{n}{k}\frac{(-1)^k}{x+k}=\frac{n!}{x(x+1)\cdots(x+n)}=x^{-1}{\binom{x+n}{n}}^{-1}\quad,x\neq0,-1,\cdots,-n}\end{align}\]</span> 两个角度看，我们对和式求出了封闭形式，或者说对 <span class="math inline">\(n!/x(x+1)\cdots(x+n)\)</span> 求得了一个部分分式的展开形式。</p><h4 id="应用2牛顿级数">应用2（牛顿级数）</h4><p>设 <span class="math inline">\(f(x)=a_dx^d+a_{d-1}x^{d-1}+\cdots+a_1x^1+a_0x^0\)</span> 是一个 <span class="math inline">\(d\)</span> 次多项式，则 <span class="math inline">\(\Delta f(x)\)</span> 是 <span class="math inline">\(d-1\)</span> 次多项式，……，<span class="math inline">\(\Delta^df(x)\)</span> 是一个常数，对于 <span class="math inline">\(n&gt;d\)</span>，<span class="math inline">\(\Delta^nf(x)=0\)</span>.</p><p>根据后面章节的内容，任何幂次都能表示成下降幂的和式，从而 <span class="math inline">\(f(x)\)</span> 可以写作：<span class="math inline">\(f(x)=b_dx^{\underline d}+b_{d-1}x^{\underline{d-1}}+\cdots b_1x^{\underline 1}+b_0x^{\underline 0}\)</span>. 又设 <span class="math inline">\(c_k=k!\cdot b_k\)</span>，则有： <span class="math display">\[f(x)=c_d\binom{x}{d}+c_{d-1}\binom{x}{d-1}+\cdots+c_1\binom{x}{1}+c_0\binom{x}{0}\]</span> 即任何多项式都可以表示成二项式系数的倍数之和。该展开式称为<strong>牛顿级数</strong>。</p><p>由加法恒等式： <span class="math display">\[\Delta\binom{x}{k}=\binom{x+1}{k}-\binom{x}{k}=\binom{x}{k-1}\]</span> 可得牛顿级数的 <span class="math inline">\(n\)</span> 阶差分为： <span class="math display">\[\Delta^nf(x)=c_d\binom{x}{d-n}+c_{d-1}\binom{x}{d-1-n}+\cdots+c_1\binom{x}{1-n}+c_0\binom{x}{-n}\]</span> 令 <span class="math inline">\(x=0\)</span>，除了 <span class="math inline">\(k=n\)</span> 的项 <span class="math inline">\(c_k\binom{x}{k-n}=c_k\)</span>，其他项都是 <span class="math inline">\(0\)</span>，于是： <span class="math display">\[\Delta^nf(0)=\begin{cases}c_n&amp;n\leqslant d\\0&amp;n&gt;d\end{cases}\]</span> 因此牛顿级数写作： <span class="math display">\[\color{purple}{f(x)=\Delta^df(0)\binom{x}{d}+\Delta^{d-1}f(0)\binom{x}{d-1}+\cdots+\Delta f(0)\binom{x}{1}+f(0)\binom{x}{0}=\sum_{k=0}^d\Delta^kf(0)\binom{x}{k}}\]</span> 我们可以对上式进行推广，如果 <span class="math inline">\(x\)</span> 是非负整数，显然把它写成无穷项也是没有问题的（后面的都是 <span class="math inline">\(0\)</span>），但是如果 <span class="math inline">\(x\)</span> 不是非负整数，<strong>无穷级数是否收敛，收敛到的值是否是原来的值</strong>都是问题。不过我们还是将其写出为好： <span class="math display">\[\color{purple}{\begin{align}f(x)&amp;=f(0)\binom{x}{0}+\Delta f(0)\binom{x}{1}+\Delta^2f(0)\binom{x}{2}+\cdots\\&amp;=\frac{f(0)}{0!}x^{\underline 0}+\frac{\Delta f(0)}{1!}x^{\underline 1}+\frac{\Delta^2f(0)}{2!}x^{\underline 2}+\cdots\end{align}}\]</span> 想到了什么？没错，正式麦克劳林级数： <span class="math display">\[f(x)=\frac{f(0)}{0!}x^0+\frac{f&#39;(0)}{1!}x+\frac{f&#39;&#39;(0)}{2!}x^2+\cdots\]</span> 如果设 <span class="math inline">\(f(x)=g(x+a)\)</span>，我们可以得到类比泰勒级数的结果： <span class="math display">\[\color{purple}{g(x+a)=\frac{g(a)}{0!}x^{\underline 0}+\frac{\Delta g(a)}{1!}x^{\underline 1}+\frac{\Delta^2g(a)}{2!}x^{\underline 2}+\cdots}\]</span> 换句话说，<strong>牛顿级数是泰勒级数的离散形式</strong>。</p><h4 id="应用3textbfjames-stirling-对阶乘的推广">应用3（<span class="math inline">\(\textbf{James Stirling}\)</span> 对阶乘的推广）</h4><p><span class="math inline">\(\textbf{James Stirling}\)</span> 利用牛顿级数把阶乘函数推广到非整数的值，他将 <span class="math inline">\(\ln x!\)</span> 展开： <span class="math display">\[\ln x!=\sum_n s_n\binom{x}{n}\]</span> 根据牛顿级数的内容和高阶差分公式： <span class="math display">\[\begin{align}s_n&amp;=\Delta^n (\ln x!)|_{x=0}\\&amp;=\Delta^{n-1}(\ln (x+1)!-\ln x!)|_{x=0}\\&amp;=\Delta^{n-1}\ln(x+1)|_{x=0}\\&amp;=\sum_{k}\binom{n-1}{k}(-1)^{n-1-k}\ln (x+k+1)|_{x=0}\\&amp;=\sum_k\binom{n-1}{k}(-1)^{n-1-k}\ln(k+1)\end{align}\]</span></p><blockquote><p>上述推导中，先使用了一个差分把阶乘干掉了，再套用高阶差分公式，着实妙哉！</p></blockquote><p>于是，<span class="math inline">\(s_0=s_1=0,s_2=\ln 2,s_3=\ln\frac{3}{4},s_4=\ln\frac{32}{27},\cdots\)</span>. 我们得到的级数可以证明是收敛的，这样代入非整数的值 <span class="math inline">\(x\)</span> 进入 <span class="math inline">\(\ln x!\)</span> 的展开式就可以知道 <span class="math inline">\(x!\)</span> 的值了。</p><h3 id="技巧3反演">技巧3：反演</h3><p><strong>二项式反演</strong>： <span class="math display">\[\color{purple}{g(n)=\sum_k\binom{n}{k}(-1)^kf(k)\iff f(n)=\sum_k\binom{n}{k}(-1)^kg(k)}\]</span> 证明：由于 <span class="math inline">\(f\)</span> 与 <span class="math inline">\(g\)</span> 完全对称，只需要证明必要性。已知 <span class="math inline">\(g(n)=\sum\limits_k\binom{n}{k}(-1)^kf(k)\)</span>，那么： <span class="math display">\[\begin{align}\sum_k\binom{n}{k}(-1)^kg(k)&amp;=\sum_k\binom{n}{k}(-1)^k\sum_j\binom{k}{j}(-1)^jf(j)\\&amp;=\sum_jf(j)\sum_k\binom{n}{k}\binom{k}{j}(-1)^{j+k}&amp;&amp;求和号换序\\&amp;=\sum_jf(j)\sum_k(-1)^{j+k}\binom{n}{j}\binom{n-j}{k-j}&amp;&amp;三项式版恒等式\\&amp;=\sum_jf(j)\binom{n}{j}\sum_t(-1)^t\binom{n-j}{t}&amp;&amp;令\;t=k-j\\&amp;=\sum_jf(j)\binom{n}{j}(1-1)^{n-j}&amp;&amp;二项式定理\\&amp;=\sum_jf(j)\binom{n}{j}[n=j]\\&amp;=f(n)\end{align}\]</span> 证毕。</p><p>另一个形式： <span class="math display">\[\color{purple}{g(n)=\sum_k\binom{n}{k}f(k)\iff f(n)=(-1)^n\sum_k\binom{n}{k}(-1)^kg(k)}\]</span> 只需要把 <span class="math inline">\((-1)^kf(k)\)</span> 视为一个整体就可以得到上式。</p><h4 id="应用足球胜利问题">应用（足球胜利问题）</h4><p><span class="math inline">\(n\)</span> 个球迷把他们的帽子抛向空中，这些帽子随机落下，使得每个球迷都随机得到一个帽子，有多少种方式 <span class="math inline">\(h(n,k)\)</span> 使恰好 <span class="math inline">\(k\)</span> 个球迷拿到他们自己的帽子？</p><blockquote><p><span class="math inline">\(D_n=h(n,0)\)</span> 就是著名的<strong>错排问题</strong>。</p></blockquote><p>显然，<span class="math inline">\(h(n,k)=\binom{n}{k}h(n-k,0)\)</span>，即选出 <span class="math inline">\(k\)</span> 人拿自己的帽子，其余 <span class="math inline">\(n-k\)</span> 人都不是自己的帽子。</p><p>一个简单的观察是，对于所有 <span class="math inline">\(k\)</span>，<span class="math inline">\(h(n,k)\)</span> 的和就是所有可能的排列，即 <span class="math inline">\(n!\)</span>，所以我们有： <span class="math display">\[\sum_k h(n,k)=\sum_k\binom{n}{k}h(n-k,0)=\sum_k\binom{n}{n-k}h(k,0)=\sum_k\binom{n}{k}h(k,0)=n!\]</span></p><p>实施二项式反演，得到： <span class="math display">\[D_n=h(n,0)=(-1)^n\sum_k\binom{n}{k}(-1)^kk!\]</span> 化简该式： <span class="math display">\[\begin{align}D_n&amp;=(-1)^n\sum_k\binom{n}{k}(-1)^kk!\\&amp;=\sum_{k=0}^n\frac{n!}{(n-k)!}(-1)^{n+k}\\&amp;=n!\sum_{k=0}^n\frac{(-1)^k}{k!}\\\end{align}\]</span></p><p>我们知道：<span class="math inline">\(e^{-x}=\sum\limits_{n=0}^\infty\frac{(-x)^n}{n!}\implies e^{-1}=\sum\limits_{n=0}^\infty\frac{(-1)^n}{n!}\)</span>，于是 <span class="math inline">\(D_n\)</span> 和 <span class="math inline">\(n!/e\)</span> 之间的差值是 <span class="math inline">\(n!\sum\limits_{k=n+1}^\infty\frac{(-1)^k}{k!}=(-1)^{n+1}\left(\frac{1}{n+1}-\frac{1}{(n+1)(n+2)}+\frac{1}{(n+1)(n+2)(n+3)}-\cdots\right)\)</span>，括号中的值介于 <span class="math inline">\(\frac{1}{n+1}\)</span> 和 <span class="math inline">\(\frac{1}{n+1}-\frac{1}{(n+1)(n+2)}=\frac{1}{n+2}\)</span> 之间，所以在 <span class="math inline">\(n&gt;0\)</span> 的时候，我们只需要将 <span class="math inline">\(n!/e\)</span> 四舍五入即可得到 <span class="math inline">\(D_n\)</span> 的封闭形式，即： <span class="math display">\[\color{purple}{D_n=\left\lfloor\frac{n!}{e}+\frac{1}{2}\right\rfloor+[n=0]}\]</span> <br></p><p>从上述推导中的 <span class="math inline">\(D_n=n!\sum\limits_{k=0}^n\frac{(-1)^k}{k!}\)</span> 可以得到：<span class="math inline">\(D_n-nD_{n-1}=n!\sum\limits_{k=0}^n\frac{(-1)^k}{k!}-n!\sum\limits_{k=0}^{n-1}\frac{(-1)^k}{k!}=n!\frac{(-1)^n}{n!}=(-1)^n\)</span>，于是我们得到 <span class="math inline">\(D_n\)</span> 的一个递归式： <span class="math display">\[\color{purple}{D_n=nD_{n-1}+(-1)^n}\]</span> 如果把 <span class="math inline">\(D_{n-1}\)</span> 也按照该递归式展开，我们可以得到一个更常见（大家应该已经很熟悉）的递归式： <span class="math display">\[\color{purple}{D_n=(n-1)(D_{n-1}+D_{n-2})}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>具体数学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[具体数学]第四章·数论</title>
    <link href="/blog-main/2020/08/07/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E5%9B%9B%E7%AB%A0%C2%B7%E6%95%B0%E8%AE%BA/"/>
    <url>/blog-main/2020/08/07/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E5%9B%9B%E7%AB%A0%C2%B7%E6%95%B0%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<blockquote><p>第四章·数论分9节，从最基础的整除一直讲到了莫比乌斯函数和欧拉函数</p><p><strong>注意</strong>：过于基础的数论知识就略去了</p></blockquote><h2 id="整除性">整除性</h2><h3 id="定义">定义</h3><p>整除定义略去。</p><p><span class="math inline">\(\gcd(m,n)\)</span> 和 <span class="math inline">\(\text{lcm}(m,n)\)</span> 的定义略去，注意它们有关系式： <span class="math display">\[\gcd(m,n)\cdot\text{lcm}(m,n)=mn\]</span> 考虑 <span class="math inline">\(m,n\)</span> 的质因数分解即可证。</p><h3 id="欧几里得算法">欧几里得算法</h3><p><strong>欧几里得算法（辗转相除法）</strong> 解 <span class="math inline">\(\gcd\)</span>，基于定理： <span class="math display">\[\gcd(m,n)=\gcd(n\bmod m,m),\quad m&gt;0\]</span> 证：设 <span class="math inline">\(d\)</span> 是 <span class="math inline">\(m,n\)</span> 的一个公因数，<span class="math inline">\(n=ad,\,m=bd\)</span>，则 <span class="math inline">\(n\bmod m=n-m\lfloor n/m\rfloor=ad-bd\lfloor a/b\rfloor\)</span> 也是 <span class="math inline">\(d\)</span> 的倍数。这说明任意 <span class="math inline">\(m,n\)</span> 的公因数都是 <span class="math inline">\(n\bmod m,m\)</span> 的公因数，<span class="math inline">\(\gcd\)</span> 自然不例外。<span class="math inline">\(\square\)</span></p><p><br></p><p>可以从上述的证明过程中提出来一个定理： <span class="math display">\[k\mid m\;且\;k\mid n\iff k\mid\gcd(m,n)\]</span> <br></p><p>由欧几里得算法可以得到<strong>扩展欧几里得算法</strong>，用于解不定方程。（竞赛基础，略去）</p><h3 id="常用的和式变换">常用的和式变换</h3><p>在之后的数论推导中，我们经常遇到带有整除的和式，为此，需要熟悉几个常用的和式变换。</p><p><strong>变换1</strong>： <span class="math display">\[\sum_{d\mid n}f(d)=\sum_{d\mid n}f(n/d)\]</span> 很简单。</p><p><strong>变换2</strong>： <span class="math display">\[\sum_{d\mid n}f(d)=\sum_{m}\sum_{d&gt;0}[n=md]f(d)\]</span> 相当于一次变量替换 <span class="math inline">\(n=md\)</span>.</p><p><strong>变换3</strong>： <span class="math display">\[\sum_{m\mid n}\sum_{k\mid m}f(k,m)=\sum_{k\mid n}\sum_{l\mid (n/k)}f(k,kl)\]</span> 和式换序+变量替换 <span class="math inline">\(m=kl\)</span>.</p><h2 id="素数">素数</h2><h3 id="算术基本定理">算术基本定理</h3><p><strong>算术基本定理（唯一分解定理）</strong>：每一个正整数都能用唯一的方式写成： <span class="math display">\[n=\prod_pp^{n_p},\quad n_p\geqslant0\]</span> 这玩意儿看起来很显然，但是我们仍然需要证明。</p><p>证：归纳法。<span class="math inline">\(n=1\)</span> 显然；现在研究 <span class="math inline">\(n&gt;1\)</span> 的情况。设小于 <span class="math inline">\(n\)</span> 的数都满足该定理，假设 <span class="math inline">\(n=p_1p_2\cdots p_m=q_1q_2\cdots q_k\)</span>，其中 <span class="math inline">\(p_1\leqslant\cdots\leqslant p_m\)</span> 且 <span class="math inline">\(q_1\leqslant\cdots\leqslant q_k\)</span>，我们只需要证明 <span class="math inline">\(m=k\)</span> 且 <span class="math inline">\(\forall i\;(p_i=q_i)\)</span>。取第一个使得 <span class="math inline">\(p_i\neq q_i\)</span> 的 <span class="math inline">\(i\)</span>，不妨假设 <span class="math inline">\(p_1&lt;q_1\)</span>，那么根据贝祖定理，存在整数 <span class="math inline">\(a,b\)</span> 使得 <span class="math inline">\(ap_1+bq_1=1\)</span>，于是 <span class="math inline">\(ap_1q_2\cdots q_k+bq_1q_2\cdots q_k=q_2\cdots q_k\)</span>。由于 <span class="math inline">\(p_1\)</span> 整除左式，所以 <span class="math inline">\(p_1\)</span> 整除右式，即 <span class="math inline">\(p_1\mid q_2\cdots q_k\)</span>。但是 <span class="math inline">\(p_1\)</span> 比 <span class="math inline">\(q_2,\cdots,q_k\)</span> 都小且 <span class="math inline">\(q_2,\cdots,q_k\)</span> 都是质数，所以矛盾，假设不成立，<span class="math inline">\(p_1=q_1\)</span>。用 <span class="math inline">\(p_1\)</span> 除分解式两边，得到 <span class="math inline">\(p_2\cdots p_m=q_2\cdots q_k\)</span>，由归纳假设，这些数对应相等。 <span class="math inline">\(\square\)</span></p><blockquote><p>为什么说这么显然的东西需要证明呢？因为我们稍微扩展一下“质数”的定义，它就不对了。</p><p>考虑集合 <span class="math inline">\(Z(\sqrt{10})=\{m+n\sqrt{10}\mid m,n\in\mathbb{Z}\}\)</span>。如果 <span class="math inline">\(m^2-10n^2=\pm1\)</span>，称 <span class="math inline">\(m+n\sqrt{10}\)</span> 是<strong>单位</strong>，因为它具有逆元（<span class="math inline">\((m+n\sqrt{10})\times\pm(m-n\sqrt{10})=1\)</span>）。称 <span class="math inline">\(Z(\sqrt{10})\)</span> 中不能写成两个非单位数乘积的非单位数为<strong>素元</strong>。可以证明，<span class="math inline">\(2,3,4\pm\sqrt{10}\)</span> 都是 <span class="math inline">\(Z(\sqrt{10})\)</span> 中的素元，于是 <span class="math inline">\(6=2\times3=(4+\sqrt{10})(4-\sqrt{10})\)</span> 被分解成了两种表示，不是唯一分解。</p></blockquote><h3 id="欧几里得数">欧几里得数</h3><p>欧几里得提出过一个素数无穷性的著名证明：</p><p>假设只有有限个素数，设为 <span class="math inline">\(p_1,p_2,\cdots,p_k\)</span>，那么考虑数 <span class="math inline">\(M=p_1p_2\cdots p_k+1\)</span>，它不被任何这 <span class="math inline">\(k\)</span> 个素数整除，于是 <span class="math inline">\(M\)</span> 被另一个素数整除或者本身就是素数，均与假设矛盾，故素数无穷。<span class="math inline">\(\square\)</span></p><p>据此，我们可以用递归式定义出<strong>欧几里得数</strong> <span class="math inline">\(\textbf{Euclid numbers}\)</span>（也称 <span class="math inline">\(\textbf{Sylvester&#39;s sequence}\)</span>）： <span class="math display">\[e_n=e_1e_2\cdots e_{n-1}+1,\quad n\geqslant 1\]</span> 所有的欧几里得数都是互质的，但是不都是质数。</p><p>我们试着解出它的封闭形式： <span class="math display">\[e_n=e_1\cdots e_{n-1}+1=(e_{n-1}-1)e_{n-1}+1=e_{n-1}^2-e_{n-1}+1\]</span> 可以证明：存在一个常数 <span class="math inline">\(E\approx1.264\)</span>，使得 <span class="math display">\[e_n=\left\lfloor E^{2^n}+\frac{1}{2}\right\rfloor\]</span></p><h3 id="梅森数">梅森数</h3><p>形如 <span class="math inline">\(2^p-1\)</span>（<span class="math inline">\(p\)</span> 为素数）的数称为梅森数 <span class="math inline">\(\textbf{Mersenne number}\)</span>。</p><p>注意 <span class="math inline">\(2^n-1\)</span>，当 <span class="math inline">\(n\)</span> 为合数时必不是质数，因为 <span class="math inline">\(2^m-1\mid 2^{km}-1\)</span>；但 <span class="math inline">\(n\)</span> 为质数时 <span class="math inline">\(2^n-1\)</span> 不一定是质数。</p><h3 id="素数定理">素数定理</h3><p><span class="math display">\[\pi(x)\sim\frac{x}{\ln x}\]</span></p><p>或 <span class="math display">\[P_n\sim n\ln n\]</span></p><h3 id="孪生素数">孪生素数</h3><p><span class="math inline">\(p\)</span> 和 <span class="math inline">\(p+2\)</span> 均是素数，称它们为孪生素数。至今仍未证明出有无穷多对孪生素数。</p><h2 id="阶乘的因子">阶乘的因子</h2><p>考虑阶乘 <span class="math inline">\(n!\)</span>，记 <span class="math inline">\(\varepsilon_p(n!)\)</span> 表示能整除 <span class="math inline">\(n!\)</span> 的素数 <span class="math inline">\(p\)</span> 的最大幂（例如，<span class="math inline">\(\varepsilon_2(10!)=8\)</span>，因为 <span class="math inline">\(2^8\mid 10!\)</span> 而 <span class="math inline">\(2^9\nmid 10!\)</span>）</p><p>考虑 <span class="math inline">\([1,n]\)</span> 中的数，如果它能被 <span class="math inline">\(p\)</span> 整除，它给答案的贡献为 <span class="math inline">\(1\)</span>；如果它能被 <span class="math inline">\(p^2\)</span> 整除，它给答案的贡献在 <span class="math inline">\(1\)</span> 的基础上再 <span class="math inline">\(+1\)</span>；……而 <span class="math inline">\([1,n]\)</span> 中能被 <span class="math inline">\(a\)</span> 整除的数有 <span class="math inline">\(\left\lfloor\frac{n}{a}\right\rfloor\)</span> 个，所以我们有公式： <span class="math display">\[\varepsilon_p(n!)=\left\lfloor\frac{n}{p}\right\rfloor+\left\lfloor\frac{n}{p^2}\right\rfloor+\left\lfloor\frac{n}{p^3}\right\rfloor+\cdots=\sum_{k\geqslant 1}\left\lfloor\frac{n}{p^k}\right\rfloor\]</span> 我们可以做一个简单的放缩，得到 <span class="math inline">\(\varepsilon_p(n!)\)</span> 的一个上界： <span class="math display">\[\varepsilon_p(n!)&lt;\sum_{k\geqslant 1}\frac{n}{p^k}=\frac{n}{p}\cdot\frac{1}{1-\frac{1}{p}}=\frac{n}{p-1}\]</span> 而事实上，这个放缩已经很好了。</p><p>该上界可以反过来得到 <span class="math inline">\(p^{\varepsilon_p(n!)}\)</span> 的上界，注意这个式子表示的是 <span class="math inline">\(p\)</span> 对 <span class="math inline">\(n!\)</span> 的贡献： <span class="math display">\[p^{\varepsilon_p(n!)}&lt;p^{\frac{n}{p-1}}\]</span> 我们再做一步很大的放缩 <span class="math inline">\(p\leqslant 2^{p-1}\)</span>： <span class="math display">\[p^{\varepsilon_p(n!)}&lt;p^{\frac{n}{p-1}}\leqslant 2^n\]</span> 也就是说，任何一个素数 <span class="math inline">\(p\)</span> 对 <span class="math inline">\(n!\)</span> 的贡献都小于 <span class="math inline">\(2^n\)</span>. 用这个结论，我们可以给出一个素数无穷的证明方法：</p><p>假设只有 <span class="math inline">\(k\)</span> 个素数 <span class="math inline">\(p_1,p_2,\cdots p_k\)</span>，由于每一个素数对 <span class="math inline">\(n!\)</span> 的贡献 <span class="math inline">\(&lt;2^n\)</span>，故 <span class="math inline">\(n!&lt;2^{kn}\)</span>. 但是该式对足够大的 <span class="math inline">\(n\)</span> 显然不成立（阶乘增长比指数要快），故假设不成立。<span class="math inline">\(\square\)</span></p><h2 id="互素">互素</h2><h3 id="符号">符号</h3><p><span class="math display">\[m\perp n\iff m,n\in\mathbb{Z},\;\gcd(m,n)=1\]</span></p><p>也可以是 <span class="math inline">\((m,n)=1\)</span>. （<span class="math inline">\((m,n)\)</span> 在无歧义时指代 <span class="math inline">\(\gcd(m,n)\)</span>）</p><p>重要法则： <span class="math display">\[k\perp m\;且\;k\perp n\iff k\perp mn\]</span></p><h3 id="textbfstern-brocot-树和-textbffarey-级数"><span class="math inline">\(\textbf{Stern-Brocot}\)</span> 树和 <span class="math inline">\(\textbf{Farey}\)</span> 级数</h3><p>从两个分数：<span class="math inline">\(\frac{0}{1},\frac{1}{0}\)</span> 出发，不断重复操作：在两个相邻的分数 <span class="math inline">\(\frac{m}{n},\frac{m&#39;}{n&#39;}\)</span> 之间插入它们的<strong>中位分数</strong> <span class="math inline">\(\frac{m+m&#39;}{n+n&#39;}\)</span>.</p><p>上述操作过程可以形成一棵树：</p><p><img src="stern-brocot.png" /></p><p><strong>基本性质</strong>：如果 <span class="math inline">\(m/n\)</span> 和 <span class="math inline">\(m&#39;/n&#39;\)</span> 是这个构造中任何一个阶段的相邻分数，那么有： <span class="math display">\[m&#39;n-mn&#39;=1\]</span> 证：归纳法。首先对于 <span class="math inline">\(0/1,1/0\)</span> 时正确的；其次，插入 <span class="math inline">\((m+m&#39;)/(n+n&#39;)\)</span> 时，有： <span class="math display">\[\begin{align}(m+m&#39;)n-m(n+n&#39;)&amp;=m&#39;n-mn&#39;=1\\m&#39;(n+n&#39;)-(m+m&#39;)n&#39;&amp;=m&#39;n-mn&#39;=1\end{align}\]</span> <span class="math inline">\(\square\)</span></p><p><strong>性质</strong>：</p><ul><li><p>出现在 <span class="math inline">\(\textbf{Stern-Brocot}\)</span> 树中的分数都是最简分数。</p><p>证：设 <span class="math inline">\(m/n\)</span> 出现在树中，<span class="math inline">\(d\)</span> 是 <span class="math inline">\(m,n\)</span> 的任何一个公因子，那么 <span class="math inline">\(d\mid m&#39;n-mn&#39;\)</span>。由基本性质可知：<span class="math inline">\(d\mid 1\)</span>，于是 <span class="math inline">\(d=1\)</span>，因此 <span class="math inline">\(m\perp n\)</span>. <span class="math inline">\(\square\)</span></p></li><li><p><strong>所有</strong>的最简分数都会在 <span class="math inline">\(\textbf{Stern-Brocot}\)</span> 树中出现。</p><p>证：对于最简分数 <span class="math inline">\(a/b\)</span>，假设我们在某阶段有：<span class="math inline">\(\frac{m}{n}&lt;\frac{a}{b}&lt;\frac{m&#39;}{n&#39;}\)</span>，若 <span class="math inline">\((m+m&#39;)/(n+n&#39;)=a/b\)</span>，则找到了 <span class="math inline">\(a/b\)</span>；若 <span class="math inline">\((m+m&#39;)/(n+n&#39;)&gt;a/b\)</span>，则在 <span class="math inline">\(m/n\)</span> 和 <span class="math inline">\((m+m&#39;)/(n+n&#39;)\)</span> 中继续寻找；若 <span class="math inline">\((m+m&#39;)/(n+n&#39;)&lt;a/b\)</span>，则在 <span class="math inline">\((m+m&#39;)/(n+n&#39;)\)</span> 和 <span class="math inline">\(m&#39;/n&#39;\)</span> 中继续寻找。这个过程必定有限，因为我们有： <span class="math display">\[\begin{align}\begin{cases}\frac{a}{b}-\frac{m}{n}&gt;0\\\frac{m&#39;}{n&#39;}-\frac{a}{b}&gt;0\end{cases}&amp;\implies\begin{cases}an-bm\geqslant1\\bm&#39;-an&#39;\geqslant 1\end{cases}\\&amp;\implies (m&#39;+n&#39;)(an-bm)+(m+n)(bm&#39;-an&#39;)\geqslant m+n+m&#39;+n&#39;\\&amp;\implies a+b\geqslant m+n+m&#39;+n&#39;\end{align}\]</span> 而 <span class="math inline">\(m,n,m&#39;,n&#39;\)</span> 始终在增加，故最多 <span class="math inline">\(a+b\)</span> 步就能找到 <span class="math inline">\(a/b\)</span>.</p></li></ul><p><br></p><p>阶为 <span class="math inline">\(N\)</span> 的法里级数 <span class="math inline">\(\textbf{Farey series}\)</span> 记为 <span class="math inline">\(\mathcal{F}_N\)</span>，表示介于 <span class="math inline">\(0\)</span> 和 <span class="math inline">\(1\)</span> 之间的，分母不超过 <span class="math inline">\(N\)</span> 的所有最简分数组成的集合，按递增的次序排列。例如：<span class="math inline">\(\mathcal{F}_6=\frac{0}{1},\frac{1}{6},\frac{1}{5},\frac{1}{4},\frac{1}{3},\frac{2}{5},\frac{1}{2},\frac{3}{5},\frac{2}{3},\frac{3}{4},\frac{4}{5},\frac{5}{6},\frac{1}{1}\)</span>.</p><p><span class="math inline">\(\mathcal{F}_N\)</span> 的构造和之前类似，从 <span class="math inline">\(\mathcal{F}_1=\frac{0}{1},\frac{1}{0}\)</span> 出发，不断插入中位分数，不过不插入分母大于 <span class="math inline">\(N\)</span> 的数。换句话说，<span class="math inline">\(\mathcal{F}_N\)</span> 定义了 <span class="math inline">\(\textbf{Stern-Brocot}\)</span> 树的一颗子树，去除了分母 <span class="math inline">\(&gt;N\)</span> 的分支。于是，<span class="math inline">\(\textbf{Farey}\)</span> 级数也满足性质：设 <span class="math inline">\(m/n\)</span> 和 <span class="math inline">\(m&#39;/n&#39;\)</span> 是其中的相邻元素，则 <span class="math inline">\(m&#39;n-mn&#39;=1\)</span>.</p><p><br></p><p>我们可以把 <span class="math inline">\(\textbf{Stern-Brocot}\)</span> 树看成一个表示有理数的<strong>数系</strong>，因为每一个有理数都在其中恰好出现一次。用 <span class="math inline">\(L,R\)</span> 表示从树根往下走的方式，这样每一个有理数都可以唯一地被一个 <span class="math inline">\(L,R\)</span> 构成的字符串表示，例如 <span class="math inline">\(\frac{5}{7}\)</span> 可以表示为 <span class="math inline">\(LRRL\)</span>. 特别地，<span class="math inline">\(\frac{1}{1}\)</span> 由空串表示，记为 <span class="math inline">\(I\)</span>.</p><p>现在有两个问题：给定一个最简分数，它的字符串表示是什么？给定一个字符串，它表示的最简分数是什么？</p><p>编程的话，我们直接按照规则在 <span class="math inline">\(\textbf{Stern-Brocot}\)</span> 树上往左或者往右走即可。但是如果要在数学上很好地表达出来，我们可以这样考虑：</p><p>设 <span class="math inline">\(f(S)\)</span> 表示 <span class="math inline">\(S\)</span> 对应的最简分数，<span class="math inline">\(m/n,m&#39;/n&#39;\)</span> 是树的上一层中 <span class="math inline">\(f(S)\)</span> 前后的树，则 <span class="math inline">\(f(S)=(m+m&#39;)/(n+n&#39;)\)</span>. 考虑矩阵： <span class="math display">\[M(S)=\begin{bmatrix}n&amp;n&#39;\\m&amp;m&#39;\end{bmatrix}\]</span> 向左走一步时，<span class="math inline">\(M(S)\)</span> 要变成 <span class="math inline">\(\begin{bmatrix}n&amp;n+n&#39;\\m&amp;m+m&#39;\end{bmatrix}\)</span>，需要右乘 <span class="math inline">\(\begin{bmatrix}1&amp;1\\0&amp;1\end{bmatrix}\)</span>；向右走一步时，<span class="math inline">\(M(S)\)</span> 要变成 <span class="math inline">\(\begin{bmatrix}n+n&#39;&amp;n&#39;\\m+m&#39;&amp;m&#39;\end{bmatrix}\)</span>，需要右乘 <span class="math inline">\(\begin{bmatrix}1&amp;0\\1&amp;1\end{bmatrix}\)</span>. 也就是说：定义 <span class="math display">\[L=\begin{bmatrix}1&amp;1\\0&amp;1\end{bmatrix},\;R=\begin{bmatrix}1&amp;0\\1&amp;1\end{bmatrix}\]</span> 则： <span class="math display">\[M(SL)=M(S)L,\;M(SR)=M(S)R\]</span> 结合初始条件 <span class="math inline">\(M(I)=I\)</span>，容易归纳得到： <span class="math display">\[M(S)=S\]</span> 例如，<span class="math inline">\(M(LRRL)=LRRL=\begin{bmatrix}1&amp;1\\0&amp;1\end{bmatrix}\begin{bmatrix}1&amp;0\\1&amp;1\end{bmatrix}\begin{bmatrix}1&amp;0\\1&amp;1\end{bmatrix}\begin{bmatrix}1&amp;1\\0&amp;1\end{bmatrix}=\begin{bmatrix}3&amp;4\\2&amp;3\end{bmatrix}\)</span>，说明 <span class="math inline">\(LRRL\)</span> 对应 <span class="math inline">\(\frac{2}{3}\)</span> 和 <span class="math inline">\(\frac{3}{4}\)</span> 的中位分数 <span class="math inline">\(\frac{5}{7}\)</span>.</p><p>另外，如果我们给定的是一个无理数，我们会在树上无限地走下去，并不断更新所给无理数的上下界，这么做可以让我们去近似一个无理数。</p><p><br></p><p>欧几里得算法和有理数的 <span class="math inline">\(\textbf{Stern-Brocot}\)</span> 表示之间有密切的联系。给定 <span class="math inline">\(\alpha=m/n\)</span>，我们得到 <span class="math inline">\(\lfloor m/n\rfloor\)</span> 个 <span class="math inline">\(R\)</span>，然后得到 <span class="math inline">\(\lfloor n/(m\bmod n)\rfloor\)</span> 个 <span class="math inline">\(L\)</span>，然后得到 <span class="math inline">\(\lfloor(m\bmod n)/(n\bmod(m\bmod n))\rfloor\)</span> 个 <span class="math inline">\(R\)</span>……这些数正好是欧几里得算法中出现的数。</p><h2 id="bmod同余关系"><span class="math inline">\(\bmod\)</span>：同余关系</h2><p>定义略。</p><p>同余是一个<strong>等价关系</strong>，满足自反律、对称律、传递律。</p><p>满足加、减、乘、幂。</p><p><strong>除法</strong>：</p><ul><li><p><span class="math inline">\(d\perp m\)</span> 时，满足消去律：<span class="math inline">\(ad\equiv bd\pmod m\iff a\equiv b\pmod m\)</span>.</p></li><li><p>一般的，有： <span class="math display">\[ac\equiv bc\pmod m\iff a\equiv b\pmod{\frac{m}{\gcd(c,m)}}\]</span></p></li></ul><p>定理： <span class="math display">\[a\equiv b\pmod {md}\implies a\equiv b\pmod m\]</span> 定理： <span class="math display">\[a\equiv b\pmod m\;且\;a\equiv b\pmod n\iff a\equiv b\pmod {\text{lcm}(m,n)}\]</span> 当 <span class="math inline">\(m\perp n\)</span> 时，有上述定理的特殊情况： <span class="math display">\[a\equiv b\pmod {mn}\iff a\equiv b\pmod m\;且\;a\equiv b\pmod n,\quad m\perp n\]</span></p><h2 id="独立剩余">独立剩余</h2><p>一个整数 <span class="math inline">\(x\)</span> 表示成为关于一组互素的模的剩余序列： <span class="math display">\[\text{Res}(x)=(x\bmod m_1,\cdots,x\bmod m_r),\quad m_j\perp m_k,\forall 1\leqslant j&lt;k\leqslant r\]</span> 注意<strong>独立性</strong>：以 <span class="math inline">\(m=15=3\times 5\)</span> 为例，<span class="math inline">\(\text{Res}(13)=(1,3)\)</span>，<span class="math inline">\(\text{Res}(7)=(1,2)\)</span>，那么 <span class="math inline">\(\text{Res}(13\times 7)=(1\times 1\bmod 3,3\times 2\bmod 5)=(1,1)\)</span>. 对每个 <span class="math inline">\(m_i\)</span> 分别做运算即可。</p><p>如果知道了一个给定的剩余序列，如何反过来求出 <span class="math inline">\(x\bmod m\)</span>？<strong>中国剩余定理</strong>。</p><p><br></p><p>上述理论的一个小应用：<span class="math inline">\(x^2\equiv 1\pmod m\)</span> 在模 <span class="math inline">\(m\)</span> 下有多少个解？</p><ul><li><p>先考虑 <span class="math inline">\(m=p^k,\;p\in\text{prime},\,k&gt;0\)</span> 的情形：</p><p>当 <span class="math inline">\(p\neq 2\)</span> 时，这是<strong>二次探测定理</strong>： <span class="math display">\[x^2\equiv 1\pmod {p^k}\iff p^k\mid (x-1)(x+1)\iff p^k\mid x-1\;或\;p^k\mid x+1\iff x\equiv\pm1\pmod {p^k}\]</span> 仅有两解：<span class="math inline">\(x\equiv \pm1\)</span>.</p><p>当 <span class="math inline">\(p=2\)</span> 时，<span class="math inline">\(2^k\mid (x-1)(x+1)\)</span>，由于 <span class="math inline">\(x-1\)</span> 和 <span class="math inline">\(x+1\)</span> 中有一个数能被 <span class="math inline">\(2\)</span> 整除但不能被 <span class="math inline">\(4\)</span> 整除，所以另一个必定能被 <span class="math inline">\(2^{k-1}\)</span> 整除。因此，当 <span class="math inline">\(k\geqslant 3\)</span> 时有 <span class="math inline">\(4\)</span> 解：<span class="math inline">\(x\equiv \pm1,2^{k-1}\pm1\)</span>.</p></li><li><p>考虑 <span class="math inline">\(m=\prod {p_i}^{m_{p_i}}\)</span>，由于素数之间<strong>相互独立</strong>，我们只需要把每个素数的解的数量乘起来即可。对 <span class="math inline">\(p\neq2\)</span> 都是两解，对 <span class="math inline">\(p=2\)</span> 需要进行一些修正，如果非要写成一个式子，那就是： <span class="math display">\[2^{r+[8\mid m]+[4\mid m]-[2\mid m]}\]</span></p></li></ul><h2 id="几个定理">几个定理</h2><h3 id="费马小定理">费马小定理</h3><p>若 <span class="math inline">\(p\)</span> 是质数，则： <span class="math display">\[a^p \equiv a\pmod p\]</span> 特别地，若 <span class="math inline">\(a\)</span> 不是 <span class="math inline">\(p\)</span> 的倍数，则： <span class="math display">\[a^{p-1} \equiv 1\pmod p\]</span> <em>Proof</em>：设 <span class="math inline">\(p\)</span> 为质数，<span class="math inline">\(a\)</span> 不是 <span class="math inline">\(p\)</span> 的倍数（即 <span class="math inline">\(\gcd(a,p)=1\)</span>）。取一模 <span class="math inline">\(p\)</span> 的完全剩余系 <span class="math inline">\(A=\{1,2,\cdots,p-1\}\)</span>，容易证明 <span class="math inline">\(\{aA_i\}\)</span> 也是模 <span class="math inline">\(p\)</span> 的完全剩余系。于是有： <span class="math display">\[\begin{align}aA_1aA_2\cdots aA_{p-1}&amp;\equiv A_1A_2\cdots A_{p-1}\pmod p\\a^{p-1}&amp;\equiv 1\pmod p\end{align}\]</span> <span class="math inline">\(\square\)</span></p><h3 id="欧拉定理">欧拉定理</h3><p>若 <span class="math inline">\((a,m)=1\)</span>，则： <span class="math display">\[a^{\varphi(m)}\equiv 1\pmod m\]</span> <em>Proof</em>：取一模 <span class="math inline">\(m\)</span> 的缩剩余系 <span class="math inline">\(\{r_1,r_2,\cdots,r_{\varphi(m)}\}\)</span>，容易证明 <span class="math inline">\(\{ar_i\}\)</span> 也是模 <span class="math inline">\(m\)</span> 的一个缩剩余系。于是有： <span class="math display">\[\begin{align}ar_1ar_2\cdots ar_{\varphi(m)}&amp;\equiv r_1r_2\cdots r_{\varphi(m)}\pmod m\\a^{\varphi(m)}&amp;\equiv 1\pmod m\end{align}\]</span> <span class="math inline">\(\square\)</span></p><h3 id="威尔逊定理">威尔逊定理</h3><p><span class="math inline">\(p\)</span> 为质数的<strong>充要条件</strong>是： <span class="math display">\[(p-1)!\equiv -1\pmod p\]</span> <em>Proof</em>：</p><p>充分性：<span class="math inline">\((p-1)!\equiv -1\pmod p\iff p\mid(p-1)!+1\)</span>。设 <span class="math inline">\(a\mid p\quad(a&lt;p)\)</span>，则 <span class="math inline">\(a\mid (p-1)!+1\)</span>；又 <span class="math inline">\(a\mid (p-1)!\)</span>，而 <span class="math inline">\(\gcd((p-1)!,(p-1)!+1)=1\)</span>，故只能是 <span class="math inline">\(a=1\)</span>，所以 <span class="math inline">\(p\)</span> 是质数；</p><p>必要性： 只需证：<span class="math inline">\((p-2)!\equiv1\pmod p\)</span>。当 <span class="math inline">\(p=2\)</span> 时，成立；当 <span class="math inline">\(p&gt;2\)</span> 时，<span class="math inline">\(p\)</span> 是奇数，现在证明：<span class="math inline">\(2,3,\cdots,p-2\)</span> 这偶数个数字可以两两配对，使得每一对都互为模 <span class="math inline">\(p\)</span> 意义下的逆元。首先，对于 <span class="math inline">\(x&lt;p\)</span>，一定能找到 <span class="math inline">\(y&lt;p\)</span> 使得 <span class="math inline">\(x,y\)</span> 互为逆元；其次，对于 <span class="math inline">\(2\leqslant x\leqslant p-2\)</span>，<span class="math inline">\(x\)</span> 不可能是自己的逆元；再次，假设 <span class="math inline">\(x,y\)</span> 互为逆元，则必不会有第三者 <span class="math inline">\(z&lt;p\)</span> 与 <span class="math inline">\(x\)</span> 或 <span class="math inline">\(y\)</span> 为逆元，综上易知，<span class="math inline">\(2,3,\cdots,p-2\)</span> 可以两两配对形成逆元对。于是，<span class="math inline">\((p-2)!\equiv 1\pmod p\)</span> 成立。</p><p><span class="math inline">\(\square\)</span></p><h2 id="varphi-函数与-mu-函数"><span class="math inline">\(\varphi\)</span> 函数与 <span class="math inline">\(\mu\)</span> 函数</h2><h3 id="欧拉函数">欧拉函数</h3><p><span class="math inline">\(\varphi(n)\)</span> 表示 <span class="math inline">\(1\sim n\)</span> 中与 <span class="math inline">\(n\)</span> 互质的数的个数，即：<span class="math inline">\(\varphi(n)=\sum\limits_{k=1}^n[\gcd(k,n)=1]\)</span>.</p><blockquote><p>totient 这个单词是一个喜欢发明新词汇的英国数学家 <span class="math inline">\(\textbf{J.J.Sylvester}\)</span> 发明的。</p></blockquote><ul><li><p>对于素数 <span class="math inline">\(p\)</span>，显然 <span class="math display">\[\varphi(p)=p-1\]</span></p></li><li><p>对于素数幂 <span class="math inline">\(p^k\)</span>，所有与 <span class="math inline">\(p^k\)</span> 互质的数 <span class="math inline">\(n\)</span> 满足：<span class="math inline">\(n\perp p^k\iff p\nmid n\)</span>，而在 <span class="math inline">\(1\sim p^k\)</span> 中 <span class="math inline">\(p\)</span> 的倍数有 <span class="math inline">\(p^{k-1}\)</span> 个，所以 <span class="math inline">\(\varphi(p^k)\)</span> 就是总数减掉这些倍数： <span class="math display">\[\varphi(p^k)=p^k-p^{k-1}\]</span></p></li><li><p>对于非素数幂 <span class="math inline">\(m\)</span>，可以写作：<span class="math inline">\(m=m_1m_2\)</span>，其中 <span class="math inline">\(m_1\perp m_2\)</span>。如果 <span class="math inline">\(n\perp m\)</span>，那么 <span class="math inline">\(n\perp m_1\;且\;n\perp m_2\)</span>，于是 <span class="math inline">\(n\bmod m_1\perp m_1\;且\;n\bmod m_2\perp m_2\)</span>（由 <span class="math inline">\(\gcd(a,b)=\gcd(b,a\bmod b)\)</span> 可知）。于是乎，<span class="math inline">\(n\bmod m_1\)</span> 有 <span class="math inline">\(\varphi(m_1)\)</span> 种选法，<span class="math inline">\(n\bmod m_2\)</span> 有 <span class="math inline">\(\varphi(m_2)\)</span> 种选法，根据中国剩余定理，给定一个 <span class="math inline">\(n\bmod m_1\)</span> 和 <span class="math inline">\(n\bmod m_2\)</span> 就能唯一确定下 <span class="math inline">\(n\bmod m\)</span>，所以一共就有 <span class="math inline">\(\varphi(m_1)\varphi(m_2)\)</span> 种 <span class="math inline">\(n\)</span> 的选法，也就是说： <span class="math display">\[\varphi(m_1m_2)=\varphi(m_1)\varphi(m_2)\]</span></p></li></ul><p><br></p><p><strong>积性函数</strong>：如果 <span class="math inline">\(f(1)=1\)</span>，并且 <span class="math display">\[f(m_1m_2)=f(m_1)f(m_2)\quad \forall m_1\perp m_2\]</span> 那么称正整数的函数 <span class="math inline">\(f(m)\)</span> 是<strong>积性的</strong>（multiplicable）。从刚才的叙述中可以知道，欧拉函数 <span class="math inline">\(\varphi(n)\)</span> 是积性的。</p><p>从定义可以看出，积性函数的值完全由它在素数幂的值所决定。若把 <span class="math inline">\(m\)</span> 质因子分解：<span class="math inline">\(m=\prod\limits_pp^{m_p}\)</span>，那么根据积性有： <span class="math display">\[f(m)=\prod_pf(p^{m_p})\]</span> <br></p><p>把欧拉函数套入公式，得到： <span class="math display">\[\varphi(m)=\prod_{p\mid m}\varphi(p^{m_p})=\prod_{p\mid m}(p^{m_p}-p^{m_p-1})=m\prod_{p\mid m}\left(1-\frac{1}{p}\right)\]</span> 考虑以 <span class="math inline">\(m\)</span> 为分母的所有真分数：<span class="math inline">\(\frac{0}{m},\frac{1}{m},\cdots,\frac{m-1}{m}\)</span>，把它们化简约分之后可以按照分母分类，它们的分母全是 <span class="math inline">\(m\)</span> 的约数，而以 <span class="math inline">\(d\)</span> 为分母的所有分数恰有 <span class="math inline">\(\varphi(d)\)</span> 个。于是乎，我们得到了如下公式： <span class="math display">\[\sum_{d\mid m}\varphi(d)=m\]</span></p><blockquote><p>上面一段话不太“数学”，用“更数学”的话来说是：设 <span class="math inline">\(f(i)\)</span> 表示小于等于 <span class="math inline">\(m\)</span> 的数中与 <span class="math inline">\(m\)</span> 的 <span class="math inline">\(\gcd\)</span> 为 <span class="math inline">\(i\)</span> 的数的个数，即 <span class="math inline">\(f(i)=\sum\limits_{k=1}^m[\gcd(k,m)=i]\)</span>. 那么显然有：<span class="math inline">\(m=\sum\limits_{d\mid m}f(d)\)</span>. 又因为 <span class="math inline">\(\gcd(k,m)=i\iff\gcd\left(\frac{k}{i},\frac{m}{i}\right)=1\)</span>，所以 <span class="math inline">\(f(i)=\varphi\left(\frac{m}{i}\right)\)</span>，于是 <span class="math inline">\(m=\sum\limits_{d\mid m}\varphi\left(\frac{m}{d}\right)=\sum\limits_{d\mid m}\varphi(d)\)</span>. <span class="math inline">\(\square\)</span></p><p>用 <span class="math inline">\(\textbf{Dirichlet}\)</span> 卷积可以写成： <span class="math display">\[\varphi*1=\text{id}\]</span></p></blockquote><p>如果 <span class="math inline">\(f(m)\)</span> 和 <span class="math inline">\(g(m)\)</span> 满足：<span class="math inline">\(g(m)=\sum\limits_{d\mid m}f(d)\)</span>，那么知道 <span class="math inline">\(f(m)\)</span> 和 <span class="math inline">\(g(m)\)</span> 其中一个是积性的，就可以推出另一个也是积性的。</p><blockquote><p>更一般地，设 <span class="math inline">\(h=f*g\)</span>，那么 <span class="math inline">\(h,f,g\)</span> 三者的积性知二推一。</p></blockquote><h3 id="莫比乌斯函数">莫比乌斯函数</h3><p>莫比乌斯函数 <span class="math inline">\(\mu(m)\)</span> 由下式<strong>定义</strong>： <span class="math display">\[\sum_{d\mid m}\mu(d)=[m=1]\]</span></p><blockquote><p>用 <span class="math inline">\(\textbf{Dirichlet}\)</span> 卷积的形式写作： <span class="math display">\[\mu * 1=\epsilon\]</span></p></blockquote><p>由于 <span class="math inline">\(g(m)=[m=1]\)</span> 显然积性，所以 <span class="math inline">\(\mu(m)\)</span> 是积性函数。又根据定义可知 <span class="math inline">\(\mu(1)=1,\,\mu(p)=-1,\,\mu(p^k)=0(k&gt;1)\)</span>，于是套用积性函数的公式可得： <span class="math display">\[\mu(m)=\begin{cases}1&amp;,m=1\\(-1)^r&amp;,m=p_1\cdots p_r\\0&amp;,m\;有平方因子\end{cases}\]</span> <span class="math inline">\(\textbf{Richar Dedekind}\)</span> 和 <span class="math inline">\(\textbf{Joseph Liouville}\)</span> 在1857年注意到了“反演原理”： <span class="math display">\[g(m)=\sum_{d\mid m}f(d)\iff f(m)=\sum_{d\mid m}\mu(d)g\left(\frac{m}{d}\right)\]</span></p><blockquote><p>用 <span class="math inline">\(\textbf{Dirichlet}\)</span> 卷积的形式写作： <span class="math display">\[g=f*1\iff f=\mu*g\]</span> 注意到 <span class="math inline">\(\mu*1=\epsilon\)</span>，即 <span class="math inline">\(\mu\)</span> 是 <span class="math inline">\(1\)</span> 的逆元，推导也就简单了。</p></blockquote><p>如果对我们之前得到的公式 <span class="math inline">\(\sum\limits_{d\mid m}\varphi(d)=m\)</span> 进行莫比乌斯反演，就可以得到： <span class="math display">\[\varphi(m)=\sum_{d\mid m}\mu(d)\frac{m}{d}\]</span></p><blockquote><p>用 <span class="math inline">\(\textbf{Dirichlet}\)</span> 卷积的形式写作： <span class="math display">\[\varphi=\mu*\text{id}\]</span> 我们可以把它与欧拉函数的决定式 <span class="math inline">\(\varphi(m)=m\prod\limits_{p\mid m}\left(1-\frac{1}{p}\right)\)</span> 之间建立起关系：注意到 <span class="math inline">\(\varphi(m)=\sum\limits_{d\mid m}\mu(d)\frac{m}{d}\)</span> 中 <span class="math inline">\(\mu(d)\)</span> 常常取 <span class="math inline">\(0\)</span>，事实上，仅有 <span class="math inline">\(2^r\)</span> 项不是 <span class="math inline">\(0\)</span>. 把 <span class="math inline">\(\prod\limits_{p\mid m}\left(1-\frac{1}{p}\right)\)</span> 展开来得到的 <span class="math inline">\(2^r\)</span> 项，正好与之对应。</p></blockquote><p><br></p><h3 id="欧拉函数前缀和">欧拉函数前缀和</h3><p>定义： <span class="math display">\[\Phi(x)=\sum_{k=1}^x\varphi(k)\]</span> 为欧拉函数的前缀和函数。注意这里 <span class="math inline">\(x\in\mathbb{R}\)</span>，所以 <span class="math inline">\(\Phi(x)=\Phi(\lfloor x\rfloor)\)</span>.</p><p>我们有恒等式： <span class="math display">\[\sum_{d\geqslant1}\Phi\left(\frac{x}{d}\right)=\frac{1}{2}\lfloor x\rfloor\lfloor 1+x\rfloor\]</span> 证：对于所有满足 <span class="math inline">\(0\leqslant m&lt;n\leqslant x\)</span> 的分数 <span class="math inline">\(\frac{m}{n}\)</span>，如果我们既计入最简分数，也计入未化简的分数，那么一共有 <span class="math inline">\(\sum\limits_{n=1}^x\sum\limits_{m=0}^{n-1}1=\sum\limits_{n=1}^x n=\frac{1}{2}{\lfloor x\rfloor \lfloor 1+x\rfloor}\)</span> 种；换一个角度，考虑分子分母公因数为 <span class="math inline">\(d\)</span> 的所有分数个数，相当于 <span class="math inline">\(0\leqslant m&#39;&lt;n&#39;\leqslant \frac{x}{d}\)</span> 的所有最简分数个数，即 <span class="math inline">\(\Phi\left(\frac{x}{d}\right)\)</span>. 枚举 <span class="math inline">\(d\)</span> 得到左式 <span class="math inline">\(\sum\limits_{d\geqslant 1}\Phi\left(\frac{x}{d}\right)\)</span>. <span class="math inline">\(\square\)</span></p><p>这个恒等式可以看成隐含的 <span class="math inline">\(\Phi(x)\)</span> 的递归式，不过为了更方便计算，我们引入莫比乌斯反演： <span class="math display">\[g(x)=\sum_{d\geqslant 1}f(x/d)\iff f(x)=\sum_{d\geqslant 1}\mu(d)g(x/d)\]</span></p><blockquote><p>事实上，莫比乌斯是因为这个反演原理而非之前叙述的那个而创造的莫比乌斯函数。</p></blockquote><p>证：</p><p>必要性： <span class="math display">\[\begin{align}\sum_{d\geqslant 1}\mu(d)g(x/d)&amp;=\sum_{d\geqslant 1}\mu(d)\sum_{k\geqslant 1}f(x/kd)\\&amp;=\sum_{m\geqslant 1}\sum_{d\mid m}\mu(d)f(x/m)&amp;&amp;令\;m=kd\\&amp;=\sum_{m\geqslant 1}f(x/m)[m=1]&amp;&amp;运用\;\mu*1=\epsilon\\&amp;=f(x)\end{align}\]</span> 充分性： <span class="math display">\[\begin{align}\sum_{d\geqslant 1}f(x/d)&amp;=\sum_{d\geqslant 1}\sum_{k\geqslant 1}\mu(k)g(x/kd)\\&amp;=\sum_{m\geqslant 1}\sum_{k\mid m}\mu(k)g(x/m)&amp;&amp;令\;m=kd\\&amp;=\sum_{m\geqslant 1}g(x/m)[m=1]&amp;&amp;运用\;\mu*1=\epsilon\\&amp;=g(x)\end{align}\]</span> <span class="math inline">\(\square\)</span></p><p>运用上述莫比乌斯反演，我们反解出 <span class="math inline">\(\Phi(x)\)</span>： <span class="math display">\[\sum_{d\geqslant1}\Phi\left(\frac{x}{d}\right)=\frac{1}{2}\lfloor x\rfloor\lfloor 1+x\rfloor\implies\Phi(x)=\frac{1}{2}\sum_{d\geqslant 1}\mu(d)\lfloor x/d\rfloor\lfloor 1+x/d\rfloor\]</span></p><blockquote><p>在这里你可以发现，欧拉函数前缀和可以 <span class="math inline">\(O(\sqrt n)\)</span> 地求出，比杜教筛优秀（但是依赖于莫比乌斯函数前缀和，所以还是得杜教筛）</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>具体数学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[论文阅读]The “Sales Tax” Theorem</title>
    <link href="/blog-main/2020/08/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-The%20%E2%80%9CSales%20Tax%E2%80%9D%20Theorem/"/>
    <url>/blog-main/2020/08/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-The%20%E2%80%9CSales%20Tax%E2%80%9D%20Theorem/</url>
    
    <content type="html"><![CDATA[<p>Link to this article: <a href="https://doi.org/10.1080/0025570X.1976.11976573" class="uri">https://doi.org/10.1080/0025570X.1976.11976573</a></p><span id="more"></span><p>在数论中，我们有一个神奇的结论：</p><blockquote><p><span class="math inline">\(\textbf{Theorem A}\)</span>：设 <span class="math inline">\(p_n\)</span> 是第 <span class="math inline">\(n\)</span> 个质数，<span class="math inline">\(\pi(n)\)</span> 表示 <span class="math inline">\(\leqslant n\)</span> 的质数个数，那么序列 <span class="math inline">\(\{n+\pi(n)\}\)</span> 和序列 <span class="math inline">\(\{p_n+n-1\}\)</span> 构成了对正整数的一个划分。</p></blockquote><p>事实上，这个结论的正确性与质数没有关系！更普遍的情形是：</p><blockquote><p><span class="math inline">\(\textbf{Theorem B}\)</span>：设 <span class="math inline">\(Q=\{q_n\}\)</span> 是正整数序列的一个子序列（即 <span class="math inline">\(1\leqslant q_1&lt;q_2&lt;\cdots\)</span>），令 <span class="math inline">\(\tau(n)\)</span> 表示 <span class="math inline">\(Q\)</span> 中 <span class="math inline">\(\leqslant n\)</span> 的数的个数，那么序列 <span class="math inline">\(\{n+\tau(n)\}\)</span> 和 <span class="math inline">\(\{q_n+n-1\}\)</span> 构成了对正整数的一个划分。</p></blockquote><p>为形象化表述，我们视 <span class="math inline">\(\{q_n\}\)</span> 是一个税收表，每遇到 <span class="math inline">\(\{q_n\}\)</span> 中的一项，税收 <span class="math inline">\(+1\)</span>。那么对于原价为 <span class="math inline">\(n\)</span> 的商品，它的税收为 <span class="math inline">\(\tau(n)\)</span>，总价为 <span class="math inline">\(n+\tau(n)\)</span>，于是所有商品的总价构成序列 <span class="math inline">\(\{n+\tau(n)\}\)</span>。</p><p>考虑哪些数不会出现在总价序列中。注意，如果没有税收，那么总价随原价每加一而加一，构成一个连续的正整数序列；但是现在有了税收，于是每碰到一个 <span class="math inline">\(q_n\)</span>，总价不仅随原价加一，还因为 <span class="math inline">\(q_n\)</span> 再次加一，一共加二，这就导致总价跳过了一个数，正是这个数不会出现在总价序列中。假设我们从 <span class="math inline">\(n-1\)</span> 到 <span class="math inline">\(n\)</span> 变化时发生了这种情况，说明存在某个 <span class="math inline">\(k\)</span> 使得 <span class="math inline">\(q_k=n\)</span>，于是这个被跳过的数就是 <span class="math inline">\(n+\tau(n)-1=q_k+k-1\)</span>（注意 <span class="math inline">\(\tau(q_k)=k\)</span>）。所有被跳过的数构成序列 <span class="math inline">\(\{q_n+n-1\}\)</span>，证毕。</p><p>应用 <span class="math inline">\(\textbf{Theorem B}\)</span>，我们就可以得到一些结论。</p><p>设 <span class="math inline">\(\alpha&gt;1\)</span> 是一个实数，令 <span class="math inline">\(\{q_n\}=\{\lceil\alpha n\rceil\}\)</span>，那么得到：<span class="math inline">\(\tau(n)=\left\lfloor n/\alpha\right\rfloor\)</span>。根据 <span class="math inline">\(\textbf{Theorem B}\)</span>，有：<span class="math inline">\(\left\{n+\left\lfloor n/\alpha\right\rfloor\right\}\)</span> 和 <span class="math inline">\(\{\lceil\alpha n\rceil+n-1\}\)</span> 构成了对正整数的一个划分。化简一下：<span class="math inline">\(\left\{n+\left\lfloor n/\alpha\right\rfloor\right\}=\left\{\left\lfloor n\left(1+1/\alpha\right)\right\rfloor\right\}\)</span>，<span class="math inline">\(\{\lceil\alpha n\rceil+n-1\}=\{\lceil(\alpha+1) n\rceil-1\}\)</span>。做代换：<span class="math inline">\(u=1+1/\alpha,\,v=1+\alpha\)</span>，得到：</p><blockquote><p><span class="math inline">\(\textbf{Theorem C}\)</span>：对于 <span class="math inline">\(\forall u,v&gt;1\)</span> 满足 <span class="math inline">\(\frac{1}{u}+\frac{1}{v}=1\)</span>，<span class="math inline">\(\{\lfloor un\rfloor\}\)</span> 和 <span class="math inline">\(\{\lceil vn\rceil-1\}\)</span> 构成了对正整数的一个划分。</p></blockquote><p>如果进一步对 <span class="math inline">\(\textbf{Theorem C}\)</span> 特殊化处理，欲使 <span class="math inline">\(\lceil vn\rceil-1=\lfloor vn\rfloor\)</span>，只需要 <span class="math inline">\(\forall n,\,vn\notin\mathbb{Z}\)</span>，也即 <span class="math inline">\(v\)</span> 是无理数；此时，<span class="math inline">\(u\)</span> 显然也是无理数，于是我们有下述定理：</p><blockquote><p><span class="math inline">\(\textbf{Theorem D}\)</span>：对于<strong>无理数</strong> <span class="math inline">\(u,v&gt;1\)</span> 满足 <span class="math inline">\(\frac{1}{u}+\frac{1}{v}=1\)</span>，<span class="math inline">\(\{\lfloor un\rfloor]\}\)</span> 和 <span class="math inline">\(\{\lfloor vn\rfloor\}\)</span> 构成了对正整数的一个划分。</p></blockquote><p>我们把 <span class="math inline">\(\textbf{Theorem B}\)</span> 应用到另一个函数上：设 <span class="math inline">\(m\in\mathbb{Z},\,m&gt;1\)</span>，<span class="math inline">\(q_n=n^m\)</span>，则 <span class="math inline">\(\tau(n)=\left\lfloor\sqrt[m]{n}\right\rfloor\)</span>，于是：</p><blockquote><p><span class="math inline">\(\textbf{Theorem E}\)</span>：设 <span class="math inline">\(m\in\mathbb{Z},\,m&gt;1\)</span>，序列 <span class="math inline">\(\{n+\lfloor n^{1/m}\rfloor\}\)</span> 和 <span class="math inline">\(\{n^m+n-1\}\)</span> 构成了对正整数的一个划分。</p></blockquote><p>如果把 <span class="math inline">\(\textbf{Theorem B}\)</span> 应用到 <span class="math inline">\(q_n=b^n\;(b\in\mathbb{Z},\,b&gt;1)\)</span> 上，得到：</p><blockquote><p><span class="math inline">\(\textbf{Theorem F}\)</span>：设 <span class="math inline">\(b\in\mathbb{Z},\,b&gt;1\)</span>，序列 <span class="math inline">\(\{n+\lfloor\log_bn\rfloor\}\)</span> 和 <span class="math inline">\(\{b^n+n-1\}\)</span> 构成了对正整数的一个划分。</p></blockquote><p>总而言之，我们可以根据自己的意愿取 <span class="math inline">\(q_n\)</span> 为某种正整数的子序列，然后套用 <span class="math inline">\(\textbf{Theorem B}\)</span>，就可以得到新的结论。</p>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>数学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[具体数学]第三章·整值函数</title>
    <link href="/blog-main/2020/08/03/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E4%B8%89%E7%AB%A0%C2%B7%E6%95%B4%E5%80%BC%E5%87%BD%E6%95%B0/"/>
    <url>/blog-main/2020/08/03/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E4%B8%89%E7%AB%A0%C2%B7%E6%95%B4%E5%80%BC%E5%87%BD%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<blockquote><p>第三章·整值函数分5节，包括底和顶的相关知识以及取模运算。底和顶（即取整函数）在许多问题以及编程中经常用到，很重要。</p></blockquote><h2 id="底和顶及其应用">底和顶及其应用</h2><p><span class="math inline">\(\lfloor x\rfloor,\lceil x\rceil\)</span> 定义不说了，就是向下/向上取整，书中翻译为“底”和“顶”。</p><p><strong>性质</strong>： <span class="math display">\[\lceil x\rceil-\lfloor x\rfloor=[x不是整数]\]</span></p><p><strong>反射律</strong>： <span class="math display">\[\lfloor-x\rfloor=-\lceil x\rceil\quad\lceil-x\rceil=-\lfloor x\rfloor\]</span></p><p><br></p><p><strong>互相转化</strong>：对 <span class="math inline">\(\forall n\in\mathbb Z,m\in\mathbb N^*\)</span>，有： <span class="math display">\[\left\lceil\frac{n}{m}\right\rceil=\left\lfloor\frac{n+m-1}{m}\right\rfloor\]</span> 证：两边同时减去 <span class="math inline">\(\left\lfloor\frac{n}{m}\right\rfloor\)</span> 有： <span class="math display">\[\begin{align}左式&amp;=\left\lceil\frac{n}{m}\right\rceil-\left\lfloor\frac{n}{m}\right\rfloor=[m\nmid n]\\右式&amp;=\left\lfloor\frac{n+m-1}{m}\right\rfloor-\left\lfloor\frac{n}{m}\right\rfloor=1-\left(\left\lfloor\frac{n}{m}\right\rfloor-\left\lfloor\frac{n-1}{m}\right\rfloor\right)=1-[m\mid n]\end{align}\]</span> 其中，右式最后一步基于：<span class="math inline">\(\left\lfloor\frac{n}{m}\right\rfloor-\left\lfloor\frac{n-1}{m}\right\rfloor=[m\mid n]\)</span>，证一下：若 <span class="math inline">\(m\mid n\)</span>，显然 <span class="math inline">\(\left\lfloor\frac{n}{m}\right\rfloor-\left\lfloor\frac{n-1}{m}\right\rfloor=1\)</span>；否则，设 <span class="math inline">\(n=km+r\;(1\leqslant r&lt;m)\)</span>，则 <span class="math inline">\(\left\lfloor\frac{n}{m}\right\rfloor=k\)</span>，<span class="math inline">\(\left\lfloor\frac{n-1}{m}\right\rfloor=\left\lfloor k+\frac{r-1}{m}\right\rfloor=k\)</span>（注意 <span class="math inline">\(0\leqslant \frac{r-1}{m}&lt;1\)</span>），所以 <span class="math inline">\(\left\lfloor\frac{n}{m}\right\rfloor-\left\lfloor\frac{n-1}{m}\right\rfloor=0\)</span>；综上，<span class="math inline">\(\left\lfloor\frac{n}{m}\right\rfloor-\left\lfloor\frac{n-1}{m}\right\rfloor=[m\mid n]\)</span>.</p><p>显然，<span class="math inline">\(左式=[m\nmid n]=1-[m\mid n]=右式\)</span>，证毕。</p><p><br></p><p><strong>分数部分</strong>： <span class="math display">\[\{x\}=x-\lfloor x\rfloor\]</span></p><h3 id="常用法则">常用法则</h3><p><strong>常用法则</strong>：取整函数等式与不等式之间的转化 <span class="math display">\[\boxed{\begin{align}\lfloor x\rfloor=n\iff n\leqslant x&lt;n+1\iff x-1&lt;n\leqslant x\\\lceil x\rceil=n\iff n-1&lt;x\leqslant n\iff x\leqslant n&lt;x+1\end{align}}\tag{1}\]</span></p><blockquote><p>上述法则很简单很显然，但是也很重要，证明的时候可以用来把取整符号去掉。</p></blockquote><p><strong>常用法则</strong>：实数和整数之间的任何不等式都<strong>等价于</strong>整数之间的一个有关取整函数的不等式。 <span class="math display">\[\boxed{\begin{align}x&lt;n&amp;\iff \lfloor x\rfloor&lt; n\\n&lt;x&amp;\iff n&lt;\lceil x\rceil\\x\leqslant n&amp;\iff\lceil x\rceil\leqslant n\\n\leqslant x&amp;\iff n\leqslant\lfloor x\rfloor\end{align}}\tag{2}\]</span></p><blockquote><p>上述法则很容易证明，也很重要。</p></blockquote><h3 id="一个非常有用的定理">一个非常有用的定理</h3><p>设 <span class="math inline">\(f(x)\)</span> 是任意一个具有如下性质且在一个实数区间<u>连续</u>的<u>单调递增函数</u>： <span class="math display">\[f(x)=整数\implies x=整数\]</span> 那么我们有： <span class="math display">\[\begin{align}\left\lfloor f\left(\lfloor x\rfloor\right)\right\rfloor=\left\lfloor f(x)\right\rfloor\\\left\lceil f\left(\lceil x\rceil\right)\right\rceil=\left\lceil f(x)\right\rceil\end{align}\]</span> 证明：只对底进行证明，顶的情况类似。若 <span class="math inline">\(x\)</span> 是整数，显然成立；否则，<span class="math inline">\(\lfloor x\rfloor&lt;x\)</span>，由于 <span class="math inline">\(f(x)\)</span> 单调递增，有：<span class="math inline">\(f\left(\lfloor x\rfloor\right)&lt;f(x)\)</span>，于是 <span class="math inline">\(\left\lfloor f\left(\lfloor x\rfloor\right)\right\rfloor\leqslant\left\lfloor f(x)\right\rfloor\)</span>。假若 <span class="math inline">\(\left\lfloor f\left(\lfloor x\rfloor\right)\right\rfloor&lt;\left\lfloor f(x)\right\rfloor\)</span>，由连续性知，介值定理成立：存在一个 <span class="math inline">\(y\in\left(\lfloor x\rfloor,x\right]\)</span>，使得 <span class="math inline">\(f(y)=\left\lfloor f(x)\right\rfloor\)</span>。根据 <span class="math inline">\(f(x)\)</span> 的题设性质可知，<span class="math inline">\(y\)</span> 是整数，然而我们无法在 <span class="math inline">\(\left(\lfloor x\rfloor,x\right]\)</span> 中找到一个整数，故假设不成立。综上，<span class="math inline">\(\left\lfloor f\left(\lfloor x\rfloor\right)\right\rfloor=\left\lfloor f(x)\right\rfloor\)</span>. <span class="math inline">\(\square\)</span></p><blockquote><p>为什么说这个定理很有用呢？它可以推导出： <span class="math display">\[\left\lfloor\frac{\left\lfloor x\right\rfloor+m}{n}\right\rfloor=\left\lfloor\frac{x+m}{n}\right\rfloor\]</span> 这是取 <span class="math inline">\(f(x)=\frac{x+m}{n}\)</span> 得到的结果。进一步特殊化，有： <span class="math display">\[\left\lfloor\frac{\left\lfloor \frac{a}{b}\right\rfloor}{c}\right\rfloor=\left\lfloor\frac{a}{bc}\right\rfloor\]</span> 这一点在写代码的时候很有用啊。比如在推导一些数论函数的题的时候，就会经常遇到上述式子。</p><p>另外，如果我们取 <span class="math inline">\(f(x)=\sqrt x\)</span>，可以知道： <span class="math display">\[\left\lfloor\sqrt{\lfloor x\rfloor}\right\rfloor=\left\lfloor\sqrt x\right\rfloor,\;\left\lceil\sqrt{\lceil x\rceil}\right\rceil=\left\lceil\sqrt x\right\rceil\]</span></p></blockquote><h3 id="区间内整数个数">区间内整数个数</h3><table><thead><tr class="header"><th style="text-align: center;">区间</th><th style="text-align: center;">整数个数</th><th style="text-align: center;">限制条件</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><span class="math inline">\([\alpha\cdots\beta]\)</span></td><td style="text-align: center;"><span class="math inline">\(\lfloor\beta\rfloor-\lceil\alpha\rceil+1\)</span></td><td style="text-align: center;"><span class="math inline">\(\alpha\leqslant\beta\)</span></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\([\alpha\cdots\beta)\)</span></td><td style="text-align: center;"><span class="math inline">\(\lceil\beta\rceil-\lceil\alpha\rceil\)</span></td><td style="text-align: center;"><span class="math inline">\(\alpha\leqslant\beta\)</span></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\((\alpha\cdots\beta]\)</span></td><td style="text-align: center;"><span class="math inline">\(\lfloor\beta\rfloor-\lfloor\alpha\rfloor\)</span></td><td style="text-align: center;"><span class="math inline">\(\alpha\leqslant\beta\)</span></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\((\alpha\cdots\beta)\)</span></td><td style="text-align: center;"><span class="math inline">\(\lceil\beta\rceil-\lfloor\alpha\rfloor-1\)</span></td><td style="text-align: center;"><span class="math inline">\(\alpha&lt;\beta\)</span></td></tr></tbody></table><h3 id="来看一个例子">来看一个例子</h3><p>求： <span class="math display">\[\sum_{n=1}^N\big[\left\lfloor\sqrt[3]{n}\right\rfloor\mid n\big]\]</span> 解： <span class="math display">\[\begin{align}\sum_{n=1}^N\big[\left\lfloor\sqrt[3]{n}\right\rfloor\mid n\big]&amp;=\sum_{n=1}^{N}\sum_{k\mid N}\big[k=\left\lfloor\sqrt[3]{n}\right\rfloor\big]&amp;&amp;枚举\;k\\&amp;=\sum_{k=1}^{K}\sum_{m=1}^{\left\lfloor\frac{N}{k}\right\rfloor}\big[k=\left\lfloor\sqrt[3]{n}\right\rfloor\big]&amp;&amp;设\;K=\left\lfloor\sqrt[3]{N}\right\rfloor，求和号换序并置\;n=km\\&amp;=\sum_{k=1}^{K-1}\sum_{m=1}^{\left\lfloor\frac{N}{k}\right\rfloor}[k^3\leqslant km&lt;(k+1)^3]+\sum_{m}[K^3\leqslant Km\leqslant N] &amp;&amp;用不等式转化，注意最后一项单独计算\\&amp;=\sum_{k=1}^{K-1}\sum_{m=1}^{\left\lfloor\frac{N}{k}\right\rfloor}\left[m\in\left[k^2,\frac{(k+1)^3}{k}\right)\right]+\sum_{m}\left[m\in\left[K^2,\frac{N}{K}\right]\right]\\&amp;=\left(\sum_{k=1}^N\left\lceil k^2+3k+3+\frac{1}{k}\right\rceil-\lceil k^2\rceil\right)+\left\lfloor\frac{N}{K}\right\rfloor-\lceil K^2\rceil+1&amp;&amp;应用区间内整数个数的结论\\&amp;=\left(\sum_{k=1}^{K-1}3k+4\right)+\left\lfloor\frac{N}{K}\right\rfloor-K^2+1\\&amp;=\frac{(K-1)(3K+8)}{2}+\left\lfloor\frac{N}{K}\right\rfloor-K^2+1\\&amp;=\frac{1}{2}K^2+\frac{5}{2}K-3+\left\lfloor\frac{N}{K}\right\rfloor\end{align}\]</span></p><blockquote><p><strong>常用法则<span class="math inline">\((1)\)</span></strong> 中的不等式在上述证明中起到了关键的作用，否则我们无法很好地处理向下取整。</p></blockquote><h3 id="谱">谱</h3><p>定义实数 <span class="math inline">\(\alpha\)</span> 的<strong>谱</strong>（spectrum）是一个无限多重集合： <span class="math display">\[\text{Spec}(\alpha)=\left\{ \lfloor\alpha\rfloor,\lfloor2\alpha\rfloor,\lfloor3\alpha\rfloor,\cdots \right\}\]</span> 性质：</p><ul><li><p>没有两个实数的谱相等。</p><p>证明：假设 <span class="math inline">\(\alpha&lt;\beta\)</span>，则必定存在 <span class="math inline">\(N\)</span> 使得 <span class="math inline">\(N(\beta-\alpha)\geqslant 1\)</span>，则 <span class="math inline">\(N\beta\geqslant N\alpha+1\)</span>，故 <span class="math inline">\(\lfloor N\beta\rfloor&gt;\lfloor N\alpha\rfloor\)</span>，它们的谱在 <span class="math inline">\(N\)</span> 处不相等。<span class="math inline">\(\square\)</span></p></li><li><p><span class="math inline">\(\text{Spec}(\alpha)\)</span> 和 <span class="math inline">\(\text{Spec}(\beta)\)</span> 给出了正整数的一个划分当且仅当 <span class="math inline">\(\alpha,\beta\)</span> 均是无理数且 <span class="math inline">\(\frac{1}{\alpha}+\frac{1}{\beta}=1\)</span>. （例如 <span class="math inline">\(\text{Spec}(2)\)</span> 和 <span class="math inline">\(\text{Spec}(2+\sqrt 2)\)</span> 给出了正整数的一个划分）</p><p>证：<strong>充分性</strong>：只需要证明对于任意的 <span class="math inline">\(n\)</span>，<span class="math inline">\(\text{Spec}(\alpha)\)</span> 中 <span class="math inline">\(\leqslant n\)</span> 的元素个数 <span class="math inline">\(N(\alpha,n)\)</span> 和 <span class="math inline">\(\text{Spec}(\beta)\)</span> 中 <span class="math inline">\(\leqslant n\)</span> 的元素个数 <span class="math inline">\(N(\beta,n)\)</span> 之和为 <span class="math inline">\(n\)</span> 即可。<span class="math inline">\(N\)</span> 可如下求得： <span class="math display">\[\begin{align}N(\alpha,n)&amp;=\sum_{k&gt;0}\big[\lfloor k\alpha\rfloor\leqslant n\big]\\&amp;=\sum_{k&gt;0}\big[\lfloor k\alpha\rfloor&lt;n+1\big]\\&amp;=\sum_{k&gt;0}\big[k\alpha&lt;n+1\big]&amp;&amp;常用法则(2)\\&amp;=\sum_k\left[0&lt;k&lt;\frac{n+1}{\alpha}\right]\\&amp;=\left\lceil\frac{n+1}{\alpha}\right\rceil-1\end{align}\]</span> 于是 <span class="math display">\[\begin{align}N(\alpha,n)+N(\beta,n)&amp;=\left\lceil\frac{n+1}{\alpha}\right\rceil-1+\left\lceil\frac{n+1}{\beta}\right\rceil-1=\left\lfloor\frac{n+1}{\alpha}\right\rfloor+\left\lfloor\frac{n+1}{\beta}\right\rfloor\\&amp;=\frac{n+1}{\alpha}+\frac{n+1}{\beta}-\left\{\frac{n+1}{\alpha}\right\}-\left\{\frac{n+1}{\beta}\right\}\\&amp;=n+1-\left[\left\{\frac{n+1}{\alpha}\right\}+\left\{\frac{n+1}{\beta}\right\}\right]\\&amp;=n\end{align}\]</span> （注：上面的证明多次用到了 <span class="math inline">\(\frac{n+1}{\alpha}\)</span> 和 <span class="math inline">\(\frac{n+1}{\beta}\)</span> 必然是<strong>小数</strong>的条件。）</p><p><strong>必要性</strong>：若 <span class="math inline">\(\alpha\)</span> 是有理数，那么 <span class="math inline">\(\beta=\frac{1}{1-\frac{1}{\alpha}}\)</span> 也是有理数，把它们写成分数，设它们<strong>分子</strong>的最小公倍数为 <span class="math inline">\(m\)</span>，那么二者的谱中都会出现 <span class="math inline">\(m\)</span> 这个数，不能构成一个划分，故 <span class="math inline">\(\alpha,\beta\)</span> 均为无理数。此时，<span class="math inline">\(N(\alpha,n)+N(\beta,n)=(n+1)\left(\frac{1}{\alpha}+\frac{1}{\beta}\right)-1=n\)</span>，解得：<span class="math inline">\(\frac{1}{\alpha}+\frac{1}{\beta}=1\)</span>. 证毕。</p></li></ul><blockquote><p>针对第二个性质，<span class="math inline">\(\textbf{Solomon W. Golomb}\)</span> 在他的论文 <a href="https://doi.org/10.1080/0025570X.1976.11976573"><span class="math inline">\(\text{The &#39;Sales Tax&#39; Theorem}\)</span></a> 中指出，当 <span class="math inline">\(\frac{1}{\alpha}+\frac{1}{\beta}=1\)</span> 时（不要求无理数），<span class="math inline">\(\{\lfloor n\alpha\rfloor\mid n\geqslant 1\}\)</span> 和 <span class="math inline">\(\{\lceil n\beta\rceil-1\mid n\geqslant 1\}\)</span> 总会构成一个划分。</p></blockquote><h2 id="底和顶的递归式">底和顶的递归式</h2><h3 id="高德纳数">高德纳数</h3><p><span class="math display">\[\begin{cases}K_0=1\\K_{n+1}=1+\min\left(2K_{\lfloor n/2\rfloor},3K_{\lfloor n/3\rfloor}\right)\end{cases}\]</span></p><p>证明或证伪： <span class="math display">\[K_n\geqslant n\]</span> 证：我们来证一个<strong>更强的结论</strong>： <span class="math display">\[K_n&gt;n\]</span> 归纳法：首先，<span class="math inline">\(K_0=1&gt;0\)</span> 成立；其次，假设对 <span class="math inline">\(K_0\cdots K_n\)</span> 都成立，那么 <span class="math inline">\(K_{\lfloor n/2\rfloor}&gt;\lfloor n/2\rfloor\)</span>，注意到 <span class="math inline">\(K_{\lfloor n/2\rfloor}\)</span> 是整数，所以由<strong>常用法则<span class="math inline">\((2)\)</span></strong>可知： <span class="math display">\[K_{\lfloor n/2\rfloor}&gt;n/2\]</span> 同理， <span class="math display">\[K_{\lfloor n/3\rfloor}&gt;n/3\]</span> 所以， <span class="math display">\[K_{n+1}=1+\min\left(2K_{\lfloor n/2\rfloor},3K_{\lfloor n/3\rfloor}\right)&gt;1+\min(2\cdot n/2+3\cdot n/3)=1+n\]</span> 证毕。</p><blockquote><p>一件很有趣的事情是，这个更强的结论容易归纳证明，但是原来更弱的结论却难以归纳证明。</p><p>如果我们尝试对原结论归纳，会有： <span class="math display">\[K_{n+1}=1+\min\left(2K_{\lfloor n/2\rfloor},3K_{\lfloor n/3\rfloor}\right)\geqslant1+\min\left(2\left\lfloor \frac{n}{2}\right\rfloor,3\left\lfloor \frac{n}{3}\right\rfloor\right)\]</span> 然而，<span class="math inline">\(2\left\lfloor\frac{n}{2}\right\rfloor\)</span> 可以小到 <span class="math inline">\(n-1\)</span>（当 <span class="math inline">\(n\)</span> 是奇数时），<span class="math inline">\(3\left\lfloor\frac{n}{3}\right\rfloor\)</span> 可以小到 <span class="math inline">\(n-2\)</span>（当 <span class="math inline">\(n=3k+2\)</span> 时），所以我们只能得到： <span class="math display">\[K_{n+1}\geqslant1+\min(n-1,n-2)=n-1\]</span> 不能归纳出结论。</p><p>这种现象在证明时还是比较常见的，即证明更强的结论反而更简单。数学之有趣大概于此。</p></blockquote><h3 id="再论约瑟夫问题">再论约瑟夫问题</h3><p>我们不从递归式，而是换一种角度思考约瑟夫问题。这里探究每隔 <span class="math inline">\(q-1\)</span> 个人死一个的一般情形。我们不断地指定新的编号：刚开始是 <span class="math inline">\(1,2,\cdots,n\)</span>，随后 <span class="math inline">\(1\)</span> 号变成 <span class="math inline">\(n+1\)</span> 号、……、<span class="math inline">\(q-1\)</span> 号变成 <span class="math inline">\(n+q-1\)</span> 号，<strong><span class="math inline">\(q\)</span> 号处死</strong>，<span class="math inline">\(q+1\)</span> 号变成 <span class="math inline">\(n+q\)</span> 号，<span class="math inline">\(q+2\)</span> 号变成 <span class="math inline">\(n+q+1\)</span> 号……，<strong><span class="math inline">\(2q\)</span> 号处死</strong>，<span class="math inline">\(2q+1\)</span> 号变成 <span class="math inline">\(n+2q-1\)</span> 号，……，<span class="math inline">\(kq+1\)</span> 号变成 <span class="math inline">\(n+kq+1-k=n+(q-1)k+1\)</span> 号，……，最后剩下 <span class="math inline">\(qn\)</span> 号。</p><p><del>上面一段话没人想看</del>，举个例子好了：<span class="math inline">\(n=10\)</span>，<span class="math inline">\(q=3\)</span> 时，编号是这样变化的： <span class="math display">\[\begin{matrix}1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;10\\11&amp;12&amp;&amp;13&amp;14&amp;&amp;15&amp;16&amp;&amp;17\\18&amp;&amp;&amp;19&amp;20&amp;&amp;&amp;21&amp;&amp;22\\&amp;&amp;&amp;23&amp;24&amp;&amp;&amp;&amp;&amp;25\\&amp;&amp;&amp;26&amp;&amp;&amp;&amp;&amp;&amp;27\\&amp;&amp;&amp;28\\&amp;&amp;&amp;29\\&amp;&amp;&amp;30\end{matrix}\]</span></p><blockquote><p>如何理解上述编号过程？想象两个指针一前一后的扫，前一个指针从 <span class="math inline">\(1\)</span> 开始递增地、遇到一个数就赋值；后一个指针遇到 <span class="math inline">\(q\)</span> 的倍数就把它删掉（删掉的数不被第一个指针赋值）。于是乎，当一个数被赋值为 <span class="math inline">\(q\)</span> 的倍数时，它就被敲响了丧钟。</p></blockquote><p>对于编号 <span class="math inline">\(qk+r\;(1\leqslant r&lt;q)\)</span>，我们考虑它的下一个编号是多少。<span class="math inline">\(qk+r\)</span> 意味着它被下一次赋值时，会有 <span class="math inline">\(k\)</span> 个人死亡，所以它的下一个编号是 <span class="math inline">\(qk+r+(n-k)\)</span>；反过来，编号 <span class="math inline">\(N=n+k(q-1)+r\;(1\leqslant r&lt;q)\)</span> 的上一个编号就是 <span class="math inline">\(qk+r=qk+N-n-k(q-1)=N-n+k\)</span>。由于我们已经知道最后一个存活的人是 <span class="math inline">\(qn\)</span>，所以可以据此逆推出幸存者的原号码： <span class="math display">\[\begin{align}&amp;N:=qn\\&amp;\textbf{while}\quad N&gt;n\quad\textbf{do}\quad N:=\left\lfloor\frac{N-n-1}{q-1}\right\rfloor+N-n\\&amp;J_q(n):=N\end{align}\]</span> 如果作变量代换：<span class="math inline">\(D=qn+1-N\)</span>，上述代码可以简化： <span class="math display">\[\begin{align}&amp;D=1\\&amp;\textbf{while}\quad D\leqslant(q-1)n\quad\textbf{do}\quad D:=\left\lceil\frac{q}{q-1}D\right\rceil\\&amp;J_q(n):=qn+1-D\end{align}\]</span></p><h2 id="bmod二元运算"><span class="math inline">\(\bmod\)</span>：二元运算</h2><h3 id="定义">定义</h3><p><span class="math display">\[x\bmod y=x-y\lfloor x/y\rfloor,\quad y\neq0\]</span></p><p>注意上述定义中，<span class="math inline">\(x,y\in\mathbb{R}\)</span>.</p><p>特殊定义： <span class="math display">\[x\bmod0=x\]</span></p><blockquote><p><span class="math inline">\(\LaTeX\)</span> 里，<code>\bmod</code> 能像其他二元运算符一样打出一个漂亮的 <span class="math inline">\(\bmod\)</span>，而如果直接用 <code>\mod</code> 前后会有一大片空格。</p><p>另外，同余式中用 <code>\pmod</code>，它会自动在两边加上括号，例如：<span class="math inline">\(x\equiv1\pmod m\)</span> 的代码是：<code>x\equiv1\pmod m</code>.</p></blockquote><p><br></p><blockquote><p>务必注意不同的编程语言中的模运算和数学语言的 <span class="math inline">\(\bmod\)</span> 可能会有差别。</p><p>例如： <span class="math display">\[\begin{align}&amp;5\bmod 3=2\\&amp;5\bmod-3=5-(-3)\lfloor5/(-3)\rfloor=-1\\&amp;-5\bmod3=-5-3\lfloor(-5)/3\rfloor=1\\&amp;-5\bmod-3=-5-(-3)\lfloor(-5)/(-3)\rfloor=-2\end{align}\]</span> 而在 <span class="math inline">\(\text{C++}\)</span> 下：</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs gcode"><span class="hljs-number">5</span> <span class="hljs-meta">%</span> <span class="hljs-number">3</span> = <span class="hljs-number">2</span><br><span class="hljs-number">5</span> <span class="hljs-meta">%</span> <span class="hljs-comment">(-3)</span> = <span class="hljs-number">2</span><br><span class="hljs-comment">(-5)</span> <span class="hljs-meta">%</span> <span class="hljs-number">3</span> = <span class="hljs-number">-2</span><br><span class="hljs-comment">(-5)</span> <span class="hljs-meta">%</span> <span class="hljs-comment">(-3)</span> = <span class="hljs-number">-2</span><br></code></pre></td></tr></table></figure><p>但是 <span class="math inline">\(\text{python}\)</span> 下与数学语言又是一致的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-number">5</span>%<span class="hljs-number">3</span><br><span class="hljs-number">2</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-number">5</span>%(-<span class="hljs-number">3</span>)<br>-<span class="hljs-number">1</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>(-<span class="hljs-number">5</span>)%<span class="hljs-number">3</span><br><span class="hljs-number">1</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>(-<span class="hljs-number">5</span>)%(-<span class="hljs-number">3</span>)<br>-<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure></blockquote><h3 id="分配律">分配律</h3><p>对 <span class="math inline">\(\forall c,x,y\in\mathbb R\)</span>，有： <span class="math display">\[c(x\bmod y)=(cx)\bmod(cy)\]</span> 证明： <span class="math display">\[c(x\bmod y)=c(x-y\lfloor x /y\rfloor)=cx-cy\lfloor x/y\rfloor=(cx)-(cy)\lfloor(cx)/(cy)\rfloor=(cx)\bmod (cy)\]</span> <span class="math inline">\(\square\)</span></p><h3 id="一个例子">一个例子</h3><p>把 <span class="math inline">\(n\)</span> 个物品分成 <span class="math inline">\(m\)</span> 组，把组按照不增的次序排列，使分得尽可能均匀。</p><p>答案很简单：前 <span class="math inline">\(n\bmod m\)</span> 组分 <span class="math inline">\(\left\lceil\frac{n}{m}\right\rceil\)</span> 个，后 <span class="math inline">\(m-n\bmod m\)</span> 组分 <span class="math inline">\(\left\lfloor\frac{n}{m}\right\rfloor\)</span> 个。</p><p>如果问第 <span class="math inline">\(k\)</span> 组有多少个物品，我们想找到一个式子，当 <span class="math inline">\(k\leqslant n\bmod m\)</span> 时值为 <span class="math inline">\(\left\lceil\frac{n}{m}\right\rceil\)</span>，当 <span class="math inline">\(k&gt;n\bmod m\)</span> 时值为 <span class="math inline">\(\left\lfloor\frac{n}{m}\right\rfloor\)</span>. 可以验证，答案是： <span class="math display">\[\text{num}(k)=\left\lceil\frac{n-k+1}{m}\right\rceil\]</span> 又根据 <span class="math inline">\(\sum\limits_{k=1}^m\text{num}(k)=n\)</span>，我们可以得到一个恒等式： <span class="math display">\[n=\left\lceil\frac{n}{m}\right\rceil+\left\lceil\frac{n-1}{m}\right\rceil+\cdots+\left\lceil\frac{n-m+1}{m}\right\rceil\]</span> 该式对 <span class="math inline">\(\forall n\in\mathbb Z,\,m\in\mathbb{N}^*\)</span> 恒成立。</p><p><br></p><p>对应的，如果把原题改成<strong>不减</strong>，我们也可以得到一个恒等式： <span class="math display">\[n=\left\lfloor\frac{n}{m}\right\rfloor+\left\lfloor\frac{n+1}{m}\right\rfloor+\cdots+\left\lfloor\frac{n+m-1}{m}\right\rfloor\]</span></p><h2 id="底和顶的和式">底和顶的和式</h2><p>我们不是很能直接处理取整函数，所以往往一个有效的技巧是<strong>引入变量</strong>来规避取整。</p><h3 id="第一个例子">第一个例子</h3><p>求 <span class="math display">\[\sum_{0\leqslant k&lt;n}\lfloor\sqrt k\rfloor\]</span> 的封闭形式。</p><p>解： <span class="math display">\[\begin{align}\sum_{0\leqslant k&lt;n}\lfloor\sqrt k\rfloor&amp;=\sum_{m,k\geqslant 0}m[k&lt;n]\big[m=\lfloor\sqrt k\rfloor\big]&amp;&amp;变量替换\\&amp;=\sum_{m,k\geqslant 0}m[k&lt; n][\sqrt k-1&lt;m\leqslant \sqrt k]&amp;&amp;常用法则(1)\\&amp;=\sum_{m,k\geqslant 0}m[k&lt; n][m^2\leqslant k&lt;(m+1)^2]\\&amp;=\sum_{m,k\geqslant 0}m[m^2\leqslant k&lt;(m+1)^2\leqslant n]+\sum_{m,k\geqslant 0}m[m^2\leqslant k&lt; n&lt;(m+1)^2]&amp;&amp;拆分\end{align}\]</span> 其中，第一个和式计算如下： <span class="math display">\[\begin{align}\sum_{m,k\geqslant 0}m[m^2\leqslant k&lt;(m+1)^2\leqslant n]&amp;=\sum_{0\leqslant m\leqslant \lfloor\sqrt n\rfloor-1}m\left((m+1)^2-m^2\right)\\&amp;=\sum_0^{a-1}2m^2+m&amp;&amp;记\;a=\lfloor\sqrt n\rfloor\\&amp;=2\cdot\frac{(a-1)a(2a-1)}{6}+\frac{a(a-1)}{2}\\&amp;=\frac{2}{3}a^3-\frac{1}{2}a^2-\frac{1}{6}a\end{align}\]</span> 第二个和式计算如下： <span class="math display">\[\sum_{m,k\geqslant 0}m[m^2\leqslant k&lt; n&lt;(m+1)^2]=a(n-a^2)\]</span> （注意到仅当 <span class="math inline">\(m=a=\lfloor\sqrt n\rfloor\)</span> 的时候才满足条件）</p><p>因此， <span class="math display">\[\sum_{0\leqslant k&lt;n}\lfloor\sqrt k\rfloor=\frac{2}{3}a^3-\frac{1}{2}a^2-\frac{1}{6}a+na-a^3=na-\frac{1}{3}a^3-\frac{1}{2}a^2-\frac{1}{6}a\]</span></p><h3 id="第二个例子">第二个例子</h3><p>接下来两个大坑先留着。。。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>具体数学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[具体数学]第二章·和式（第二部分）</title>
    <link href="/blog-main/2020/08/01/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E4%BA%8C%E7%AB%A0%C2%B7%E5%92%8C%E5%BC%8F%EF%BC%88%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%89/"/>
    <url>/blog-main/2020/08/01/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E4%BA%8C%E7%AB%A0%C2%B7%E5%92%8C%E5%BC%8F%EF%BC%88%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<blockquote><p>第二章·和式分7节，这是我的笔记第二部分，主要是<strong>有限微积分</strong>的内容。为什么单独把它拿出来呢？因为实在是太具有启发性了——特别是当把有限微积分和传统的无限微积分做<strong>类比</strong>的时候。</p></blockquote><h2 id="算子">算子</h2><p>我们在学习传统的无限微积分时，运算基于<strong>微分(derivative)算子</strong> <span class="math inline">\(D\)</span>： <span class="math display">\[Df(x)=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}=\frac{\mathrm df}{\mathrm dx}\]</span> 而有限微积分基于的是<strong>差分(difference)算子</strong> <span class="math inline">\(\Delta\)</span>： <span class="math display">\[\Delta f(x)=f(x+1)-f(x)\]</span></p><h2 id="下降上升阶乘幂">下降/上升阶乘幂</h2><p>我们知道 <span class="math inline">\(D\)</span> 作用在幂函数 <span class="math inline">\(f(x)=x^m\)</span> 上得到：<span class="math inline">\(D(x^m)=mx^{m-1}\)</span>，但是显然 <span class="math inline">\(\Delta(x^m)\)</span> 没有这么好的规律。不过它对“下降阶乘幂”有这样的好规律。</p><p><br></p><p><strong>下降阶乘幂</strong>： <span class="math display">\[x^{\underline m}=x(x-1)(x-2)\cdots(x-m+1)\]</span> <strong>上升阶乘幂</strong>： <span class="math display">\[x^{\overline m}=x(x+1)(x+2)\cdots(x+m-1)\]</span> 下面说明差分算子适用于下降阶乘幂： <span class="math display">\[\begin{align}\Delta(x^{\underline m})&amp;={(x+1)}^{\underline m}-x^{\underline m}\\&amp;=(x+1)x\cdots(x-m)-x(x-1)\cdots(x-m+1)\\&amp;=mx(x-1)\cdots(x-m)\\&amp;=mx^{\underline {m-1}}\end{align}\]</span> （注意：对上升阶乘幂适用的算子是：<span class="math inline">\(\nabla f(x)=f(x)-f(x-1)\)</span> ）</p><h2 id="积分与和式">积分与和式</h2><p>我们知道无限微积分中，微分的逆运算是积分；类似地，有限微积分中，差分的逆运算就是和式。</p><p><br></p><p><strong>不定积分与不定和式</strong>： <span class="math display">\[\begin{align}g(x)=Df(x)&amp;\iff f(x)=\int g(x)\mathrm dx+C\\g(x)=\Delta f(x)&amp;\iff f(x)=\sum g(x)\delta x+C\end{align}\]</span> 在不定和式中，<span class="math inline">\(\sum g(x)\delta x\)</span> 表示差分等于 <span class="math inline">\(f(x)\)</span> 的一个函数类，<span class="math inline">\(C\)</span> 可以是满足 <span class="math inline">\(p(x+1)=p(x)\)</span> 的任意一个函数 <span class="math inline">\(p(x)\)</span>.</p><p><br></p><p><strong>定积分与和式</strong>： <span class="math display">\[\begin{align}\int_a^b g(x)\mathrm dx&amp;=f(b)-f(a)\\\sum\nolimits_a^b g(x)\delta x&amp;=f(b)-f(a)\end{align}\]</span> <br></p><p>基于我们已经建立的理论，我们类比无限微积分，可以得到一些结果： <span class="math display">\[\sum_{k=0}^{n-1}k^{\underline m}=\sum\nolimits_0^nk^{\underline m}\delta k=\frac{k^{\underline{m+1}}}{m+1}\Bigg|_0^n=\frac{n^{\underline{m+1}}}{m+1}\]</span></p><blockquote><p>应用：求 <span class="math inline">\(\square_n=\sum\limits_{k=0}^nk^2\)</span> 的封闭形式。为此，只需要注意到：<span class="math inline">\(k^2=k^{\underline 2}+k^{\underline 1}\)</span>，那么有： <span class="math display">\[\sum_{k=0}^nk^2=\sum\nolimits_0^{n+1}(k^{\underline 2}+k^{\underline 1})\delta k=\frac{(n+1)^{\underline 3}}{3}+\frac{(n+1)^{\underline 2}}{2}=\frac{n(n+1)(2n+1)}{6}\]</span> 事实上，根据斯特林数，我们总能在幂和阶乘幂之间进行转换。</p></blockquote><p>阶乘幂还有一些其他性质，比如满足二项式定理：<span class="math inline">\((x+y)^{\underline m}=\sum\limits_{k=0}^m\binom{m}{k}x^{\underline k}y^{\underline {m-k}}\)</span>，<span class="math inline">\((x+y)^{\overline m}=\sum\limits_{k=0}^m\binom{m}{k}x^{\overline k}y^{\overline {m-k}}\)</span>.</p><h2 id="阶乘幂推广到负指数">阶乘幂推广到负指数</h2><p><span class="math display">\[x^{\underline {-m}}=\frac{1}{(x+1)(x+2)\cdots(x+m)}\quad,m&gt;0\]</span></p><p>有了它之后，我们可以写出类似于 <span class="math inline">\(x^{m+n}=x^m\cdot x^n\)</span> 的法则： <span class="math display">\[x^{\underline {m+n}}=x^{\underline m}\cdot{(x-m)}^{\underline{n}}\]</span></p><p>并且差分算子依旧适用于负指数： <span class="math display">\[\begin{align}\Delta x^{\underline{-m}}&amp;=\frac{1}{(x+2)\cdots(x+m+1)}-\frac{1}{(x+1)\cdots(x+m)}\\&amp;=\frac{1}{(x+2)\cdots(x+m)}\left(\frac{1}{x+m+1}-\frac{1}{x+1}\right)\\&amp;=-m\frac{1}{(x+1)\cdots(x+m+1)}\\&amp;=-mx^{\underline {-(m+1)}}\end{align}\]</span> 于是乎，类比幂函数的积分，我们有对阶乘幂的求和： <span class="math display">\[\begin{align}&amp;\int_a^bx^m\mathrm dx=\frac{x^{m+1}}{m+1}\Bigg|_a^b=\frac{b^{m+1}}{m+1}-\frac{a^{m+1}}{m+1}\quad,m\neq-1\\&amp;\sum\nolimits_a^bx^{\underline m}\delta x=\frac{x^{\underline{m+1}}}{m+1}\Bigg|_a^b=\frac{b^{\underline{m+1}}}{m+1}-\frac{a^{\underline{m+1}}}{m+1}\quad,m\neq-1\end{align}\]</span> 而当 <span class="math inline">\(m=-1\)</span> 时，<span class="math inline">\(\int_a^bx^{-1}\mathrm dx=\ln x\Big|_a^b=\ln b-\ln a\)</span>，我们想找到一个 <span class="math inline">\(\ln x\)</span> 的<strong>有限模拟</strong>，即找到函数 <span class="math inline">\(f(x)\)</span>，使得 <span class="math inline">\(\Delta f(x)=f(x+1)-f(x)=x^{\underline{-1}}=\frac{1}{x+1}\)</span>. 显然，调和数 <span class="math inline">\(H_x=\sum\limits_{k=1}^x\frac{1}{k}\)</span> 满足条件。所以我们有： <span class="math display">\[\sum\nolimits_a^bx^{\underline m}\delta x=\begin{cases}\frac{x^{\underline{m+1}}}{m+1}\Bigg|_a^b&amp;,m\neq-1\\H_x\Big|_a^b&amp;,m=-1\end{cases}\]</span> 如果我们想找到一个 <span class="math inline">\(e^x\)</span> 的<strong>有限模拟</strong>，即找到函数使得 <span class="math inline">\(\Delta f(x)=f(x+1)-f(x)=f(x)\)</span>，容易知道：<span class="math inline">\(f(x)=2^x\)</span>.</p><h2 id="分部求和">分部求和</h2><p>回忆一下<strong>分部积分</strong>公式的推导过程： <span class="math display">\[\mathrm d(uv)=u\mathrm dv+v\mathrm du\implies\int u\mathrm dv=uv-\int v\mathrm du\]</span> 我们推导一下<strong>分部求和</strong>公式： <span class="math display">\[\begin{align}\Delta(u(x)v(x))&amp;=u(x+1)v(x+1)-u(x)v(x)\\&amp;=v(x+1)\big[u(x+1)-u(x)\big]+u(x)\big[v(x+1)-v(x)\big]\\&amp;=v(x+1)\Delta u(x)+u(x)\Delta v(x)\end{align}\]</span> 引入<strong>移位算子</strong> <span class="math inline">\(E\)</span>： <span class="math display">\[Ef(x)=f(x+1)\]</span> 那么： <span class="math display">\[\begin{align}&amp;\Delta\big(u(x)v(x)\big)=Ev(x)\Delta u(x)+u(x)\Delta v(x)\\\implies&amp;\sum u\Delta v=uv-\sum Ev\Delta u\end{align}\]</span></p><blockquote><p>练习：求 <span class="math inline">\(\sum\limits_{k=0}^nk2^k\)</span> 的封闭形式。</p><p>解： <span class="math display">\[\begin{align}\sum_{k=0}^nk2^k&amp;=\sum\nolimits_0^{n+1}k2^k\delta k=\sum\nolimits_0^{n+1}k\delta2^k=k2^k\Big|_0^{n+1}-\sum\nolimits_0^{n+1}2^{k+1}\delta k\\&amp;=(n+1)2^{n+1}-2^{n+2}+2=(n-1)2^{n+1}+2\end{align}\]</span></p></blockquote><h2 id="差分-求和表">差分-求和表</h2><p>学无限微积分的时候会背常用的导数和积分表，那对于有限微积分可以总结如下：</p><table><thead><tr class="header"><th style="text-align: center;"><span class="math inline">\(f=\sum g\)</span></th><th style="text-align: center;"><span class="math inline">\(\Delta f=g\)</span></th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><span class="math inline">\(x^{\underline m}\)</span></td><td style="text-align: center;"><span class="math inline">\(mx^{\underline{m-1}}\)</span></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(H_x\)</span></td><td style="text-align: center;"><span class="math inline">\(\frac{1}{x+1}\)</span></td></tr><tr class="odd"><td style="text-align: center;"><span class="math inline">\(c^x\)</span></td><td style="text-align: center;"><span class="math inline">\((c-1)c^x\)</span></td></tr><tr class="even"><td style="text-align: center;"><span class="math inline">\(uv\)</span></td><td style="text-align: center;"><span class="math inline">\(u\Delta v+Ev\Delta u\)</span></td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>具体数学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[具体数学]第二章·和式（第一部分）</title>
    <link href="/blog-main/2020/07/31/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E4%BA%8C%E7%AB%A0%C2%B7%E5%92%8C%E5%BC%8F%EF%BC%88%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%89/"/>
    <url>/blog-main/2020/07/31/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E4%BA%8C%E7%AB%A0%C2%B7%E5%92%8C%E5%BC%8F%EF%BC%88%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<blockquote><p>第二章·和式分7节，这是我的笔记第一部分，包括：和式与递归式的联系，和式和多重和式的处理方法，无限和式。</p></blockquote><h2 id="和式和递归式">和式和递归式</h2><h3 id="和式-to-递归式">和式 <span class="math inline">\(\to\)</span> 递归式</h3><p>和式 <span class="math display">\[S_n=\sum_{k=0}^na_k\]</span> 等价于递归式 <span class="math display">\[\begin{cases}S_0=0\\S_n=S_{n-1}+a_n\end{cases}\]</span> 于是我们可以用第一章中解递归式的方法（成套方法）解出和式。</p><h3 id="递归式-to-和式">递归式 <span class="math inline">\(\to\)</span> 和式</h3><blockquote><p>通过求出和式解递归式，这个实用多了。</p></blockquote><p>设有递归式： <span class="math display">\[a_nT_n=b_nT_{n-1}+c_n\]</span> 我们用一个<strong>求和因子</strong>（summation factor）<span class="math inline">\(s_n\)</span> 乘两边，其中 <span class="math inline">\(s_n\)</span> 满足：<span class="math inline">\(s_nb_n=s_{n-1}a_{n-1}\)</span>，得到： <span class="math display">\[s_na_nT_n=s_nb_nT_{n-1}+s_nc_n=s_{n-1}a_{n-1}T_{n-1}+s_nc_n\]</span> 解得： <span class="math display">\[s_na_nT_n=s_0a_0T_0+\sum_{i=1}^ns_ic_i=s_1b_1T_0+\sum_{i=1}^ns_ic_i\]</span> 故 <span class="math display">\[T_n=\frac{1}{s_na_n}\left(s_1b_1T_0+\sum_{i=1}^ns_ic_i\right)\]</span> 现在关键在于求 <span class="math inline">\(s_n\)</span>，由于 <span class="math inline">\(s_nb_n=s_{n-1}a_{n-1}\)</span>，所以我们不难知道： <span class="math display">\[s_n=\frac{a_{n-1}a_{n-2}\cdots a_1}{b_nb_{n-1}b_2}s_1\]</span> 其中 <span class="math inline">\(s_1\)</span> 任取一个常数（比如 <span class="math inline">\(1\)</span>）即可。</p><h3 id="例子">例子</h3><ul><li><p>解 <span class="math inline">\(\textbf{Hanoi}\)</span> 塔递归式： <span class="math display">\[\begin{cases}T_0=0\\T_n=2T_{n-1}+1\end{cases}\]</span> 这个我们高中就会做，两边 <span class="math inline">\(+1\)</span> 就变成了等比数列，或者两边除以 <span class="math inline">\(2^n\)</span> 就变成了等差数列。后者其实就是求和因子的做法（<span class="math inline">\(s_n=\frac{1}{2^n}\)</span>）。</p></li><li><p>解快速排序中出现的递归式：典型的快速排序算法所做的比较步骤的平均次数满足递归式： <span class="math display">\[\begin{cases}C_0=C_1=0\\C_n=n+1+\frac{2}{n}\sum\limits_{k=0}^{n-1}C_k\end{cases}\]</span> 这个 <span class="math inline">\(C_n\)</span> 依赖于前 <span class="math inline">\(n-1\)</span> 项，我们先简化一下：两边同乘 <span class="math inline">\(n\)</span>，列出 <span class="math inline">\(C_n\)</span> 和 <span class="math inline">\(C_{n-1}\)</span> 的递归式： <span class="math display">\[\begin{cases}nC_n=n^2+n+2\sum\limits_{k=0}^{n-1}C_k\\(n-1)C_{n-1}=(n-1)^2+n-1+2\sum\limits_{k=0}^{n-2}C_k\end{cases}\]</span> 两式相减，化简得到： <span class="math display">\[nC_n=(n+1)C_{n-1}+2n\]</span> 这个我们高中也会做，同除 <span class="math inline">\(\frac{1}{n(n+1)}\)</span> 就好。现在我们算求和因子：<span class="math inline">\(s_n=\frac{(n-1)\cdots 2\cdot1}{(n+1)n\cdots3}=\frac{2}{n(n+1)}\)</span> 也验证了这个做法。</p><p>最后解出来是： <span class="math display">\[C_n=2(n+1)\sum_{k=1}^n\frac{1}{k+1}-\frac{2}{3}(n+1)=2(n+1)H_n-\frac{8}{3}n-\frac{2}{3}\]</span></p></li></ul><h2 id="和式的处理">和式的处理</h2><p>我们可以对和式的下标进行灵活的变换： <span class="math display">\[\sum_{k\in K}a_k=\sum_{p(k)\in K}a_{p(k)}\]</span> 更普通的写法： <span class="math display">\[\sum_{j\in J}a_{f(j)}=\sum_{k\in K}a_k\#f^-(k)\]</span> 其中，#<span class="math inline">\(f^-(k)\)</span> 表示集合 <span class="math inline">\(f^-(k)=\{j\mid f(j)=k\}\)</span> 中元素的个数。</p><p><br></p><p><strong>扰动法</strong>（<span class="math inline">\(\text{perturbation method}\)</span>）：计算和式 <span class="math inline">\(S_n=\sum\limits_{k=0}^na_k\)</span> 可以这么做： <span class="math display">\[S_{n+1}=S_n+a_{n+1}=a_0+\sum_{k=0}^na_{k+1}\]</span> 左式是<strong>抽出最后一项</strong>，右式是<strong>抽出第一项</strong>，然后试着把 <span class="math inline">\(\sum\limits_{k=0}^na_{k+1}\)</span> 写成 <span class="math inline">\(S_n\)</span> 的函数，这样我们就有了一个关于 <span class="math inline">\(S_n\)</span> 的方程，解出来就可以了。</p><p>例如：求解 <span class="math inline">\(S_n=\sum\limits_{k=0}^nk2^k\)</span>，用扰动法： <span class="math display">\[S_n+(n+1)2^{n+1}=\sum\limits_{k=0}^n(k+1)2^{k+1}=2\sum\limits_{k=0}^nk2^k+\sum\limits_{k=0}^n2^{k+1}=2S_n+2^{n+2}-2\]</span> 解得：<span class="math inline">\(S_n=(n+1)2^{n+1}-2^{n+2}+2=(n-1)2^{n+1}+2\)</span>.</p><blockquote><p>当然，这玩意儿也可以求导做，变成无穷级数的话就是个很常规的高数题。</p></blockquote><h2 id="多重和式">多重和式</h2><p>主要就是求和号换序的问题。</p><h3 id="简易型">简易型</h3><p><span class="math inline">\(j,k\)</span> 的范围相互无关： <span class="math display">\[\sum_{j\in J}\sum_{k\in K}a_{j,k}=\sum_{k\in K}\sum_{j\in J}a_{j,k}\]</span> 特别地，有一般分配律： <span class="math display">\[\sum_{j\in J}\sum_{k\in K}a_jb_k=\left(\sum_{j\in J}a_j\right)\left(\sum_{k\in K}b_k\right)\]</span></p><h3 id="复杂型">复杂型</h3><p><span class="math inline">\(j,k\)</span> 的范围相关： <span class="math display">\[\sum_{j\in J}\sum_{k\in K(j)}a_{j,k}=\sum_{k\in K&#39;}\sum_{j\in J&#39;(k)}a_{j,k}\]</span> 换序时保证两式的范围相同即可，即保证：<span class="math inline">\([j\in J][k\in K(j)]=[k\in K&#39;][j\in J&#39;(k)]\)</span>.</p><blockquote><p>比如说， <span class="math display">\[\sum_{j=1}^n\sum_{k=j}^na_{j,k}=\sum_{1\leqslant j\leqslant k\leqslant n}a_{j,k}=\sum_{k=1}^n\sum_{j=1}^ka_{j,k}\]</span> 可以理解成 <span class="math inline">\(x\)</span> 型区域和 <span class="math inline">\(y\)</span> 型区域的两种表示。</p></blockquote><blockquote><p>再比如说，做莫比乌斯反演的题的时候经常要用： <span class="math display">\[\sum_{i=1}^n\sum_{d\mid i}f(d,i)=\sum_{d=1}^n\sum_{i=1}^n[d\mid i]f(d,i)=\sum_{d=1}^n\sum_{i=1}^{\left\lfloor\frac{n}{d}\right\rfloor}f(d,di)\]</span> 或者说： <span class="math display">\[\sum_{i=1}^n\sum_{j=1}^m\sum_{d\mid \gcd(i,j)}f(d,i,j)=\sum_{d=1}^{\min(n,m)}\sum_{i=1}^{\left\lfloor\frac{n}{d}\right\rfloor}\sum_{j=1}^{\left\lfloor\frac{m}{d}\right\rfloor}f(d,di,dj)\]</span> 也是如此。</p></blockquote><h3 id="一个有趣的例子">一个有趣的例子</h3><p>计算 <span class="math display">\[S_n=\sum_{1\leqslant j&lt;k\leqslant n}\frac{1}{k-j}\]</span> <strong>第一次尝试</strong>：（先对 <span class="math inline">\(j\)</span> 求和，再对 <span class="math inline">\(k\)</span> 求和） <span class="math display">\[\begin{align}S_n=\sum_{k=1}^n\sum_{j=1}^k\frac{1}{k-j}&amp;=\sum_{k=1}^n\sum_{j=1}^{k-1}\frac{1}{j}&amp;&amp;用\;k-j\;代替\;j\\&amp;=\sum_{k=1}^nH_{k-1}&amp;&amp;调和数的定义\\&amp;=\sum_{k=0}^{n-1}H_k\end{align}\]</span> 没法进行了。</p><p><strong>第二次尝试</strong>：（先对 <span class="math inline">\(k\)</span> 求和，再对 <span class="math inline">\(j\)</span> 求和） <span class="math display">\[\begin{align}S_n=\sum_{j=1}^n\sum_{k=j+1}^n\frac{1}{k-j}&amp;=\sum_{j=1}^n\sum_{k=1}^{n-j}\frac{1}{k}&amp;&amp;用\;k+j\;替换\;k\\&amp;=\sum_{j=1}^nH_{n-j}&amp;&amp;调和数的定义\\&amp;=\sum_{j=0}^{n-1}H_j&amp;&amp;用\;n-j\;替换\;j\end{align}\]</span> 和第一次尝试结果一样。</p><p><strong>第三次尝试</strong>：（先代换） <span class="math display">\[\begin{align}S_n=\sum_{1\leqslant j&lt;k\leqslant n}\frac{1}{k-j}&amp;=\sum_{1\leqslant j&lt;u+j\leqslant n}\frac{1}{u}&amp;&amp;令\;u=k-j\\&amp;=\sum_{j=1}^{n}\sum_{u=1}^{n-j}\frac{1}{u}\\&amp;=\sum_{u=1}^{n}\sum_{j=1}^{n-u}\frac{1}{u}&amp;&amp;求和号换序\\&amp;=\sum_{u=1}^{n}\left(\frac{n}{u}-1\right)\\&amp;=nH_n-n\end{align}\]</span> 搞定。</p><p>上述尝试还给了我们一个恒等式： <span class="math display">\[\sum_{k=0}^{n-1}H_k=nH_n-n\]</span></p><h2 id="一般性方法">一般性方法</h2><p>这一节算是一个小总结吧。</p><p>以求解 <span class="math display">\[\square_n=\sum_{k=0}^nk^2\]</span> 的封闭形式为例，探究至少 <span class="math inline">\(8\)</span> 种解决问题的方法。</p><h3 id="方法0查找公式">方法0：查找公式</h3><p>作为 OIer/ACMer，我们都会一个方法：算出前几项，然后愉快地 <span class="math inline">\(\text{OEIS}\)</span>. <a href="http://oeis.org/">链接</a></p><blockquote><p>练习：在毕导的<a href="https://www.bilibili.com/video/BV1RT4y1j7pP?from=search&amp;seid=4095880483690619223">视频</a>中，男厕尴尬定理的递归式长这样：<span class="math inline">\(\begin{cases}f(1)=f(2)=1\\f(3)=2\\f(2n+1)=2f(n+1)-1\\f(2n)=f(n)+f(n+1)-1\end{cases}\)</span>，我们可以打表找规律求解（毕导也是这么做的），这个规律还是比较好找的。如果我们试着 <span class="math inline">\(\text{OEIS}\)</span>，输入 <span class="math inline">\(1,1,2,2,3,3,3,4,5,5,5,5,5\)</span>，就会发现，早有人给出了这个问题以及解，只不过他没有以厕所而是电话亭举例。</p></blockquote><h3 id="方法1猜测答案归纳证明">方法1：猜测答案+归纳证明</h3><p>假设我们现在神奇地知道了 <span class="math inline">\(\square_n=\frac{n\left(n+\frac{1}{2}\right)(n+1)}{3}\)</span>，于是归纳证明：</p><ul><li>对于 <span class="math inline">\(n=0\)</span>，有 <span class="math inline">\(\square_0=0=\frac{0\left(0+\frac{1}{2}\right)(0+1)}{3}\)</span>，成立；</li><li>假设 <span class="math inline">\(n-1\)</span> 成立，则 <span class="math inline">\(3\square_n=3\square_{n-1}+3n^2=(n-1)\left(n-\frac{1}{2}\right)n+3n^2=n\left(n+\frac{1}{2}\right)(n+1)\)</span>，成立。证毕。</li></ul><h3 id="方法2对和式用扰动法">方法2：对和式用扰动法</h3><p><strong>抽出最后一项和第一项</strong>： <span class="math display">\[\begin{align}\square_{n+1}&amp;=\square_{n}+(n+1)^2\\&amp;=\sum_{k=1}^{n+1}k^2=\sum_{k=0}^n(k+1)^2=\sum_{k=0}^nk^2+\sum_{k=0}^n2k+\sum_{k=0}^n1\\&amp;=\square_n+2\sum_{k=0}^nk+n+1\end{align}\]</span> 完蛋，<span class="math inline">\(\square_n\)</span> 被约掉了！但是！注意它给出了 <span class="math inline">\(\sum\limits_{k=0}^nk\)</span> 的封闭形式，所以不妨“升维打击”一波（设 <span class="math inline">\(\triangle_n=\sum\limits_{k=0}^nk^3\)</span>）： <span class="math display">\[\begin{align}\triangle_{n+1}&amp;=\triangle_n+(n+1)^3\\&amp;=\sum_{k=1}^{n+1}k^3=\sum_{k=0}^n(k+1)^3=\sum_{k=0}^n(k^3+3k^2+3k+1)\\&amp;=\triangle_n+3\square_n+3\cdot\frac{n(n+1)}{2}+n+1\end{align}\]</span> 耶！解得：<span class="math inline">\(\square_n=\frac{n\left(n+\frac{1}{2}\right)(n+1)}{3}\)</span>.</p><h3 id="方法3建立成套方法">方法3：建立成套方法</h3><p>设有递归式： <span class="math display">\[\begin{cases}R_0=\alpha\\R_n=R_{n-1}+\beta+\gamma n+\delta n^2\end{cases}\]</span> 它有解：<span class="math inline">\(R_n=A(n)\alpha+B(n)\beta+C(n)\gamma+D(n)\delta\)</span>.</p><p>当 <span class="math inline">\(\delta=0\)</span> 时，问题转化成我们已经解决过的问题，所以 <span class="math inline">\(A(n),B(n),C(n)\)</span> 已经搞定了；</p><p>然后设 <span class="math inline">\(R_n=n^3\)</span>，解得：<span class="math inline">\((\alpha,\beta,\gamma,\delta)=(0,1,-3,3)\)</span>，得到：<span class="math inline">\(B(n)-3C(n)+3D(n)=n^3\)</span>，于是 <span class="math inline">\(D(n)\)</span> 也搞定了。</p><p>由于 <span class="math inline">\(\square_n=\square_{n-1}+n^2\)</span>，我们只需要设 <span class="math inline">\((\alpha,\beta,\gamma,\delta)=(0,0,0,1)\)</span>，那么 <span class="math inline">\(R_n=D(n)=\square_n\)</span>.</p><blockquote><p>练习：求 <span class="math inline">\(\sum\limits_{k=0}^n(-1)^kk^2\)</span> 的封闭形式。</p><p>解：有递归式：<span class="math inline">\(\begin{cases}R_0=\alpha\\R_n=R_{n-1}+(-1)^n(\beta+\gamma n+\delta n^2)\end{cases}\)</span>，设解为 <span class="math inline">\(R_n=A(n)\alpha+B(n)\beta+C(n)\gamma+D(n)\delta\)</span>，需要找到四个特解：</p><ul><li>令 <span class="math inline">\(R_n=1\)</span>，解得：<span class="math inline">\((\alpha,\beta,\gamma,\delta)=(1,0,0,0)\)</span>，得到：<span class="math inline">\(A(n)=1\)</span>；</li><li>令 <span class="math inline">\(R_n=(-1)^n\)</span>，解得：<span class="math inline">\((\alpha,\beta,\gamma,\delta)=(1,2,0,0)\)</span>，得到：<span class="math inline">\(A(n)+B(n)=(-1)^n\)</span>；</li><li>令 <span class="math inline">\(R_n=(-1)^nn\)</span>，解得：<span class="math inline">\((\alpha,\beta,\gamma,\delta)=(0,-1,2,0)\)</span>，得到：<span class="math inline">\(-B(n)+2C(n)=(-1)^nn\)</span>；</li><li>令 <span class="math inline">\(R_n=(-1)^nn^2\)</span>，解得：<span class="math inline">\((\alpha,\beta,\gamma,\delta)=(0,1,-2,2)\)</span>，得到：<span class="math inline">\(B(n)-2C(n)+2D(n)=(-1)^nn^2\)</span>.</li></ul><p>我们只需要设 <span class="math inline">\((\alpha,\beta,\gamma,\delta)=(0,0,0,1)\)</span>，就知道了：<span class="math inline">\(原式=D(n)=\frac{(-1)^n}{2}(n^2+n)\)</span>.</p></blockquote><h3 id="方法4积分代替和式">方法4：积分代替和式</h3><p>由于 <span class="math inline">\(\int_0^nx^2\mathrm dx=\frac{n^3}{3}\)</span>，所以我们有近似：<span class="math inline">\(\square_n\sim\frac{n^3}{3}\)</span>.</p><p>然后我们来对误差进行<strong>精确</strong>分析：设误差 <span class="math inline">\(E_n=\square_n-\frac{n^3}{3}\)</span>，由于 <span class="math inline">\(\square_n=\square_{n-1}+n^2\)</span>，所以我们有： <span class="math display">\[E_n=E_{n-1}+n-\frac{1}{3}\]</span> 这个递归式很好解，解出来就行了。</p><h3 id="方法5展开和收缩">方法5：展开和收缩</h3><p>大致思路：一重和式 <span class="math inline">\(\to\)</span> 二重和式 <span class="math inline">\(\to\)</span> 封闭形式。 <span class="math display">\[\begin{align}\square_n&amp;=\sum_{k=1}^nk^2=1+(2+2)+(3+3+3)+\cdots+(n+n+\cdots+n)=\sum_{k=1}^n\sum_{j=1}^kk\\&amp;=\sum_{j=1}^n\sum_{k=j}^nk=\sum_{j=1}^n\frac{(j+n)(n-j+1)}{2}\\&amp;=\frac{1}{2}\sum_{j=1}^n\left(n(n+1)+j-j^2\right)\\&amp;=\frac{1}{2}n^2(n+1)+\frac{1}{4}n(n+1)-\frac{1}{2}\square_n\end{align}\]</span> 解出来即可。</p><blockquote><p>你以为它变麻烦了，实际上它变简单了……</p><p>练习：求 <span class="math inline">\(\triangle_n=\sum\limits_{k=1}^nk^3\)</span> 的封闭形式。</p><p>解： <span class="math display">\[\begin{align}\triangle_n&amp;=\sum_{k=1}^nk^3=\sum_{k=1}^n\sum_{j=1}^kk^2=\sum_{j=1}^n\sum_{k=j}^nk^2=\sum_{j=1}^n(\square_n-\square_{j-1})\\&amp;=\sum_{j=1}^n\left(\frac{n(n+1)(2n+1)}{6}-\frac{(j-1)j(2j-1)}{6}\right)\\&amp;=\frac{n^2(n+1)(2n+1)}{6}-\frac{1}{6}\sum_{j=1}^n(2j^3-3j^2+j)\\&amp;=\frac{n^2(n+1)(2n+1)}{6}-\frac{1}{3}\triangle_n+\frac{n(n+1)(2n+1)}{12}-\frac{n(n+1)}{12}\end{align}\]</span> 解得：<span class="math inline">\(\triangle_n=\frac{n^2(n+1)^2}{4}\)</span>.</p></blockquote><h3 id="方法6有限微积分">方法6：有限微积分</h3><p>见第二部分</p><h3 id="方法7生成函数">方法7：生成函数</h3><p>见之后的章节</p><h2 id="无限和式">无限和式</h2><p>其实就是无穷级数，高数都学过，略去。</p>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>具体数学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>[具体数学]第一章·递归问题</title>
    <link href="/blog-main/2020/07/29/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E4%B8%80%E7%AB%A0%C2%B7%E9%80%92%E5%BD%92%E9%97%AE%E9%A2%98/"/>
    <url>/blog-main/2020/07/29/%E5%85%B7%E4%BD%93%E6%95%B0%E5%AD%A6-%E7%AC%AC%E4%B8%80%E7%AB%A0%C2%B7%E9%80%92%E5%BD%92%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<blockquote><p>第一章·递归问题分 3 节，主要包括 <span class="math inline">\(\textbf{Hanoi}\)</span> 塔问题、平面上直线分割问题、约瑟夫问题。其中，个人认为约瑟夫问题的讨论相当具有启发性。</p></blockquote><h2 id="hanoi-塔问题">Hanoi 塔问题</h2><h3 id="notes">Notes</h3><p>这是个超级经典的递归问题了，不多做解释。设 <span class="math inline">\(T_n\)</span> 是 <span class="math inline">\(n\)</span> 个圆盘的移动次数，那么有递归式： <span class="math display">\[\begin{cases}T_0=0\\T_n=2T_{n-1}+1\end{cases}\]</span> 两边同时加一转化成等比数列，或者用数学归纳法，最后解得： <span class="math display">\[T_n=2^n-1\]</span></p><h3 id="习题">习题</h3><p>书上的习题有一些 <span class="math inline">\(\textbf{Hanoi}\)</span> 塔问题的变式，在此挑一些记录一下：</p><ul><li><p>不允许在最左边的 <span class="math inline">\(A\)</span> 柱和最右边的 <span class="math inline">\(B\)</span> 柱之间直接移动： <span class="math display">\[T_n=T_{n-1}+1+T_{n-1}+1+T_{n-1}=3T_{n-1}+2\implies T_n=3^n-1\]</span> 证明：这种移动规则下，<span class="math inline">\(3\)</span> 根柱子上都会出现 <span class="math inline">\(n\)</span> 个圆盘的每一种正确叠放。</p><p>证：宏观来看，一共就 <span class="math inline">\(3^n\)</span> 种可能，而 <span class="math inline">\(T_n=3^n-1\)</span>，一定遍历了所有可能的叠放方式。<span class="math inline">\(\square\)</span></p><blockquote><p>批注：这问题问得很玄学，转换角度思考就变简单了。</p></blockquote></li><li><p>是否存在某种初始情形和结束情形，使得需要多于 <span class="math inline">\(2^n-1\)</span> 的移动？</p><p>不存在，<strong>归纳</strong>：如果最大盘不动，最多 <span class="math inline">\(2^{n-1}-1\)</span> 次；否则，最多 <span class="math inline">\((2^{n-1}-1)+1+(2^{n-1}-1)=2^n-1\)</span> 次。<span class="math inline">\(\square\)</span></p><blockquote><p>批注：不知道咋整就归纳，哈哈</p></blockquote></li><li><p>要求移动必须是顺时针方向（只能 <span class="math inline">\(A\to B,B\to C,C\to A\)</span>），设 <span class="math inline">\(Q_n\)</span> 是 <span class="math inline">\(A\to B\)</span> 的最少移动次数，<span class="math inline">\(R_n\)</span> 是 <span class="math inline">\(B\to A\)</span> 的最少移动次数，求证： <span class="math display">\[Q_n=\begin{cases}0&amp;,n=0\\2R_{n-1}+1&amp;,n&gt;0\end{cases}\quad\quad R_n=\begin{cases}0&amp;,n=0\\Q_n+Q_{n-1}+1&amp;,n&gt;0\end{cases}\]</span> 证：意思就是 <span class="math inline">\(Q_n\)</span> 是跨一步，<span class="math inline">\(R_n\)</span> 是跨两步。</p><p>跨一步这样完成：<span class="math inline">\(n-1\)</span> 个盘 <span class="math inline">\(A\to C\)</span>，<span class="math inline">\(1\)</span> 个盘 <span class="math inline">\(A\to B\)</span>，<span class="math inline">\(n-1\)</span> 个盘 <span class="math inline">\(C\to B\)</span>，故 <span class="math inline">\(Q_n=R_{n-1}+1+R_{n-1}=2R_{n-1}+1\)</span>；</p><p>跨两步这样完成：<span class="math inline">\(n-1\)</span> 个盘 <span class="math inline">\(A\to C\)</span>，<span class="math inline">\(1\)</span> 个盘 <span class="math inline">\(A\to B\)</span>，<span class="math inline">\(n-1\)</span> 个盘 <span class="math inline">\(C\to A\)</span>，<span class="math inline">\(1\)</span> 个盘 <span class="math inline">\(B\to C\)</span>，<span class="math inline">\(n-1\)</span> 个盘 <span class="math inline">\(A\to C\)</span>，故 <span class="math inline">\(R_n=R_{n-1}+1+Q_{n-1}+1+R_{n-1}=(2R_{n-1}+1)+Q_{n-1}+1=Q_n+Q_{n-1}+1\)</span>. <span class="math inline">\(\square\)</span></p><blockquote><p>注：遗留问题：为什么跨两步不能这样：<span class="math inline">\(n-1\)</span> 个盘 <span class="math inline">\(A\to C\)</span>，<span class="math inline">\(1\)</span> 个盘 <span class="math inline">\(A\to B\)</span>，<span class="math inline">\(n-1\)</span> 个盘 <span class="math inline">\(C\to B\)</span>，<span class="math inline">\(n\)</span> 个盘 <span class="math inline">\(B\to C\)</span>。这样是 <span class="math inline">\(R_n=R_{n-1}+1+R_{n-1}+Q_n=2Q_n\)</span>. 可以验证答案更差，但是怎样才能避免这样想？</p></blockquote></li><li><p><span class="math inline">\(n\)</span> 种尺寸，第 <span class="math inline">\(k\)</span> 种有 <span class="math inline">\(m_k\)</span> 个圆盘，求最少移动次数 <span class="math inline">\(A(m_1,\cdots,m_k)\)</span>. <span class="math display">\[A(m_1,\cdots,m_n)=2A(m_1,\cdots,m_{n-1})+m_n=4A(m_1,\cdots,m_{n-2})+2m_{n-1}+m_n=\cdots=\sum\limits_{i=0}^{n-1}2^im_{n-i}\]</span></p></li></ul><h2 id="平面上的直线">平面上的直线</h2><h3 id="notes-1">Notes</h3><p>也是一个经典问题了，一条新的直线和原来的 <span class="math inline">\(n-1\)</span> 条直线最多有 <span class="math inline">\(n-1\)</span> 个交点，就最多增加 <span class="math inline">\(n\)</span> 个面。</p><blockquote><p>为什么？注意这条直线被划分成了 <span class="math inline">\(n\)</span> 段，每一段两侧是两个现在不同、但是之前相同的面，所以净增加了 <span class="math inline">\(n\)</span> 个面。</p></blockquote><p>由此，我们有递归式： <span class="math display">\[\begin{cases}L_0=1\\L_n=L_{n-1}+n\end{cases}\]</span> 累和，或者数学归纳法，解得： <span class="math display">\[L_n=\frac{n(n+1)}{2}+1\]</span></p><h3 id="变式">变式</h3><ul><li><p>把直线改成折线：</p><p><img src="http://acm.hdu.edu.cn/data/images/C40-1008-1.jpg" alt="图片来源：http://acm.hdu.edu.cn/showproblem.php?pid=2050" style="zoom:67%;" /></p><p>注意到，一条折线相当于两条直线相交后抹掉一半，抹掉一半后平面数减 <span class="math inline">\(2\)</span>，所以 <span class="math inline">\(Z_n=L_{2n}-2n\)</span>.</p><blockquote><p>可能还是不好理解（至少我理解了很久），这么考虑：新加一条折线进来，从它的顶点向被抹掉的那个方向看去，抹掉的直线本应该把这个区域分成 <span class="math inline">\(3\)</span> 部分，但现在它是 <span class="math inline">\(1\)</span> 部分，所以少了 <span class="math inline">\(2\)</span>。</p><p>基于此我们也可以得到递归式：加入新折线的一条边，有 <span class="math inline">\(2n-2\)</span> 个新交点，增加 <span class="math inline">\(2n-1\)</span> 个面，再加入另一条边，有 <span class="math inline">\(2n-1\)</span> 个新交点（包括新折线的顶点），增加 <span class="math inline">\(2n\)</span> 个面，但是顶点那个交点不争气要减 <span class="math inline">\(2\)</span>，所以总共新增 <span class="math inline">\(4n-3\)</span> 个面，即 <span class="math inline">\(Z_n=Z_{n-1}+4n-3\)</span>.</p></blockquote></li><li><p><span class="math inline">\(\text{Z}\)</span> 字型（两平行半直线加一条直线段）划分平面：沿用上面的思路可得递归式：<span class="math inline">\(Z_n=Z_{n-1}+(3n-3+1)+(3n-3+1)+(3n-1+1)-2-2=Z_{n-1}+9n-8\)</span>，解得：<span class="math inline">\(Z_n=\frac{9}{2}n^2-\frac{7}{2}n+1\)</span>.</p></li><li><p>厚奶酪上划直切痕，得到的三维区域最大个数：新增一个面，前 <span class="math inline">\(n-1\)</span> 个面在这个新平面上最多得到 <span class="math inline">\(L_{n-1}\)</span> 个区域，考虑每个区域两侧可以知道新增了 <span class="math inline">\(L_{n-1}\)</span> 个块，所以 <span class="math inline">\(P_n=P_{n-1}+L_{n-1}\)</span>.</p><blockquote><p>这波降维打击实在有趣！</p></blockquote></li></ul><h2 id="约瑟夫问题">约瑟夫问题</h2><p>重点来了！</p><h3 id="基础问题">基础问题</h3><p><span class="math inline">\(n\)</span> 个人围圈，隔一个死一个，问最后剩下的人的编号 <span class="math inline">\(J(n)\)</span>.</p><ul><li>偶数情况：<span class="math inline">\(2n\)</span> 个人，死了一圈之后，剩下 <span class="math inline">\(n\)</span> 个人编号是：<span class="math inline">\(1,3,5,\cdots,2n-1\)</span>，这就是个把原编号 <span class="math inline">\(k\)</span> 改成 <span class="math inline">\(2k-1\)</span> 之后的 <span class="math inline">\(n\)</span> 人约瑟夫问题，所以 <span class="math inline">\(J(2n)=2J(n)-1\)</span>；</li><li>奇数情况：<span class="math inline">\(2n+1\)</span> 个人，死了一圈之后，剩下 <span class="math inline">\(n\)</span> 个人编号是：<span class="math inline">\(3,5,7,\cdots,2n+1\)</span>，这就是个把原编号 <span class="math inline">\(k\)</span> 改成 <span class="math inline">\(2k+1\)</span> 之后的 <span class="math inline">\(n\)</span> 人约瑟夫问题，所以 <span class="math inline">\(J(2n+1)=2J(n)+1\)</span>.</li></ul><p>综上，再加上初始条件，有递归式： <span class="math display">\[\begin{cases}J(1)=1\\J(2n)=2J(n)-1\\J(2n+1)=2J(n)+1\end{cases}\]</span> 至于怎么解这个递归式……我们可以<strong>打表</strong>（OIer &amp; ACMer 狂喜）：</p><table><thead><tr class="header"><th style="text-align: center;">n</th><th style="text-align: center;">1</th><th style="text-align: center;">2</th><th style="text-align: center;">3</th><th style="text-align: center;">4</th><th style="text-align: center;">5</th><th style="text-align: center;">6</th><th style="text-align: center;">7</th><th style="text-align: center;">8</th><th style="text-align: center;">9</th><th style="text-align: center;">10</th><th style="text-align: center;">11</th><th style="text-align: center;">12</th><th style="text-align: center;">13</th><th style="text-align: center;">14</th><th style="text-align: center;">15</th><th style="text-align: center;">16</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">J(n)​</td><td style="text-align: center;">1</td><td style="text-align: center;">1</td><td style="text-align: center;">3</td><td style="text-align: center;">1</td><td style="text-align: center;">3</td><td style="text-align: center;">5</td><td style="text-align: center;">7</td><td style="text-align: center;">1</td><td style="text-align: center;">3</td><td style="text-align: center;">5</td><td style="text-align: center;">7</td><td style="text-align: center;">9</td><td style="text-align: center;">11</td><td style="text-align: center;">13</td><td style="text-align: center;">15</td><td style="text-align: center;">1</td></tr></tbody></table><p>观察得到： <span class="math display">\[J(n)=2\left(n-2^{\lfloor\lg n\rfloor}\right)+1\]</span> 或者按书上的写法，是 <span class="math inline">\(J(2^m+l)=2l+1,\quad m\geqslant0,\quad 0\leqslant l&lt;2^m\)</span>. 证明数学归纳法即可。</p><h3 id="二进制视角">二进制视角</h3><p>这一部分是我觉得最 Amazing 的地方了。</p><p>把上表写成二进制：</p><table><thead><tr class="header"><th style="text-align: center;">n​</th><th style="text-align: center;">1</th><th style="text-align: center;">10</th><th style="text-align: center;">11</th><th style="text-align: center;">100</th><th style="text-align: center;">101</th><th style="text-align: center;">110</th><th style="text-align: center;">111</th><th style="text-align: center;">1000</th><th style="text-align: center;">1001</th><th style="text-align: center;">1010</th><th style="text-align: center;">1011</th><th style="text-align: center;">1100</th><th style="text-align: center;">1101</th><th style="text-align: center;">1110</th><th style="text-align: center;">1111</th><th style="text-align: center;">10000</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">J(n)​</td><td style="text-align: center;">1</td><td style="text-align: center;">1</td><td style="text-align: center;">11</td><td style="text-align: center;">001</td><td style="text-align: center;">011</td><td style="text-align: center;">101</td><td style="text-align: center;">111</td><td style="text-align: center;">0001</td><td style="text-align: center;">0011</td><td style="text-align: center;">0101</td><td style="text-align: center;">0111</td><td style="text-align: center;">1001</td><td style="text-align: center;">1011</td><td style="text-align: center;">1101</td><td style="text-align: center;">1111</td><td style="text-align: center;">00001</td></tr></tbody></table><p>发现一个神奇的现象：把 <span class="math inline">\(n\)</span> 的二进制<strong>向左循环滚动一位</strong>，就得到了 <span class="math inline">\(J(n)\)</span>！</p><blockquote><p>其实看看通项公式，<span class="math inline">\(n-2^{\lfloor\lg n\rfloor}\)</span> 就是删掉最高位的剩下的部分，乘 <span class="math inline">\(2\)</span> 就是左移一位，加一就是把最高位加到最低位去——所以一通操作下来，就是向左循环一位。</p></blockquote><p>如果我们进行套娃式迭代——不停地向左循环，但是注意前导 <span class="math inline">\(0\)</span> 会消失掉，所以足够多次迭代下去后得到的数就是由所有 <span class="math inline">\(1\)</span> 组成的二进制数，即 <span class="math inline">\(J(J(J(\cdots J(n))))=2^{\nu(n)}-1\)</span>，其中 <span class="math inline">\(\nu(n)\)</span> 是 <span class="math inline">\(n\)</span> 在二进制下 <span class="math inline">\(1\)</span> 的个数。</p><h3 id="成套方法求解递归式-repertoire-method">成套方法求解递归式 Repertoire Method</h3><p>我们把 <span class="math inline">\(J(n)\)</span> 的递归式给推广一下： <span class="math display">\[\begin{cases}f(1)=\alpha\\f(2n)=2f(n)+\beta\\f(2n+1)=2f(n)+\gamma\end{cases}\]</span> （<span class="math inline">\(J(n)\)</span> 是 <span class="math inline">\(\alpha=1,\beta=-1,\gamma=1\)</span> 的特殊情况）。我们仍然可以打表找规律，但是可以用<strong>成套方法</strong>（这名字好奇怪）解它。</p><p>注意到决定上式结果的是 <span class="math inline">\(3\)</span> 个参数 <span class="math inline">\(\alpha,\beta,\gamma\)</span>，所以我们可以设 <span class="math inline">\(f(n)=\alpha A(n)+\beta B(n)+\gamma C(n)\)</span> 是它的解（充分利用已知条件嘛）。下面我们找 <span class="math inline">\(3\)</span> 个特解：</p><ul><li>令 <span class="math inline">\(f(n)=1\)</span>，解得：<span class="math inline">\(\alpha=1,\beta=-1,\gamma=-1\)</span>，得到：<span class="math inline">\(A(n)-B(n)-C(n)=1\)</span>；</li><li>令 <span class="math inline">\(f(n)=n\)</span>，解得：<span class="math inline">\(\alpha=1,\beta=0,\gamma=1\)</span>，得到：<span class="math inline">\(A(n)+C(n)=n\)</span>；</li><li>令 <span class="math inline">\(\alpha=1,\beta=0,\gamma=0\)</span>，有 <span class="math inline">\(f(n)=A(n)\)</span>，得到：<span class="math inline">\(\begin{cases}A(1)=1\\A(2n)=2A(n)\\A(2n+1)=2A(n)\end{cases}\)</span>，解得：<span class="math inline">\(A(n)=2^{\lfloor\lg n\rfloor}\)</span>.</li></ul><p>联立上述 <span class="math inline">\(3\)</span> 式，解得：<span class="math inline">\(C(n)=n-2^{\lfloor\lg n\rfloor},\;B(n)=2^{\lfloor\lg n\rfloor+1}-n-1\)</span>. 所以我们得到了递归式的解： <span class="math display">\[f(n)=2^{\lfloor\lg n\rfloor}\alpha+\left(2^{\lfloor\lg n\rfloor+1}-n-1\right)\beta+\left(n-2^{\lfloor\lg n\rfloor}\right)\gamma\]</span> 或者用书上的写法（ <span class="math inline">\(n=2^m+l\)</span> ），就是：<span class="math inline">\(f(n)=2^m\alpha+\left(2^m-1-l\right)\beta+l\gamma\)</span>.</p><blockquote><p>有木有感觉很玄学！这三个特解咋冒出来的～特别是第三个特解，好生奇怪！</p><p>如果我们把函数 <span class="math inline">\(f(n)\)</span> 看作一个向量，那它就是 <span class="math inline">\(A(n),B(n),C(n)\)</span> 的线性组合，我们找到了三个特解，得到线性方程组： <span class="math display">\[\begin{bmatrix}1&amp;-1&amp;-1\\1&amp;0&amp;1\\1&amp;0&amp;0\end{bmatrix}\begin{bmatrix}A(n)\\B(n)\\C(n)\end{bmatrix}=\begin{bmatrix}1\\n\\2^m\end{bmatrix}\]</span> 理论上，给定三个线性无关的向量 <span class="math inline">\([\alpha,\beta,\gamma]\)</span> 和分别对应的的 <span class="math inline">\(f(n)\)</span>，那么就可以解出 <span class="math inline">\([A(n),B(n),C(n)]\)</span>。但是问题的关键就在于，任给一个 <span class="math inline">\([\alpha,\beta,\gamma]\)</span>，解 <span class="math inline">\(f(n)\)</span> 不一定简单。反过来，给定 <span class="math inline">\(f(n)\)</span> 解 <span class="math inline">\([\alpha,\beta,\gamma]\)</span> 倒是简单，但是给定的 <span class="math inline">\(f(n)\)</span> 可能不合法，即根本就不是原递归式的解。所以我感觉这个“成套方法”有很多凑的成分……</p></blockquote><h3 id="推广递归式的进制视角">推广递归式的进制视角</h3><p>对于推广的递归式，改写为： <span class="math display">\[\begin{cases}f(1)=\alpha\\f(2n+j)=2f(n)+\beta_j&amp;j=0,1,\;n\geqslant1\end{cases}\]</span> 我们用二进制展开： <span class="math display">\[\begin{align}f\Big({(b_mb_{m-1}\cdots b_1b_0)}_2\Big)&amp;=2f\Big({(b_mb_{m-1}\cdots b_1)}_2\Big)+\beta_{b_0}\\&amp;=4f\Big({(b_mb_{m-1}\cdots b_2)}_2\Big)+2\beta_{b_1}+\beta_{b_0}\\&amp;=\cdots\\&amp;=2^m\cdot1+2^{m-1}\beta_{b_{m-1}}+\cdots+2\beta_{b_1}+\beta_{b_0}\\&amp;={(\alpha\beta_{b_{m-1}}\cdots\beta_{b_1}\beta_{b_0})}_2\end{align}\]</span> 当然，这里不要求二进制的每一位必须是 <span class="math inline">\(0,1\)</span>，而是任意数字都行。</p><blockquote><p>怎么理解？把 <span class="math inline">\(n\)</span> 写作二进制，其中 <span class="math inline">\(0\)</span> 改成 <span class="math inline">\(\beta_0\)</span>，<span class="math inline">\(1\)</span> 改成 <span class="math inline">\(\beta_1\)</span>，最高位改成 <span class="math inline">\(\alpha\)</span>，就得到了 <span class="math inline">\(f(n)\)</span>.</p></blockquote><p><br></p><p>我们还可以推广，如果递归式长这样： <span class="math display">\[\begin{cases}f(j)=\alpha_j&amp;1\leqslant j&lt;d\\f(dn+j)=cf(n)+\beta_j&amp;0\leqslant j&lt;d,\;n\geqslant1\end{cases}\]</span> 那么用 <span class="math inline">\(d\)</span> 进制展开： <span class="math display">\[\begin{align}f\Big({(b_mb_{m-1}\cdots b_1b_0)}_d\Big)&amp;=cf\Big({(b_mb_{m-1}\cdots b_1)}_d\Big)+\beta_{b_0}\\&amp;=c^2f\Big({(b_mb_{m-1}\cdots b_2)}_d\Big)+c\beta_{b_1}+\beta_{b_0}\\&amp;=\cdots\\&amp;=c^m\cdot\alpha_{b_m}+c^{m-1}\beta_{b_{m-1}}+\cdots+c\beta_{b_1}+\beta_{b_0}\\&amp;={(\alpha_{b_m}\beta_{b_{m-1}}\cdots\beta_{b_1}\beta_{b_0})}_c\end{align}\]</span> 发现得到一个 <span class="math inline">\(c\)</span> 进制结果！实在是妙呀！</p><h3 id="习题-1">习题</h3><ul><li><p>用成套方法解四参数递归式： <span class="math display">\[\begin{cases}g(1)=\alpha\\g(2n+j)=3g(n)+\gamma n+\beta_j&amp;j=0,1,\; n\geqslant 1\end{cases}\]</span> 解：设 <span class="math inline">\(g(n)=\alpha A(n)+\gamma B(n)+\beta_0C(n)+\beta_1D(n)\)</span> 是它的解。这是四参数递归式，我们需要找到四个特解。</p><ul><li>令 <span class="math inline">\(\gamma=0\)</span>，问题转化成了之前解决的问题，有解：<span class="math inline">\(g\Big({(b_mb_{m-1}\cdots b_1b_0)}_2\Big)={(\alpha\beta_{b_{m-1}}\cdots\beta_{b_1}\beta_{b_0})}_3\)</span>. 随便取 <span class="math inline">\(3\)</span> 个线性无关的 <span class="math inline">\([\alpha,\beta_0,\beta_1]\)</span>，就能确定下 <span class="math inline">\(A(n),C(n),D(n)\)</span>；</li><li>令 <span class="math inline">\(g(n)=n\)</span>，解得：<span class="math inline">\((\alpha,\gamma,\beta_0,\beta_1)=(1,-1,0,1)\)</span>，得到：<span class="math inline">\(A(n)-B(n)+D(n)=n\)</span>.</li></ul><p>于是我们解决了这个问题（虽然没有写出显式的表达式）。</p></li><li><p>用成套方法解五参数递归式： <span class="math display">\[\begin{cases}h(1)=\alpha\\h(2n+j)=4h(n)+\gamma_j n+\beta_j&amp;j=0,1,\; n\geqslant 1\end{cases}\]</span> 解：设 <span class="math inline">\(h(n)=\alpha A(n)+\gamma_0 B(n)+\gamma_1C(n)+\beta_0D(n)+\beta_1E(n)\)</span> 是它的解。这是五参数递归式，我们需要找到五个特解。</p><ul><li>令 <span class="math inline">\(\gamma_0=\gamma_1=0\)</span>，问题转化成了之前解决的问题，可以确定 <span class="math inline">\(A(n),D(n),E(n)\)</span>；</li><li>令 <span class="math inline">\(h(n)=n\)</span>，解得：<span class="math inline">\((\alpha,\gamma_0,\gamma_1,\beta_0,\beta_1)=(1,-2,-2,0,1)\)</span>，得到：<span class="math inline">\(A(n)-2B(n)-2C(n)+E(n)=n\)</span>；</li><li>令 <span class="math inline">\(h(n)=n^2\)</span>（注意观察，平方项正好抵消，所以这么设又解），解得：<span class="math inline">\((\alpha,\gamma_0,\gamma_1,\beta_0,\beta_1)=(1,0,4,0,1)\)</span>，得到：<span class="math inline">\(A(n)+4C(n)+E(n)=n^2\)</span>.</li></ul><p>联立，问题解决。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>课程书籍笔记</category>
      
      <category>具体数学</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>天会晴 心会暖 阳光在手指间</title>
    <link href="/blog-main/2020/03/15/%E5%A4%A9%E4%BC%9A%E6%99%B4-%E5%BF%83%E4%BC%9A%E6%9A%96-%E9%98%B3%E5%85%89%E5%9C%A8%E6%89%8B%E6%8C%87%E9%97%B4/"/>
    <url>/blog-main/2020/03/15/%E5%A4%A9%E4%BC%9A%E6%99%B4-%E5%BF%83%E4%BC%9A%E6%9A%96-%E9%98%B3%E5%85%89%E5%9C%A8%E6%89%8B%E6%8C%87%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<p>足球在坑洼的草地上跳动</p><p>羽毛球划出一道过高的抛物线</p><p>小女孩的风筝摇晃着跌落</p><span id="more"></span><p>小男孩的自行车缺少了脚踏板</p><p>飞碟留下一道蓝色的光影极速下坠</p><p>野炊弥漫出一片白色的雾气看不真切</p><p>秋千在路边晃荡</p><p>车流在一侧熙攘</p><p>阳光从树顶泻下</p><p>池塘在一侧静默</p><p>足球踢正了方向</p><p>羽毛球打上了一个来回</p><p>父亲把风筝放起再交给女孩</p><p>男孩用脚蹬地骑车不亦乐乎</p><p>飞碟又一次被拾起</p><p>白雾后飘来若有若无的味道</p><hr /><p>在这个疫情即将结束的午后，我难得在公园呆了几个小时，突然发现，宅得太久，人会变得像钢筋水泥般无趣。还有，那些小时候体会过的快乐，是永远不会过时的快乐。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Python3 入门笔记</title>
    <link href="/blog-main/2020/02/16/Python3-%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/"/>
    <url>/blog-main/2020/02/16/Python3-%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p><del>为了做大一立项</del>为了提升自身姿势水平需要在寒假先把 Python 学了...</p><p>由于会 C/C++，笔记采用对比方法辅助记忆。</p><span id="more"></span><p>操作系统：macOS 10.15</p><p>参考书籍：《Python编程：从入门到实践》，笔记目录也将参考该书目录</p><hr /><h2 id="起步">起步</h2><p>macOS 10.15 自带 Python3，终端输入 <code>python3</code> 即可运行。</p><p><a href="https://www.sublimetext.com/3">Sublime text</a> 文本编辑器</p><hr /><h2 id="变量和简单数据类型">变量和简单数据类型</h2><h3 id="hello-world">Hello world</h3><p><code>print("Hello world!")</code></p><p>注意：<code>print</code> 语句末尾自动<strong>换行</strong></p><p>【与 C++ 不同】Python3 使用<strong>缩进</strong>来组织代码结构，而非大括号。</p><h3 id="变量">变量</h3><p>命名规范：【和 C++ 类似】，不赘述。</p><p>尽量用小写变量名（下划线法），这是 Python3 的主流命名方式。</p><p>【与 C++ 不同】<strong>不用</strong>声明变量类型，Python3 解释器自己解释。</p><h3 id="字符串">字符串</h3><p>【与 C++ 不同】可以用 <code>""</code> 或者 <code>''</code> 括起字符串。（小心歧义）</p><p>设 <code>str</code> 是一个字符串：</p><ul><li><code>str.title()</code>: 返回单词首字母大写，其余字母小写（不管以前是不是大写）的字符串</li><li><code>str.upper()</code> 和 <code>str.lower()</code>：返回全文大/小写的字符串</li><li><code>str1 + str2</code>: 返回用加号拼接的字符串【和 C++ 的 <code>string</code> 类似】</li><li>制表符 <code>\t</code>，换行符 <code>\n</code>【和 C++ 一样】</li><li><code>str.lstrip()</code> 和 <code>str.rstrip()</code> 和 <code>str.strip()</code>: 返回删除开头/末尾/两端空白的字符串</li><li><code>str.replace(str1, str2)</code>：将字符串中的单词 <code>str1</code> 全部替换成 <code>str2</code></li><li><code>str.split()</code>：以空格为分隔符把字符串拆成一个个单词，并返回包含这些单词的列表</li></ul><p>Python3 视字符串非空为 <code>True</code></p><h3 id="数字">数字</h3><ul><li><code>+</code> <code>-</code> <code>*</code> <code>/</code> <code>**</code>（<strong>乘方</strong>）</li><li><code>str(a)</code>: 把数字（整型或浮点型） <code>a</code> 转换成字符串</li></ul><p>【与 C++ 不同】在 Python3 中，输出 <code>3/2</code> 的值是 <code>1.5</code>，即 Python3 将其理解为浮点数除法而非整除。</p><h3 id="注释">注释</h3><p>用 <code>#</code> 标识。</p><hr /><h2 id="列表-lists">列表 Lists</h2><p>【与 C++ 的 vector 类似】</p><p>设 <code>lst</code> 是一个列表：</p><h3 id="访问元素">访问元素</h3><ul><li><p>定义列表：<code>lst = [a, b, c, d]</code> ，其中 <code>a,b,c,d</code> 等是列表的元素，<strong>类型可以不同</strong></p></li><li><p>打印列表：<code>print(lst)</code>（会将列表中的元素列出，括在方括号里）</p></li><li><p>访问元素：<code>lst[3]</code> ， <strong>索引从 0 开始！！！</strong></p><p>【与 C++ 不同】支持<strong>负数</strong>索引，<code>-1</code> 表示倒数第一个，<code>-2</code> 倒数第二个。更具体地，设列表共 <span class="math inline">\(x\)</span> 个元素，则索引范围是 <span class="math inline">\([-x,x)\)</span>.</p></li></ul><h3 id="修改添加删除元素">修改、添加、删除元素</h3><ul><li>修改：赋值就好了</li><li><code>lst.append(x)</code>: 在列表末尾添加元素 <code>x</code></li><li><code>lst.insert(idx, x)</code>: 在列表索引 <code>idx</code> 处插入一个元素 <code>x</code> （插入后，<code>x</code> 的索引是 <code>idx</code>，其后的元素后移一格）</li><li><code>del lst[3]</code>: 删除指定元素（删除后，其后元素前移一格）</li><li><code>lst.pop()</code>: 弹出<strong>并返回</strong>最后一个元素</li><li><code>lst.pop(idx)</code>: 弹出<strong>并返回</strong>指定元素</li><li><code>lst.remove(x)</code>: 删除<strong>第一个</strong>值为 <code>x</code> 的元素</li></ul><h3 id="组织列表">组织列表</h3><ul><li><code>lst.sort()</code> 和 <code>lst.sort(reverse = True)</code>: 对列表排序，永久性修改顺序</li><li><code>sorted(lst)</code> 和 <code>sorted(lst, reverse = True)</code>: 返回排序后的列表，但<strong>不改变</strong>列表原有顺序</li><li><code>lst.reverse()</code>: 翻转列表，永久性修改顺序</li><li><code>len(lst)</code>: 返回列表长度</li></ul><h3 id="遍历列表for-语句">遍历列表——<code>for</code> 语句</h3><p>从头到尾遍历列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> lst:<br>    do something<br></code></pre></td></tr></table></figure><p>注意最后有个冒号 <code>:</code></p><p>【与 C++ 不同】<code>i</code> 是列表元素，不是索引；循环结束后 <code>i</code> 停留为最后一个元素。</p><h3 id="数值列表">数值列表</h3><ul><li><code>range(l, r)</code>: 依次生成 <span class="math inline">\([l,r)\)</span> 中的整数【类似于 C++ 的 <code>for(int i = l; i &lt; r; i++)</code>】</li><li><code>range(l, r, step)</code>: 指定步长为 <code>step</code>【类似于 C++ 的 <code>for(int i = l; i &lt; r; i += step)</code>】</li><li><code>list(range(l, r))</code>: 把 <code>range(l, r)</code> 转换为列表</li><li><code>min(lis)</code> 和 <code>max(lis)</code> 和 <code>sum(lis)</code>: 返回数值列表的最小值、最大值、和</li><li><strong>列表解析</strong>：<code>列表名 = [表达式 for循环]</code>，例如：<code>lis = [value**2 for value in range(1,11)]</code> （有描述法定义集合之感）</li></ul><h3 id="列表切片">列表切片</h3><ul><li><p><code>lis[l:r]</code>: 返回一个列表，元素依次是 <code>lis</code> 列表的索引在 <span class="math inline">\([l,r)\)</span> 内的元素</p><p>省略 <code>l</code> 或 <code>r</code> 则默认从头开始或到尾结束</p></li><li><p>可以循环遍历列表切片：<code>for i in lis[l:r]:</code></p></li><li><p>复制列表：在切片中同时省略 <code>l</code> 和 <code>r</code>，即返回从头到尾的列表</p><p><strong>注意</strong>：复制列表应改写为：<code>lis2 = lis1[:]</code>，而非 <code>lis2=lis1</code>，后者<code>lis1</code> 和 <code>lis2</code> 实质是同一个列表【类似 C++ 的引用】</p></li></ul><hr /><h2 id="元组-tuples">元组 Tuples</h2><p>元组就是元素值不可更改的列表。</p><h3 id="定义元组">定义元组</h3><p><code>tpl = (a, b, c, d)</code>: 把列表定义中的方括号 <code>[]</code> 改成圆括号 <code>()</code> 即可</p><h3 id="遍历元组">遍历元组</h3><p><code>for i in tpl:</code> 和列表一样</p><h3 id="修改元组">修改元组</h3><p>元组中元素的值不能修改，但是元组变量本身可以被赋值。</p><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">tpl = (<span class="hljs-number">100</span>, <span class="hljs-number">300</span>)<br><span class="hljs-built_in">print</span>(tpl)<br>tpl = (<span class="hljs-number">300</span>, <span class="hljs-number">100</span>)<br><span class="hljs-built_in">print</span>(tpl)<br></code></pre></td></tr></table></figure><p>上述代码合法。</p><hr /><h2 id="if-语句"><code>if</code> 语句</h2><ul><li><p>等号 <code>==</code>，不等号 <code>!=</code> 【和 C++ 一样】</p></li><li><p>与 <code>and</code>，或 <code>or</code></p></li><li><p>检查特定值是否包含在列表中：<code>x in lst</code></p><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">lst = [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-number">3</span> <span class="hljs-keyword">in</span> lst)<br><span class="hljs-built_in">print</span>(<span class="hljs-number">4</span> <span class="hljs-keyword">in</span> lst)<br></code></pre></td></tr></table></figure><p>分别输出 <code>True</code> 和 <code>False</code></p></li><li><p>检查特定值不在列表中：<code>x not in lst</code></p></li><li><p><code>if</code> 语句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> 条件测试:<br>    do something<br></code></pre></td></tr></table></figure><p>注意最后有个冒号 <code>:</code></p></li><li><p><code>if-else</code> 语句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> 条件测试:<br>    do something<br><span class="hljs-keyword">else</span>:<br>    do something<br></code></pre></td></tr></table></figure><p>注意 <code>else</code> 后面也有一个冒号 <code>:</code></p></li><li><p><code>if-elif-else</code> 语句</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> 条件测试:<br>    do something<br><span class="hljs-keyword">elif</span> 条件测试:<br>    do something<br><span class="hljs-keyword">else</span>:<br>    do something<br></code></pre></td></tr></table></figure></li><li><p>检查列表是否为空：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> lst:<br>    do something<br></code></pre></td></tr></table></figure><p>以列表名为条件表达式，Python3 在列表为空时返回 <code>False</code>，非空时返回 <code>True</code>。</p></li></ul><hr /><h2 id="字典-dictionaries">字典 Dictionaries</h2><p>字典是一系列 “键-值对”，可将任何 Python 对象作为值。</p><p>【类似于更高端版本的 C++ 的 map】</p><h3 id="定义字典">定义字典</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">dic = &#123;<span class="hljs-string">&#x27;color&#x27;</span>: <span class="hljs-string">&#x27;green&#x27;</span>, <span class="hljs-string">&#x27;points&#x27;</span>: <span class="hljs-number">5</span>&#125;<br>void_dic = &#123;&#125;<br>favorite_languages = &#123;<br>    <span class="hljs-string">&#x27;jen&#x27;</span>: <span class="hljs-string">&#x27;python&#x27;</span>,<br>    <span class="hljs-string">&#x27;sarah&#x27;</span>: <span class="hljs-string">&#x27;c&#x27;</span>,<br>    <span class="hljs-string">&#x27;jason&#x27;</span>: <span class="hljs-string">&#x27;c++&#x27;</span>,<br>&#125;<br></code></pre></td></tr></table></figure><p>花括号括起一系列键-值对，键与值之间冒号 <code>:</code> 分隔，键值对之间逗号 <code>,</code> 分隔。</p><h3 id="访问字典">访问字典</h3><p><code>dic['color']</code>: 字典名[键]</p><h3 id="添加修改删除">添加、修改、删除</h3><ul><li>添加：直接赋值即可（即使键本来不存在）。例如：<code>dic['x'] = 0</code></li><li>修改：直接赋值即可</li><li>删除：<code>del dic['points']</code></li></ul><h3 id="遍历字典">遍历字典</h3><ul><li><p>遍历所有键-值对：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> favorite_languages.items():<br>    do something<br></code></pre></td></tr></table></figure><p><code>k</code> 和 <code>v</code> 是自定义的变量名，可以更换成其他名称。</p><p>方法 <code>items()</code> 返回键-值对列表。</p></li><li><p>遍历所有键：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> favorite_languages.keys():<br>    do something<br></code></pre></td></tr></table></figure><p><code>k</code> 是自定义的变量名。</p><p>方法 <code>keys()</code> 返回键的列表，可省略（即 <code>for k in favorite_languages:</code> ）</p><p>由于 <code>keys()</code> 本质是个列表，各种对列表的操作也适用，例如：</p><ul><li><p>按顺序遍历所有键：<code>for k in sorted(favorite_languages.keys()):</code></p></li><li><p>检查某值是否在键中：<code>if 'catherine' in favorite_languages.keys():</code></p></li></ul></li><li><p>遍历所有值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> favorite_languages.values():<br>    do something<br></code></pre></td></tr></table></figure><p><code>v</code> 是自定义的变量名。</p><p>方法 <code>values()</code> 返回值的列表。</p><p>去重：<code>for v in set(favorite_languages.values()):</code></p></li></ul><h3 id="嵌套">嵌套</h3><ul><li><p>字典列表：列表的元素是字典，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">alien_0 = &#123;<span class="hljs-string">&#x27;color&#x27;</span>: <span class="hljs-string">&#x27;green&#x27;</span>, <span class="hljs-string">&#x27;points&#x27;</span>: <span class="hljs-number">5</span>&#125;<br>alien_1 = &#123;<span class="hljs-string">&#x27;color&#x27;</span>: <span class="hljs-string">&#x27;yellow&#x27;</span>, <span class="hljs-string">&#x27;points&#x27;</span>: <span class="hljs-number">10</span>&#125;<br>alien_2 = &#123;<span class="hljs-string">&#x27;color&#x27;</span>: <span class="hljs-string">&#x27;red&#x27;</span>, <span class="hljs-string">&#x27;points&#x27;</span>: <span class="hljs-number">15</span>&#125;<br>aliens = [alien_0, alien_1, alien_2]<br></code></pre></td></tr></table></figure></li><li><p>字典中嵌套列表，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">pizza = &#123;<br><span class="hljs-string">&#x27;crust&#x27;</span>: <span class="hljs-string">&#x27;thick&#x27;</span>,<br><span class="hljs-string">&#x27;toppings&#x27;</span>: [<span class="hljs-string">&#x27;mushrooms&#x27;</span>, <span class="hljs-string">&#x27;extra cheese&#x27;</span>],<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>字典嵌套字典，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">users = &#123;<br>    <span class="hljs-string">&#x27;aeinstein&#x27;</span>: &#123;<br><span class="hljs-string">&#x27;first&#x27;</span>: <span class="hljs-string">&#x27;albert&#x27;</span>,<br>        <span class="hljs-string">&#x27;last&#x27;</span>: <span class="hljs-string">&#x27;einstein&#x27;</span>,<br>        <span class="hljs-string">&#x27;location&#x27;</span>: <span class="hljs-string">&#x27;princeton&#x27;</span>,<br>    &#125;,<br><span class="hljs-string">&#x27;mcurie&#x27;</span>: &#123;<br><span class="hljs-string">&#x27;first&#x27;</span>: <span class="hljs-string">&#x27;marie&#x27;</span>,<br>        <span class="hljs-string">&#x27;last&#x27;</span>: <span class="hljs-string">&#x27;curie&#x27;</span>,<br>        <span class="hljs-string">&#x27;location&#x27;</span>: <span class="hljs-string">&#x27;paris&#x27;</span>,<br>    &#125;,<br>&#125;<br></code></pre></td></tr></table></figure></li></ul><hr /><h2 id="用户输入">用户输入</h2><p>###<code>input()</code> 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">a = <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;prompt&quot;</span>)<br></code></pre></td></tr></table></figure><p>其中，<code>prompt</code> 是显示在屏幕上的提示词，<code>input()</code> 函数返回一个<strong>字符串</strong>存储在变量 <code>a</code> 中，即用户的输入内容。</p><h3 id="数值输入">数值输入</h3><p><code>int(x)</code> 函数将字符串 <code>x</code> 转换成整数值。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">a = <span class="hljs-built_in">input</span>()<br>b = <span class="hljs-built_in">input</span>()<br>a = <span class="hljs-built_in">int</span>(a)<br>b = <span class="hljs-built_in">int</span>(b)<br><span class="hljs-built_in">print</span>(a + b)<br></code></pre></td></tr></table></figure><p><code>float(x)</code> 函数将字符串 <code>x</code> 转换成浮点数值。</p><h3 id="取模运算">取模运算</h3><p><code>a % b</code></p><p>例如：<code>35 % 4 == 3</code>，<code>-35 % 4 == 1</code>，<code>35 % -4 == -1</code>，<code>-35 % -4 == -3</code></p><p>【与 C++ 不同：负数对正数取模时返回正余数，而非像 C++ 那样返回负余数】</p><hr /><h2 id="while-语句"><code>while</code> 语句</h2><ul><li><p><code>while</code> 语句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">while</span> 循环条件:<br>    do something<br></code></pre></td></tr></table></figure><p>注意最后有个冒号 <code>:</code></p></li><li><p><code>break</code> 和 <code>continue</code> 【和 C++ 一样】</p></li></ul><hr /><h2 id="函数-functions">函数 Functions</h2><h3 id="定义与调用">定义与调用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">func</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;文档字符串 docstring&quot;&quot;&quot;</span><br>    do something<br><br>func()<br></code></pre></td></tr></table></figure><h3 id="传递实参">传递实参</h3><ul><li><p>位置实参：实参位置与形参位置对应【和 C++ 一样】</p></li><li><p>关键字实参：例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pet</span>(<span class="hljs-params"><span class="hljs-built_in">type</span>, name</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;My &quot;</span> + <span class="hljs-built_in">type</span> + <span class="hljs-string">&quot;&#x27;s name is &quot;</span> + name.title() + <span class="hljs-string">&quot;.&quot;</span>)<br><br>pet(name=<span class="hljs-string">&#x27;harry&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;hamster&#x27;</span>)<br></code></pre></td></tr></table></figure></li><li><p>设定默认值：例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">add</span>(<span class="hljs-params">a, b=<span class="hljs-number">0</span></span>):<br>    <span class="hljs-built_in">print</span>(a + b)<br><br>add(<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>注意：形参列表中必须先列出无默认值的形参，再列出有默认值的形参</p></li></ul><h3 id="返回值">返回值</h3><p><code>return</code> 【和 C++ 一样】</p><p>可以 <code>return</code> 任何类型（包括列表、字典等）</p><h3 id="传递列表">传递列表</h3><ul><li><p>把列表名作为实参传过去即可。</p></li><li><p>在函数中修改列表可更改原列表【类似于 C++ 的传递数组实质是传递首地址】</p></li><li><p>不想修改原列表可传递切片，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">print_models</span>(<span class="hljs-params">unprinted_designs, completed_models</span>):<br><span class="hljs-keyword">while</span> unprinted_designs:<br>current_design = unprinted_designs.pop()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Printing model: &quot;</span> + current_design)<br>   completed_models.append(current_design)<br><br>print_models(unprinted_designs[:], completed_models)<br></code></pre></td></tr></table></figure></li></ul><h3 id="传递任意数量的实参">传递任意数量的实参</h3><ul><li><p>在形参前加星号 <code>*</code>，Python3 会创建一个该形参名称的<strong>元组</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">func</span>(<span class="hljs-params">*tpl</span>):<br>    do something<br></code></pre></td></tr></table></figure><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_pizza</span>(<span class="hljs-params">*toppings</span>):<br>    <span class="hljs-built_in">print</span>(toppings)<br><br>make_pizza(<span class="hljs-string">&#x27;pepperoni&#x27;</span>)<br>make_pizza(<span class="hljs-string">&#x27;mushrooms&#x27;</span>, <span class="hljs-string">&#x27;green peppers&#x27;</span>, <span class="hljs-string">&#x27;extra cheese&#x27;</span>)<br></code></pre></td></tr></table></figure></li><li><p>与位置实参结合：位置实参在前，任意数量实参在后</p><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_pizza</span>(<span class="hljs-params">size, *toppings</span>): <br>  <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nMaking a &quot;</span> + <span class="hljs-built_in">str</span>(size) + <span class="hljs-string">&quot;-inch pizza with the following toppings:&quot;</span>)<br>    <span class="hljs-keyword">for</span> topping <span class="hljs-keyword">in</span> toppings:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;- &quot;</span> + topping)<br></code></pre></td></tr></table></figure></li><li><p>任意数量的关键字实参：在形参前加 <code>**</code>，Python3 会创建一个该形参名称的<strong>字典</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">func</span>(<span class="hljs-params">a, b, **c</span>):<br>    do something<br></code></pre></td></tr></table></figure><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_profile</span>(<span class="hljs-params">name, **info</span>):<br>    profile = &#123;&#125;<br>    profile[<span class="hljs-string">&#x27;name&#x27;</span>] = name<br>    <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> info.items():<br>        profile[key] = value<br>    <span class="hljs-keyword">return</span> profile<br><br>user = build_profile(<span class="hljs-string">&#x27;jason&#x27;</span>, location = <span class="hljs-string">&#x27;China&#x27;</span>, university = <span class="hljs-string">&#x27;HITSZ&#x27;</span>)<br><span class="hljs-built_in">print</span>(user)<br></code></pre></td></tr></table></figure></li></ul><h3 id="将函数存储在模块中">将函数存储在模块中</h3><p>模块是扩展名为 .py 的文件，包含代码【类似于 C++ 的头文件】</p><ul><li><p>导入整个模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pizza<br></code></pre></td></tr></table></figure><p>调用时使用句点 <code>.</code>，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pizza.make_pizza(<span class="hljs-number">16</span>, <span class="hljs-string">&#x27;green peppers&#x27;</span>)<br></code></pre></td></tr></table></figure></li><li><p>导入特定函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> module_name <span class="hljs-keyword">import</span> function_name<br><span class="hljs-keyword">from</span> module_name <span class="hljs-keyword">import</span> function_0, function_1, function_2<br></code></pre></td></tr></table></figure><p>调用时无需使用句点</p></li><li><p>函数别名：<code>as</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> module_name <span class="hljs-keyword">import</span> function_name <span class="hljs-keyword">as</span> fn<br></code></pre></td></tr></table></figure><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> HelloWorld <span class="hljs-keyword">import</span> say_hello <span class="hljs-keyword">as</span> sh<br><br>sh()<br></code></pre></td></tr></table></figure></li><li><p>模块别名：<code>as</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> module_name <span class="hljs-keyword">as</span> mn<br></code></pre></td></tr></table></figure><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> HelloWorld <span class="hljs-keyword">as</span> HW<br><br>HW.say_hello()<br></code></pre></td></tr></table></figure></li><li><p>导入模块中所有函数：星号 <code>*</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> module_name <span class="hljs-keyword">import</span> *<br></code></pre></td></tr></table></figure><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> HelloWorld <span class="hljs-keyword">import</span> *<br><br>say_hello()<br>say_bye()<br></code></pre></td></tr></table></figure></li></ul><p>所有 <code>import</code> 都放在程序开头【类似于 C++ 的 #include&lt;&gt;】</p><hr /><h2 id="类-classes">类 Classes</h2><h3 id="创建和使用类">创建和使用类</h3><ul><li><p>以举例的方式说明如何创建类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Dog</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;a dog&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, name, age</span>):<br>        self.name = name<br>        self.age = age<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sit</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(self.name.title() + <span class="hljs-string">&quot; is now sitting.&quot;</span>)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">roll_over</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(self.name.title() + <span class="hljs-string">&quot; rolled over!&quot;</span>)<br></code></pre></td></tr></table></figure><ul><li>约定类的名称采用驼峰命名法</li><li><code>""" """</code> 括起一段描述</li><li>方法 <code>__init__()</code>：创建实例时<strong>自动</strong>调用【类似于 C++ 的构造函数】</li><li><code>self</code> 是 <code>__init__()</code> 中必不可少的形参，且位于第一个，所有与类相关的方法都<strong>自动</strong>传入实参 <code>self</code>，它是一个指向实例自身的引用【类似于 C++ 的 *this】</li><li><code>__init__()</code> 中定义的两个变量前都有 <code>self.</code>，表明其可供类中所有方法使用</li></ul></li><li><p>举例说明如何创建实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">my_dog = Dog(<span class="hljs-string">&#x27;willie&#x27;</span>, <span class="hljs-number">6</span>)<br><span class="hljs-built_in">print</span>(my_dog.name.title())<br>my_dog.roll_over()<br></code></pre></td></tr></table></figure><p>访问属性和调用方法都采用句点 <code>.</code></p></li></ul><h3 id="使用类和实例">使用类和实例</h3><ul><li><p>给属性指定默认值：在 <code>__init__()</code> 中赋值即可，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Car</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, make, model, year</span>):<br>        self.make = make<br>        self.model = model<br>        self.year = year<br>        self.odometer_reading = <span class="hljs-number">0</span><br><br>my_new_car = Car(<span class="hljs-string">&#x27;audi&#x27;</span>, <span class="hljs-string">&#x27;a6&#x27;</span>, <span class="hljs-number">2019</span>)<br></code></pre></td></tr></table></figure></li><li><p>修改属性值：</p><ul><li><p>直接修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Car</span>():<br>    <span class="hljs-comment"># omit</span><br><br>my_new_car.odometer_reading = <span class="hljs-number">100</span><br></code></pre></td></tr></table></figure></li><li><p>定义一个方法修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Car</span>():<br>    <span class="hljs-comment"># omit</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, mileage</span>):<br>        self.odometer_reading = mileage<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">increase</span>(<span class="hljs-params">self, mileage</span>):<br>        self.odometer_reading += mileage<br><br>my_new_car = Car(<span class="hljs-string">&#x27;audi&#x27;</span>, <span class="hljs-string">&#x27;a6&#x27;</span>, <span class="hljs-number">2019</span>)<br>my_new_car.update(<span class="hljs-number">30</span>)<br>my_new_car.increase(<span class="hljs-number">100</span>)<br></code></pre></td></tr></table></figure></li></ul></li></ul><h3 id="继承">继承</h3><p>一个类继承另一个类时，它自动获得另一个类的所有属性和方法；原有的类称为父类或超类(superclass)，而新类称为子类。子类继承父类的属性和方法后，还可以定义自己的属性和方法。</p><ul><li><p>举例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Car</span>():<br>    <span class="hljs-comment"># omit</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ElectricCar</span>(<span class="hljs-title class_ inherited__">Car</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, make, model, year</span>):<br>        <span class="hljs-built_in">super</span>().__init__(make, model, year)<br>        self.battery_size = <span class="hljs-number">70</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">print_battery</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(self.battery_size)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, mileage</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;no!&quot;</span>)<br>        <br>my_tesla = ElectricCar(<span class="hljs-string">&#x27;tesla&#x27;</span>, <span class="hljs-string">&#x27;model s&#x27;</span>, <span class="hljs-number">2016</span>)<br>my_tesla.increase(<span class="hljs-number">120</span>)<br><span class="hljs-built_in">print</span>(my_tesla.odometer_reading)<br>my_tesla.print_battery()<br>my_tesla.update(<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><ul><li>定义子类时，括号里指定父类名称，<code>__init__()</code> 接受创建父类所需的信息</li><li><code>super()</code> 函数将父类和子类关联起来。<code>super().__init__()</code> 调用父类的方法 <code>__init__()</code>，使子类包含父类所有属性</li><li><code>battery_size</code> 和 <code>print_battery()</code> 分别是子类特有的属性和方法</li><li><code>update()</code> 是父类的函数，在子类里重写了</li></ul></li><li><p>将实例作为属性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Car</span>():<br>    <span class="hljs-comment"># omit</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Battery</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, battery_size=<span class="hljs-number">70</span></span>):<br>        self.battery_size = battery_size<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">print_battery</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(self.battery_size)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ElectricCar</span>(<span class="hljs-title class_ inherited__">Car</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, make, model, year, bat</span>):<br>        <span class="hljs-built_in">super</span>().__init__(make, model, year)<br>        self.battery = Battery(bat)<br>        <br>my_tesla = ElectricCar(<span class="hljs-string">&#x27;tesla&#x27;</span>, <span class="hljs-string">&#x27;model s&#x27;</span>, <span class="hljs-number">2016</span>, <span class="hljs-number">80</span>)<br>my_tesla.battery.print_battery()<br></code></pre></td></tr></table></figure><p>【类似于 C++ 的 struct 或 class 的嵌套】</p></li></ul><h3 id="导入类">导入类</h3><p>假设上述 <code>Car</code>,<code>Battery</code>,<code>ElectricCar</code> 类都存储在 <code>car.py</code> 中：</p><ul><li><p>导入单个/多个类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> car <span class="hljs-keyword">import</span> Car<br><span class="hljs-keyword">from</span> car <span class="hljs-keyword">import</span> Car, ElectricCar<br></code></pre></td></tr></table></figure></li><li><p>导入整个模块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> car<br></code></pre></td></tr></table></figure><p>访问模块内的类是需要使用句点 <code>.</code> ，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">my_tesla = car.Electric(<span class="hljs-string">&#x27;tesla&#x27;</span>, <span class="hljs-string">&#x27;model s&#x27;</span>, <span class="hljs-number">2016</span>, <span class="hljs-number">80</span>)<br></code></pre></td></tr></table></figure></li><li><p>导入模块内所有类：星号 <code>*</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> module_name <span class="hljs-keyword">import</span> *<br></code></pre></td></tr></table></figure></li></ul><p>推荐用第二种方式导入并调用类。</p><h3 id="python-标准库">Python 标准库</h3><p><a href="https://pymotw.com/3/">链接</a></p><hr /><h2 id="文件和异常-files-and-exceptions">文件和异常 Files and Exceptions</h2><h3 id="读取文件">读取文件</h3><ul><li><p>读取整个文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;pi_digits.txt&#x27;</span>) <span class="hljs-keyword">as</span> file_object:<br>    contents = file_object.read()<br>    <span class="hljs-built_in">print</span>(contents)<br></code></pre></td></tr></table></figure><ul><li><code>open()</code> 函数打开一个文件，返回<strong>表示文件的对象</strong>并存储在 <code>as</code> 后的变量中</li><li>关键字 <code>with</code> 使得 Python3 能够在合适的时候关闭文件，而不必手动调用 <code>close()</code></li><li>方法 <code>read()</code> 读取整个文件的内容并返回<strong>字符串</strong>，注意末尾有一个空行（可通过 <code>rstrip()</code> 去掉）</li></ul></li><li><p>文件路径</p><p>相对路径：从程序所在文件夹开始找</p><p>绝对路径：从根目录开始</p></li><li><p>逐行读取：用 <code>for</code> 语句</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">filename = <span class="hljs-string">&#x27;pi_digits.txt&#x27;</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename) <span class="hljs-keyword">as</span> file_object:<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> file_object:<br>        <span class="hljs-built_in">print</span>(line)<br></code></pre></td></tr></table></figure><p>注意每一行末尾都会有一个换行符（可通过 <code>rstrip()</code> 去掉）</p></li><li><p>创建包含文件各行内容的列表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">filename = <span class="hljs-string">&#x27;pi_digits.txt&#x27;</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename) <span class="hljs-keyword">as</span> file_object:<br>    lines = file_object.readlines()<br><span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines:<br>    <span class="hljs-built_in">print</span>(line.rstrip())<br></code></pre></td></tr></table></figure><p>方法 <code>readlines()</code> 从文件读取每一行，并视为<strong>字符串</strong>将其存储在列表中，返回该列表</p></li></ul><h3 id="写入文件">写入文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">filename = <span class="hljs-string">&#x27;programming.txt&#x27;</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> file_object:<br>    file_object.write(<span class="hljs-string">&quot;I love programming.&quot;</span>)<br></code></pre></td></tr></table></figure><ul><li><code>open()</code> 函数的第二个实参 <code>w</code> 表示写入（自动创建或<strong>覆盖</strong>原内容），<code>r</code> 表示只读，<code>a</code> 表示附加（自动创建或<strong>添加到文件末尾</strong>），<code>r+</code> 表示读写。默认只读</li><li>方法 <code>write()</code> 将<strong>字符串</strong>写入文件</li><li><code>write()</code> 不会自动换行，要换行需写换行符 <code>\n</code></li></ul><p>注：要写入数值，应先用 <code>str()</code> 将其转化为字符串</p><h3 id="异常">异常</h3><p>异常是一种特殊对象，当 Python3 发生错误时会创建一个异常对象，若编写了处理异常的代码，程序将继续运行，否则将停止。</p><p>异常是使用 <code>try-except</code> 代码块处理的，该代码块告诉 Python3 出现异常时怎么办，程序也将继续运行。</p><ul><li><p><code>try-except</code> 代码块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">try</span>:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-number">5</span>/<span class="hljs-number">0</span>)<br><span class="hljs-keyword">except</span> ZeroDivisionError:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;You can&#x27;t divide by zero!&quot;</span>)<br></code></pre></td></tr></table></figure></li><li><p><code>try-except-else</code> 代码块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Give me two numbers, and I&#x27;ll divide them.&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Enter &#x27;q&#x27; to quit.&quot;</span>)<br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>    first_number = <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;\nFirst number: &quot;</span>)<br>    <span class="hljs-keyword">if</span> first_number == <span class="hljs-string">&#x27;q&#x27;</span>:<br>        <span class="hljs-keyword">break</span><br>    second_number = <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;Second number: &quot;</span>)<br>    <span class="hljs-keyword">try</span>:<br>        answer = <span class="hljs-built_in">int</span>(first_number) / <span class="hljs-built_in">int</span>(second_number)<br>    <span class="hljs-keyword">except</span> ZeroDivisionError:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;You can&#x27;t divide by 0!&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(answer)<br></code></pre></td></tr></table></figure></li><li><p>一些异常：</p><ul><li><code>ZeroDivisionError</code></li><li><code>FileNotFoundError</code></li><li><code>TypeError</code></li><li><code>ValueError</code></li></ul></li><li><p><code>pass</code> 语句：让 Python3 什么都不做</p></li></ul><h3 id="存储数据">存储数据</h3><p>使用模块 <code>json</code> 存储数据。</p><ul><li><p><code>json.dump()</code> 与 <code>json.load()</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><br>numbers = [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">11</span>, <span class="hljs-number">13</span>]<br><br>filename = <span class="hljs-string">&#x27;numbers.json&#x27;</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f_obj:<br>    json.dump(numbers, f_obj)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><br>filename = <span class="hljs-string">&#x27;numbers.json&#x27;</span><br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename) <span class="hljs-keyword">as</span> f_obj:<br>    numbers = json.load(f_obj)<br><span class="hljs-built_in">print</span>(numbers)<br></code></pre></td></tr></table></figure></li></ul><hr /><h2 id="测试-testing">测试 Testing</h2><p>测试模块 <code>unittest</code></p><p>单元测试、测试用例、全覆盖式测试</p><h3 id="测试函数">测试函数</h3><p>要为函数编写测试用例，可先导入模块 <code>unittest</code> 以及要测试的函数，再创建一个继承 <code>unittest.TestCase</code> 的类，并编写一系列方法对函数行为的不同方面进行测试。</p><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> unittest<br><span class="hljs-keyword">from</span> name_function <span class="hljs-keyword">import</span> get_formatted_name<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NamesTestCase</span>(unittest.TestCase):<br>    <span class="hljs-string">&quot;&quot;&quot;测试name_function.py&quot;&quot;&quot;</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test_first_last_name</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;能够正确地处理像Janis Joplin这样的姓名吗?&quot;&quot;&quot;</span><br>        formatted_name = get_formatted_name(<span class="hljs-string">&#x27;janis&#x27;</span>, <span class="hljs-string">&#x27;joplin&#x27;</span>)<br>        self.assertEqual(formatted_name, <span class="hljs-string">&#x27;Janis Joplin&#x27;</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test_first_last_middle_name</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;能够正确地处理像Wolfgang Amadeus Mozart这样的姓名吗?&quot;&quot;&quot;</span><br>        formatted_name = get_formatted_name( <span class="hljs-string">&#x27;wolfgang&#x27;</span>, <span class="hljs-string">&#x27;mozart&#x27;</span>, <span class="hljs-string">&#x27;amadeus&#x27;</span>)<br>        self.assertEqual(formatted_name, <span class="hljs-string">&#x27;Wolfgang Amadeus Mozart&#x27;</span>)<br><br>unittest.main()<br></code></pre></td></tr></table></figure><ul><li><code>NamesTestCase</code> 是继承 <code>unittest.TestCase</code> 的一个类</li><li>我们运行这个文件时，所有以 <code>test_</code> 打头的方法都将自动运行（方法名必须以 <code>test_</code> 打头！）</li><li><code>unittest.main()</code> 让 Python3 运行这个文件的测试</li></ul><h3 id="unittest-的断言方法"><code>unittest</code> 的断言方法</h3><table><thead><tr class="header"><th style="text-align: center;">方法</th><th style="text-align: center;">用途</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><code>assertEqual(a, b)</code></td><td style="text-align: center;">核实 <code>a == b</code></td></tr><tr class="even"><td style="text-align: center;"><code>assertNotEqual(a, b)</code></td><td style="text-align: center;">核实 <code>a != b</code></td></tr><tr class="odd"><td style="text-align: center;"><code>assertTrue(x)</code></td><td style="text-align: center;">核实 <code>a</code> 为 <code>True</code></td></tr><tr class="even"><td style="text-align: center;"><code>assertFalse(x)</code></td><td style="text-align: center;">核实 <code>a</code> 为 <code>False</code></td></tr><tr class="odd"><td style="text-align: center;"><code>assertIn(item, list)</code></td><td style="text-align: center;">核实 <code>item</code> 在 <code>list</code> 中</td></tr><tr class="even"><td style="text-align: center;"><code>assertNotIn(item, list)</code></td><td style="text-align: center;">核实 <code>item</code> 不在 <code>list</code> 中</td></tr></tbody></table><h3 id="测试类">测试类</h3><ul><li><p>与测试函数类似。</p><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> unittest<br><span class="hljs-keyword">from</span> survey <span class="hljs-keyword">import</span> AnonymousSurvey<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TestAnonymousSurvey</span>(unittest.TestCase):<br>    <span class="hljs-string">&quot;&quot;&quot;针对AnonymousSurvey类的测试&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test_store_single_response</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;测试单个答案会被妥善地存储&quot;&quot;&quot;</span><br>        question = <span class="hljs-string">&quot;What language did you first learn to speak?&quot;</span><br>        my_survey = AnonymousSurvey(question)<br>        my_survey.store_response(<span class="hljs-string">&#x27;English&#x27;</span>)<br>        self.assertIn(<span class="hljs-string">&#x27;English&#x27;</span>, my_survey.responses)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test_store_three_responses</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;测试三个答案会被妥善地存储&quot;&quot;&quot;</span><br>        question = <span class="hljs-string">&quot;What language did you first learn to speak?&quot;</span><br>        my_survey = AnonymousSurvey(question)<br>        responses = [<span class="hljs-string">&#x27;English&#x27;</span>, <span class="hljs-string">&#x27;Spanish&#x27;</span>, <span class="hljs-string">&#x27;Mandarin&#x27;</span>]<br>        <span class="hljs-keyword">for</span> response <span class="hljs-keyword">in</span> responses:<br>            my_survey.store_response(response)<br>        <span class="hljs-keyword">for</span> response <span class="hljs-keyword">in</span> responses:<br>            self.assertIn(response, my_survey.responses)<br><br>unittest.main()<br></code></pre></td></tr></table></figure></li><li><p>方法 <code>setUp()</code>：<code>unittest.TestCase</code> 类包含方法 <code>setUp()</code>，Python3 将先运行它，再运行各个以 <code>test_</code> 打头的方法。</p><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> unittest<br><span class="hljs-keyword">from</span> survey <span class="hljs-keyword">import</span> AnonymousSurvey<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TestAnonymousSurvey</span>(unittest.TestCase):<br>    <span class="hljs-string">&quot;&quot;&quot;针对AnonymousSurvey类的测试&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">setUp</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;创建一个调查对象和一组答案，供使用的测试方法使用&quot;&quot;&quot;</span><br>        question = <span class="hljs-string">&quot;What language did you first learn to speak?&quot;</span><br>        self.my_survey = AnonymousSurvey(question)<br>        self.responses = [<span class="hljs-string">&#x27;English&#x27;</span>, <span class="hljs-string">&#x27;Spanish&#x27;</span>, <span class="hljs-string">&#x27;Mandarin&#x27;</span>]<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test_store_single_response</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;测试单个答案会被妥善地存储&quot;&quot;&quot;</span><br>        self.my_survey.store_response(self.responses[<span class="hljs-number">0</span>])<br>        self.assertIn(self.responses[<span class="hljs-number">0</span>], self.my_survey.responses)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test_store_three_responses</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;测试三个答案会被妥善地存储&quot;&quot;&quot;</span><br><span class="hljs-keyword">for</span> response <span class="hljs-keyword">in</span> self.responses:<br>self.my_survey.store_response(response)<br>            <span class="hljs-keyword">for</span> response <span class="hljs-keyword">in</span> self.responses:<br>self.assertIn(response, self.my_survey.responses)<br><br>unittest.main()<br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>技术博客</category>
      
      <category>技术栈</category>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
