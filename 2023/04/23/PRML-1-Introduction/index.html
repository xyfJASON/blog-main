

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=light>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/blog-main/logo/myfavicon.png">
  <link rel="icon" href="/blog-main/logo/myfavicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="xyfJASON">
  <meta name="keywords" content="">
  
    <meta name="description" content="\[ \newcommand{\bsf}[1]{\boldsymbol{\mathsf{ #1}}} \] Example: Polynomial Curve Fitting PRML 的第一章是围绕着一个简单的回归问题——多项式拟合展开的。问题虽然简单，但其中蕴藏着许多奥妙。作者分别阐述了概率论、决策论和信息论三个贯穿全书的重要工具，展示了频率学派和贝叶斯学派面对问题的不同思考与处理手段，尤其侧">
<meta property="og:type" content="article">
<meta property="og:title" content="[PRML]1.Introduction">
<meta property="og:url" content="https://xyfjason.github.io/blog-main/2023/04/23/PRML-1-Introduction/index.html">
<meta property="og:site_name" content="xyfJASON">
<meta property="og:description" content="\[ \newcommand{\bsf}[1]{\boldsymbol{\mathsf{ #1}}} \] Example: Polynomial Curve Fitting PRML 的第一章是围绕着一个简单的回归问题——多项式拟合展开的。问题虽然简单，但其中蕴藏着许多奥妙。作者分别阐述了概率论、决策论和信息论三个贯穿全书的重要工具，展示了频率学派和贝叶斯学派面对问题的不同思考与处理手段，尤其侧">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://xyfjason.github.io/blog-main/gallery/walle.webp">
<meta property="article:published_time" content="2023-04-23T06:15:35.000Z">
<meta property="article:modified_time" content="2023-09-09T00:36:29.250Z">
<meta property="article:author" content="xyfJASON">
<meta property="article:tag" content="machine learning">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://xyfjason.github.io/blog-main/gallery/walle.webp">
  
  
  
  <title>[PRML]1.Introduction - xyfJASON</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/blog-main/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/blog-main/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/blog-main/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"xyfjason.github.io","root":"/blog-main/","version":"1.9.5-a","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":3},"lazyload":{"enable":true,"loading_img":"/logo/imageloading.png","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/blog-main/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/blog-main/js/utils.js" ></script>
  <script  src="/blog-main/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 60vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/blog-main/">
      <strong>xyfJASON</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog-main/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog-main/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog-main/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog-main/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog-main/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog-main/links/">
                <i class="iconfont icon-friends"></i>
                <span>友链</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-link-fill"></i>
                <span>链接</span>
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="https://xyfjason.github.io/homepage">
                    
                    <span>学术主页</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://xyfjason.github.io/blog-xcpc">
                    
                    <span>博客 (ICPC/CCPC)</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://xyfjason.github.io/blog-oi">
                    
                    <span>博客 (OI)</span>
                  </a>
                
              </div>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/blog-main/gallery/walle.webp') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="[PRML]1.Introduction"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-04-23 14:15" pubdate>
          2023年4月23日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          18k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          154 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">[PRML]1.Introduction</h1>
            
            
              <div class="markdown-body">
                
                <p><span class="math display">\[
\newcommand{\bsf}[1]{\boldsymbol{\mathsf{ #1}}}
\]</span></p>
<h2 id="example-polynomial-curve-fitting">Example: Polynomial Curve Fitting</h2>
<p>PRML 的第一章是围绕着一个简单的回归问题——多项式拟合展开的。问题虽然简单，但其中蕴藏着许多奥妙。作者分别阐述了概率论、决策论和信息论三个贯穿全书的重要工具，展示了频率学派和贝叶斯学派面对问题的不同思考与处理手段，尤其侧重于贝叶斯方法相比频率方法体现出的优势。对于只看过吴恩达入门机器学习的我来说，本章直接为我踹开了贝叶斯的大门，刷新了我的认知。</p>
<p>废话不多说，书中举例的回归问题如下图所示：</p>
<p><img src="fig1.2.png" srcset="/blog-main/logo/imageloading.png" lazyload width=70% /></p>
<p>在这个例子中，训练数据（蓝色空心圆点）是基于 <span class="math inline">\(\sin(2\pi x)\)</span>（绿色曲线）添加随机高斯噪声生成的。记训练集为 <span class="math inline">\(\bsf{x}\equiv (x_1,\ldots,x_N)^\mathrm T\)</span>，对应的观测值为 <span class="math inline">\(\bsf{t}\equiv(t_1,\ldots,t_N)^\mathrm T\)</span>，图 1.2 展示了包含 10 个样本的训练集。我们希望从数据集中找到一些规律，使得询问一个新的 <span class="math inline">\(x\)</span> 时能预测其对应 <span class="math inline">\(t\)</span> 的值。</p>
<p>考虑用多项式做回归： <span class="math display">\[
y(x,\mathbf w)=w_0+w_1x+w_2x^2+\cdots+w_Mx^M=\sum_{j=0}^M w_jx^j
\]</span> 其中 <span class="math inline">\(\mathbf w=(w_0,\ldots,w_M)^\mathrm T\)</span> 是模型的参数，<span class="math inline">\(M\)</span> 为多项式的阶数。这是一个线性模型（linear model），因为 <span class="math inline">\(y(x,\mathbf w)\)</span> 是关于 <span class="math inline">\(\mathbf w\)</span> 的线性函数（尽管关于 <span class="math inline">\(x\)</span> 是非线性的）。</p>
<p>为了拟合训练数据，我们会定义一个误差函数并最小化之： <span class="math display">\[
E(\mathbf w)=\frac{1}{2}\sum_{n=1}^N(y(x_n,\mathbf w)-t_n)^2
\]</span> 通过求导取零一通计算，这个优化问题可以算出一个闭式解。不过先别急着算，这里还有一个问题尚待解决——如何选择 <span class="math inline">\(M\)</span>？不同的 <span class="math inline">\(M\)</span> 对应了不同的模型，会导致效果完全不同的解，如下图所示：</p>
<p><img src="fig1.4.png" srcset="/blog-main/logo/imageloading.png" lazyload width=70% /></p>
<p>有点机器学习基础的同学都知道，<span class="math inline">\(M=0,1\)</span> 时模型能力不够，发生了欠拟合；而 <span class="math inline">\(M=9\)</span> 时模型完美地穿过了所有训练数据，但会在测试数据上表现极差，发生了过拟合；<span class="math inline">\(M=3\)</span> 则刚刚好。过拟合现象其实挺讽刺的——<span class="math inline">\(M=9\)</span> 明明包含了 <span class="math inline">\(M=3\)</span>，它理应表现得至少不比后者差；另外，如果要用多项式无限逼近 <span class="math inline">\(\sin\)</span> 函数，甚至需要无限阶的多项式（泰勒级数），所以我们期待 <span class="math inline">\(M\)</span> 越大、模型效果越好才对。</p>
<p><span class="math inline">\(M=9\)</span> 的多项式有 10 个自由参数，所以刚好能拟合大小为 10 的训练集。那如果我们加大训练集的规模呢？下图展示了 15 和 100 个数据点下的结果：</p>
<p><img src="fig1.6.png" srcset="/blog-main/logo/imageloading.png" lazyload width=70% /></p>
<p>可以看见，随着训练集规模增大，过拟合现象得到了缓解。因此，一个启发式的经验是说，参数量应该比数据量少若干倍（如 5~10 倍）。但是，这样的解决方案其实不是很让人满意，我们更希望参数量与问题的复杂程度挂钩，而不是与数据量挂钩。<strong>在下文中，我们会看到上述最小二乘法的本质其实是极大似然估计，而过拟合是极大似然估计的一般属性。相反，贝叶斯方法可以避免过拟合问题。事实上，当参数量多于数据量时，贝叶斯模型能够自适应地调节有效参数量。</strong></p>
<p>另一个常见的解决过拟合的方案是<strong>正则化。</strong>如果我们考察不同 <span class="math inline">\(M\)</span> 下解出来的 <span class="math inline">\(\mathbf w\)</span> 数值，如下表所示：</p>
<p><img src="table1.1.png" srcset="/blog-main/logo/imageloading.png" lazyload width=70% /></p>
<p>可以看见，随着 <span class="math inline">\(M\)</span> 增大，高阶项的系数（绝对值）变得异常的大。为了惩罚过大的参数值，我们可以将参数的平方和加入误差函数，并用系数 <span class="math inline">\(\lambda\)</span> 调节其大小： <span class="math display">\[
\tilde E(\mathbf w)=\frac{1}{2}\sum_{n=1}^N(y(x_n,\mathbf w)-t_n)^2+\frac{\lambda}{2}\Vert\mathbf w\Vert^2
\]</span> 依旧取 <span class="math inline">\(M=9\)</span>，在合适的正则化参数下，拟合结果就平滑了许多。当然，如果正则化过分了，拟合得就不好了：</p>
<p><img src="fig1.7.png" srcset="/blog-main/logo/imageloading.png" lazyload width=70% /></p>
<p>在正则化的作用下，<span class="math inline">\(\mathbf w\)</span> 的数值得到了控制：</p>
<p><img src="table1.2.png" srcset="/blog-main/logo/imageloading.png" lazyload width=70% /></p>
<h2 id="probability-theory">Probability Theory</h2>
<p>基础的概率知识直接跳过，我们从贝叶斯说起。</p>
<h3 id="bayesian-probabilities">Bayesian probabilities</h3>
<p>经典频率学派将概率视作随机可重复事件发生的频率，而在贝叶斯视角下，概率是对不确定性（uncertainty）的度量。考虑一个不确定的事件，例如北极的冰川是否会在世纪末消失，这可不是一个可以重复试验的事情，但是我们依旧能对冰川的融化速度有大致的评估。如果我们现在有了一些新的观测，我们也许会更新之前的评估，进而调整我们的动作，例如减少温室气体的排放。这个过程可以用贝叶斯概率做定量地描述。</p>
<p>在上文多项式拟合的例子中，我们可以用概率来表示模型参数 <span class="math inline">\(\mathbf w\)</span> 的不确定性。我们首先为其指定一个先验概率（prior） <span class="math inline">\(p(\mathbf w)\)</span>，再设观察到的数据为 <span class="math inline">\(\mathcal D=\{t_1,\ldots,t_N\}\)</span>，那么数据在当前参数下的似然（likelihood）就是 <span class="math inline">\(p(\mathcal D\vert \mathbf w)\)</span>，于是，根据贝叶斯定理，我们可以计算后验概率（posterior）： <span class="math display">\[
p(\mathbf w\vert \mathcal D)=\frac{p(\mathcal D\vert \mathbf w)p(\mathbf w)}{p(\mathcal D)}
\]</span> 其中 <span class="math inline">\(p(\mathcal D)=\int p(\mathcal D\vert \mathbf w)p(\mathbf w)\mathrm d\mathbf w\)</span> 是归一化系数。后验概率说明了我们在观察到数据 <span class="math inline">\(\mathcal D\)</span> 之后对 <span class="math inline">\(\mathbf w\)</span> 的不确定性的更新。用先验、后验和似然的术语，贝叶斯定理可以表述为： <span class="math display">\[
\text{posterior}\propto\text{likelihood}\times\text{prior}
\]</span> 可见，在贝叶斯视角下，模型参数 <span class="math inline">\(\mathbf w\)</span> 具有不确定性、是个变量，服从一个概率分布。我们还能根据新的观察更新这个概率分布。相反，在频率学派视角下，<span class="math inline">\(\mathbf w\)</span> 是一个固定的、客观存在的参数，只不过需要我们去估计它。例如，让似然 <span class="math inline">\(p(\mathcal D\vert \mathbf w)\)</span> 最大的解就是一种估计，这种方法被称作极大似然估计（MLE）。</p>
<h3 id="the-gaussian-distribution">The Gaussian distribution</h3>
<p>上文说过，最小二乘法本质就是极大似然估计，为了建立二者的联系，我们先考虑另一个问题：用高斯分布为数据的分布建模。假设数据集为 <span class="math inline">\(N\)</span> 个（独立同分布）标量 <span class="math inline">\(\bsf x=(x_1,\ldots,x_N)^\mathrm T\)</span>，那么在高斯分布 <span class="math inline">\(\mathcal N(\mu,\sigma^2)\)</span> 下，数据的似然为： <span class="math display">\[
p(\bsf x\vert\mu,\sigma^2)=\prod_{n=1}^N \mathcal N(x_n\vert\mu,\sigma^2)
\]</span> 最大化似然等价于最小化负对数似然： <span class="math display">\[
-\ln p(\bsf x\vert\mu,\sigma^2)=\frac{1}{2\sigma^2}\sum_{n=1}^N(x_n-\mu)^2+\frac{N}{2}\ln\sigma^2+\frac{N}{2}\ln(2\pi)
\]</span> 求导取零，解得： <span class="math display">\[
\begin{cases}
\mu_{\text{ML}}=\frac{1}{N}\sum_{n=1}^N x_n\\
\sigma^2_{\text{ML}}=\frac{1}{N}\sum_{n=1}^N(x_n-\mu_{\text{ML}})^2
\end{cases}
\]</span> 即极大似然估计的均值和方差分别是样本均值和样本方差。然而，这样计算得到的方差是有偏的——它比真正的方差偏小。为了验证这一点，我们可以假设数据采样自 <span class="math inline">\(\mathcal N(\mu,\sigma^2)\)</span>，那么经过计算有： <span class="math display">\[
\mathbb E[\mu_\text{ML}]=\frac{1}{N}\sum_{n=1}^N\mathbb E[x_n]=\mu
\]</span></p>
<p><span class="math display">\[
\begin{align}
\mathbb E[\sigma_\text{ML}^2]&amp;=\frac{1}{N}\sum_{n=1}^N\mathbb E\left[\left(x_n-\frac{1}{N}\sum_{i=1}^N x_i\right)^2\right]\\
&amp;=\frac{1}{N}\sum_{n=1}^N\mathbb E\left[x_n^2-\frac{2}{N}x_n\left(\sum_{i=1}^Nx_i\right)+\frac{1}{N^2}\left(\sum_{i=1}^Nx_i\right)^2\right]\\
&amp;=\frac{1}{N}\sum_{n=1}^N\mathbb E[x_n^2]-\frac{1}{N^2}\mathbb E\left[\left(\sum_{i=1}^Nx_i\right)^2\right]\\
&amp;=\mu^2+\sigma^2-\frac{1}{N^2}\left(N(\mu^2+\sigma^2)+N(N-1)\mu^2\right)\\
&amp;=\frac{N-1}{N}\sigma^2
\end{align}
\]</span></p>
<blockquote>
<p>推导过程中用了高斯分布的二阶矩为 <span class="math inline">\(\mu^2+\sigma^2\)</span> 的结论。</p>
</blockquote>
<p>下图直观地描绘了这个问题：</p>
<p><img src="fig1.15.png" srcset="/blog-main/logo/imageloading.png" lazyload width=70% /></p>
<p>当 <span class="math inline">\(N\to\infty\)</span> 时，<span class="math inline">\(\sigma^2_\text{ML}\to\sigma^2\)</span>，这个偏差（bias）不会引起太大的问题。但是本书将会考虑有很多参数的复杂模型，这时偏差问题就会很严重。<strong>事实上，我们将看到，极大似然估计带来的偏差问题正是过拟合的根源所在</strong>。</p>
<h3 id="curve-fitting-re-visited">Curve fitting re-visited</h3>
<p>现在让我们回过头来，从概率角度重新审视多项式曲线拟合问题。我们为模型的预测值赋以不确定性，并用高斯分布来建模： <span class="math display">\[
p(t\vert x,\mathbf w,\beta)=\mathcal N(t\vert y(x,\mathbf w),\beta^{-1})
\]</span> <img src="fig1.16.png" srcset="/blog-main/logo/imageloading.png" lazyload width=70% /></p>
<p>于是乎，使用极大似然估计，我们有似然： <span class="math display">\[
p(\bsf t\vert \bsf x,\mathbf w,\beta)=\prod_{n=1}^N\mathcal N(t_n\vert x_n,\mathbf w,\beta^{-1})
\]</span> 负对数似然： <span class="math display">\[
-\ln p(\bsf t\vert\bsf x,\mathbf w,\beta)=\frac{\beta}{2}\sum_{n=1}^N(y(x_n,\mathbf w)-t_n)^2-\frac{N}{2}\ln\beta+\frac{N}{2}\ln(2\pi)
\]</span> 为解 <span class="math inline">\(\mathbf w_\text{ML}\)</span>，由于后两项与 <span class="math inline">\(\mathbf w\)</span> 无关，所以可以直接丢掉；第一项的系数也与 <span class="math inline">\(\mathbf w\)</span> 无关，可以换成 <span class="math inline">\(\frac{1}{2}\)</span>——那么我们就得到了第一节中的平方和误差函数。<strong>因此，最小二乘法就是在高斯分布假设下的极大似然估计。</strong></p>
<p>当然，上式还有另一个参数 <span class="math inline">\(\beta\)</span>，根据上一小节的结论，其解为： <span class="math display">\[
\frac{1}{\beta_\text{ML}}=\frac{1}{N}\sum_{n=1}^N(y(x_n,\mathbf w)-t_n)^2
\]</span> 有了 <span class="math inline">\(\mathbf w_\text{ML}\)</span> 和 <span class="math inline">\(\beta_\text{ML}\)</span>，我们在预测时就不只是给出一个点，而是一个概率分布了： <span class="math display">\[
p(t\vert x,\mathbf w_\text{ML},\beta_\text{ML})=\mathcal N(t\vert y(x,\mathbf w_\text{ML}),\beta_\text{ML}^{-1})
\]</span> 虽然我们已经从点估计跃升为了预测概率分布，但这还不是贝叶斯，毕竟极大似然估计依旧在频率学派的范畴。前文提及，贝叶斯视角下的模型参数 <span class="math inline">\(\mathbf w\)</span> 也由概率分布描述。为简便起见，我们将先验分布设为以 <span class="math inline">\(\alpha\)</span> 为参数的多元高斯分布： <span class="math display">\[
p(\mathbf w\vert\alpha)=\mathcal N(\mathbf w\vert \mathbf 0,\alpha^{-1}\mathbf I)=\left(\frac{\alpha}{2\pi}\right)^{(M+1)/2}\exp\left(-\frac{\alpha}{2}\mathbf w^\mathrm T\mathbf w\right)
\]</span> 注意 <span class="math inline">\(\mathbf w\)</span> 有 <span class="math inline">\(M+1\)</span> 维而不是 <span class="math inline">\(M\)</span> 维。这里的 <span class="math inline">\(\alpha\)</span> 是人为设置的参数，即超参数。那么根据贝叶斯定理，我们有： <span class="math display">\[
\underbrace{p(\mathbf w\vert\bsf x,\bsf t,\alpha,\beta)}_\text{posterior}\propto \underbrace{p(\bsf t\vert\bsf x,\mathbf w,\beta)}_\text{likelihood}\ \underbrace{p(\mathbf w\vert\alpha)}_\text{prior}
\]</span> 现在，我们可以求出一个使后验分布最大的 <span class="math inline">\(\mathbf w^\ast\)</span>，这被称为最大后验估计（MAP）： <span class="math display">\[
\begin{align}
\mathbf w^\ast&amp;=\mathop{\text{argmax}}_{\mathbf w}\ p(\mathbf w\vert \bsf x,\bsf t,\alpha,\beta)\\
&amp;=\mathop{\text{argmax}}_{\mathbf w}\ p(\bsf t\vert\bsf x,\mathbf w,\beta)p(\mathbf w\vert \alpha)\\
&amp;=\mathop{\text{argmax}}_{\mathbf w}\ \ln p(\bsf t\vert\bsf x,\mathbf w,\beta)+\ln p(\mathbf w\vert\alpha)\\
&amp;=\mathop{\text{argmin}}_{\mathbf w}\ \frac{\beta}{2}(y(x_n,\mathbf w)-t_n)^2+\frac{\alpha}{2}\mathbf w^\mathrm T\mathbf w
\end{align}
\]</span> 可以看到，第一项就是极大似然估计（最小二乘法）的优化目标，而第二项就是正则项。所以说，<strong>最大后验估计等价于极大似然估计加上一个与先验分布有关的正则项</strong>。</p>
<h3 id="bayesian-curve-fitting">Bayesian curve fitting</h3>
<p>虽然我们现在用先验、后验概率分布来描述参数 <span class="math inline">\(\mathbf w\)</span>，但是 MAP 给出的是点估计，所以依旧不能算做是完全的贝叶斯。<strong>完全的贝叶斯方法要求一贯使用概率的 sum rule 和 product rule 推导（而不是推一半突然取个 argmax），这往往需要我们对所有的 <span class="math inline">\(\mathbf w\)</span> 积分</strong>。不幸的是，这个积分不总是容易计算的，如何计算、估计或绕开这个积分成为了很多研究的关注点。</p>
<blockquote>
<p>本书中，sum rule 指的是 <span class="math inline">\(p(X)=\sum_Y p(X,Y)\)</span>；product rule 指的是 <span class="math inline">\(p(X,Y)=p(Y\vert X)p(X)\)</span>.</p>
</blockquote>
<p>在多项式曲线拟合问题中，我们已知的是 <span class="math inline">\(\bsf x,\bsf t\)</span>，目标是给定 <span class="math inline">\(x\)</span>，预测对应的 <span class="math inline">\(t\)</span>，因此我们希望求的是 <span class="math inline">\(p(t\vert x,\bsf x,\bsf t)\)</span>. 这里，我们假设 <span class="math inline">\(\alpha,\beta\)</span> 是固定已知的。</p>
<p>把 <span class="math inline">\(\bsf x,\bsf t\)</span> 和 <span class="math inline">\(x,t\)</span> 联系起来的是我们的回归模型，因此： <span class="math display">\[
p(t\vert x,\bsf x,\bsf t)=\int p(t\vert x,\mathbf w)p(\mathbf w\vert \bsf x,\bsf t)\mathrm d\mathbf w
\]</span> 右式中，<span class="math inline">\(p(t\vert x,\mathbf w)\)</span> 即模型预测的概率分布，定义在上一小节的最开始处；而 <span class="math inline">\(p(\mathbf w\vert\bsf x,\bsf t)\)</span> 是参数的后验分布，由贝叶斯公式计算得到： <span class="math display">\[
p(\mathbf w\vert\bsf x,\bsf t)=\frac{p(\bsf t\vert\bsf x,\mathbf w,\beta)p(\mathbf w\vert\alpha)}{p(\bsf t\vert\bsf x)}=\frac{p(\bsf t\vert\bsf x,\mathbf w,\beta)p(\mathbf w\vert\alpha)}{\int p(\bsf t\vert\bsf x,\mathbf w,\beta)p(\mathbf w\vert\alpha)\mathrm d\mathbf w}
\]</span> 分子中的似然和先验都在上一小节有所定义；而分母出现了棘手的对 <span class="math inline">\(\mathbf w\)</span> 的积分。幸运的是，在多项式曲线拟合问题中，所有的积分都可以计算出解析形式，因此最后我们能得到解析解。具体解的过程和结果此处略去，相关内容会在书的第三章详细阐述。结果可以绘制为下图：</p>
<p><img src="fig1.17.png" srcset="/blog-main/logo/imageloading.png" lazyload width=80% /></p>
<h2 id="the-curse-of-dimensionality">The Curse of Dimensionality</h2>
<p>多项式曲线拟合问题只有一个输入变量 <span class="math inline">\(x\)</span>，但是很多实际问题会涉及到更多的变量，这时我们会遇到维度灾难。书中举了这样的一个例子：每条数据有 12 个属性，即由一个 12 维向量表示，共分为 3 类。其中 <span class="math inline">\((x_6,x_7)\)</span> 这两维的特征可以可视化为下图：</p>
<p><img src="fig1.19.png" srcset="/blog-main/logo/imageloading.png" lazyload width=70% /></p>
<p>如果我们想对 × 做分类，一个简单的做法是把空间分成若干小格，每一个格子的类别定义为落在其中的点的大多数类别；那么询问一个新的数据点时，我们看它落在哪一个格子里即可，如下图所示：</p>
<p><img src="fig1.20.png" srcset="/blog-main/logo/imageloading.png" lazyload width=70% /></p>
<p>当然，这个方法比较 naive，存在很多问题，但是最重要的问题之一就是维度灾难。当维度从 2 维上升到更高维时，用来划分高维空间的格子数量将呈指数增长，那么，为了让每个格子里有足够多的数据点，所需要的数据数量也就随之呈指数增加。如下图所示：</p>
<p><img src="fig1.21.png" srcset="/blog-main/logo/imageloading.png" lazyload width=70% /></p>
<p>我们也可以从多项式拟合问题里看到维度灾难。假设我们有 <span class="math inline">\(D\)</span> 个输入变量，那么一个 3 阶多项式将长这样： <span class="math display">\[
y(\mathbf x,\mathbf w)=w_0+\sum_{i=1}^D w_ix_i+\sum_{i=1}^D\sum_{j=1}^D w_{ij}x_ix_j+\sum_{i=1}^D\sum_{j=1}^D\sum_{k=1}^D w_{ijk}x_ix_jx_k
\]</span> 也就是说，<span class="math inline">\(M\)</span> 阶多项式的参数数量将变成 <span class="math inline">\(O(D^M)\)</span>. 虽然这是幂增长而非指数增长，但依旧增长得很快，使得模型变得笨重而难以实用。</p>
<p>当维度变高后，很多低维空间下的直觉将变得不再正确。例如，设有 <span class="math inline">\(D\)</span> 维空间下的一个单位超球体，考虑位于半径 <span class="math inline">\(r=1-\epsilon\)</span> 到 <span class="math inline">\(r=1\)</span> 之间部分的（相对）体积： <span class="math display">\[
\frac{V_D(1)-V_D(1-\epsilon)}{V_D(1)}=1-(1-\epsilon)^D
\]</span> 当 <span class="math inline">\(D\)</span> 很大时，即便 <span class="math inline">\(\epsilon\)</span> 较小，这个比例依旧会接近 <span class="math inline">\(1\)</span>，可以做图以直观展示：</p>
<p><img src="fig1.22.png" srcset="/blog-main/logo/imageloading.png" lazyload width=70% /></p>
<p>这意味着，高维空间中的一个超球体，其大部分体积都集中在接近表面的薄薄的一层上！</p>
<p>高维高斯分布也有类似的情况。在极坐标下，做出 <span class="math inline">\(p(r)\)</span> 关于 <span class="math inline">\(r\)</span> 的图，可以看见大部分概率密度集中在某一个特定 <span class="math inline">\(r\)</span> 的附近：</p>
<p><img src="fig1.23.png" srcset="/blog-main/logo/imageloading.png" lazyload width=70% /></p>
<p>虽然如此，在实践中我们依旧能够有效地处理高维数据，原因有两点：</p>
<ol type="1">
<li>真实数据往往处于低维的子空间/流形上；</li>
<li>真实数据往往有一定的光滑性，即输入变量的微小变化会引起目标变量的微小变化，因此我们可以用插值等方式对新的输入做预测。</li>
</ol>
<h2 id="decision-theory">Decision Theory</h2>
<p>本节我们围绕一个非常经典的例子——癌症诊断展开：输入一张 X 光片 <span class="math inline">\(\mathbf x\)</span>，决定病人是否患有癌症。这是一个分类问题，我们用类别 <span class="math inline">\(\mathcal C_1\)</span> 或 <span class="math inline">\(t=0\)</span> 表示患癌，类别 <span class="math inline">\(\mathcal C_2\)</span> 或 <span class="math inline">\(t=1\)</span> 表示健康。</p>
<p>使用第二节概率论的方法，我们能够推断出一些概率分布，其中核心是联合概率 <span class="math inline">\(p(\mathbf x,\mathcal C_k)\)</span>（条件概率或边缘概率都可以通过相关公式由联合概率推出来）。给定这些概率，我们如何决定病人是否真的患有癌症呢？这就是决策论要解决的问题。</p>
<p>直觉上，我们会选取 <span class="math inline">\(p(\mathcal C_k\vert \mathbf x)\)</span> 最大的那一类 <span class="math inline">\(k\)</span>. 的确，我们稍后会看到这确实是让错误率最小的方法。然而，如果我们的目标不是最小化错误率，那么这个决策可能会发生变化。</p>
<h3 id="minimizing-the-misclassification-rate">Minimizing the misclassification rate</h3>
<p>对于一种决策，设区域 <span class="math inline">\(\mathcal R_k\)</span> 内的数据点会被分类为 <span class="math inline">\(C_k\)</span>，这些决策区域的边界被称作“决策边界”。假设我们的目标是最小化错误分类率，那么在癌症诊断的例子中，错误率为： <span class="math display">\[
\begin{align}
p(\text{mistake})&amp;=p(\mathbf x\in \mathcal R_1,\mathcal C_2)+p(\mathbf x\in \mathcal R_2,\mathcal C_1)\\
&amp;=\int_{\mathcal R_1} p(\mathbf x, \mathcal C_2)\mathrm d\mathbf x+\int_{\mathcal R_2} p(\mathbf x, \mathcal C_1)\mathrm d\mathbf x
\end{align}
\]</span> 因此，要让错误率最小，如果 <span class="math inline">\(p(\mathbf x,\mathcal C_1)&gt;p(\mathbf x,\mathcal C_2)\)</span>，我们就应该把 <span class="math inline">\(\mathbf x\)</span> 分类为 <span class="math inline">\(\mathcal C_1\)</span>.</p>
<p>拓展到 <span class="math inline">\(K\)</span> 分类： <span class="math display">\[
p(\text{correct})=\sum_{k=1}^K p(\mathbf x\in\mathcal R_k,\mathcal C_k)=\sum_{k=1}^K\int_{\mathcal R_k}p(\mathbf x,\mathcal C_k)\mathrm d\mathbf x
\]</span> 要让正确率最大，我们的决策就是把数据点 <span class="math inline">\(\mathbf x\)</span> 分类给 <span class="math inline">\(p(\mathbf x,\mathcal C_k)\)</span> 最大的那个 <span class="math inline">\(k\)</span>. 考虑到 <span class="math inline">\(p(\mathbf x,\mathcal C_k)=p(C_k\vert\mathbf x)p(\mathbf x)\)</span>，这个决策等价于把 <span class="math inline">\(\mathbf x\)</span> 分类给具有最大后验概率 <span class="math inline">\(p(\mathcal C_k\vert\mathbf x)\)</span> 的 <span class="math inline">\(k\)</span>，正如上文所言。</p>
<h3 id="minimizing-the-expected-loss">Minimizing the expected loss</h3>
<p>很多应用场景中，我们的目标比单单最小化错误率要复杂。例如癌症诊断，如果把一个健康的人诊断为患癌，那么大不了再多做点检测；但如果把一个患癌的人诊断为健康，那就要对他/她的生命负责了。虽然两种情况都是误诊，但后者的后果更为严重。为此，我们可以定义一个 loss matrix <span class="math inline">\(L\)</span>，<span class="math inline">\(L_{kj}\)</span> 表示真实类别为 <span class="math inline">\(k\)</span> 但是预测类别为 <span class="math inline">\(j\)</span> 的损失值，并试图最小化损失的期望： <span class="math display">\[
\mathbb E[L]=\sum_k\sum_j\int_{\mathcal R_j} L_{kj}p(\mathbf x,\mathcal C_k)\mathrm d\mathbf x
\]</span> 那么要使上式最小，我们的决策是把 <span class="math inline">\(\mathbf x\)</span> 分类给使得 <span class="math inline">\(\sum_k L_{kj}p(\mathbf x,\mathcal C_k)\)</span> 最小的 <span class="math inline">\(j\)</span>；或等价地，使得 <span class="math display">\[
\sum_kL_{kj}p(\mathcal C_k\vert\mathbf x)
\]</span> 最小的 <span class="math inline">\(j\)</span>.</p>
<p>例如，在癌症诊断的例子中，我们可以定义 <span class="math inline">\(L=\begin{bmatrix}0&amp;1000\\1&amp;0\end{bmatrix}\)</span>，让患癌但漏诊的损失非常大，从而减小这种情况的发生。</p>
<h3 id="the-reject-option">The reject option</h3>
<p>当最大的 <span class="math inline">\(p(\mathcal C_k\vert \mathbf x)\)</span> 都比 <span class="math inline">\(1\)</span> 小很多时，我们对 <span class="math inline">\(\mathbf x\)</span> 的类别预测有很大的不确定性，这个时候不如拒绝为其分类。我们可以设置一个阈值 <span class="math inline">\(\theta\)</span>，仅当 <span class="math inline">\(\max_k p(\mathcal C_k\vert \mathbf x)&gt;\theta\)</span> 时做出分类决策，反之拒绝分类（例如交给人工检测是否患癌）。</p>
<h3 id="inference-and-decision">Inference and decision</h3>
<p>通过上文的叙述，我们看到一个分类问题被划分为了两个阶段——先推断（inference），再决策（decision）。在推断阶段，我们用概率论方法获得 <span class="math inline">\(p(\mathbf x,\mathcal C_k)\)</span> 或 <span class="math inline">\(p(\mathcal C_k\vert \mathbf x)\)</span>，然后在决策阶段根据已得的概率值决定分类结果。另一种方法是一步到位——直接学习一个函数将输入 <span class="math inline">\(\mathbf x\)</span> 映射到决策，这样的函数被称作 discriminant function.</p>
<p>事实上，我们有三种解决决策问题的方案：</p>
<ol type="1">
<li><p><strong>Generative models</strong>：首先对每个 <span class="math inline">\(\mathcal C_k\)</span> 推断 <span class="math inline">\(p(\mathbf x\vert\mathcal C_k)\)</span> 和 <span class="math inline">\(p(\mathcal C_k)\)</span>，然后运用贝叶斯定理： <span class="math display">\[
p(\mathcal C_k\vert\mathbf x)=\frac{p(\mathbf x\vert\mathcal C_k)p(\mathcal C_k)}{p(\mathbf x)}=\frac{p(\mathbf x\vert\mathcal C_k)p(\mathcal C_k)}{\sum_kp(\mathbf x\vert\mathcal C_k)p(\mathcal C_k)}
\]</span> 计算后验概率 <span class="math inline">\(p(\mathcal C_k\vert \mathbf x)\)</span>. 等价地，也可以先推断出联合分布 <span class="math inline">\(p(\mathbf x,\mathcal C_k)\)</span>，然后归一化得到后验概率。如此建模的模型被称作生成模型，因为我们可以从 <span class="math inline">\(p(\mathbf x\vert\mathcal C_k)\)</span> 中采样生成合成数据。</p></li>
<li><p><strong>Discriminative models</strong>：直接推断后验概率 <span class="math inline">\(p(\mathcal C_k\vert\mathbf x)\)</span>，然后依其做决策。如此建模的模型被称作判别模型。</p></li>
<li><p><strong>Discriminant function</strong>：将输入 <span class="math inline">\(\mathbf x\)</span> 直接映射到其类别标签 <span class="math inline">\(f(\mathbf x)\)</span>，跳过所有概率。</p></li>
</ol>
<p>对生成模型而言，在实际应用中 <span class="math inline">\(\mathbf x\)</span> 的维度常常很高，需要大量的数据来足够精确地估计 <span class="math inline">\(p(\mathbf x\vert\mathcal C_k)\)</span>. 而先验分布 <span class="math inline">\(p(\mathcal C_k)\)</span> 可以通过统计训练集获得。其优势在于能够计算边缘分布 <span class="math inline">\(p(\mathbf x)\)</span>，在离群点检测（outlier detection）等方面有所应用。然而，如果只为了解决分类问题，那么没有必要费劲建模 <span class="math inline">\(p(\mathbf x\vert\mathcal C_k)\)</span> 或联合分布，直接用判别模型建模 <span class="math inline">\(p(\mathcal C_k\vert\mathbf x)\)</span> 即可。</p>
<p>第三种方案抛弃了后验概率，但我们认为后验概率在很多时候还是很有用的：</p>
<ul>
<li><p><strong>Minimizing risk</strong>. 如果一个问题的 loss matrix 会随时间不断变化（在金融中很常见），那么有后验概率我们可以随时调整决策，但如果只有 discriminant function，每次变化就要重新训练一遍；</p></li>
<li><p><strong>Reject option</strong>. 根据后验概率的大小，我们可以拒绝分类，如上一小节所述；</p></li>
<li><p><strong>Compensating for class priors</strong>. 很多分类问题面临类别不平衡问题，比如癌症的 X 光片数量远少于健康的 X 光片数量。这时要训练一个好的分类器是很困难的，因为就算分类器无论输入是什么都输出健康，那它的正确率也非常高。可行的解决方案是人为构造一个类别平衡的数据集（如在多类中只采样和少类一样多的样本），在上面训练模型。但由于我们更改了数据类别的分布，所以回归实际应用时应该做相应的补偿。具体而言，假设原本数据的类别分布为 <span class="math inline">\(p(\mathcal C_k)\)</span>，更改后为 <span class="math inline">\(\tilde p(\mathcal C_k)\)</span>，在更改后的数据上训练的模型为 <span class="math inline">\(\tilde p(\mathcal C_k\vert\mathbf x)\)</span>，根据贝叶斯定理： <span class="math display">\[
\tilde p(\mathcal C_k\vert\mathbf x)\propto \tilde p(\mathcal C_k)p(\mathbf x\vert\mathcal C_k)
\]</span> 注意类别条件分布 <span class="math inline">\(p(\mathbf x\vert\mathcal C_k)\)</span> 是不会因为我们对数据集的更改而变化的，所以： <span class="math display">\[
p(\mathcal C_k\vert\mathbf x)\propto \tilde p(\mathcal C_k\vert\mathbf x)\cdot\frac{p(\mathcal C_k)}{\tilde p(\mathcal C_k)}
\]</span> 这样就解决了类别不平衡问题。如果我们没有后验概率，就无法完成这样的操作。</p></li>
<li><p><strong>Combining models</strong>. 对于复杂的应用，我们也许会在不同的特征上训练多个模型，这时我们能够依据它们的后验概率合并它们的输出。例如，在癌症诊断的例子中，假设除了 X 光片 <span class="math inline">\(\mathbf x_\text{I}\)</span>，我们还有血液样本 <span class="math inline">\(\mathbf x_\text{B}\)</span>，并且二者是条件独立的： <span class="math display">\[
p(\mathbf x_\text{I},\mathbf x_\text{B}\vert\mathcal C_k)=p(\mathbf x_\text{I}\vert\mathcal C_k)p(\mathbf x_\text{B}\vert\mathcal C_k)
\]</span> 那么： <span class="math display">\[
\begin{align}
p(\mathcal C_k\vert \mathbf x_\text{I},\mathbf x_\text{B})&amp;\propto p(\mathbf x_\text{I},\mathbf x_\text{B}\vert \mathcal C_k)p(\mathcal C_k)\\
&amp;\propto p(\mathbf x_\text{I}\vert\mathcal C_k)p(\mathbf x_\text{B}\vert\mathcal C_k)p(\mathcal C_k)\\
&amp;\propto \frac{p(\mathcal C_k\vert \mathbf x_\text{I})p(\mathcal C_k\vert \mathbf x_\text{B})}{p(\mathcal C_k)}
\end{align}
\]</span> 其中，条件独立假设的引入就是朴素贝叶斯模型的思想。</p></li>
</ul>
<h3 id="loss-functions-for-regression">Loss functions for regression</h3>
<p>这一节前面一直在讨论分类模型，其实回归模型也有类似的决策阶段。仍然以多项式拟合问题为例，在推断阶段我们已经计算了联合概率分布 <span class="math inline">\(p(\mathbf x,t)\)</span>，那么在决策阶段我们要为每个 <span class="math inline">\(\mathbf x\)</span> 确定其 <span class="math inline">\(y(\mathbf x)\)</span>，使得期望损失最小： <span class="math display">\[
\mathbb E[L]=\iint L(t,y(\mathbf x))p(\mathbf x,t)\mathrm d\mathbf x\mathrm d t
\]</span> 如果用平方误差作为损失函数，优化目标就是： <span class="math display">\[
\mathbb E[L]=\iint (y(\mathbf x)-t)^2p(\mathbf x,t)\mathrm d\mathbf x\mathrm d t
\]</span> 由于要优化的变量是函数 <span class="math inline">\(y(\mathbf x)\)</span>，所以可以把 <span class="math inline">\(\mathbb E[L]\)</span> 视为 <span class="math inline">\(y(\mathbf x)\)</span> 的泛函，运用变分法： <span class="math display">\[
\frac{\delta \mathbb E[L]}{\delta y(\mathbf x)}=2\int (y(\mathbf x)-t)p(\mathbf x,t)\mathrm dt=0
\]</span> 解得： <span class="math display">\[
y(\mathbf x)=\frac{\int tp(\mathbf x,t)\mathrm dt}{p(\mathbf x)}=\int t p(\mathrm t\vert \mathbf x)\mathrm dt=\mathbb E[t\vert\mathbf x]
\]</span> 即以 <span class="math inline">\(\mathbf x\)</span> 为条件下的均值。</p>
<p>当然，平方误差并不是唯一的损失函数的选择，如果用绝对值误差，那么解就是 <span class="math inline">\(\mathbf x\)</span> 条件下的中位数……</p>
<p>同分类问题一样，对于回归问题我们也有生成模型、判别模型和判别函数三种解决问题的方案，且有着同样的优缺点。</p>
<h2 id="information-theory">Information Theory</h2>
<p>信息论也是模式识别和机器学习中的重要工具。设有一个离散随机变量 <span class="math inline">\(x\)</span>，如果我们观测到了一个不太可能发生的事件，那么它带给我们的信息量是巨大的；相反，如果我们观测到一个一定会发生的事件，那我们也没有获取到什么信息。因此，信息量 <span class="math inline">\(h(x)\)</span> 应该与 <span class="math inline">\(p(x)\)</span> 有关。对于两个独立事件 <span class="math inline">\(x,y\)</span>，我们希望它们同时被观测到的信息量是二者信息量之和：<span class="math inline">\(h(x,y)=h(x)+h(y)\)</span>. 而考虑到此时 <span class="math inline">\(p(x,y)=p(x)p(y)\)</span>，所以一个自然的选择是取信息量为 <span class="math inline">\(p(x)\)</span> 的对数形式： <span class="math display">\[
h(x)=-\log_2 p(x)
\]</span> 负号是为了让 <span class="math inline">\(h(x)\geq 0\)</span>. 当对数底数为 <span class="math inline">\(2\)</span> 时，信息量的单位为比特（bits, binary digits）。</p>
<p>对于随机变量 <span class="math inline">\(x\)</span>，其平均信息量就是： <span class="math display">\[
H[x]=-\sum_xp(x)\log_2 p(x)
\]</span> 这被称作随机变量 <span class="math inline">\(x\)</span> 的熵（entropy）。</p>
<p>上述信息量和熵的定义方式显得非常“启发式”，给人不够严谨的感觉。事实上，熵最早来源于物理学中的热力学，并被用来表述一个系统的混乱程度。我们可以考虑将 <span class="math inline">\(N\)</span> 个相同物体分到若干个桶内，使得第 <span class="math inline">\(i\)</span> 个桶有 <span class="math inline">\(n_i\)</span> 个物体。这是个经典的计数问题，总方案数为： <span class="math display">\[
W=\frac{N!}{\prod_i n_i!}
\]</span> 那么熵 <span class="math inline">\(H\)</span> 被定义为其自然对数乘上一个缩放因子： <span class="math display">\[
H=\frac{1}{N}\ln W=\frac{1}{N}\ln N!-\frac{1}{N}\sum_{i}\ln n_i!
\]</span> 考虑取 <span class="math inline">\(N\to\infty\)</span>，且保证 <span class="math inline">\(p_i=n_i/N\)</span> 不变，根据 Stirling 近似，有： <span class="math display">\[
\ln N!\simeq N\ln N-N
\]</span> 于是： <span class="math display">\[
\begin{align}
H&amp;=\lim_{N\to\infty}\left(\ln N-1-\frac{1}{N}\sum_i\left(n_i\ln n_i-n_i\right)\right)\\
&amp;=\lim_{N\to\infty}\sum_i\left(\frac{n_i}{N}\ln N-\frac{n_i}{N}-\frac{1}{N}(n_i\ln n_i-n_i)\right)\\
&amp;=-\lim_{N\to\infty}\sum_i\left(\frac{n_i}{N}\ln\frac{N}{n_i}\right)\\
&amp;=-\sum_i p_i\ln p_i
\end{align}
\]</span> 得到了和之前类似的定义。</p>
<p>显然，熵的最小值为 <span class="math inline">\(0\)</span>，当某个 <span class="math inline">\(p_i=1\)</span> 并且 <span class="math inline">\(p_{j\neq i}=0\)</span> 时取到。要求熵的最大值，我们可以利用拉格朗日乘数法，定义拉格朗日函数： <span class="math display">\[
-\sum_i p_i\ln p_i+\lambda\left(\sum_i p_i-1\right)
\]</span> 求导取零，解得<strong>熵的最大值为 <span class="math inline">\(\ln M\)</span>，当且仅当 <span class="math inline">\(p_i=\frac{1}{M},\,\forall i\)</span> 时取到</strong>。其中 <span class="math inline">\(M\)</span> 是 <span class="math inline">\(x\)</span> 可能的状态数（桶的数量）。</p>
<p>上面都是离散情形。对于连续分布 <span class="math inline">\(p(x)\)</span>，我们按如下方式推导。首先将 <span class="math inline">\(x\)</span> 划分为宽度为 <span class="math inline">\(\Delta\)</span> 的若干桶，那么中值定理告诉我们，对于每个桶，存在一个 <span class="math inline">\(x_i\)</span> 使得： <span class="math display">\[
\int_{i\Delta}^{(i+1)\Delta} p(x)\mathrm dx=p(x_i)\Delta
\]</span> 于是，我们可以把连续变量 <span class="math inline">\(x\)</span> 量化到各个桶的 <span class="math inline">\(x_i\)</span> 上，那么观察到 <span class="math inline">\(x_i\)</span> 概率就是 <span class="math inline">\(p(x_i)\Delta\)</span>. 因此，熵为： <span class="math display">\[
\begin{align}
H_\Delta&amp;=-\sum_ip(x_i)\Delta\ln (p(x_i)\Delta)\\
&amp;=-\sum_ip(x_i)\Delta\ln p(x_i)-\sum_ip(x_i)\Delta\ln\Delta\\
&amp;=-\sum_ip(x_i)\Delta\ln p(x_i)-\ln\Delta
\end{align}
\]</span> 当 <span class="math inline">\(\Delta\to0\)</span> 时，第一项： <span class="math display">\[
\lim_{\Delta\to0}-\sum_i p(x_i)\Delta \ln p(x_i)=-\int p(x)\ln p(x)\mathrm dx
\]</span> 称为微分熵（differential entropy）。<strong>注意离散情形下的熵和连续情形下的微分熵相差了一个 <span class="math inline">\(\ln \Delta\)</span>，而当 <span class="math inline">\(\Delta\to0\)</span> 时它是发散的，二者并不是等价的</strong>。</p>
<p>前文证明了，离散情形下的熵在均匀类别分布下取到最大，那么连续情形下也是如此吗？事实上这取决于约束条件<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="为什么熵值最大的分布状态是正态分布而不是均匀分布？ - 椎名的回答 - 知乎 https://www.zhihu.com/question/357032828/answer/907586249">[1]</span></a></sup>。如果我们约束分布的均值为 <span class="math inline">\(\mu\)</span>，方差为 <span class="math inline">\(\sigma^2\)</span>，结合归一化条件，那么我们有三个约束条件： <span class="math display">\[
\begin{align}
&amp;\int_{-\infty}^{+\infty}p(x)\mathrm dx=1\\
&amp;\int_{-\infty}^{+\infty}xp(x)\mathrm dx=\mu\\
&amp;\int_{-\infty}^{+\infty}(x-\mu)^2p(x)\mathrm dx=\sigma^2
\end{align}
\]</span> 运用拉格朗日乘数法和变分法，最终可解得最大值在正态分布时取到： <span class="math display">\[
p(x)=\frac{1}{(2\pi\sigma^2)^{1/2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
\]</span> 且最大值为： <span class="math display">\[
\begin{align}
H[x]&amp;=-\int p(x)\ln p(x)\mathrm dx\\
&amp;=\int p(x)\left(\frac{1}{2}\ln(2\pi\sigma^2)+\frac{(x-\mu)^2}{2\sigma^2}\right)\mathrm dx\\
&amp;=\frac{1}{2}\ln(2\pi\sigma^2)+\int\frac{(x-\mu)^2}{2\sigma^2}p(x)\mathrm dx\\
&amp;=\frac{1}{2}\ln(2\pi\sigma^2)+\frac{1}{2\sigma^2}\sigma^2\\
&amp;=\frac{1}{2}\left(1+\ln(2\pi\sigma^2)\right)
\end{align}
\]</span> <strong>可以看见，不同于离散情形的熵，微分熵可以是负的</strong>。</p>
<p>如果我们将约束条件更改为在支撑集 <span class="math inline">\([a,b]\)</span> 上，即： <span class="math display">\[
\int_a^bp(x)\mathrm dx=1
\]</span> 那么运用拉格朗日乘数法和变分法，解得最大值在均匀分布时取到： <span class="math display">\[
p(x)=\dfrac{1}{b-a}\mathbf 1_{[a,b]}(x)
\]</span> 如果我们有两个随机变量 <span class="math inline">\(\mathbf x,\mathbf y\)</span>，那么在 <span class="math inline">\(\mathbf x\)</span> 的条件下，<span class="math inline">\(\mathbf y\)</span> 的条件熵（conditional entropy）定义为： <span class="math display">\[
H[\mathbf y\vert\mathbf x]=-\iint p(\mathbf y\vert\mathbf x)\ln p(\mathbf y\vert\mathbf x)\mathrm d\mathbf y\mathrm d\mathbf x
\]</span> 容易证明： <span class="math display">\[
H[\mathbf x,\mathbf y]=H[\mathbf y\vert\mathbf x]+H[\mathbf x]
\]</span></p>
<h3 id="relative-entropy-and-mutual-information">Relative entropy and mutual information</h3>
<p>在机器学习中，一个常见的需求是用一个分布 <span class="math inline">\(q(\mathbf x)\)</span> 为另一个分布 <span class="math inline">\(p(\mathbf x)\)</span> 建模. 由于二者并不一定完全相同，所以用 <span class="math inline">\(q(\mathbf x)\)</span> 的平均信息量就比原本的熵多出了： <span class="math display">\[
\begin{align}
\text{KL}(p\Vert q)&amp;=-\int p(\mathbf x)\ln q(\mathbf x)\mathrm dx-\left(-\int p(\mathbf x)\ln p(\mathbf x)\mathrm d\mathbf x\right)\\
&amp;=-\int p(\mathbf x)\ln\frac{q(\mathbf x)}{p(\mathbf x)}\mathrm d\mathbf x
\end{align}
\]</span> 这就是相对熵或 KL 散度。KL 散度并不对称，即 <span class="math inline">\(\text{KL}(p\Vert q)\not\equiv\text{KL}(q\Vert p)\)</span>.</p>
<p>我们可以用琴生不等式证明 KL 散度非负，并且当且仅当 <span class="math inline">\(p(\mathbf x)=q(\mathbf x)\)</span> 时取等： <span class="math display">\[
\text{KL}(p\Vert q)=-\int p(\mathbf x)\ln\frac{q(\mathbf x)}{p(\mathbf x)}\mathrm d\mathbf x\geq-\ln\left(\int p(\mathbf x)\frac{q(\mathbf x)}{p(\mathbf x)}\mathrm d\mathbf x\right)=0
\]</span> 因此 KL 散度被视作两个分布之间的距离。</p>
<p>在应用中，我们常常用一个参数化概率分布 <span class="math inline">\(q(\mathbf x\vert \theta)\)</span>（例如混合高斯）来近似一个未知的数据分布 <span class="math inline">\(p(\mathbf x)\)</span>. 一个自然的想法就是以最小化二者的 KL 散度为目标来优化 <span class="math inline">\(\theta\)</span>. 然而，由于我们不知道 <span class="math inline">\(p(\mathbf x)\)</span> 的形式，所以无法直接计算 KL 散度。一般而言，我们有的是从 <span class="math inline">\(p(\mathbf x)\)</span> 中采样的数据集 <span class="math inline">\(\{\mathbf x_1,\ldots,\mathbf x_N\}\)</span>. 因此，通过采样近似期望，有： <span class="math display">\[
\text{KL}(p\Vert q)\simeq \sum_{n=1}^N\left[-\ln q(\mathbf x_n\vert\theta)+\ln p(\mathbf x_n)\right]
\]</span></p>
<p>第二项与 <span class="math inline">\(\theta\)</span> 无关，第一项是 <span class="math inline">\(q(\mathbf x\vert\theta)\)</span> 下的负对数似然。因此，<strong>最小化 KL 散度等最大化似然函数</strong>。</p>
<p>现在考虑两个随机变量 <span class="math inline">\(\mathbf x,\mathbf y\)</span>，如果二者相互独立，那么 <span class="math inline">\(p(\mathbf x)p(\mathbf y)=p(\mathbf x,\mathbf y)\)</span>. 如果它们不是独立的，我们希望知道它们有多接近独立，可以用 <span class="math inline">\(p(\mathbf x)p(\mathbf y)\)</span> 和 <span class="math inline">\(p(\mathbf x,\mathbf y)\)</span> 之间的 KL 散度作为指标： <span class="math display">\[
\begin{align}
\text{I}[\mathbf x,\mathbf y]&amp;=\text{KL}(p(\mathbf x,\mathbf y)\Vert p(\mathbf x)p(\mathbf y))\\
&amp;=-\iint p(\mathbf x,\mathbf y)\ln\frac{p(\mathbf x)p(\mathbf y)}{p(\mathbf x,\mathbf y)}\mathrm d\mathbf x\mathrm d\mathbf y
\end{align}
\]</span> 这个量被称为 <span class="math inline">\(\mathbf x\)</span> 与 <span class="math inline">\(\mathbf y\)</span> 之间的互信息。根据 KL 散度的性质，我们知道 <span class="math inline">\(\text{I}[\mathbf x,\mathbf y]\geq 0\)</span>，当且仅当二者独立时取等。另外，容易证明： <span class="math display">\[
\text{I}[\mathbf x,\mathbf y]=H[\mathbf x]-H[\mathbf x\vert\mathbf y]=H[\mathbf y]-H[\mathbf y\vert\mathbf x]
\]</span> 因此，互信息可以看作是观察到 <span class="math inline">\(\mathbf y\)</span> 给 <span class="math inline">\(\mathbf x\)</span> 的不确定性（熵）带来的减少量。</p>
<h2 id="references">References</h2>
<section class="footnotes">
<div class="footnote-list">
<ol>
<li>
<span id="fn:1" class="footnote-text"><span>为什么熵值最大的分布状态是正态分布而不是均匀分布？ - 椎名的回答 - 知乎 https://www.zhihu.com/question/357032828/answer/907586249 <a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩︎</a></span></span>
</li>
</ol>
</div>
</section>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/blog-main/categories/%E8%AF%BE%E7%A8%8B%E4%B9%A6%E7%B1%8D%E7%AC%94%E8%AE%B0/" class="category-chain-item">课程书籍笔记</a>
  
  
    <span>></span>
    
  <a href="/blog-main/categories/%E8%AF%BE%E7%A8%8B%E4%B9%A6%E7%B1%8D%E7%AC%94%E8%AE%B0/PRML/" class="category-chain-item">PRML</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/blog-main/tags/machine-learning/" class="print-no-link">#machine learning</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>[PRML]1.Introduction</div>
      <div>https://xyfjason.github.io/blog-main/2023/04/23/PRML-1-Introduction/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>xyfJASON</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年4月23日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/blog-main/2023/04/25/PRML-Appendix-D-Calculus-of-Variations/" title="[PRML]Appendix D.Calculus of Variations">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">[PRML]Appendix D.Calculus of Variations</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/blog-main/2023/03/29/Vector-Quantization/" title="Vector-Quantization">
                        <span class="hidden-mobile">Vector-Quantization</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/blog-main/js/events.js" ></script>
<script  src="/blog-main/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/blog-main/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/blog-main/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/blog-main/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
